>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Tue 29 Mar 2022 07:41:29 PM EDT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  airplane ; Model ID: 3db61220251b3c9de719b5362fe06bbb
--------------------
Training baseline regression model:  2022-03-29 19:41:31.996441
Detector:  point_transformer
Object:  airplane
--------------------
device is  cuda
--------------------
Number of trainable parameters:  900778
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.05721908062696457
Batch  11  loss:  0.03809642046689987
Batch  21  loss:  0.026747163385152817
Batch  31  loss:  0.01986132375895977
Batch  41  loss:  0.01493741199374199
Batch  51  loss:  0.005732764955610037
Batch  61  loss:  0.011671395972371101
Batch  71  loss:  0.01868564262986183
Batch  81  loss:  0.005464628804475069
Batch  91  loss:  0.007091466803103685
Batch  101  loss:  0.006071813404560089
Batch  111  loss:  0.002892482792958617
Batch  121  loss:  0.0036008828319609165
Batch  131  loss:  0.003798730205744505
Batch  141  loss:  0.003398523898795247
Batch  151  loss:  0.002347527537494898
Batch  161  loss:  0.001951243495568633
Batch  171  loss:  0.0023531767074018717
Batch  181  loss:  0.0014189318753778934
Batch  191  loss:  0.0014964417787268758
Validation on real data: 
LOSS supervised-train 0.009741952969343402, valid 0.0019048657268285751
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.0009861105354502797
Batch  11  loss:  0.0012885350733995438
Batch  21  loss:  0.0018415703671053052
Batch  31  loss:  0.001502375234849751
Batch  41  loss:  0.0014815684407949448
Batch  51  loss:  0.00384120037779212
Batch  61  loss:  0.0008028522715903819
Batch  71  loss:  0.0017993516521528363
Batch  81  loss:  0.0011562998406589031
Batch  91  loss:  0.0017564031295478344
Batch  101  loss:  0.003001888282597065
Batch  111  loss:  0.0012172380229458213
Batch  121  loss:  0.0016460893675684929
Batch  131  loss:  0.0011580606224015355
Batch  141  loss:  0.0022967399563640356
Batch  151  loss:  0.0014191308291628957
Batch  161  loss:  0.001639744034036994
Batch  171  loss:  0.0011133317602798343
Batch  181  loss:  0.0011781965149566531
Batch  191  loss:  0.0010750838555395603
Validation on real data: 
LOSS supervised-train 0.0017265894392039626, valid 0.001128192525357008
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0006388224428519607
Batch  11  loss:  0.0009209473500959575
Batch  21  loss:  0.0010226948652416468
Batch  31  loss:  0.0009309924789704382
Batch  41  loss:  0.0008714116993360221
Batch  51  loss:  0.0022710973862558603
Batch  61  loss:  0.0008262561750598252
Batch  71  loss:  0.0018337409710511565
Batch  81  loss:  0.0007691096980124712
Batch  91  loss:  0.0014706748770549893
Batch  101  loss:  0.0024101368617266417
Batch  111  loss:  0.0009526025969535112
Batch  121  loss:  0.000699180702213198
Batch  131  loss:  0.0008701013866811991
Batch  141  loss:  0.002114728558808565
Batch  151  loss:  0.001146069262176752
Batch  161  loss:  0.0009392455103807151
Batch  171  loss:  0.0008993152878247201
Batch  181  loss:  0.0007545819971710443
Batch  191  loss:  0.0008128182380460203
Validation on real data: 
LOSS supervised-train 0.0012713696305581834, valid 0.0008210737723857164
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0005246953805908561
Batch  11  loss:  0.0009152825805358589
Batch  21  loss:  0.0005678244633600116
Batch  31  loss:  0.0006638356717303395
Batch  41  loss:  0.0008145449100993574
Batch  51  loss:  0.0018514400580897927
Batch  61  loss:  0.0006864823517389596
Batch  71  loss:  0.0017574595985934138
Batch  81  loss:  0.0005842697573825717
Batch  91  loss:  0.0011831215815618634
Batch  101  loss:  0.0015208765398710966
Batch  111  loss:  0.0008933976059779525
Batch  121  loss:  0.0005073061911389232
Batch  131  loss:  0.0005560100544244051
Batch  141  loss:  0.0012425401946529746
Batch  151  loss:  0.0007934169261716306
Batch  161  loss:  0.0007302133599296212
Batch  171  loss:  0.0009799026884138584
Batch  181  loss:  0.0009619261254556477
Batch  191  loss:  0.00047151639591902494
Validation on real data: 
LOSS supervised-train 0.000994127563899383, valid 0.00041066939593292773
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0004560822853818536
Batch  11  loss:  0.0006708133150823414
Batch  21  loss:  0.0004741135926451534
Batch  31  loss:  0.0007468457915820181
Batch  41  loss:  0.00047381361946463585
Batch  51  loss:  0.0015281880041584373
Batch  61  loss:  0.0006750699249096215
Batch  71  loss:  0.0015124332858249545
Batch  81  loss:  0.0005297333118505776
Batch  91  loss:  0.0008356606704182923
Batch  101  loss:  0.0009247033158317208
Batch  111  loss:  0.0007208121242001653
Batch  121  loss:  0.00034970566048286855
Batch  131  loss:  0.0004452638968359679
Batch  141  loss:  0.0012058101128786802
Batch  151  loss:  0.0007272204384207726
Batch  161  loss:  0.0005499248509295285
Batch  171  loss:  0.0007059910567477345
Batch  181  loss:  0.0006871356745250523
Batch  191  loss:  0.00037603138480335474
Validation on real data: 
LOSS supervised-train 0.0008089048553665634, valid 0.0005842808168381453
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.00047734592226333916
Batch  11  loss:  0.0006092175026424229
Batch  21  loss:  0.0003604420053306967
Batch  31  loss:  0.0004377768491394818
Batch  41  loss:  0.0005454409983940423
Batch  51  loss:  0.0011445459676906466
Batch  61  loss:  0.0006024906178936362
Batch  71  loss:  0.001250852714292705
Batch  81  loss:  0.0005602757446467876
Batch  91  loss:  0.0007720267749391496
Batch  101  loss:  0.0006982950726523995
Batch  111  loss:  0.0006153637659735978
Batch  121  loss:  0.00035206638858653605
Batch  131  loss:  0.0004882549401372671
Batch  141  loss:  0.0012149173999205232
Batch  151  loss:  0.0005782248917967081
Batch  161  loss:  0.0005730885313823819
Batch  171  loss:  0.0006256358465179801
Batch  181  loss:  0.000525322335306555
Batch  191  loss:  0.0003074101114179939
Validation on real data: 
LOSS supervised-train 0.0006861016704351641, valid 0.0002741996431723237
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.00040093515417538583
Batch  11  loss:  0.0005226386128924787
Batch  21  loss:  0.0004508306737989187
Batch  31  loss:  0.0004554974439088255
Batch  41  loss:  0.0004738822462968528
Batch  51  loss:  0.0009240445797331631
Batch  61  loss:  0.0004982611280865967
Batch  71  loss:  0.0012998096644878387
Batch  81  loss:  0.00042382656829431653
Batch  91  loss:  0.0005556512624025345
Batch  101  loss:  0.0007463680813089013
Batch  111  loss:  0.000537250132765621
Batch  121  loss:  0.00027203618083149195
Batch  131  loss:  0.000429582956712693
Batch  141  loss:  0.0008159505669027567
Batch  151  loss:  0.0005631293752230704
Batch  161  loss:  0.00043814547825604677
Batch  171  loss:  0.0006271045422181487
Batch  181  loss:  0.00047823257045820355
Batch  191  loss:  0.0003734584024641663
Validation on real data: 
LOSS supervised-train 0.0005849254937493242, valid 0.00031578438938595355
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0003308485320303589
Batch  11  loss:  0.0006248487625271082
Batch  21  loss:  0.0003275109047535807
Batch  31  loss:  0.0004980583908036351
Batch  41  loss:  0.00040264896233566105
Batch  51  loss:  0.0006324721616692841
Batch  61  loss:  0.00045190146192908287
Batch  71  loss:  0.0009780983673408628
Batch  81  loss:  0.00041152691119350493
Batch  91  loss:  0.0006324027199298143
Batch  101  loss:  0.0005877637886442244
Batch  111  loss:  0.00048033168422989547
Batch  121  loss:  0.00024397786182817072
Batch  131  loss:  0.00035585532896220684
Batch  141  loss:  0.0006502768374048173
Batch  151  loss:  0.0005318857147358358
Batch  161  loss:  0.0003699586377479136
Batch  171  loss:  0.0006110395188443363
Batch  181  loss:  0.0004476807080209255
Batch  191  loss:  0.0003101980546489358
Validation on real data: 
LOSS supervised-train 0.000505061187359388, valid 0.0002626101777423173
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0003273649199400097
Batch  11  loss:  0.0005065269069746137
Batch  21  loss:  0.0002692387206479907
Batch  31  loss:  0.0003205010143574327
Batch  41  loss:  0.0002827904827427119
Batch  51  loss:  0.0006771258194930851
Batch  61  loss:  0.0004810327955055982
Batch  71  loss:  0.0010209556203335524
Batch  81  loss:  0.0002922422136180103
Batch  91  loss:  0.0005635623238049448
Batch  101  loss:  0.0005248480010777712
Batch  111  loss:  0.0003870548098348081
Batch  121  loss:  0.0002192908141296357
Batch  131  loss:  0.0003393616934772581
Batch  141  loss:  0.0006182906217873096
Batch  151  loss:  0.0004916788311675191
Batch  161  loss:  0.0003002118319272995
Batch  171  loss:  0.0005448051379062235
Batch  181  loss:  0.0004648522299248725
Batch  191  loss:  0.000317425059620291
Validation on real data: 
LOSS supervised-train 0.0004485688320710324, valid 0.00034506560768932104
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0002897772646974772
Batch  11  loss:  0.0004050802090205252
Batch  21  loss:  0.0003062069299630821
Batch  31  loss:  0.00030299308127723634
Batch  41  loss:  0.0002502788556739688
Batch  51  loss:  0.0004947643610648811
Batch  61  loss:  0.0003412674705032259
Batch  71  loss:  0.0007147236028686166
Batch  81  loss:  0.0002991212531924248
Batch  91  loss:  0.0005927280872128904
Batch  101  loss:  0.0005318071343936026
Batch  111  loss:  0.0003124777285847813
Batch  121  loss:  0.00027997259167023003
Batch  131  loss:  0.00028851194656454027
Batch  141  loss:  0.0005434316117316484
Batch  151  loss:  0.00038390292320400476
Batch  161  loss:  0.0003599894407670945
Batch  171  loss:  0.00043126201489940286
Batch  181  loss:  0.0003555455768946558
Batch  191  loss:  0.0003008211206179112
Validation on real data: 
LOSS supervised-train 0.0004049648084765067, valid 0.0002210375969298184
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0003667279670480639
Batch  11  loss:  0.0003789816109929234
Batch  21  loss:  0.00024068844504654408
Batch  31  loss:  0.0003033004468306899
Batch  41  loss:  0.000277643499430269
Batch  51  loss:  0.00048512531793676317
Batch  61  loss:  0.00038265431066975
Batch  71  loss:  0.0008304939838126302
Batch  81  loss:  0.00034902343759313226
Batch  91  loss:  0.00042309120181016624
Batch  101  loss:  0.0004793470143340528
Batch  111  loss:  0.0003129994438495487
Batch  121  loss:  0.00023528793826699257
Batch  131  loss:  0.0003340067632962018
Batch  141  loss:  0.0004756007983814925
Batch  151  loss:  0.000367072963854298
Batch  161  loss:  0.0002823438262566924
Batch  171  loss:  0.0004236848617438227
Batch  181  loss:  0.00034832110395655036
Batch  191  loss:  0.000247299118200317
Validation on real data: 
LOSS supervised-train 0.000371181836744654, valid 0.0002219835005234927
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0002596710983198136
Batch  11  loss:  0.00041541518294252455
Batch  21  loss:  0.00022297738178167492
Batch  31  loss:  0.00022567450650967658
Batch  41  loss:  0.0002017281949520111
Batch  51  loss:  0.00045222582411952317
Batch  61  loss:  0.0003204070671927184
Batch  71  loss:  0.0006525312783196568
Batch  81  loss:  0.00033204868668690324
Batch  91  loss:  0.0003208479902241379
Batch  101  loss:  0.0004107819404453039
Batch  111  loss:  0.00023548508761450648
Batch  121  loss:  0.00024159764871001244
Batch  131  loss:  0.00026445050025358796
Batch  141  loss:  0.00039487346657551825
Batch  151  loss:  0.000342903338605538
Batch  161  loss:  0.00026384161901660264
Batch  171  loss:  0.00044241209980100393
Batch  181  loss:  0.0003032112726941705
Batch  191  loss:  0.0002578338608145714
Validation on real data: 
LOSS supervised-train 0.0003335266435169615, valid 0.00020884693367406726
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0002868461888283491
Batch  11  loss:  0.00038946777931414545
Batch  21  loss:  0.00029086426366120577
Batch  31  loss:  0.00035792760900221765
Batch  41  loss:  0.0002926620654761791
Batch  51  loss:  0.00045389877050183713
Batch  61  loss:  0.0003025277692358941
Batch  71  loss:  0.0006112103001214564
Batch  81  loss:  0.0002690541441552341
Batch  91  loss:  0.0003828997432719916
Batch  101  loss:  0.00038503322866745293
Batch  111  loss:  0.00020616485562641174
Batch  121  loss:  0.0002271287958137691
Batch  131  loss:  0.0002766806283034384
Batch  141  loss:  0.00041393417632207274
Batch  151  loss:  0.0002652146795298904
Batch  161  loss:  0.00028688597376458347
Batch  171  loss:  0.00039979268331080675
Batch  181  loss:  0.0002705281076487154
Batch  191  loss:  0.00022471570991910994
Validation on real data: 
LOSS supervised-train 0.00031662672634411135, valid 0.00020083750132471323
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00023620104184374213
Batch  11  loss:  0.0002511109341867268
Batch  21  loss:  0.0002557958650868386
Batch  31  loss:  0.0002312073775101453
Batch  41  loss:  0.0001957095810212195
Batch  51  loss:  0.00040696572978049517
Batch  61  loss:  0.000235627347137779
Batch  71  loss:  0.0006504283519461751
Batch  81  loss:  0.00026980179245583713
Batch  91  loss:  0.0003242865495849401
Batch  101  loss:  0.000322141480864957
Batch  111  loss:  0.0002533207880333066
Batch  121  loss:  0.0001570230524521321
Batch  131  loss:  0.0003025114128831774
Batch  141  loss:  0.00030188303207978606
Batch  151  loss:  0.00028642459074035287
Batch  161  loss:  0.00022566679399460554
Batch  171  loss:  0.00033731473376974463
Batch  181  loss:  0.0002455666253808886
Batch  191  loss:  0.00020770130504388362
Validation on real data: 
LOSS supervised-train 0.0002885272216371959, valid 0.00023148272885009646
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.00026641818112693727
Batch  11  loss:  0.00024637035676278174
Batch  21  loss:  0.00022903166245669127
Batch  31  loss:  0.00020282273180782795
Batch  41  loss:  0.0002581756270956248
Batch  51  loss:  0.0003046799683943391
Batch  61  loss:  0.00038184598088264465
Batch  71  loss:  0.0006089344969950616
Batch  81  loss:  0.0002640185703057796
Batch  91  loss:  0.0003328023012727499
Batch  101  loss:  0.00024484089226461947
Batch  111  loss:  0.0002037619415204972
Batch  121  loss:  0.00017249080701731145
Batch  131  loss:  0.0002892026968766004
Batch  141  loss:  0.0002727180253714323
Batch  151  loss:  0.00026029415312223136
Batch  161  loss:  0.00024984803167171776
Batch  171  loss:  0.0003457711136434227
Batch  181  loss:  0.00022565870312973857
Batch  191  loss:  0.00019035364675801247
Validation on real data: 
LOSS supervised-train 0.00027421634818892927, valid 0.0001406818482792005
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.00028076846501789987
Batch  11  loss:  0.00029871161677874625
Batch  21  loss:  0.0002594029647298157
Batch  31  loss:  0.00015806255396455526
Batch  41  loss:  0.0001900974748423323
Batch  51  loss:  0.0002942782884929329
Batch  61  loss:  0.0002670029934961349
Batch  71  loss:  0.000416603812482208
Batch  81  loss:  0.0002717976167332381
Batch  91  loss:  0.0002407540159765631
Batch  101  loss:  0.0002165101614082232
Batch  111  loss:  0.0002519106201361865
Batch  121  loss:  0.00018381880363449454
Batch  131  loss:  0.0002582537999842316
Batch  141  loss:  0.00023716289433650672
Batch  151  loss:  0.000268866861006245
Batch  161  loss:  0.00027576438151299953
Batch  171  loss:  0.0003438094863668084
Batch  181  loss:  0.00018852036737371236
Batch  191  loss:  0.00018444977467879653
Validation on real data: 
LOSS supervised-train 0.00025347304363094737, valid 0.0001371192338410765
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.00028373717213980854
Batch  11  loss:  0.00018662116781342775
Batch  21  loss:  0.00023748572857584804
Batch  31  loss:  0.00020424311514943838
Batch  41  loss:  0.00014332101272884756
Batch  51  loss:  0.0002582660235930234
Batch  61  loss:  0.00027810054598376155
Batch  71  loss:  0.0005388612044043839
Batch  81  loss:  0.00022014910064172
Batch  91  loss:  0.00030802658875472844
Batch  101  loss:  0.0002935028460342437
Batch  111  loss:  0.00019937354954890907
Batch  121  loss:  0.0002061952545773238
Batch  131  loss:  0.0002450495958328247
Batch  141  loss:  0.00030461850110441446
Batch  151  loss:  0.00026078170049004257
Batch  161  loss:  0.00019212666666135192
Batch  171  loss:  0.0002731449785642326
Batch  181  loss:  0.0001846633676905185
Batch  191  loss:  0.00021647464018315077
Validation on real data: 
LOSS supervised-train 0.00024088461061182897, valid 0.0001231371716130525
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.00021452516375575215
Batch  11  loss:  0.0003319868992548436
Batch  21  loss:  0.00020412610319908708
Batch  31  loss:  0.00022055167937651277
Batch  41  loss:  0.00015415424422826618
Batch  51  loss:  0.00023931905161589384
Batch  61  loss:  0.00029097095830366015
Batch  71  loss:  0.0005049635074101388
Batch  81  loss:  0.00021864734299015254
Batch  91  loss:  0.00023276946740224957
Batch  101  loss:  0.00020507557201199234
Batch  111  loss:  0.00014121504500508308
Batch  121  loss:  0.00014260047464631498
Batch  131  loss:  0.0002469411410856992
Batch  141  loss:  0.0003181553620379418
Batch  151  loss:  0.00022396478743758053
Batch  161  loss:  0.00021695600298698992
Batch  171  loss:  0.0003142990462947637
Batch  181  loss:  0.0002064540603896603
Batch  191  loss:  0.00019264440925326198
Validation on real data: 
LOSS supervised-train 0.0002332848067089799, valid 0.00014625710900872946
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.00020975588995497674
Batch  11  loss:  0.00022430486569646746
Batch  21  loss:  0.00017285306239500642
Batch  31  loss:  0.00017319883045274764
Batch  41  loss:  0.0001569474843563512
Batch  51  loss:  0.00018773670308291912
Batch  61  loss:  0.00025683027342893183
Batch  71  loss:  0.00035356983426027
Batch  81  loss:  0.00022953710868023336
Batch  91  loss:  0.00022860689205117524
Batch  101  loss:  0.0002234997518826276
Batch  111  loss:  0.00017056158685591072
Batch  121  loss:  0.00013576076889876276
Batch  131  loss:  0.0002216500142822042
Batch  141  loss:  0.00020473104086704552
Batch  151  loss:  0.0002329573326278478
Batch  161  loss:  0.00023699249140918255
Batch  171  loss:  0.00021423707948997617
Batch  181  loss:  0.0001947082782862708
Batch  191  loss:  0.00019901506311725825
Validation on real data: 
LOSS supervised-train 0.0002136314679228235, valid 0.00016881397459656
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.00016981808585114777
Batch  11  loss:  0.0002100880810758099
Batch  21  loss:  0.0001959247310878709
Batch  31  loss:  0.00016987937851808965
Batch  41  loss:  0.00015099564916454256
Batch  51  loss:  0.0002212611580034718
Batch  61  loss:  0.00019853688718285412
Batch  71  loss:  0.00043257392826490104
Batch  81  loss:  0.00022504832304548472
Batch  91  loss:  0.00022023098426871002
Batch  101  loss:  0.00023548692115582526
Batch  111  loss:  0.0001541207020636648
Batch  121  loss:  0.0001435471058357507
Batch  131  loss:  0.0002362634550081566
Batch  141  loss:  0.00018992487457580864
Batch  151  loss:  0.00020284052880015224
Batch  161  loss:  0.00017674210539553314
Batch  171  loss:  0.00028333684895187616
Batch  181  loss:  0.0002150640939362347
Batch  191  loss:  0.0001911116560222581
Validation on real data: 
LOSS supervised-train 0.00021111818314238916, valid 0.00015421290299855173
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0002162909077014774
Batch  11  loss:  0.0002003567642532289
Batch  21  loss:  0.00021549772645812482
Batch  31  loss:  0.00017647544154897332
Batch  41  loss:  0.00013581728853750974
Batch  51  loss:  0.00018501335580367595
Batch  61  loss:  0.00021959621517453343
Batch  71  loss:  0.0003747361770365387
Batch  81  loss:  0.000191077750059776
Batch  91  loss:  0.00020056104403920472
Batch  101  loss:  0.00015086031635291874
Batch  111  loss:  0.00014525580627378076
Batch  121  loss:  0.00013899942860007286
Batch  131  loss:  0.0001971915626199916
Batch  141  loss:  0.0002551647194195539
Batch  151  loss:  0.00017683599435258657
Batch  161  loss:  0.00015861164138186723
Batch  171  loss:  0.00028176698833703995
Batch  181  loss:  0.0001774132833816111
Batch  191  loss:  0.00019971575238741934
Validation on real data: 
LOSS supervised-train 0.00019478739643091102, valid 0.00013849118840880692
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.00017560958804097027
Batch  11  loss:  0.00020996271632611752
Batch  21  loss:  0.00020103076531086117
Batch  31  loss:  0.00012796703958883882
Batch  41  loss:  0.00013880178448744118
Batch  51  loss:  0.0002486315497662872
Batch  61  loss:  0.00018171570263803005
Batch  71  loss:  0.0003064968332182616
Batch  81  loss:  0.00020309106912463903
Batch  91  loss:  0.00021887618640903383
Batch  101  loss:  0.00017218897119164467
Batch  111  loss:  0.00011135911336168647
Batch  121  loss:  0.0001507424603914842
Batch  131  loss:  0.0001871249987743795
Batch  141  loss:  0.00015417704707942903
Batch  151  loss:  0.00020645766926463693
Batch  161  loss:  0.00018960471788886935
Batch  171  loss:  0.00020983822469133884
Batch  181  loss:  0.00015448803605977446
Batch  191  loss:  0.00018723974062595516
Validation on real data: 
LOSS supervised-train 0.00018840108652511844, valid 0.0001543765392852947
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00019315521058160812
Batch  11  loss:  0.000266336282948032
Batch  21  loss:  0.0001436932507203892
Batch  31  loss:  0.00015094350965227932
Batch  41  loss:  0.00012445179163478315
Batch  51  loss:  0.00016637954104226083
Batch  61  loss:  0.00019465488730929792
Batch  71  loss:  0.0003204939712304622
Batch  81  loss:  0.00018112899851985276
Batch  91  loss:  0.0001659985282458365
Batch  101  loss:  0.00014770486450288445
Batch  111  loss:  0.00015042946324683726
Batch  121  loss:  0.00014220509910956025
Batch  131  loss:  0.00019896931189578027
Batch  141  loss:  0.00018758831720333546
Batch  151  loss:  0.00018175046716351062
Batch  161  loss:  0.0001794261479517445
Batch  171  loss:  0.0002516367530915886
Batch  181  loss:  0.00012375217920634896
Batch  191  loss:  0.00018855731468647718
Validation on real data: 
LOSS supervised-train 0.00018448416762112174, valid 0.0001408788957633078
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00019458036695141345
Batch  11  loss:  0.00025834498228505254
Batch  21  loss:  0.00020608879276551306
Batch  31  loss:  0.00021048018243163824
Batch  41  loss:  0.0001616387744434178
Batch  51  loss:  0.00020128102914895862
Batch  61  loss:  0.00015850414638407528
Batch  71  loss:  0.0004176947695668787
Batch  81  loss:  0.0001998841908061877
Batch  91  loss:  0.00016508753469679505
Batch  101  loss:  0.00016095489263534546
Batch  111  loss:  0.00014890212332829833
Batch  121  loss:  0.00014466109860222787
Batch  131  loss:  0.000174080953001976
Batch  141  loss:  0.0002197165013058111
Batch  151  loss:  0.00018103855836670846
Batch  161  loss:  0.00019939012418035418
Batch  171  loss:  0.00029276375425979495
Batch  181  loss:  0.0001630631450098008
Batch  191  loss:  0.00017075927462428808
Validation on real data: 
LOSS supervised-train 0.00018283989404153545, valid 0.00015534638077951968
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0001988464209716767
Batch  11  loss:  0.00021119184384588152
Batch  21  loss:  0.00019733117369469255
Batch  31  loss:  0.00014434396871365607
Batch  41  loss:  0.00017019464576151222
Batch  51  loss:  0.00018804013961926103
Batch  61  loss:  0.00014566184836439788
Batch  71  loss:  0.0003938441222999245
Batch  81  loss:  0.00017069472232833505
Batch  91  loss:  0.00020299310563132167
Batch  101  loss:  0.0001421465422026813
Batch  111  loss:  0.0001530442532384768
Batch  121  loss:  0.00017643383762333542
Batch  131  loss:  0.00014926372386980802
Batch  141  loss:  0.00020732550183311105
Batch  151  loss:  0.0001841972698457539
Batch  161  loss:  0.00015908088244032115
Batch  171  loss:  0.00017137719260063022
Batch  181  loss:  0.00014270209067035466
Batch  191  loss:  0.0001694145321380347
Validation on real data: 
LOSS supervised-train 0.0001744936788600171, valid 0.0001213801879202947
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0001341363531537354
Batch  11  loss:  0.0001661338028497994
Batch  21  loss:  0.00017408742860425264
Batch  31  loss:  0.00014351039135362953
Batch  41  loss:  0.00014194978575687855
Batch  51  loss:  0.00014458110672421753
Batch  61  loss:  0.0001177443191409111
Batch  71  loss:  0.00029645435279235244
Batch  81  loss:  0.000159669405547902
Batch  91  loss:  0.00021790726168546826
Batch  101  loss:  0.0001555861090309918
Batch  111  loss:  0.00011958499089814723
Batch  121  loss:  0.00015707760758232325
Batch  131  loss:  0.00016979868814814836
Batch  141  loss:  0.00017132755601778626
Batch  151  loss:  0.0001567952276673168
Batch  161  loss:  0.00016780616715550423
Batch  171  loss:  0.0002280099579365924
Batch  181  loss:  0.0001285102334804833
Batch  191  loss:  0.00016489767585881054
Validation on real data: 
LOSS supervised-train 0.00016610236845735927, valid 0.00012711904128082097
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00014339140034280717
Batch  11  loss:  0.00018114145495928824
Batch  21  loss:  0.00018510312656871974
Batch  31  loss:  0.00012588909885380417
Batch  41  loss:  0.00012153144052717835
Batch  51  loss:  0.0001472233998356387
Batch  61  loss:  0.0001868887775344774
Batch  71  loss:  0.00023346660600509495
Batch  81  loss:  0.0001711048826109618
Batch  91  loss:  0.00012432399671524763
Batch  101  loss:  0.00014892933540977538
Batch  111  loss:  0.00014642930182162672
Batch  121  loss:  0.00017986759485211223
Batch  131  loss:  0.00014840092626400292
Batch  141  loss:  0.00019340885046403855
Batch  151  loss:  0.00020723044872283936
Batch  161  loss:  0.00015371316112577915
Batch  171  loss:  0.0002653180272318423
Batch  181  loss:  0.00015280998195521533
Batch  191  loss:  0.00016013313143048435
Validation on real data: 
LOSS supervised-train 0.0001667797340269317, valid 0.00012367713497951627
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0001827230880735442
Batch  11  loss:  0.00018364793504588306
Batch  21  loss:  0.0001730529183987528
Batch  31  loss:  0.00013512128498405218
Batch  41  loss:  0.00011780483328038827
Batch  51  loss:  0.00016087382391560823
Batch  61  loss:  0.00014859565999358892
Batch  71  loss:  0.0003203036612831056
Batch  81  loss:  0.00015114333655219525
Batch  91  loss:  0.000159056595293805
Batch  101  loss:  0.00013548815331887454
Batch  111  loss:  0.0001585789432283491
Batch  121  loss:  0.0001447118993382901
Batch  131  loss:  0.00014848384307697415
Batch  141  loss:  0.00015883776359260082
Batch  151  loss:  0.00016487977700307965
Batch  161  loss:  0.00017100175318773836
Batch  171  loss:  0.00023812051222193986
Batch  181  loss:  0.0001211199487443082
Batch  191  loss:  0.00016992349992506206
Validation on real data: 
LOSS supervised-train 0.00015815478818694828, valid 0.00010461927013238892
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00014536328671965748
Batch  11  loss:  0.0001802511978894472
Batch  21  loss:  0.00015700451331213117
Batch  31  loss:  0.00013936491450294852
Batch  41  loss:  0.0001373886043438688
Batch  51  loss:  0.00012143419735366479
Batch  61  loss:  0.00011932405323022977
Batch  71  loss:  0.00031491753179579973
Batch  81  loss:  0.0001447009271942079
Batch  91  loss:  0.00017629051581025124
Batch  101  loss:  0.0001450426789233461
Batch  111  loss:  0.0001457442995160818
Batch  121  loss:  0.00014202944294083863
Batch  131  loss:  0.00011703425843734294
Batch  141  loss:  0.00017020819359458983
Batch  151  loss:  0.00013100601790938526
Batch  161  loss:  0.00013524216774385422
Batch  171  loss:  0.0002378875360591337
Batch  181  loss:  0.00012111529213143513
Batch  191  loss:  0.00015764261479489505
Validation on real data: 
LOSS supervised-train 0.00015851545464101946, valid 0.00010110226867254823
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.00015605564112775028
Batch  11  loss:  0.00019377059652470052
Batch  21  loss:  0.00017941177065949887
Batch  31  loss:  0.00011858178186230361
Batch  41  loss:  9.901649173116311e-05
Batch  51  loss:  0.00014324145740829408
Batch  61  loss:  0.00015667751722503453
Batch  71  loss:  0.00025406229542568326
Batch  81  loss:  0.00015428844199050218
Batch  91  loss:  0.00013853404379915446
Batch  101  loss:  0.0001364542549708858
Batch  111  loss:  0.00016855198191478848
Batch  121  loss:  0.000125013742945157
Batch  131  loss:  0.00011928665480809286
Batch  141  loss:  0.00014632620150223374
Batch  151  loss:  0.00014946478768251836
Batch  161  loss:  0.00013783248141407967
Batch  171  loss:  0.0002850776945706457
Batch  181  loss:  0.00010636966180754825
Batch  191  loss:  0.00015829796029720455
Validation on real data: 
LOSS supervised-train 0.0001532972959103063, valid 0.00012787980085704476
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0001947165437741205
Batch  11  loss:  0.00018098844157066196
Batch  21  loss:  0.0001804971689125523
Batch  31  loss:  0.00013524889072868973
Batch  41  loss:  0.00013675150694325566
Batch  51  loss:  0.00011628763604676351
Batch  61  loss:  0.00020134844817221165
Batch  71  loss:  0.00020994212536606938
Batch  81  loss:  0.00016048092220444232
Batch  91  loss:  0.0001657264947425574
Batch  101  loss:  0.00011811565491370857
Batch  111  loss:  0.00013331785157788545
Batch  121  loss:  0.00010313956590835005
Batch  131  loss:  0.00012390142364893109
Batch  141  loss:  0.00016139171202667058
Batch  151  loss:  0.00013284917804412544
Batch  161  loss:  0.00013294327072799206
Batch  171  loss:  0.0001723028253763914
Batch  181  loss:  0.00011276909935986623
Batch  191  loss:  0.0001708025229163468
Validation on real data: 
LOSS supervised-train 0.00014762827351660234, valid 0.00010733380622696131
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.00015549207455478609
Batch  11  loss:  0.0001507698034401983
Batch  21  loss:  0.00018042839656118304
Batch  31  loss:  0.00012293529289308935
Batch  41  loss:  0.0001324693439528346
Batch  51  loss:  0.0001573773770360276
Batch  61  loss:  0.00017705929349176586
Batch  71  loss:  0.00018986068607773632
Batch  81  loss:  0.0001549024455016479
Batch  91  loss:  0.00013561004016082734
Batch  101  loss:  0.00011148615158163011
Batch  111  loss:  0.00012070446973666549
Batch  121  loss:  0.00010813367407536134
Batch  131  loss:  0.00014818862837273628
Batch  141  loss:  0.00015225267270579934
Batch  151  loss:  0.0001280504948226735
Batch  161  loss:  0.0001330922677880153
Batch  171  loss:  0.0001696125400485471
Batch  181  loss:  0.00011778117914218456
Batch  191  loss:  0.00012591473932843655
Validation on real data: 
LOSS supervised-train 0.0001438391677220352, valid 0.00010824845230672508
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00015484329196624458
Batch  11  loss:  0.00016409037925768644
Batch  21  loss:  0.0001688868651399389
Batch  31  loss:  0.00012286804849281907
Batch  41  loss:  0.00010270108032273129
Batch  51  loss:  0.00013367414067033678
Batch  61  loss:  0.00014640357403550297
Batch  71  loss:  0.0002262728230562061
Batch  81  loss:  0.00013960021897219121
Batch  91  loss:  0.0001267654006369412
Batch  101  loss:  0.00010281230788677931
Batch  111  loss:  0.00012540866737253964
Batch  121  loss:  0.0001527619024273008
Batch  131  loss:  0.00017653295071795583
Batch  141  loss:  0.00014111562632024288
Batch  151  loss:  0.00013198380474932492
Batch  161  loss:  0.00012955375132150948
Batch  171  loss:  0.00018063462630379945
Batch  181  loss:  0.00012228154810145497
Batch  191  loss:  0.00016513740411028266
Validation on real data: 
LOSS supervised-train 0.00014262135955505072, valid 0.00011478202213766053
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00016308348858729005
Batch  11  loss:  0.00017312086129095405
Batch  21  loss:  0.000137410155730322
Batch  31  loss:  0.00014102691784501076
Batch  41  loss:  0.00012207243707962334
Batch  51  loss:  0.00014509486209135503
Batch  61  loss:  0.00013607274740934372
Batch  71  loss:  0.00018133029516320676
Batch  81  loss:  0.00015392362547572702
Batch  91  loss:  0.00011796173203038052
Batch  101  loss:  0.00010277796536684036
Batch  111  loss:  0.000105061917565763
Batch  121  loss:  0.00011057268420699984
Batch  131  loss:  0.0001289433566853404
Batch  141  loss:  0.0001458267797715962
Batch  151  loss:  0.0001635808584978804
Batch  161  loss:  0.00011418105714255944
Batch  171  loss:  0.00020694818522315472
Batch  181  loss:  0.00010877539898501709
Batch  191  loss:  0.00017301227489951998
Validation on real data: 
LOSS supervised-train 0.0001389529386869981, valid 9.492890967521816e-05
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.00013649286120198667
Batch  11  loss:  0.000217852444620803
Batch  21  loss:  0.00014570362691301852
Batch  31  loss:  0.00012048600910929963
Batch  41  loss:  0.00012562444317154586
Batch  51  loss:  0.00010003719216911122
Batch  61  loss:  0.00013721722643822432
Batch  71  loss:  0.00023599369160365313
Batch  81  loss:  0.00012804201105609536
Batch  91  loss:  0.00013491131539922208
Batch  101  loss:  0.00012190773850306869
Batch  111  loss:  0.00011409349826863036
Batch  121  loss:  0.00010555340850260109
Batch  131  loss:  0.00012829041224904358
Batch  141  loss:  0.00013779517030343413
Batch  151  loss:  0.0001263261801796034
Batch  161  loss:  0.0001324243494309485
Batch  171  loss:  0.00015293022443074733
Batch  181  loss:  0.00010617814405122772
Batch  191  loss:  0.00015866055036894977
Validation on real data: 
LOSS supervised-train 0.00013652528778038687, valid 9.355509246233851e-05
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00018795236246660352
Batch  11  loss:  0.00011484573042253032
Batch  21  loss:  0.0001246750762220472
Batch  31  loss:  0.0001105644550989382
Batch  41  loss:  9.857980330707505e-05
Batch  51  loss:  0.00013527143164537847
Batch  61  loss:  0.00012439006241038442
Batch  71  loss:  0.00023110458278097212
Batch  81  loss:  0.0001383885828545317
Batch  91  loss:  0.00012282955867704004
Batch  101  loss:  0.00014528092287946492
Batch  111  loss:  0.0001044003656716086
Batch  121  loss:  0.00011344596714479849
Batch  131  loss:  9.244777174899355e-05
Batch  141  loss:  0.00013402670447248966
Batch  151  loss:  0.0001546223065815866
Batch  161  loss:  0.00012637116014957428
Batch  171  loss:  0.00016794574912637472
Batch  181  loss:  9.400548151461408e-05
Batch  191  loss:  0.00015611117123626173
Validation on real data: 
LOSS supervised-train 0.00013460331119858894, valid 0.00012677487393375486
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.00012901423906441778
Batch  11  loss:  0.00017565913731232285
Batch  21  loss:  0.0001655620289966464
Batch  31  loss:  0.00010617850057315081
Batch  41  loss:  0.00010818910232046619
Batch  51  loss:  0.00010644183930708095
Batch  61  loss:  0.00014132463547866791
Batch  71  loss:  0.0002426293067401275
Batch  81  loss:  0.00012222956866025925
Batch  91  loss:  0.00011147624900331721
Batch  101  loss:  0.00011973972141277045
Batch  111  loss:  0.0001306976773776114
Batch  121  loss:  8.921617700252682e-05
Batch  131  loss:  0.00012540955503936857
Batch  141  loss:  0.00014533475041389465
Batch  151  loss:  0.00011508482566569
Batch  161  loss:  0.0001423818030161783
Batch  171  loss:  0.000125559774460271
Batch  181  loss:  0.00012450235954020172
Batch  191  loss:  0.00017082880367524922
Validation on real data: 
LOSS supervised-train 0.0001316382885488565, valid 9.771685290616006e-05
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0001579797244630754
Batch  11  loss:  0.0001810230314731598
Batch  21  loss:  0.00012431974755600095
Batch  31  loss:  0.00010371191456215456
Batch  41  loss:  0.00012444466119632125
Batch  51  loss:  0.0001156847138190642
Batch  61  loss:  0.00014024905976839364
Batch  71  loss:  0.0001696944236755371
Batch  81  loss:  0.00013332266826182604
Batch  91  loss:  0.00011790853022830561
Batch  101  loss:  0.0001004144360194914
Batch  111  loss:  0.00010000907786889002
Batch  121  loss:  0.00012380081170704216
Batch  131  loss:  0.00012612785212695599
Batch  141  loss:  0.00016177228826563805
Batch  151  loss:  0.0001280929136555642
Batch  161  loss:  0.00014384470705408603
Batch  171  loss:  0.00018918905698228627
Batch  181  loss:  0.00012929155491292477
Batch  191  loss:  0.00011703017662512138
Validation on real data: 
LOSS supervised-train 0.00012768167311151047, valid 0.00012044598406646401
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0001290748914470896
Batch  11  loss:  0.0001541682140668854
Batch  21  loss:  0.0001665919553488493
Batch  31  loss:  0.00013279967242851853
Batch  41  loss:  0.0001315370318479836
Batch  51  loss:  0.00010905511589953676
Batch  61  loss:  0.00015318246732931584
Batch  71  loss:  0.00022316623653750867
Batch  81  loss:  0.0001402367342961952
Batch  91  loss:  0.0001008903855108656
Batch  101  loss:  8.815872570266947e-05
Batch  111  loss:  0.0001229513727594167
Batch  121  loss:  0.0001223196304636076
Batch  131  loss:  8.982879808172584e-05
Batch  141  loss:  9.819203842198476e-05
Batch  151  loss:  0.00012768199667334557
Batch  161  loss:  0.00011512798664625734
Batch  171  loss:  0.00015247745614033192
Batch  181  loss:  7.097666821209714e-05
Batch  191  loss:  0.00014605498290620744
Validation on real data: 
LOSS supervised-train 0.0001295264511645655, valid 9.424568270333111e-05
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00014118137187324464
Batch  11  loss:  0.00013251988275442272
Batch  21  loss:  0.00012189695553388447
Batch  31  loss:  9.251150186173618e-05
Batch  41  loss:  0.00010960353392874822
Batch  51  loss:  0.00010562840907368809
Batch  61  loss:  0.00018425365851726383
Batch  71  loss:  0.00022022600751370192
Batch  81  loss:  0.0001249757333425805
Batch  91  loss:  0.00013017081073485315
Batch  101  loss:  0.0001046236211550422
Batch  111  loss:  0.00016657629748806357
Batch  121  loss:  0.0001228931942023337
Batch  131  loss:  0.00013126585690770298
Batch  141  loss:  0.0001261524303117767
Batch  151  loss:  0.00011991646169917658
Batch  161  loss:  0.0001120657252613455
Batch  171  loss:  0.00018714596808422357
Batch  181  loss:  0.00010394389391876757
Batch  191  loss:  0.0001463853841414675
Validation on real data: 
LOSS supervised-train 0.00012563904689159245, valid 9.637672337703407e-05
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.00012297523790039122
Batch  11  loss:  0.000174289831193164
Batch  21  loss:  0.00011239959712838754
Batch  31  loss:  9.626078099245206e-05
Batch  41  loss:  0.00010831814142875373
Batch  51  loss:  0.00010945618123514578
Batch  61  loss:  9.119488095166162e-05
Batch  71  loss:  0.0001419697073288262
Batch  81  loss:  0.00010393289267085493
Batch  91  loss:  9.133057756116614e-05
Batch  101  loss:  0.00010667395690688863
Batch  111  loss:  0.0001233226212207228
Batch  121  loss:  9.786562441149727e-05
Batch  131  loss:  0.0001195750737679191
Batch  141  loss:  0.00015037180855870247
Batch  151  loss:  0.0001028224069159478
Batch  161  loss:  0.000116662064101547
Batch  171  loss:  0.0001195782606373541
Batch  181  loss:  9.175000741379336e-05
Batch  191  loss:  0.00012702921230811626
Validation on real data: 
LOSS supervised-train 0.00012123123931814917, valid 8.874684863258153e-05
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00012836155656259507
Batch  11  loss:  0.0001592093612998724
Batch  21  loss:  0.00019088767294306308
Batch  31  loss:  9.253894677385688e-05
Batch  41  loss:  0.00014072183694224805
Batch  51  loss:  0.0001394114806316793
Batch  61  loss:  0.00016723913722671568
Batch  71  loss:  0.00018839605036191642
Batch  81  loss:  0.00010628203745000064
Batch  91  loss:  9.911351662594825e-05
Batch  101  loss:  0.00011472165351733565
Batch  111  loss:  9.193851292366162e-05
Batch  121  loss:  9.131745173363015e-05
Batch  131  loss:  0.00010228358587482944
Batch  141  loss:  0.00011079939577030018
Batch  151  loss:  0.00013220170512795448
Batch  161  loss:  0.00013042613863945007
Batch  171  loss:  0.00016669520118739456
Batch  181  loss:  9.47336811805144e-05
Batch  191  loss:  0.00011650558008113876
Validation on real data: 
LOSS supervised-train 0.0001226728590336279, valid 8.11403151601553e-05
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00012136954319430515
Batch  11  loss:  0.00014373213343787938
Batch  21  loss:  9.815667726797983e-05
Batch  31  loss:  0.00011195539991604164
Batch  41  loss:  9.938531002262607e-05
Batch  51  loss:  8.620582229923457e-05
Batch  61  loss:  0.00013356086856219918
Batch  71  loss:  0.00017375644529238343
Batch  81  loss:  0.00011970914056291804
Batch  91  loss:  9.234934259438887e-05
Batch  101  loss:  0.0001185236033052206
Batch  111  loss:  0.000163931748829782
Batch  121  loss:  0.00011038334923796356
Batch  131  loss:  0.00014208507491275668
Batch  141  loss:  0.00015219773922581226
Batch  151  loss:  0.00015821329725440592
Batch  161  loss:  0.00012110410170862451
Batch  171  loss:  0.0001690304052317515
Batch  181  loss:  9.69702159636654e-05
Batch  191  loss:  0.00013999092334415764
Validation on real data: 
LOSS supervised-train 0.00012116858135414077, valid 0.00010744133760454133
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00012423988664522767
Batch  11  loss:  0.00014807924162596464
Batch  21  loss:  9.553135896567255e-05
Batch  31  loss:  0.00011512298078741878
Batch  41  loss:  0.00011192706733709201
Batch  51  loss:  0.00012478545249905437
Batch  61  loss:  9.88376050372608e-05
Batch  71  loss:  0.00016481225611642003
Batch  81  loss:  0.00011039892706321552
Batch  91  loss:  9.7573189123068e-05
Batch  101  loss:  7.630498294020072e-05
Batch  111  loss:  0.0001086446427507326
Batch  121  loss:  0.00010249461774947122
Batch  131  loss:  9.791651245905086e-05
Batch  141  loss:  0.00011701858602464199
Batch  151  loss:  9.957003931049258e-05
Batch  161  loss:  0.00013344554463401437
Batch  171  loss:  0.0001279144489672035
Batch  181  loss:  9.824252629186958e-05
Batch  191  loss:  0.0001602924894541502
Validation on real data: 
LOSS supervised-train 0.00011743526523787296, valid 8.066642476478592e-05
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00013176277570892125
Batch  11  loss:  0.00012376514496281743
Batch  21  loss:  0.00011609493230935186
Batch  31  loss:  9.49995664996095e-05
Batch  41  loss:  9.716687782201916e-05
Batch  51  loss:  9.921213495545089e-05
Batch  61  loss:  0.00013478279288392514
Batch  71  loss:  0.0001777455472620204
Batch  81  loss:  0.00010864953947020695
Batch  91  loss:  8.864753908710554e-05
Batch  101  loss:  6.45402615191415e-05
Batch  111  loss:  0.00010572217433946207
Batch  121  loss:  0.00010767023195512593
Batch  131  loss:  0.00011500174150569364
Batch  141  loss:  0.00010211821063421667
Batch  151  loss:  0.00012293002509977669
Batch  161  loss:  0.00011004840780515224
Batch  171  loss:  0.00015697402704972774
Batch  181  loss:  0.00010877560271183029
Batch  191  loss:  0.00013421410403680056
Validation on real data: 
LOSS supervised-train 0.00011585819156607613, valid 8.712211274541914e-05
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00011836555495392531
Batch  11  loss:  0.00013110539293847978
Batch  21  loss:  0.0001465206587454304
Batch  31  loss:  0.00010931910946965218
Batch  41  loss:  0.00010277281398884952
Batch  51  loss:  0.00011988291225861758
Batch  61  loss:  0.0001124100381275639
Batch  71  loss:  0.000190905891940929
Batch  81  loss:  9.839606354944408e-05
Batch  91  loss:  9.457297710468993e-05
Batch  101  loss:  0.00010409151582280174
Batch  111  loss:  0.00010229609324596822
Batch  121  loss:  0.00010395675053587183
Batch  131  loss:  0.00010220531112281606
Batch  141  loss:  0.00010750335059128702
Batch  151  loss:  0.0001325179182458669
Batch  161  loss:  0.00010155414929613471
Batch  171  loss:  0.0001549120497656986
Batch  181  loss:  9.216126636601985e-05
Batch  191  loss:  0.00011826916306745261
Validation on real data: 
LOSS supervised-train 0.00011307853113976308, valid 0.00011703533527906984
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0001842388155637309
Batch  11  loss:  0.00015535420970991254
Batch  21  loss:  0.0001043418378685601
Batch  31  loss:  8.803432865533978e-05
Batch  41  loss:  9.752233017934486e-05
Batch  51  loss:  9.493503603152931e-05
Batch  61  loss:  0.00010140387166757137
Batch  71  loss:  0.00012780433462467045
Batch  81  loss:  0.00010174797353101894
Batch  91  loss:  9.097529982682317e-05
Batch  101  loss:  8.663300104672089e-05
Batch  111  loss:  0.00010605945863062516
Batch  121  loss:  0.00010600177483865991
Batch  131  loss:  9.638769552111626e-05
Batch  141  loss:  0.00010931159340543672
Batch  151  loss:  0.00012607841927092522
Batch  161  loss:  9.940269956132397e-05
Batch  171  loss:  0.00015885359607636929
Batch  181  loss:  8.716298907529563e-05
Batch  191  loss:  0.000115376096800901
Validation on real data: 
LOSS supervised-train 0.00011221717773878482, valid 8.045852882787585e-05
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00011860554513987154
Batch  11  loss:  0.00013239566760603338
Batch  21  loss:  0.0001098704306059517
Batch  31  loss:  0.00012619113840628415
Batch  41  loss:  9.710001904750243e-05
Batch  51  loss:  9.259475336875767e-05
Batch  61  loss:  0.00011702618940034881
Batch  71  loss:  0.00012837565736845136
Batch  81  loss:  8.21799403638579e-05
Batch  91  loss:  8.390398579649627e-05
Batch  101  loss:  8.490575419273227e-05
Batch  111  loss:  0.00011315311712678522
Batch  121  loss:  9.508478979114443e-05
Batch  131  loss:  9.498035069555044e-05
Batch  141  loss:  0.0001142700930358842
Batch  151  loss:  0.00012513315596152097
Batch  161  loss:  0.00010971076699206606
Batch  171  loss:  0.0001743383618304506
Batch  181  loss:  9.774104546522722e-05
Batch  191  loss:  0.00010933962767012417
Validation on real data: 
LOSS supervised-train 0.00011264134729572107, valid 8.816406625555828e-05
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0001243844162672758
Batch  11  loss:  0.0001252449001185596
Batch  21  loss:  0.00012082230387022719
Batch  31  loss:  9.570194379193708e-05
Batch  41  loss:  9.933482942869887e-05
Batch  51  loss:  9.676445188233629e-05
Batch  61  loss:  0.00013764957839157432
Batch  71  loss:  0.00019966687250416726
Batch  81  loss:  0.00012025576870655641
Batch  91  loss:  9.142896306002513e-05
Batch  101  loss:  9.200540807796642e-05
Batch  111  loss:  8.698862802702934e-05
Batch  121  loss:  8.134495874401182e-05
Batch  131  loss:  0.00010037617175839841
Batch  141  loss:  0.00013296263932716101
Batch  151  loss:  0.00012447651533875614
Batch  161  loss:  9.634509478928521e-05
Batch  171  loss:  0.00013869290705770254
Batch  181  loss:  9.070679516298696e-05
Batch  191  loss:  0.00011725903459591791
Validation on real data: 
LOSS supervised-train 0.0001101132753683487, valid 7.673755317227915e-05
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00012460278230719268
Batch  11  loss:  0.00014433660544455051
Batch  21  loss:  0.000126838349387981
Batch  31  loss:  9.244077955372632e-05
Batch  41  loss:  0.00012460803554859012
Batch  51  loss:  8.895891369320452e-05
Batch  61  loss:  9.62832709774375e-05
Batch  71  loss:  0.00018138764426112175
Batch  81  loss:  0.00010039687185781077
Batch  91  loss:  8.772793808020651e-05
Batch  101  loss:  6.539391324622557e-05
Batch  111  loss:  0.00010034073056885973
Batch  121  loss:  7.649459439562634e-05
Batch  131  loss:  0.00012472711387090385
Batch  141  loss:  0.00012574807624332607
Batch  151  loss:  9.640304051572457e-05
Batch  161  loss:  0.00010317941632820293
Batch  171  loss:  0.0001358339941361919
Batch  181  loss:  8.783965313341469e-05
Batch  191  loss:  9.93967623799108e-05
Validation on real data: 
LOSS supervised-train 0.00010837282590728137, valid 0.00010302029841113836
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00013641284022014588
Batch  11  loss:  0.0001449324918212369
Batch  21  loss:  0.00012228403647895902
Batch  31  loss:  8.525708835804835e-05
Batch  41  loss:  0.00011048974556615576
Batch  51  loss:  9.601916826795787e-05
Batch  61  loss:  0.00010070155985886231
Batch  71  loss:  0.00018798756354954094
Batch  81  loss:  0.00011865224223583937
Batch  91  loss:  8.43822126626037e-05
Batch  101  loss:  8.636220445623621e-05
Batch  111  loss:  9.186175157083198e-05
Batch  121  loss:  9.910121298162267e-05
Batch  131  loss:  7.299488788703457e-05
Batch  141  loss:  0.00012812351633328944
Batch  151  loss:  9.007369226310402e-05
Batch  161  loss:  0.00010284665040671825
Batch  171  loss:  0.00015478418208658695
Batch  181  loss:  8.679654274601489e-05
Batch  191  loss:  0.0001247098989551887
Validation on real data: 
LOSS supervised-train 0.00010710387865401571, valid 0.00010663254943210632
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00011627878848230466
Batch  11  loss:  9.897012932924554e-05
Batch  21  loss:  0.00010509787534829229
Batch  31  loss:  9.144907380687073e-05
Batch  41  loss:  0.00011470744357211515
Batch  51  loss:  0.00010659333929652348
Batch  61  loss:  9.427899203728884e-05
Batch  71  loss:  0.00018199600162915885
Batch  81  loss:  9.699475049274042e-05
Batch  91  loss:  9.01262101251632e-05
Batch  101  loss:  0.00010681918502086774
Batch  111  loss:  0.00010270623897667974
Batch  121  loss:  8.6112784629222e-05
Batch  131  loss:  9.702425450086594e-05
Batch  141  loss:  0.00012719763617496938
Batch  151  loss:  0.00010005724470829591
Batch  161  loss:  0.00011277146404609084
Batch  171  loss:  0.00011799723870353773
Batch  181  loss:  9.034277900354937e-05
Batch  191  loss:  0.000115277296572458
Validation on real data: 
LOSS supervised-train 0.00010473233054653974, valid 8.576340042054653e-05
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00014923348498996347
Batch  11  loss:  0.00018071051454171538
Batch  21  loss:  0.00013368416693992913
Batch  31  loss:  8.088363392744213e-05
Batch  41  loss:  7.677198300370947e-05
Batch  51  loss:  8.834968321025372e-05
Batch  61  loss:  0.00012124916247557849
Batch  71  loss:  0.00012001964933006093
Batch  81  loss:  0.00011914275091839954
Batch  91  loss:  9.123169729718938e-05
Batch  101  loss:  9.989099635276943e-05
Batch  111  loss:  8.06851385277696e-05
Batch  121  loss:  7.623040437465534e-05
Batch  131  loss:  8.069973409874365e-05
Batch  141  loss:  0.00012253530439920723
Batch  151  loss:  8.695357973920181e-05
Batch  161  loss:  0.0001191332921735011
Batch  171  loss:  0.0001306871126871556
Batch  181  loss:  7.117781933629885e-05
Batch  191  loss:  0.00010337327694287524
Validation on real data: 
LOSS supervised-train 0.00010461770434631035, valid 6.216835754457861e-05
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00010156656207982451
Batch  11  loss:  0.0001139313681051135
Batch  21  loss:  0.00010914096492342651
Batch  31  loss:  9.912586392601952e-05
Batch  41  loss:  8.986148168332875e-05
Batch  51  loss:  8.149935456458479e-05
Batch  61  loss:  9.651279833633453e-05
Batch  71  loss:  0.0001641594571992755
Batch  81  loss:  0.00010432623093947768
Batch  91  loss:  7.455977174686268e-05
Batch  101  loss:  7.507065311074257e-05
Batch  111  loss:  8.923864515963942e-05
Batch  121  loss:  9.902290912577882e-05
Batch  131  loss:  9.784724534256384e-05
Batch  141  loss:  0.00012551284453365952
Batch  151  loss:  8.239701855927706e-05
Batch  161  loss:  9.704561671242118e-05
Batch  171  loss:  0.00013596843928098679
Batch  181  loss:  7.31200270820409e-05
Batch  191  loss:  0.00013106493861414492
Validation on real data: 
LOSS supervised-train 0.00010264101927532466, valid 7.850895781302825e-05
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00011082328273914754
Batch  11  loss:  0.00011272133269812912
Batch  21  loss:  0.00014850303705316037
Batch  31  loss:  8.726086525712162e-05
Batch  41  loss:  7.983438263181597e-05
Batch  51  loss:  7.494322198908776e-05
Batch  61  loss:  9.770612814463675e-05
Batch  71  loss:  0.00011070164327975363
Batch  81  loss:  0.00011344641097821295
Batch  91  loss:  9.869466157397255e-05
Batch  101  loss:  7.542173261754215e-05
Batch  111  loss:  8.263777999673039e-05
Batch  121  loss:  9.328805754194036e-05
Batch  131  loss:  0.00010122144158231094
Batch  141  loss:  0.00011243767949054018
Batch  151  loss:  0.00011201403685845435
Batch  161  loss:  0.00012277835048735142
Batch  171  loss:  0.0001861998753156513
Batch  181  loss:  8.999618148664013e-05
Batch  191  loss:  0.00012114522542105988
Validation on real data: 
LOSS supervised-train 0.00010373946213803719, valid 7.413596904370934e-05
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00012089333904441446
Batch  11  loss:  0.00010977569036185741
Batch  21  loss:  8.073891513049603e-05
Batch  31  loss:  6.876869883853942e-05
Batch  41  loss:  8.834201435092837e-05
Batch  51  loss:  8.439709927188233e-05
Batch  61  loss:  0.00011144375457661226
Batch  71  loss:  0.00013043121725786477
Batch  81  loss:  9.111498366110027e-05
Batch  91  loss:  8.557226101402193e-05
Batch  101  loss:  8.212606917368248e-05
Batch  111  loss:  9.934080298990011e-05
Batch  121  loss:  6.508195656351745e-05
Batch  131  loss:  8.069974137470126e-05
Batch  141  loss:  0.00011383822129573673
Batch  151  loss:  7.731275400146842e-05
Batch  161  loss:  0.0001075064719771035
Batch  171  loss:  0.00014780971105210483
Batch  181  loss:  7.234427175717428e-05
Batch  191  loss:  9.916137787513435e-05
Validation on real data: 
LOSS supervised-train 0.00010075394555315143, valid 7.602196274092421e-05
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  9.440279973205179e-05
Batch  11  loss:  0.00014056620420888066
Batch  21  loss:  0.00014410968287847936
Batch  31  loss:  7.098402420524508e-05
Batch  41  loss:  9.534392302157357e-05
Batch  51  loss:  9.542209591018036e-05
Batch  61  loss:  7.472780998796225e-05
Batch  71  loss:  0.00015577554586343467
Batch  81  loss:  0.00011562024883460253
Batch  91  loss:  0.0001018480907077901
Batch  101  loss:  9.259033686248586e-05
Batch  111  loss:  8.004657865967602e-05
Batch  121  loss:  9.243327076546848e-05
Batch  131  loss:  7.322034798562527e-05
Batch  141  loss:  9.657000191509724e-05
Batch  151  loss:  8.390198490815237e-05
Batch  161  loss:  0.00012843676086049527
Batch  171  loss:  0.00012478706776164472
Batch  181  loss:  8.65823749336414e-05
Batch  191  loss:  0.00014037173241376877
Validation on real data: 
LOSS supervised-train 0.00010092679254739778, valid 8.706244989298284e-05
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0001602591946721077
Batch  11  loss:  0.00013047239917796105
Batch  21  loss:  0.00010675465455278754
Batch  31  loss:  8.038487430894747e-05
Batch  41  loss:  0.00010196393122896552
Batch  51  loss:  0.00010181728430325165
Batch  61  loss:  7.965818076627329e-05
Batch  71  loss:  8.916160004446283e-05
Batch  81  loss:  0.00010732039663707837
Batch  91  loss:  9.381047857459635e-05
Batch  101  loss:  7.819657912477851e-05
Batch  111  loss:  8.715361036593094e-05
Batch  121  loss:  8.383504609810188e-05
Batch  131  loss:  7.444104267051443e-05
Batch  141  loss:  0.00011452344915596768
Batch  151  loss:  9.903363388730213e-05
Batch  161  loss:  0.00010924465459538624
Batch  171  loss:  0.00012382506974972785
Batch  181  loss:  7.384337368421257e-05
Batch  191  loss:  0.00010471736459294334
Validation on real data: 
LOSS supervised-train 9.87201682801242e-05, valid 8.030039316508919e-05
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.00012204252561787143
Batch  11  loss:  0.00011563781299628317
Batch  21  loss:  9.573135321261361e-05
Batch  31  loss:  7.734071550657973e-05
Batch  41  loss:  7.690064376220107e-05
Batch  51  loss:  8.302332571474835e-05
Batch  61  loss:  9.90796834230423e-05
Batch  71  loss:  0.00013445749937091023
Batch  81  loss:  0.0001014466179185547
Batch  91  loss:  7.841241313144565e-05
Batch  101  loss:  7.423447095789015e-05
Batch  111  loss:  8.167856867657974e-05
Batch  121  loss:  7.908073894213885e-05
Batch  131  loss:  9.292583126807585e-05
Batch  141  loss:  0.00011188363714609295
Batch  151  loss:  9.991438855649903e-05
Batch  161  loss:  0.00011095432273577899
Batch  171  loss:  0.00011895563511643559
Batch  181  loss:  7.878682663431391e-05
Batch  191  loss:  9.83099453151226e-05
Validation on real data: 
LOSS supervised-train 9.517605234577786e-05, valid 7.88891629781574e-05
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00013472794671542943
Batch  11  loss:  0.00011773045844165608
Batch  21  loss:  0.0001254918024642393
Batch  31  loss:  9.155727457255125e-05
Batch  41  loss:  8.22938236524351e-05
Batch  51  loss:  8.855382475303486e-05
Batch  61  loss:  0.00010141907841898501
Batch  71  loss:  0.00013366872735787183
Batch  81  loss:  0.00010947098780889064
Batch  91  loss:  9.037504787556827e-05
Batch  101  loss:  8.66180271259509e-05
Batch  111  loss:  8.212839748011902e-05
Batch  121  loss:  7.112217281246558e-05
Batch  131  loss:  7.792213000357151e-05
Batch  141  loss:  9.231983131030574e-05
Batch  151  loss:  9.518416482023895e-05
Batch  161  loss:  9.487671923125163e-05
Batch  171  loss:  0.0001514981413492933
Batch  181  loss:  7.195773650892079e-05
Batch  191  loss:  0.00010512195876799524
Validation on real data: 
LOSS supervised-train 9.83790411191876e-05, valid 8.642983448226005e-05
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.00013561095693148673
Batch  11  loss:  9.312881593359634e-05
Batch  21  loss:  0.00013593529001809657
Batch  31  loss:  8.906422590371221e-05
Batch  41  loss:  7.629006722709164e-05
Batch  51  loss:  7.246961467899382e-05
Batch  61  loss:  9.103814227273688e-05
Batch  71  loss:  0.00013286556350067258
Batch  81  loss:  7.746399933239445e-05
Batch  91  loss:  7.299328717635944e-05
Batch  101  loss:  7.711711077718064e-05
Batch  111  loss:  0.00014724265201948583
Batch  121  loss:  7.116921915439889e-05
Batch  131  loss:  7.812013063812628e-05
Batch  141  loss:  9.30893947952427e-05
Batch  151  loss:  7.904106314526871e-05
Batch  161  loss:  0.00010234247747575864
Batch  171  loss:  0.00010662552813300863
Batch  181  loss:  6.966868386371061e-05
Batch  191  loss:  0.00012222271470818669
Validation on real data: 
LOSS supervised-train 9.272132556361611e-05, valid 6.757381925126538e-05
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00012287766730878502
Batch  11  loss:  0.00012353026249911636
Batch  21  loss:  0.00010562006355030462
Batch  31  loss:  7.786238711560145e-05
Batch  41  loss:  8.627923671156168e-05
Batch  51  loss:  8.673316915519536e-05
Batch  61  loss:  8.217905269702896e-05
Batch  71  loss:  0.00010602083057165146
Batch  81  loss:  0.00011649290536297485
Batch  91  loss:  7.145567360566929e-05
Batch  101  loss:  6.772085907869041e-05
Batch  111  loss:  7.792671385686845e-05
Batch  121  loss:  7.449650729540735e-05
Batch  131  loss:  7.759656000416726e-05
Batch  141  loss:  8.29616910777986e-05
Batch  151  loss:  7.413270213874057e-05
Batch  161  loss:  8.023172267712653e-05
Batch  171  loss:  0.0001089973738999106
Batch  181  loss:  5.8975903812097386e-05
Batch  191  loss:  0.00012082519970135763
Validation on real data: 
LOSS supervised-train 9.111671870414284e-05, valid 6.873781239846721e-05
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00011229163646930829
Batch  11  loss:  0.00011309560795780271
Batch  21  loss:  7.786684000166133e-05
Batch  31  loss:  7.344243203988299e-05
Batch  41  loss:  7.679680129513144e-05
Batch  51  loss:  7.399790047202259e-05
Batch  61  loss:  8.744794467929751e-05
Batch  71  loss:  0.0001401922490913421
Batch  81  loss:  8.373422315344214e-05
Batch  91  loss:  6.09338931099046e-05
Batch  101  loss:  8.02225258667022e-05
Batch  111  loss:  7.508338603656739e-05
Batch  121  loss:  8.128464105539024e-05
Batch  131  loss:  8.450495079159737e-05
Batch  141  loss:  9.84486541710794e-05
Batch  151  loss:  6.387470057234168e-05
Batch  161  loss:  0.00010077206388814375
Batch  171  loss:  0.0001054559470503591
Batch  181  loss:  8.639770385343581e-05
Batch  191  loss:  0.00010492783621884882
Validation on real data: 
LOSS supervised-train 9.399312210007339e-05, valid 6.38773781247437e-05
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00013355327246244997
Batch  11  loss:  0.0001250363129656762
Batch  21  loss:  8.293303835671395e-05
Batch  31  loss:  7.140685920603573e-05
Batch  41  loss:  7.125575939426199e-05
Batch  51  loss:  8.007125870790333e-05
Batch  61  loss:  0.00010378543811384588
Batch  71  loss:  9.78204989223741e-05
Batch  81  loss:  7.413274579448625e-05
Batch  91  loss:  6.272491009440273e-05
Batch  101  loss:  5.332253931555897e-05
Batch  111  loss:  6.717068026773632e-05
Batch  121  loss:  8.018950757104903e-05
Batch  131  loss:  8.102149877231568e-05
Batch  141  loss:  9.666403639130294e-05
Batch  151  loss:  8.71199372340925e-05
Batch  161  loss:  9.017979755299166e-05
Batch  171  loss:  0.00011013615585397929
Batch  181  loss:  8.27559779281728e-05
Batch  191  loss:  0.00010553495667409152
Validation on real data: 
LOSS supervised-train 9.081061491087894e-05, valid 7.523800013586879e-05
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00010678041144274175
Batch  11  loss:  9.737168875290081e-05
Batch  21  loss:  0.00011349327542120591
Batch  31  loss:  0.00011034563067369163
Batch  41  loss:  8.398365753237158e-05
Batch  51  loss:  7.506243855459616e-05
Batch  61  loss:  6.918371946085244e-05
Batch  71  loss:  0.00010153325274586678
Batch  81  loss:  9.365945152239874e-05
Batch  91  loss:  6.343279528664425e-05
Batch  101  loss:  6.524354830617085e-05
Batch  111  loss:  8.605870243627578e-05
Batch  121  loss:  7.734651444479823e-05
Batch  131  loss:  8.251592225860804e-05
Batch  141  loss:  0.00010483768710400909
Batch  151  loss:  8.431892638327554e-05
Batch  161  loss:  0.00010571969323791564
Batch  171  loss:  0.00010513540473766625
Batch  181  loss:  8.317924221046269e-05
Batch  191  loss:  9.40260651987046e-05
Validation on real data: 
LOSS supervised-train 9.060722624781192e-05, valid 9.183509973809123e-05
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00010347158240620047
Batch  11  loss:  0.00012941462046001107
Batch  21  loss:  9.83528298093006e-05
Batch  31  loss:  9.429290366824716e-05
Batch  41  loss:  9.678889182396233e-05
Batch  51  loss:  8.10378696769476e-05
Batch  61  loss:  9.337833034805954e-05
Batch  71  loss:  0.00010741974983830005
Batch  81  loss:  9.583738574292511e-05
Batch  91  loss:  5.935060835327022e-05
Batch  101  loss:  7.992424798430875e-05
Batch  111  loss:  9.269816655432805e-05
Batch  121  loss:  8.199380681617185e-05
Batch  131  loss:  8.251931285485625e-05
Batch  141  loss:  8.286299998871982e-05
Batch  151  loss:  8.392956806346774e-05
Batch  161  loss:  9.344082354800776e-05
Batch  171  loss:  0.00012583655188791454
Batch  181  loss:  6.647589179920033e-05
Batch  191  loss:  0.000125665552332066
Validation on real data: 
LOSS supervised-train 9.072835311599192e-05, valid 7.577555516036227e-05
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  9.439121640753001e-05
Batch  11  loss:  0.00015104135673027486
Batch  21  loss:  0.00010309004574082792
Batch  31  loss:  8.035689097596332e-05
Batch  41  loss:  7.427225500578061e-05
Batch  51  loss:  8.905898721423e-05
Batch  61  loss:  6.268495781114325e-05
Batch  71  loss:  8.436039206571877e-05
Batch  81  loss:  7.922975055407733e-05
Batch  91  loss:  8.770927524892613e-05
Batch  101  loss:  7.058530172798783e-05
Batch  111  loss:  7.513401214964688e-05
Batch  121  loss:  9.075697016669437e-05
Batch  131  loss:  6.283607217483222e-05
Batch  141  loss:  9.654713358031586e-05
Batch  151  loss:  8.601966692367569e-05
Batch  161  loss:  8.117148536257446e-05
Batch  171  loss:  0.00010871316044358537
Batch  181  loss:  7.675299275433645e-05
Batch  191  loss:  0.00013149221194908023
Validation on real data: 
LOSS supervised-train 8.894465674529783e-05, valid 8.248013182310387e-05
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  9.536366997053847e-05
Batch  11  loss:  0.00012148878158768639
Batch  21  loss:  0.00012198091280879453
Batch  31  loss:  7.54704451537691e-05
Batch  41  loss:  9.564407810103148e-05
Batch  51  loss:  7.433012797264382e-05
Batch  61  loss:  0.0001283997844439
Batch  71  loss:  9.470704389968887e-05
Batch  81  loss:  7.967096462380141e-05
Batch  91  loss:  8.025042188819498e-05
Batch  101  loss:  5.959984628134407e-05
Batch  111  loss:  7.862572965677828e-05
Batch  121  loss:  7.749281940050423e-05
Batch  131  loss:  6.46107000648044e-05
Batch  141  loss:  9.520784806227311e-05
Batch  151  loss:  9.697557834442705e-05
Batch  161  loss:  9.903548198053613e-05
Batch  171  loss:  0.0001519345969427377
Batch  181  loss:  6.65326151647605e-05
Batch  191  loss:  9.575024159858003e-05
Validation on real data: 
LOSS supervised-train 8.781675000136601e-05, valid 7.036520401015878e-05
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  9.641327051213011e-05
Batch  11  loss:  0.00010709208436310291
Batch  21  loss:  9.260045044356957e-05
Batch  31  loss:  6.424623279599473e-05
Batch  41  loss:  7.215818186523393e-05
Batch  51  loss:  6.650760769844055e-05
Batch  61  loss:  8.093665383057669e-05
Batch  71  loss:  0.00012081288150511682
Batch  81  loss:  9.827809117268771e-05
Batch  91  loss:  6.831451901234686e-05
Batch  101  loss:  6.748907617293298e-05
Batch  111  loss:  6.977443263167515e-05
Batch  121  loss:  8.754615555517375e-05
Batch  131  loss:  6.750932516297325e-05
Batch  141  loss:  7.436903979396448e-05
Batch  151  loss:  6.193073932081461e-05
Batch  161  loss:  7.33446649974212e-05
Batch  171  loss:  0.000128986343042925
Batch  181  loss:  8.098002581391484e-05
Batch  191  loss:  0.0001188103633467108
Validation on real data: 
LOSS supervised-train 8.829790654999669e-05, valid 7.16553331585601e-05
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00011329675180604681
Batch  11  loss:  8.400371007155627e-05
Batch  21  loss:  8.838062785798684e-05
Batch  31  loss:  8.279797475552186e-05
Batch  41  loss:  8.048995368881151e-05
Batch  51  loss:  7.165684655774385e-05
Batch  61  loss:  0.00010018310422310606
Batch  71  loss:  0.00014577334513887763
Batch  81  loss:  7.44120407034643e-05
Batch  91  loss:  6.605037197005004e-05
Batch  101  loss:  5.982835136819631e-05
Batch  111  loss:  8.573625382268801e-05
Batch  121  loss:  8.300726767629385e-05
Batch  131  loss:  6.07599868089892e-05
Batch  141  loss:  9.881208097795025e-05
Batch  151  loss:  7.459559856215492e-05
Batch  161  loss:  9.383956785313785e-05
Batch  171  loss:  0.00011834951874334365
Batch  181  loss:  7.890041888458654e-05
Batch  191  loss:  9.037409472512081e-05
Validation on real data: 
LOSS supervised-train 8.7311126953864e-05, valid 6.433221278712153e-05
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00011214966798434034
Batch  11  loss:  0.0001003990473691374
Batch  21  loss:  9.232923912350088e-05
Batch  31  loss:  7.652341446373612e-05
Batch  41  loss:  8.242422336479649e-05
Batch  51  loss:  6.794215005356818e-05
Batch  61  loss:  7.31867112335749e-05
Batch  71  loss:  9.624980884836987e-05
Batch  81  loss:  9.281497477786615e-05
Batch  91  loss:  6.22633087914437e-05
Batch  101  loss:  4.675683521782048e-05
Batch  111  loss:  9.74948561633937e-05
Batch  121  loss:  6.747582665411755e-05
Batch  131  loss:  5.753741424996406e-05
Batch  141  loss:  7.891529821790755e-05
Batch  151  loss:  7.062283111736178e-05
Batch  161  loss:  9.025119652505964e-05
Batch  171  loss:  0.00011413334868848324
Batch  181  loss:  7.301619916688651e-05
Batch  191  loss:  8.246862853411585e-05
Validation on real data: 
LOSS supervised-train 8.613940639406792e-05, valid 5.8312340115662664e-05
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00010256061068503186
Batch  11  loss:  9.52301561483182e-05
Batch  21  loss:  8.468003943562508e-05
Batch  31  loss:  5.737877290812321e-05
Batch  41  loss:  8.346117829205468e-05
Batch  51  loss:  9.276271885028109e-05
Batch  61  loss:  8.694229472894222e-05
Batch  71  loss:  9.031547961058095e-05
Batch  81  loss:  9.570811380399391e-05
Batch  91  loss:  8.081984560703859e-05
Batch  101  loss:  6.135898729553446e-05
Batch  111  loss:  7.820027531124651e-05
Batch  121  loss:  6.306505383690819e-05
Batch  131  loss:  6.41764490865171e-05
Batch  141  loss:  8.334921585628763e-05
Batch  151  loss:  6.660742656094953e-05
Batch  161  loss:  8.415713818976656e-05
Batch  171  loss:  0.00012978351151105016
Batch  181  loss:  7.510155410273e-05
Batch  191  loss:  0.0001060596332536079
Validation on real data: 
LOSS supervised-train 8.43587549206859e-05, valid 7.842351624276489e-05
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0001091745070880279
Batch  11  loss:  0.00010319573630113155
Batch  21  loss:  9.066948405234143e-05
Batch  31  loss:  7.883687067078426e-05
Batch  41  loss:  8.147041808115318e-05
Batch  51  loss:  6.119978206697851e-05
Batch  61  loss:  6.635498721152544e-05
Batch  71  loss:  9.808305185288191e-05
Batch  81  loss:  7.487993570975959e-05
Batch  91  loss:  7.078834460116923e-05
Batch  101  loss:  5.5881326261442155e-05
Batch  111  loss:  9.488103387411684e-05
Batch  121  loss:  6.612732249777764e-05
Batch  131  loss:  7.943324453663081e-05
Batch  141  loss:  0.00010229285544482991
Batch  151  loss:  7.963126699905843e-05
Batch  161  loss:  6.91900058882311e-05
Batch  171  loss:  0.00012451007205527276
Batch  181  loss:  7.745817129034549e-05
Batch  191  loss:  0.00010290017235092819
Validation on real data: 
LOSS supervised-train 8.454727510979865e-05, valid 6.349096656776965e-05
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  9.564876381773502e-05
Batch  11  loss:  8.807865378912538e-05
Batch  21  loss:  9.326741565018892e-05
Batch  31  loss:  7.44333810871467e-05
Batch  41  loss:  5.332648288458586e-05
Batch  51  loss:  9.756375220604241e-05
Batch  61  loss:  9.15211858227849e-05
Batch  71  loss:  0.00011249972158111632
Batch  81  loss:  8.395049371756613e-05
Batch  91  loss:  5.983705341350287e-05
Batch  101  loss:  7.848863606341183e-05
Batch  111  loss:  7.371788524324074e-05
Batch  121  loss:  7.185195136116818e-05
Batch  131  loss:  7.501526124542579e-05
Batch  141  loss:  9.287478314945474e-05
Batch  151  loss:  7.198886305559427e-05
Batch  161  loss:  8.402861567446962e-05
Batch  171  loss:  0.00010482782090548426
Batch  181  loss:  6.503826443804428e-05
Batch  191  loss:  9.51284819166176e-05
Validation on real data: 
LOSS supervised-train 8.409967467741808e-05, valid 7.049728446872905e-05
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00011222770990571007
Batch  11  loss:  0.00012012085790047422
Batch  21  loss:  8.52276716614142e-05
Batch  31  loss:  6.0805185057688504e-05
Batch  41  loss:  7.483694207621738e-05
Batch  51  loss:  7.636640657437965e-05
Batch  61  loss:  6.980915350141004e-05
Batch  71  loss:  9.15856653591618e-05
Batch  81  loss:  7.868368265917525e-05
Batch  91  loss:  7.610917964484543e-05
Batch  101  loss:  6.275106716202572e-05
Batch  111  loss:  6.900888547534123e-05
Batch  121  loss:  6.33646413916722e-05
Batch  131  loss:  6.290763121796772e-05
Batch  141  loss:  0.00010102741362061352
Batch  151  loss:  9.052016685018316e-05
Batch  161  loss:  8.611407974967733e-05
Batch  171  loss:  0.00010018316970672458
Batch  181  loss:  8.313630678458139e-05
Batch  191  loss:  9.204209345625713e-05
Validation on real data: 
LOSS supervised-train 8.349854102561949e-05, valid 7.819695747457445e-05
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  9.659109491622075e-05
Batch  11  loss:  0.00010797809227369726
Batch  21  loss:  8.870146848494187e-05
Batch  31  loss:  8.462942787446082e-05
Batch  41  loss:  7.531389564974234e-05
Batch  51  loss:  6.974928692216054e-05
Batch  61  loss:  8.803541277302429e-05
Batch  71  loss:  9.850398055277765e-05
Batch  81  loss:  7.602153345942497e-05
Batch  91  loss:  7.695148815400898e-05
Batch  101  loss:  5.7635112170828506e-05
Batch  111  loss:  6.877606938360259e-05
Batch  121  loss:  7.30636966181919e-05
Batch  131  loss:  7.271613867487758e-05
Batch  141  loss:  8.095449447864667e-05
Batch  151  loss:  6.736122304573655e-05
Batch  161  loss:  8.492185588693246e-05
Batch  171  loss:  0.00010136937635252252
Batch  181  loss:  7.785677735228091e-05
Batch  191  loss:  0.0001137212457251735
Validation on real data: 
LOSS supervised-train 8.172338028089143e-05, valid 7.107019337126985e-05
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  8.770434942562133e-05
Batch  11  loss:  8.8120112195611e-05
Batch  21  loss:  9.988655801862478e-05
Batch  31  loss:  6.176331226015463e-05
Batch  41  loss:  6.189659325173125e-05
Batch  51  loss:  6.375502562150359e-05
Batch  61  loss:  7.23831617506221e-05
Batch  71  loss:  0.00010438799654366449
Batch  81  loss:  6.725717685185373e-05
Batch  91  loss:  7.338705472648144e-05
Batch  101  loss:  6.387745088431984e-05
Batch  111  loss:  6.769702304154634e-05
Batch  121  loss:  5.879329546587542e-05
Batch  131  loss:  6.219561328180134e-05
Batch  141  loss:  8.743869693716988e-05
Batch  151  loss:  6.284347909968346e-05
Batch  161  loss:  9.396672248840332e-05
Batch  171  loss:  9.142525232164189e-05
Batch  181  loss:  9.679709182819352e-05
Batch  191  loss:  0.00010353885591030121
Validation on real data: 
LOSS supervised-train 8.151935990099445e-05, valid 7.902650395408273e-05
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00010486137034604326
Batch  11  loss:  0.0001147606162703596
Batch  21  loss:  6.6978762333747e-05
Batch  31  loss:  8.171185618266463e-05
Batch  41  loss:  7.49514510971494e-05
Batch  51  loss:  6.998152093729004e-05
Batch  61  loss:  9.022047743201256e-05
Batch  71  loss:  0.00010206437582382932
Batch  81  loss:  7.611361797899008e-05
Batch  91  loss:  7.466815441148356e-05
Batch  101  loss:  6.554265564773232e-05
Batch  111  loss:  7.489955169148743e-05
Batch  121  loss:  7.48994352761656e-05
Batch  131  loss:  6.578156899195164e-05
Batch  141  loss:  7.894336886238307e-05
Batch  151  loss:  7.022062345640734e-05
Batch  161  loss:  9.040944132721052e-05
Batch  171  loss:  0.00011358794290572405
Batch  181  loss:  8.539573173038661e-05
Batch  191  loss:  9.577262244420126e-05
Validation on real data: 
LOSS supervised-train 8.276998041765183e-05, valid 8.204420009860769e-05
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  9.431823127670214e-05
Batch  11  loss:  9.675318142399192e-05
Batch  21  loss:  0.0001045332828653045
Batch  31  loss:  8.063899440458044e-05
Batch  41  loss:  8.328117110067979e-05
Batch  51  loss:  6.745946302544326e-05
Batch  61  loss:  9.793517529033124e-05
Batch  71  loss:  8.604196045780554e-05
Batch  81  loss:  7.997603825060651e-05
Batch  91  loss:  7.045597885735333e-05
Batch  101  loss:  5.044439240009524e-05
Batch  111  loss:  5.989881174173206e-05
Batch  121  loss:  6.443226448027417e-05
Batch  131  loss:  7.65631120884791e-05
Batch  141  loss:  7.164140697568655e-05
Batch  151  loss:  6.154949369374663e-05
Batch  161  loss:  0.00010934190504485741
Batch  171  loss:  0.00011348329280735925
Batch  181  loss:  6.245083204703405e-05
Batch  191  loss:  8.727477688807994e-05
Validation on real data: 
LOSS supervised-train 8.116322920614039e-05, valid 7.714747334830463e-05
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00011277609155513346
Batch  11  loss:  0.00011619983706623316
Batch  21  loss:  8.104443259071559e-05
Batch  31  loss:  6.384075823007151e-05
Batch  41  loss:  8.277194137917832e-05
Batch  51  loss:  7.077831105561927e-05
Batch  61  loss:  7.453329453710467e-05
Batch  71  loss:  8.441281534032896e-05
Batch  81  loss:  7.478926272597164e-05
Batch  91  loss:  7.445644587278366e-05
Batch  101  loss:  6.107900844654068e-05
Batch  111  loss:  7.448844553437084e-05
Batch  121  loss:  8.232433174271137e-05
Batch  131  loss:  7.38622693461366e-05
Batch  141  loss:  8.968225301941857e-05
Batch  151  loss:  6.874412792967632e-05
Batch  161  loss:  7.99500267021358e-05
Batch  171  loss:  9.657425835030153e-05
Batch  181  loss:  8.447744767181575e-05
Batch  191  loss:  0.000113365073048044
Validation on real data: 
LOSS supervised-train 8.03587269547279e-05, valid 6.117009615991265e-05
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  8.249846723629162e-05
Batch  11  loss:  0.00014036803622730076
Batch  21  loss:  9.566477092448622e-05
Batch  31  loss:  8.630478259874508e-05
Batch  41  loss:  9.742433758219704e-05
Batch  51  loss:  6.246781413210556e-05
Batch  61  loss:  7.804208871675655e-05
Batch  71  loss:  8.308968972414732e-05
Batch  81  loss:  7.028109394013882e-05
Batch  91  loss:  5.043644705438055e-05
Batch  101  loss:  4.627341331797652e-05
Batch  111  loss:  7.429075049003586e-05
Batch  121  loss:  6.219965871423483e-05
Batch  131  loss:  7.161302346503362e-05
Batch  141  loss:  9.047047205967829e-05
Batch  151  loss:  7.453613943653181e-05
Batch  161  loss:  6.807154568377882e-05
Batch  171  loss:  0.00010303637100150809
Batch  181  loss:  7.656510570086539e-05
Batch  191  loss:  9.494074765825644e-05
Validation on real data: 
LOSS supervised-train 8.022011517823558e-05, valid 6.984434003243223e-05
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  9.51278634602204e-05
Batch  11  loss:  7.220316183520481e-05
Batch  21  loss:  7.872447167756036e-05
Batch  31  loss:  6.839642446720973e-05
Batch  41  loss:  8.690510003361851e-05
Batch  51  loss:  5.877440344193019e-05
Batch  61  loss:  6.338476669043303e-05
Batch  71  loss:  7.371613901341334e-05
Batch  81  loss:  7.3076575063169e-05
Batch  91  loss:  5.471659096656367e-05
Batch  101  loss:  4.546938725979999e-05
Batch  111  loss:  7.19545132596977e-05
Batch  121  loss:  6.0340011259540915e-05
Batch  131  loss:  7.09600280970335e-05
Batch  141  loss:  7.675844972254708e-05
Batch  151  loss:  6.505637429654598e-05
Batch  161  loss:  8.800677460385486e-05
Batch  171  loss:  9.276298806071281e-05
Batch  181  loss:  7.692309009144083e-05
Batch  191  loss:  8.498089300701395e-05
Validation on real data: 
LOSS supervised-train 7.971682160132332e-05, valid 6.401640712283552e-05
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  7.895120506873354e-05
Batch  11  loss:  8.973471267381683e-05
Batch  21  loss:  8.908178278943524e-05
Batch  31  loss:  7.34121713321656e-05
Batch  41  loss:  7.203346467576921e-05
Batch  51  loss:  6.430871871998534e-05
Batch  61  loss:  7.905361417215317e-05
Batch  71  loss:  0.00011774491576943547
Batch  81  loss:  8.07579854154028e-05
Batch  91  loss:  6.679748184978962e-05
Batch  101  loss:  4.727957639261149e-05
Batch  111  loss:  7.287836342584342e-05
Batch  121  loss:  6.750063766958192e-05
Batch  131  loss:  6.4814870711416e-05
Batch  141  loss:  8.598811837146059e-05
Batch  151  loss:  6.521463365061209e-05
Batch  161  loss:  8.121041173581034e-05
Batch  171  loss:  0.0001177449303213507
Batch  181  loss:  7.383678166661412e-05
Batch  191  loss:  0.0001074836400221102
Validation on real data: 
LOSS supervised-train 8.005623370991089e-05, valid 5.6079268688336015e-05
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  9.649082494433969e-05
Batch  11  loss:  0.00010432151611894369
Batch  21  loss:  8.646420610602945e-05
Batch  31  loss:  8.223123586503789e-05
Batch  41  loss:  5.8372894272906706e-05
Batch  51  loss:  5.6566132116131485e-05
Batch  61  loss:  5.842525206389837e-05
Batch  71  loss:  0.00010774032125482336
Batch  81  loss:  7.920018106233329e-05
Batch  91  loss:  6.881708395667374e-05
Batch  101  loss:  6.147546810097992e-05
Batch  111  loss:  6.95298658683896e-05
Batch  121  loss:  7.152964826673269e-05
Batch  131  loss:  6.486223719548434e-05
Batch  141  loss:  8.723001519683748e-05
Batch  151  loss:  6.329829921014607e-05
Batch  161  loss:  9.369088365929201e-05
Batch  171  loss:  0.00011394759349059314
Batch  181  loss:  8.237794099841267e-05
Batch  191  loss:  9.09038499230519e-05
Validation on real data: 
LOSS supervised-train 7.885630662713084e-05, valid 7.117323548300192e-05
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  9.542025509290397e-05
Batch  11  loss:  8.720861660549417e-05
Batch  21  loss:  9.53686612774618e-05
Batch  31  loss:  6.841013237135485e-05
Batch  41  loss:  6.602469511562958e-05
Batch  51  loss:  7.339068542933092e-05
Batch  61  loss:  8.960690320236608e-05
Batch  71  loss:  0.00010335334081901237
Batch  81  loss:  6.775264773750678e-05
Batch  91  loss:  5.2801318815909326e-05
Batch  101  loss:  6.170752021716908e-05
Batch  111  loss:  5.090479680802673e-05
Batch  121  loss:  7.378527516266331e-05
Batch  131  loss:  6.795838271500543e-05
Batch  141  loss:  8.246257493738085e-05
Batch  151  loss:  6.220101931830868e-05
Batch  161  loss:  9.569134272169322e-05
Batch  171  loss:  0.00011177262058481574
Batch  181  loss:  6.045832924428396e-05
Batch  191  loss:  8.044153219088912e-05
Validation on real data: 
LOSS supervised-train 7.927058040877455e-05, valid 5.910725303692743e-05
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  9.166500967694446e-05
Batch  11  loss:  7.699096750002354e-05
Batch  21  loss:  9.13839103304781e-05
Batch  31  loss:  7.716492837062106e-05
Batch  41  loss:  6.589101394638419e-05
Batch  51  loss:  6.558141467394307e-05
Batch  61  loss:  8.539872214896604e-05
Batch  71  loss:  0.0001226848689839244
Batch  81  loss:  6.194809247972444e-05
Batch  91  loss:  5.1859740779036656e-05
Batch  101  loss:  7.288660708582029e-05
Batch  111  loss:  5.3727275371784344e-05
Batch  121  loss:  6.938353180885315e-05
Batch  131  loss:  7.061602082103491e-05
Batch  141  loss:  6.138990283943713e-05
Batch  151  loss:  9.178516484098509e-05
Batch  161  loss:  9.87499370239675e-05
Batch  171  loss:  0.0001090303630917333
Batch  181  loss:  8.593862730776891e-05
Batch  191  loss:  8.796092151897028e-05
Validation on real data: 
LOSS supervised-train 7.626997045008466e-05, valid 7.893428846728057e-05
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  7.751744851702824e-05
Batch  11  loss:  9.76532101049088e-05
Batch  21  loss:  6.327447044895962e-05
Batch  31  loss:  7.733450911473483e-05
Batch  41  loss:  6.422801379812881e-05
Batch  51  loss:  6.140595360193402e-05
Batch  61  loss:  9.907200001180172e-05
Batch  71  loss:  8.055861690081656e-05
Batch  81  loss:  8.317142783198506e-05
Batch  91  loss:  6.165252125356346e-05
Batch  101  loss:  6.932196265552193e-05
Batch  111  loss:  0.00012534279085230082
Batch  121  loss:  6.299925007624552e-05
Batch  131  loss:  7.766945782350376e-05
Batch  141  loss:  7.136132626328617e-05
Batch  151  loss:  6.970517279114574e-05
Batch  161  loss:  7.74529471527785e-05
Batch  171  loss:  0.00010808048682520166
Batch  181  loss:  7.677162648178637e-05
Batch  191  loss:  9.22296749195084e-05
Validation on real data: 
LOSS supervised-train 7.585196060972522e-05, valid 6.244497490115464e-05
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  8.574662206228822e-05
Batch  11  loss:  8.850768062984571e-05
Batch  21  loss:  8.955474913818762e-05
Batch  31  loss:  6.816835957579315e-05
Batch  41  loss:  7.467430987162516e-05
Batch  51  loss:  4.8347948904847726e-05
Batch  61  loss:  6.925369234522805e-05
Batch  71  loss:  7.800432649673894e-05
Batch  81  loss:  7.84014628152363e-05
Batch  91  loss:  5.675506326952018e-05
Batch  101  loss:  6.756083166692406e-05
Batch  111  loss:  5.3939500503474846e-05
Batch  121  loss:  6.011686127749272e-05
Batch  131  loss:  8.40270658954978e-05
Batch  141  loss:  8.255799912149087e-05
Batch  151  loss:  7.03748592059128e-05
Batch  161  loss:  7.016528979875147e-05
Batch  171  loss:  0.00012172075366834179
Batch  181  loss:  7.304232713067904e-05
Batch  191  loss:  8.309772238135338e-05
Validation on real data: 
LOSS supervised-train 7.619399344548584e-05, valid 4.4607906602323055e-05
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  8.26733885332942e-05
Batch  11  loss:  7.520951476180926e-05
Batch  21  loss:  7.85149895818904e-05
Batch  31  loss:  7.06232531229034e-05
Batch  41  loss:  5.786358087789267e-05
Batch  51  loss:  6.36386321275495e-05
Batch  61  loss:  8.00768393673934e-05
Batch  71  loss:  7.889942935435101e-05
Batch  81  loss:  6.434241367969662e-05
Batch  91  loss:  4.364471897133626e-05
Batch  101  loss:  5.4947966418694705e-05
Batch  111  loss:  7.817372534191236e-05
Batch  121  loss:  7.128947618184611e-05
Batch  131  loss:  6.871637015137821e-05
Batch  141  loss:  7.836931035853922e-05
Batch  151  loss:  6.64082690491341e-05
Batch  161  loss:  6.679502985207364e-05
Batch  171  loss:  0.00011201646702829748
Batch  181  loss:  6.44981482764706e-05
Batch  191  loss:  9.32464754441753e-05
Validation on real data: 
LOSS supervised-train 7.540402704762527e-05, valid 6.732490146532655e-05
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00011302571510896087
Batch  11  loss:  7.388247468043119e-05
Batch  21  loss:  8.141417492879555e-05
Batch  31  loss:  6.219222268555313e-05
Batch  41  loss:  5.166465416550636e-05
Batch  51  loss:  6.261999806156382e-05
Batch  61  loss:  0.0001042602671077475
Batch  71  loss:  0.00010333100362913683
Batch  81  loss:  7.04473422956653e-05
Batch  91  loss:  5.8736957726068795e-05
Batch  101  loss:  4.906080721411854e-05
Batch  111  loss:  6.440436118282378e-05
Batch  121  loss:  6.914536061231047e-05
Batch  131  loss:  6.649436545558274e-05
Batch  141  loss:  7.96723470557481e-05
Batch  151  loss:  6.079753438825719e-05
Batch  161  loss:  6.99334850651212e-05
Batch  171  loss:  8.762858487898484e-05
Batch  181  loss:  6.993149145273492e-05
Batch  191  loss:  7.819326128810644e-05
Validation on real data: 
LOSS supervised-train 7.485392556191073e-05, valid 6.968095840420574e-05
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  8.667030488140881e-05
Batch  11  loss:  7.160461973398924e-05
Batch  21  loss:  8.056640217546374e-05
Batch  31  loss:  6.321746332105249e-05
Batch  41  loss:  5.79962543270085e-05
Batch  51  loss:  6.792388739995658e-05
Batch  61  loss:  7.173290941864252e-05
Batch  71  loss:  6.922939064679667e-05
Batch  81  loss:  7.127641583792865e-05
Batch  91  loss:  5.533083822228946e-05
Batch  101  loss:  6.32516312180087e-05
Batch  111  loss:  6.493883120128885e-05
Batch  121  loss:  6.663913518423215e-05
Batch  131  loss:  5.803758176625706e-05
Batch  141  loss:  5.8501675084698945e-05
Batch  151  loss:  4.887459726887755e-05
Batch  161  loss:  6.958271842449903e-05
Batch  171  loss:  9.670940198702738e-05
Batch  181  loss:  6.147850945126265e-05
Batch  191  loss:  7.3464289016556e-05
Validation on real data: 
LOSS supervised-train 7.378085700111115e-05, valid 5.812349991174415e-05
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00011111762432847172
Batch  11  loss:  8.199852891266346e-05
Batch  21  loss:  6.775995279895142e-05
Batch  31  loss:  5.816100019728765e-05
Batch  41  loss:  6.81992678437382e-05
Batch  51  loss:  6.096831930335611e-05
Batch  61  loss:  8.74630713951774e-05
Batch  71  loss:  8.089011680567637e-05
Batch  81  loss:  7.338826253544539e-05
Batch  91  loss:  6.704997940687463e-05
Batch  101  loss:  6.42485756543465e-05
Batch  111  loss:  7.917061157058924e-05
Batch  121  loss:  5.583829988609068e-05
Batch  131  loss:  7.400505273835734e-05
Batch  141  loss:  7.321629527723417e-05
Batch  151  loss:  6.505594501504675e-05
Batch  161  loss:  7.483015360776335e-05
Batch  171  loss:  0.00010622511035762727
Batch  181  loss:  6.585708615602925e-05
Batch  191  loss:  8.282053750008345e-05
Validation on real data: 
LOSS supervised-train 7.471351929780212e-05, valid 5.868351581739262e-05
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  9.51757247094065e-05
Batch  11  loss:  8.997079567052424e-05
Batch  21  loss:  6.74765178700909e-05
Batch  31  loss:  5.887258521397598e-05
Batch  41  loss:  6.229905557120219e-05
Batch  51  loss:  6.428486813092604e-05
Batch  61  loss:  8.665913628647104e-05
Batch  71  loss:  7.883906073402613e-05
Batch  81  loss:  7.9144119808916e-05
Batch  91  loss:  6.0692866099998355e-05
Batch  101  loss:  5.463444904307835e-05
Batch  111  loss:  8.263652125606313e-05
Batch  121  loss:  6.147595559014007e-05
Batch  131  loss:  8.035935024963692e-05
Batch  141  loss:  6.365557783283293e-05
Batch  151  loss:  7.165750139392912e-05
Batch  161  loss:  6.814594962634146e-05
Batch  171  loss:  0.00010620670218486339
Batch  181  loss:  4.964792969985865e-05
Batch  191  loss:  6.886610935907811e-05
Validation on real data: 
LOSS supervised-train 7.377430181804812e-05, valid 5.871514804312028e-05
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  9.120535105466843e-05
Batch  11  loss:  0.000101105120847933
Batch  21  loss:  6.387374014593661e-05
Batch  31  loss:  7.926829130155966e-05
Batch  41  loss:  9.36041833483614e-05
Batch  51  loss:  5.140921348356642e-05
Batch  61  loss:  6.001539441058412e-05
Batch  71  loss:  7.822701445547864e-05
Batch  81  loss:  8.182298915926367e-05
Batch  91  loss:  5.37328960490413e-05
Batch  101  loss:  5.483032146003097e-05
Batch  111  loss:  6.077315629227087e-05
Batch  121  loss:  6.206235411809757e-05
Batch  131  loss:  5.9069585404358804e-05
Batch  141  loss:  7.943200034787878e-05
Batch  151  loss:  6.645778194069862e-05
Batch  161  loss:  7.303313759621233e-05
Batch  171  loss:  0.00010520793875912204
Batch  181  loss:  6.572117854375392e-05
Batch  191  loss:  6.507810030598193e-05
Validation on real data: 
LOSS supervised-train 7.262111401360016e-05, valid 6.24512176727876e-05
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  7.287167682079598e-05
Batch  11  loss:  8.74080287758261e-05
Batch  21  loss:  7.127014396246523e-05
Batch  31  loss:  6.0796290199505165e-05
Batch  41  loss:  9.301308455178514e-05
Batch  51  loss:  5.126222094986588e-05
Batch  61  loss:  8.241645991802216e-05
Batch  71  loss:  6.80251105222851e-05
Batch  81  loss:  6.759553798474371e-05
Batch  91  loss:  4.9215435865335166e-05
Batch  101  loss:  4.862476998823695e-05
Batch  111  loss:  6.004399619996548e-05
Batch  121  loss:  5.798383062938228e-05
Batch  131  loss:  5.8953726693289354e-05
Batch  141  loss:  9.079887968255207e-05
Batch  151  loss:  7.060660573188215e-05
Batch  161  loss:  7.025546801742166e-05
Batch  171  loss:  8.327258547069505e-05
Batch  181  loss:  6.909365765750408e-05
Batch  191  loss:  0.00010970171570079401
Validation on real data: 
LOSS supervised-train 7.225103243399644e-05, valid 5.558598058996722e-05
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  9.10000890144147e-05
Batch  11  loss:  9.839890117291361e-05
Batch  21  loss:  7.083705713739619e-05
Batch  31  loss:  5.9283487644279376e-05
Batch  41  loss:  8.232898835558444e-05
Batch  51  loss:  6.857856351416558e-05
Batch  61  loss:  6.745843711541966e-05
Batch  71  loss:  7.481502689188346e-05
Batch  81  loss:  7.732036465313286e-05
Batch  91  loss:  4.8924404836725444e-05
Batch  101  loss:  7.286098843906075e-05
Batch  111  loss:  6.106756336521357e-05
Batch  121  loss:  5.493704156833701e-05
Batch  131  loss:  5.3312302043195814e-05
Batch  141  loss:  7.528615242335945e-05
Batch  151  loss:  6.223574018804356e-05
Batch  161  loss:  7.317601557588205e-05
Batch  171  loss:  0.00010223875869996846
Batch  181  loss:  5.3955940529704094e-05
Batch  191  loss:  8.870408055372536e-05
Validation on real data: 
LOSS supervised-train 7.292039872481837e-05, valid 6.277487409533933e-05
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  8.147866901708767e-05
Batch  11  loss:  7.065272075124085e-05
Batch  21  loss:  5.9075224271509796e-05
Batch  31  loss:  5.3133422625251114e-05
Batch  41  loss:  6.280811066972092e-05
Batch  51  loss:  5.4125812312122434e-05
Batch  61  loss:  6.700864469166845e-05
Batch  71  loss:  7.794030534569174e-05
Batch  81  loss:  6.2009428802412e-05
Batch  91  loss:  6.1021408328088e-05
Batch  101  loss:  7.314544927794486e-05
Batch  111  loss:  6.26755936536938e-05
Batch  121  loss:  5.868057269253768e-05
Batch  131  loss:  6.038333594915457e-05
Batch  141  loss:  8.370857540285215e-05
Batch  151  loss:  8.504433208145201e-05
Batch  161  loss:  6.186000973684713e-05
Batch  171  loss:  9.940504969563335e-05
Batch  181  loss:  5.3162428230280057e-05
Batch  191  loss:  8.6623927927576e-05
Validation on real data: 
LOSS supervised-train 6.96613115906075e-05, valid 5.0944221584359184e-05
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  6.680352089460939e-05
Batch  11  loss:  9.24090709304437e-05
Batch  21  loss:  7.194926001830027e-05
Batch  31  loss:  6.46645130473189e-05
Batch  41  loss:  6.446048791985959e-05
Batch  51  loss:  4.819931200472638e-05
Batch  61  loss:  7.192294287960976e-05
Batch  71  loss:  5.878707088413648e-05
Batch  81  loss:  6.844698509667069e-05
Batch  91  loss:  5.963255898677744e-05
Batch  101  loss:  6.270142330322415e-05
Batch  111  loss:  5.083479481982067e-05
Batch  121  loss:  5.8602861827239394e-05
Batch  131  loss:  4.631370029528625e-05
Batch  141  loss:  6.863686576252803e-05
Batch  151  loss:  5.316406532074325e-05
Batch  161  loss:  7.841795741114765e-05
Batch  171  loss:  0.00010308079799870029
Batch  181  loss:  6.573038263013586e-05
Batch  191  loss:  6.188508268678561e-05
Validation on real data: 
LOSS supervised-train 7.150071060095798e-05, valid 5.401024463935755e-05
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  6.188586849020794e-05
Batch  11  loss:  8.862891991157085e-05
Batch  21  loss:  7.608708983752877e-05
Batch  31  loss:  6.19790589553304e-05
Batch  41  loss:  5.798196798423305e-05
Batch  51  loss:  5.756975951953791e-05
Batch  61  loss:  5.798797064926475e-05
Batch  71  loss:  6.716388452332467e-05
Batch  81  loss:  6.436419789679348e-05
Batch  91  loss:  5.579303979175165e-05
Batch  101  loss:  5.759077612310648e-05
Batch  111  loss:  6.647950067417696e-05
Batch  121  loss:  7.643151184311137e-05
Batch  131  loss:  5.238705489318818e-05
Batch  141  loss:  7.089815335348248e-05
Batch  151  loss:  6.207781552802771e-05
Batch  161  loss:  5.4921598348300904e-05
Batch  171  loss:  9.51928086578846e-05
Batch  181  loss:  8.337380131706595e-05
Batch  191  loss:  0.00010035050945589319
Validation on real data: 
LOSS supervised-train 7.009891387497192e-05, valid 7.280454155988991e-05
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  7.552254101028666e-05
Batch  11  loss:  9.721305104903877e-05
Batch  21  loss:  9.593916911398992e-05
Batch  31  loss:  6.395929085556418e-05
Batch  41  loss:  5.54928628844209e-05
Batch  51  loss:  5.3038780606584623e-05
Batch  61  loss:  5.411754682427272e-05
Batch  71  loss:  9.31566464714706e-05
Batch  81  loss:  6.331742770271376e-05
Batch  91  loss:  5.409978984971531e-05
Batch  101  loss:  5.32486337760929e-05
Batch  111  loss:  7.618799281772226e-05
Batch  121  loss:  5.517000317922793e-05
Batch  131  loss:  5.568989581661299e-05
Batch  141  loss:  5.774615055997856e-05
Batch  151  loss:  5.062206037109718e-05
Batch  161  loss:  7.857492892071605e-05
Batch  171  loss:  0.00010207237210124731
Batch  181  loss:  6.851943180663511e-05
Batch  191  loss:  8.421381062362343e-05
Validation on real data: 
LOSS supervised-train 7.062495604259311e-05, valid 6.318596570054069e-05
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  bathtub ; Model ID: 90b6e958b359c1592ad490d4d7fae486
--------------------
Training baseline regression model:  2022-03-29 20:46:54.405493
Detector:  point_transformer
Object:  bathtub
--------------------
device is  cuda
--------------------
Number of trainable parameters:  897700
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.16008608043193817
Batch  11  loss:  0.12471217662096024
Batch  21  loss:  0.11933138221502304
Batch  31  loss:  0.10975668579339981
Batch  41  loss:  0.09727660566568375
Batch  51  loss:  0.07705923169851303
Batch  61  loss:  0.07946574687957764
Batch  71  loss:  0.07346014678478241
Batch  81  loss:  0.0616181455552578
Batch  91  loss:  0.024919327348470688
Batch  101  loss:  0.020181413739919662
Batch  111  loss:  0.013848485425114632
Batch  121  loss:  0.01597331091761589
Batch  131  loss:  0.02772238664329052
Batch  141  loss:  0.014923309907317162
Batch  151  loss:  0.028789278119802475
Batch  161  loss:  0.007340902928262949
Batch  171  loss:  0.013371750712394714
Batch  181  loss:  0.00831399392336607
Batch  191  loss:  0.005759202875196934
Validation on real data: 
LOSS supervised-train 0.04949685397092253, valid 0.020530663430690765
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.008481604047119617
Batch  11  loss:  0.007541475351899862
Batch  21  loss:  0.02950669638812542
Batch  31  loss:  0.009552539326250553
Batch  41  loss:  0.011175215244293213
Batch  51  loss:  0.0038517809007316828
Batch  61  loss:  0.0035223981831222773
Batch  71  loss:  0.004540746565908194
Batch  81  loss:  0.004856838844716549
Batch  91  loss:  0.003993596415966749
Batch  101  loss:  0.007203794550150633
Batch  111  loss:  0.003509572008624673
Batch  121  loss:  0.0067652687430381775
Batch  131  loss:  0.010404097847640514
Batch  141  loss:  0.006312324665486813
Batch  151  loss:  0.01676606945693493
Batch  161  loss:  0.0044394806027412415
Batch  171  loss:  0.005119714420288801
Batch  181  loss:  0.0052172159776091576
Batch  191  loss:  0.003103978931903839
Validation on real data: 
LOSS supervised-train 0.006133767888532021, valid 0.003618457820266485
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.002456093905493617
Batch  11  loss:  0.0029007457196712494
Batch  21  loss:  0.006692445836961269
Batch  31  loss:  0.005441542714834213
Batch  41  loss:  0.0028203323017805815
Batch  51  loss:  0.0034056478179991245
Batch  61  loss:  0.002821053145453334
Batch  71  loss:  0.0020534778013825417
Batch  81  loss:  0.0045258766040205956
Batch  91  loss:  0.0027318219654262066
Batch  101  loss:  0.004943370819091797
Batch  111  loss:  0.0018309345468878746
Batch  121  loss:  0.004078469704836607
Batch  131  loss:  0.006301404908299446
Batch  141  loss:  0.004535101819783449
Batch  151  loss:  0.0061669317074120045
Batch  161  loss:  0.0028914534486830235
Batch  171  loss:  0.002729142317548394
Batch  181  loss:  0.0033417921513319016
Batch  191  loss:  0.002064917702227831
Validation on real data: 
LOSS supervised-train 0.0034851416747551413, valid 0.001476906007155776
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.001637409906834364
Batch  11  loss:  0.0013108941493555903
Batch  21  loss:  0.003741127671673894
Batch  31  loss:  0.004289824981242418
Batch  41  loss:  0.0018570685060694814
Batch  51  loss:  0.0017816730542108417
Batch  61  loss:  0.0015807306626811624
Batch  71  loss:  0.00248821172863245
Batch  81  loss:  0.0019155413610860705
Batch  91  loss:  0.0018359193345531821
Batch  101  loss:  0.002386267064139247
Batch  111  loss:  0.0011186905903741717
Batch  121  loss:  0.0023897443898022175
Batch  131  loss:  0.0028280147816985846
Batch  141  loss:  0.0028524191584438086
Batch  151  loss:  0.005697657819837332
Batch  161  loss:  0.0023657854180783033
Batch  171  loss:  0.0022548732813447714
Batch  181  loss:  0.0025137625634670258
Batch  191  loss:  0.001817187643609941
Validation on real data: 
LOSS supervised-train 0.002277319911227096, valid 0.0017034264747053385
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.001504035317339003
Batch  11  loss:  0.0012699234066531062
Batch  21  loss:  0.0024848433677107096
Batch  31  loss:  0.003681114176288247
Batch  41  loss:  0.0019611141178756952
Batch  51  loss:  0.0017179602291435003
Batch  61  loss:  0.0010333003010600805
Batch  71  loss:  0.0012073703110218048
Batch  81  loss:  0.0015553268603980541
Batch  91  loss:  0.001191099057905376
Batch  101  loss:  0.0015201802598312497
Batch  111  loss:  0.00106038770172745
Batch  121  loss:  0.0021639910992234945
Batch  131  loss:  0.0024476032704114914
Batch  141  loss:  0.0024994495324790478
Batch  151  loss:  0.004749048035591841
Batch  161  loss:  0.0018493279349058867
Batch  171  loss:  0.0021248303819447756
Batch  181  loss:  0.0019664873834699392
Batch  191  loss:  0.0012471135705709457
Validation on real data: 
LOSS supervised-train 0.0017897449946030974, valid 0.00119810551404953
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0010867243399843574
Batch  11  loss:  0.0011267766822129488
Batch  21  loss:  0.0024508333299309015
Batch  31  loss:  0.003035434987396002
Batch  41  loss:  0.001355126965790987
Batch  51  loss:  0.0013689725892618299
Batch  61  loss:  0.0010194400092586875
Batch  71  loss:  0.0014636459527537227
Batch  81  loss:  0.0012925538467243314
Batch  91  loss:  0.001127566210925579
Batch  101  loss:  0.001420490094460547
Batch  111  loss:  0.000723704812116921
Batch  121  loss:  0.0015318153891712427
Batch  131  loss:  0.00233104033395648
Batch  141  loss:  0.0022216569632291794
Batch  151  loss:  0.004020342603325844
Batch  161  loss:  0.0022819912992417812
Batch  171  loss:  0.0015219023916870356
Batch  181  loss:  0.0016640063840895891
Batch  191  loss:  0.0012937444262206554
Validation on real data: 
LOSS supervised-train 0.0015533640322973952, valid 0.0012746171560138464
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0009933656547218561
Batch  11  loss:  0.0009438925189897418
Batch  21  loss:  0.0016187612200155854
Batch  31  loss:  0.002289558295160532
Batch  41  loss:  0.0011907895095646381
Batch  51  loss:  0.0010712940711528063
Batch  61  loss:  0.0008497993694618344
Batch  71  loss:  0.0010114259785041213
Batch  81  loss:  0.0012796490918844938
Batch  91  loss:  0.0009194709127768874
Batch  101  loss:  0.0012485269689932466
Batch  111  loss:  0.0007279902347363532
Batch  121  loss:  0.0016410595271736383
Batch  131  loss:  0.0016024517826735973
Batch  141  loss:  0.0017610922222957015
Batch  151  loss:  0.003568291664123535
Batch  161  loss:  0.0026089688763022423
Batch  171  loss:  0.0014593375381082296
Batch  181  loss:  0.0011633082758635283
Batch  191  loss:  0.0009727797587402165
Validation on real data: 
LOSS supervised-train 0.0013259124412434175, valid 0.0009942856850102544
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.001164770219475031
Batch  11  loss:  0.0008929130272008479
Batch  21  loss:  0.00175865413621068
Batch  31  loss:  0.002095562405884266
Batch  41  loss:  0.0012156992452219129
Batch  51  loss:  0.0009108075173571706
Batch  61  loss:  0.0007055644528008997
Batch  71  loss:  0.001226192805916071
Batch  81  loss:  0.0011725958902388811
Batch  91  loss:  0.0009292458998970687
Batch  101  loss:  0.001295042922720313
Batch  111  loss:  0.0005970873753540218
Batch  121  loss:  0.0010726004838943481
Batch  131  loss:  0.0012338216183707118
Batch  141  loss:  0.001551183988340199
Batch  151  loss:  0.0034363737795501947
Batch  161  loss:  0.0017878804355859756
Batch  171  loss:  0.0011913594789803028
Batch  181  loss:  0.0010229932377114892
Batch  191  loss:  0.0009572661947458982
Validation on real data: 
LOSS supervised-train 0.0011841608010581695, valid 0.0006592552526853979
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.000929377565626055
Batch  11  loss:  0.0008561369613744318
Batch  21  loss:  0.0014867127174511552
Batch  31  loss:  0.0019673204515129328
Batch  41  loss:  0.0011656155111268163
Batch  51  loss:  0.0009301324025727808
Batch  61  loss:  0.0007338243303820491
Batch  71  loss:  0.0011989056365564466
Batch  81  loss:  0.0012530412059277296
Batch  91  loss:  0.0007413221173919737
Batch  101  loss:  0.0009140126057900488
Batch  111  loss:  0.0005558097036555409
Batch  121  loss:  0.0012073616962879896
Batch  131  loss:  0.0015221629291772842
Batch  141  loss:  0.0015101962490007281
Batch  151  loss:  0.0022879873868077993
Batch  161  loss:  0.0018117473227903247
Batch  171  loss:  0.0010670773917809129
Batch  181  loss:  0.0009471900411881506
Batch  191  loss:  0.001024879515171051
Validation on real data: 
LOSS supervised-train 0.0010513847120455467, valid 0.0007988941506482661
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0009773928904905915
Batch  11  loss:  0.0007771477685309947
Batch  21  loss:  0.001271533314138651
Batch  31  loss:  0.0018018560949712992
Batch  41  loss:  0.001088429125957191
Batch  51  loss:  0.0007771647069603205
Batch  61  loss:  0.0006984904175624251
Batch  71  loss:  0.0008917885716073215
Batch  81  loss:  0.0011525690788403153
Batch  91  loss:  0.0007650413317605853
Batch  101  loss:  0.0006821127026341856
Batch  111  loss:  0.0006383761647157371
Batch  121  loss:  0.0013564203400164843
Batch  131  loss:  0.0013838732847943902
Batch  141  loss:  0.0011821385705843568
Batch  151  loss:  0.0026196157559752464
Batch  161  loss:  0.0015276712365448475
Batch  171  loss:  0.0009648078703321517
Batch  181  loss:  0.0008685714565217495
Batch  191  loss:  0.0008690149406902492
Validation on real data: 
LOSS supervised-train 0.0009664614862413145, valid 0.0006592830177396536
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0010586788412183523
Batch  11  loss:  0.000590206531342119
Batch  21  loss:  0.001194671494886279
Batch  31  loss:  0.001570841297507286
Batch  41  loss:  0.000806972966529429
Batch  51  loss:  0.0006435759132727981
Batch  61  loss:  0.0008518398972228169
Batch  71  loss:  0.0007898164913058281
Batch  81  loss:  0.0011789301643148065
Batch  91  loss:  0.0007239683764055371
Batch  101  loss:  0.0008732671267352998
Batch  111  loss:  0.00045444126590155065
Batch  121  loss:  0.0009392958600074053
Batch  131  loss:  0.0009810602059587836
Batch  141  loss:  0.0009250902803614736
Batch  151  loss:  0.0019237297819927335
Batch  161  loss:  0.0016264203004539013
Batch  171  loss:  0.0007906359969638288
Batch  181  loss:  0.0006852660444565117
Batch  191  loss:  0.0009969583479687572
Validation on real data: 
LOSS supervised-train 0.0009147349890554323, valid 0.0007119158399291337
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0007932883454486728
Batch  11  loss:  0.0006919032311998308
Batch  21  loss:  0.001105712610296905
Batch  31  loss:  0.0014153262600302696
Batch  41  loss:  0.0008271586848422885
Batch  51  loss:  0.0005792875308543444
Batch  61  loss:  0.0006350345211103559
Batch  71  loss:  0.0007036969182081521
Batch  81  loss:  0.000834603444673121
Batch  91  loss:  0.0007462457870133221
Batch  101  loss:  0.0008884841809049249
Batch  111  loss:  0.00047003134386613965
Batch  121  loss:  0.0006844670861028135
Batch  131  loss:  0.0007943651871755719
Batch  141  loss:  0.0009658726630732417
Batch  151  loss:  0.0016649080207571387
Batch  161  loss:  0.0013156248023733497
Batch  171  loss:  0.0007287879125215113
Batch  181  loss:  0.0007378560258075595
Batch  191  loss:  0.0007005557999946177
Validation on real data: 
LOSS supervised-train 0.0008427661459427327, valid 0.0006277250358834863
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0007967106648720801
Batch  11  loss:  0.0006465245387516916
Batch  21  loss:  0.00111819535959512
Batch  31  loss:  0.0014662167523056269
Batch  41  loss:  0.0009151170961558819
Batch  51  loss:  0.0006554506835527718
Batch  61  loss:  0.0006686585256829858
Batch  71  loss:  0.0007005799561738968
Batch  81  loss:  0.0008980664424598217
Batch  91  loss:  0.0006130910478532314
Batch  101  loss:  0.0006388829788193107
Batch  111  loss:  0.0005994358798488975
Batch  121  loss:  0.0011746127856895328
Batch  131  loss:  0.0009090341627597809
Batch  141  loss:  0.0006941720494069159
Batch  151  loss:  0.0018890632782131433
Batch  161  loss:  0.0012937167193740606
Batch  171  loss:  0.0006202957592904568
Batch  181  loss:  0.0006289980374276638
Batch  191  loss:  0.0006868205964565277
Validation on real data: 
LOSS supervised-train 0.0007953465871105436, valid 0.0006707553984597325
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0006096744909882545
Batch  11  loss:  0.0006194534944370389
Batch  21  loss:  0.0010201939148828387
Batch  31  loss:  0.0011414031032472849
Batch  41  loss:  0.0008630598313175142
Batch  51  loss:  0.0007430752739310265
Batch  61  loss:  0.00047403297503478825
Batch  71  loss:  0.0006629120907746255
Batch  81  loss:  0.0008599527063779533
Batch  91  loss:  0.0007237994577735662
Batch  101  loss:  0.0007380558527074754
Batch  111  loss:  0.0005012975889258087
Batch  121  loss:  0.0007583957631140947
Batch  131  loss:  0.0009669197024777532
Batch  141  loss:  0.0009820668492466211
Batch  151  loss:  0.0019345111213624477
Batch  161  loss:  0.0011643200414255261
Batch  171  loss:  0.0007940441719256341
Batch  181  loss:  0.0007155187777243555
Batch  191  loss:  0.000768698868341744
Validation on real data: 
LOSS supervised-train 0.0007692529550695326, valid 0.0008833447354845703
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.000733031309209764
Batch  11  loss:  0.0006453683599829674
Batch  21  loss:  0.0009491096716374159
Batch  31  loss:  0.0011705516371876001
Batch  41  loss:  0.0009162379428744316
Batch  51  loss:  0.0005079510156065226
Batch  61  loss:  0.0005393582978285849
Batch  71  loss:  0.00059660361148417
Batch  81  loss:  0.000747477519325912
Batch  91  loss:  0.0006904241163283587
Batch  101  loss:  0.000646477856207639
Batch  111  loss:  0.00038279659929685295
Batch  121  loss:  0.0008754685404710472
Batch  131  loss:  0.0008192001841962337
Batch  141  loss:  0.000910385453607887
Batch  151  loss:  0.0021538990549743176
Batch  161  loss:  0.0012430887436494231
Batch  171  loss:  0.0007228440372273326
Batch  181  loss:  0.0006716765346936882
Batch  191  loss:  0.0006049831863492727
Validation on real data: 
LOSS supervised-train 0.0007505156546540093, valid 0.001008259947411716
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0006711503956466913
Batch  11  loss:  0.0005687098600901663
Batch  21  loss:  0.0010426297085359693
Batch  31  loss:  0.0011895464267581701
Batch  41  loss:  0.0008104764274321496
Batch  51  loss:  0.0004804575291927904
Batch  61  loss:  0.0004003085778094828
Batch  71  loss:  0.0006279314402490854
Batch  81  loss:  0.0008056461811065674
Batch  91  loss:  0.0006511043757200241
Batch  101  loss:  0.0008717762539163232
Batch  111  loss:  0.0004787998041138053
Batch  121  loss:  0.0007760130683891475
Batch  131  loss:  0.000726947677321732
Batch  141  loss:  0.0008446496794931591
Batch  151  loss:  0.0015604415675625205
Batch  161  loss:  0.000886484223883599
Batch  171  loss:  0.0007250622729770839
Batch  181  loss:  0.000510194746311754
Batch  191  loss:  0.0005387973506003618
Validation on real data: 
LOSS supervised-train 0.0006836812327674124, valid 0.0006846360629424453
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.000615927332546562
Batch  11  loss:  0.0004944602260366082
Batch  21  loss:  0.0008507236489094794
Batch  31  loss:  0.0011623732279986143
Batch  41  loss:  0.0007467953837476671
Batch  51  loss:  0.0004933102754876018
Batch  61  loss:  0.0005044964491389692
Batch  71  loss:  0.0005940276314504445
Batch  81  loss:  0.0007506076944991946
Batch  91  loss:  0.000555130245629698
Batch  101  loss:  0.0006569658289663494
Batch  111  loss:  0.00046567112440243363
Batch  121  loss:  0.0008338972693309188
Batch  131  loss:  0.0008235914865508676
Batch  141  loss:  0.0007589103188365698
Batch  151  loss:  0.0010992586612701416
Batch  161  loss:  0.000786412856541574
Batch  171  loss:  0.0006427306798286736
Batch  181  loss:  0.0006451158551499248
Batch  191  loss:  0.0006194162997417152
Validation on real data: 
LOSS supervised-train 0.0006537950833444484, valid 0.0004302268789615482
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0006406864849850535
Batch  11  loss:  0.0006609064876101911
Batch  21  loss:  0.0007891309214755893
Batch  31  loss:  0.0010732825612649322
Batch  41  loss:  0.0007091534789651632
Batch  51  loss:  0.0004321036103647202
Batch  61  loss:  0.0004195427754893899
Batch  71  loss:  0.0006405754829756916
Batch  81  loss:  0.0007491679862141609
Batch  91  loss:  0.0005158743006177247
Batch  101  loss:  0.0005243006162345409
Batch  111  loss:  0.0004346476634964347
Batch  121  loss:  0.0007148910081014037
Batch  131  loss:  0.0008580845315009356
Batch  141  loss:  0.0006566945812664926
Batch  151  loss:  0.0012131232069805264
Batch  161  loss:  0.0012430805945768952
Batch  171  loss:  0.0009603273938409984
Batch  181  loss:  0.0005962210707366467
Batch  191  loss:  0.0005619528237730265
Validation on real data: 
LOSS supervised-train 0.0006330329859338235, valid 0.0005041704280301929
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0005568148335441947
Batch  11  loss:  0.00046110342373140156
Batch  21  loss:  0.0007994641782715917
Batch  31  loss:  0.0009704805561341345
Batch  41  loss:  0.0007496419129893184
Batch  51  loss:  0.00042842922266572714
Batch  61  loss:  0.0004758455615956336
Batch  71  loss:  0.0006769473548047245
Batch  81  loss:  0.0006384914740920067
Batch  91  loss:  0.0004828156088478863
Batch  101  loss:  0.0006748649175278842
Batch  111  loss:  0.0003913623804692179
Batch  121  loss:  0.000570238393265754
Batch  131  loss:  0.0007525041000917554
Batch  141  loss:  0.0005215730634517968
Batch  151  loss:  0.001321111572906375
Batch  161  loss:  0.0006910441443324089
Batch  171  loss:  0.0005652665859088302
Batch  181  loss:  0.000490736507344991
Batch  191  loss:  0.0005005486309528351
Validation on real data: 
LOSS supervised-train 0.0006051961187040434, valid 0.0003374904044903815
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.000547508941963315
Batch  11  loss:  0.0004396034637466073
Batch  21  loss:  0.0008449007873423398
Batch  31  loss:  0.0009336789953522384
Batch  41  loss:  0.0006816299282945693
Batch  51  loss:  0.0004592776531353593
Batch  61  loss:  0.0005754823214374483
Batch  71  loss:  0.0005518291727639735
Batch  81  loss:  0.0006751168984919786
Batch  91  loss:  0.00047677531256340444
Batch  101  loss:  0.000678776646964252
Batch  111  loss:  0.0004156672512181103
Batch  121  loss:  0.0005426148418337107
Batch  131  loss:  0.0006720599485561252
Batch  141  loss:  0.0005507299792952836
Batch  151  loss:  0.0013296910328790545
Batch  161  loss:  0.0006058087456040084
Batch  171  loss:  0.0005726665258407593
Batch  181  loss:  0.00045146787306293845
Batch  191  loss:  0.0004773630353156477
Validation on real data: 
LOSS supervised-train 0.0005783105715818237, valid 0.00040010351222008467
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0006021022563800216
Batch  11  loss:  0.000442648510215804
Batch  21  loss:  0.0007714738603681326
Batch  31  loss:  0.0009311464382335544
Batch  41  loss:  0.0006722326506860554
Batch  51  loss:  0.0004706034669652581
Batch  61  loss:  0.0003563325444702059
Batch  71  loss:  0.000511451973579824
Batch  81  loss:  0.0005546220927499235
Batch  91  loss:  0.0004710010252892971
Batch  101  loss:  0.0005033473717048764
Batch  111  loss:  0.0003909233200829476
Batch  121  loss:  0.0005680139875039458
Batch  131  loss:  0.0006878845160827041
Batch  141  loss:  0.0005304655060172081
Batch  151  loss:  0.0014947477029636502
Batch  161  loss:  0.0006959760212339461
Batch  171  loss:  0.0006196933100000024
Batch  181  loss:  0.0004667324828915298
Batch  191  loss:  0.00041161812259815633
Validation on real data: 
LOSS supervised-train 0.0005570677899231669, valid 0.0005379659123718739
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0006374725489877164
Batch  11  loss:  0.00036930188070982695
Batch  21  loss:  0.0007564261322841048
Batch  31  loss:  0.0008680330938659608
Batch  41  loss:  0.0006134369177743793
Batch  51  loss:  0.00039857084630057216
Batch  61  loss:  0.00040985326631926
Batch  71  loss:  0.0004929513088427484
Batch  81  loss:  0.0005317737231962383
Batch  91  loss:  0.000551973411347717
Batch  101  loss:  0.0004389244131743908
Batch  111  loss:  0.00037399327266030014
Batch  121  loss:  0.0007351679960265756
Batch  131  loss:  0.0006962440675124526
Batch  141  loss:  0.0005028719897381961
Batch  151  loss:  0.001249672961421311
Batch  161  loss:  0.000573923229239881
Batch  171  loss:  0.0004942587111145258
Batch  181  loss:  0.0005129437195137143
Batch  191  loss:  0.00046292852493934333
Validation on real data: 
LOSS supervised-train 0.0005470059000072069, valid 0.0006387330940924585
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0006527981604449451
Batch  11  loss:  0.0005133812082931399
Batch  21  loss:  0.0006875941180624068
Batch  31  loss:  0.0007995298365131021
Batch  41  loss:  0.0005179293802939355
Batch  51  loss:  0.00039719894994050264
Batch  61  loss:  0.0004255979729350656
Batch  71  loss:  0.0005285055958665907
Batch  81  loss:  0.0007532520685344934
Batch  91  loss:  0.0004447719838935882
Batch  101  loss:  0.0005871385801583529
Batch  111  loss:  0.0003529180830810219
Batch  121  loss:  0.0004883526125922799
Batch  131  loss:  0.0005620979936793447
Batch  141  loss:  0.000549279386177659
Batch  151  loss:  0.0007654540822841227
Batch  161  loss:  0.0008108536712825298
Batch  171  loss:  0.00043942712363786995
Batch  181  loss:  0.0004707537591457367
Batch  191  loss:  0.00045198193402029574
Validation on real data: 
LOSS supervised-train 0.000522821693593869, valid 0.0006037263083271682
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0006513867992907763
Batch  11  loss:  0.00042134668910875916
Batch  21  loss:  0.000593588047195226
Batch  31  loss:  0.0007701059803366661
Batch  41  loss:  0.0005379864596761763
Batch  51  loss:  0.0002903271815739572
Batch  61  loss:  0.000341729522915557
Batch  71  loss:  0.0004609034222085029
Batch  81  loss:  0.0006692425813525915
Batch  91  loss:  0.0003871787339448929
Batch  101  loss:  0.00042392799514345825
Batch  111  loss:  0.00030615931609645486
Batch  121  loss:  0.00047662685392424464
Batch  131  loss:  0.000637963239569217
Batch  141  loss:  0.00041348033118993044
Batch  151  loss:  0.0008660626481287181
Batch  161  loss:  0.0006821956485509872
Batch  171  loss:  0.0004024422960355878
Batch  181  loss:  0.0004106365086045116
Batch  191  loss:  0.0003676725900731981
Validation on real data: 
LOSS supervised-train 0.0004954031930537895, valid 0.00034974655136466026
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0005127356271259487
Batch  11  loss:  0.0003529967798385769
Batch  21  loss:  0.0005850637098774314
Batch  31  loss:  0.0008600142900831997
Batch  41  loss:  0.00044311038800515234
Batch  51  loss:  0.0002738141920417547
Batch  61  loss:  0.0004057174955960363
Batch  71  loss:  0.0004653925425373018
Batch  81  loss:  0.00066773482831195
Batch  91  loss:  0.0003661501395981759
Batch  101  loss:  0.0004956365446560085
Batch  111  loss:  0.0002520146081224084
Batch  121  loss:  0.0005388258723542094
Batch  131  loss:  0.0006043537869118154
Batch  141  loss:  0.0005149271455593407
Batch  151  loss:  0.0010702883591875434
Batch  161  loss:  0.0006451004883274436
Batch  171  loss:  0.00040356494719162583
Batch  181  loss:  0.0004043061926495284
Batch  191  loss:  0.00040542802889831364
Validation on real data: 
LOSS supervised-train 0.0004896008672949392, valid 0.00041347977821715176
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.00046486291103065014
Batch  11  loss:  0.00045907910680398345
Batch  21  loss:  0.0005572321242652833
Batch  31  loss:  0.0010252121137455106
Batch  41  loss:  0.0006141236517578363
Batch  51  loss:  0.0004093815223313868
Batch  61  loss:  0.00047941270167939365
Batch  71  loss:  0.0004025305388495326
Batch  81  loss:  0.0006227311096154153
Batch  91  loss:  0.0005173702375032008
Batch  101  loss:  0.0004765491758007556
Batch  111  loss:  0.0002637954312376678
Batch  121  loss:  0.0006838009576313198
Batch  131  loss:  0.0004999957163818181
Batch  141  loss:  0.00045073172077536583
Batch  151  loss:  0.0008692730916664004
Batch  161  loss:  0.0007311212830245495
Batch  171  loss:  0.00038960823439992964
Batch  181  loss:  0.0003992619167547673
Batch  191  loss:  0.0004032993165310472
Validation on real data: 
LOSS supervised-train 0.00048407223832327875, valid 0.00041537208016961813
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0004887873656116426
Batch  11  loss:  0.0003659139038063586
Batch  21  loss:  0.000538376159965992
Batch  31  loss:  0.0007201869739219546
Batch  41  loss:  0.0006015580147504807
Batch  51  loss:  0.0003825596650131047
Batch  61  loss:  0.0003197180340066552
Batch  71  loss:  0.00043868436478078365
Batch  81  loss:  0.0005026776343584061
Batch  91  loss:  0.00042835623025894165
Batch  101  loss:  0.0005478695384226739
Batch  111  loss:  0.0002957309770863503
Batch  121  loss:  0.0004294658428989351
Batch  131  loss:  0.0005984207382425666
Batch  141  loss:  0.00036718143383041024
Batch  151  loss:  0.000673523813020438
Batch  161  loss:  0.0011367365950718522
Batch  171  loss:  0.00042594922706484795
Batch  181  loss:  0.0003304507990833372
Batch  191  loss:  0.00040502339834347367
Validation on real data: 
LOSS supervised-train 0.0004691222518158611, valid 0.00027805165154859424
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0004902102518826723
Batch  11  loss:  0.00036438481765799224
Batch  21  loss:  0.0005963582661934197
Batch  31  loss:  0.0006296949577517807
Batch  41  loss:  0.000424910249421373
Batch  51  loss:  0.0002428946172585711
Batch  61  loss:  0.0004991893074475229
Batch  71  loss:  0.0005193501128815114
Batch  81  loss:  0.0005316564929671586
Batch  91  loss:  0.000408837862778455
Batch  101  loss:  0.0005290409899316728
Batch  111  loss:  0.00029292848194018006
Batch  121  loss:  0.0005436589126475155
Batch  131  loss:  0.00043053406989201903
Batch  141  loss:  0.00041225546738132834
Batch  151  loss:  0.0009794844081625342
Batch  161  loss:  0.0005794399185106158
Batch  171  loss:  0.000349606154486537
Batch  181  loss:  0.0004081235674675554
Batch  191  loss:  0.0003478907747194171
Validation on real data: 
LOSS supervised-train 0.00045637160052137916, valid 0.00046302820555865765
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0005472995107993484
Batch  11  loss:  0.00039591945824213326
Batch  21  loss:  0.0004845313960686326
Batch  31  loss:  0.0006777405505999923
Batch  41  loss:  0.0004482415970414877
Batch  51  loss:  0.0003740673419088125
Batch  61  loss:  0.0004293007659725845
Batch  71  loss:  0.0003803712024819106
Batch  81  loss:  0.0006393094081431627
Batch  91  loss:  0.000365015264833346
Batch  101  loss:  0.0005480209365487099
Batch  111  loss:  0.0002492328349035233
Batch  121  loss:  0.00035282864701002836
Batch  131  loss:  0.00045960169518366456
Batch  141  loss:  0.00034596477053128183
Batch  151  loss:  0.0005651971441693604
Batch  161  loss:  0.00046614365419372916
Batch  171  loss:  0.00034772834624163806
Batch  181  loss:  0.0003984526847489178
Batch  191  loss:  0.0003472096286714077
Validation on real data: 
LOSS supervised-train 0.00042943367072439286, valid 0.00032321689650416374
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0005250948597677052
Batch  11  loss:  0.0004513881867751479
Batch  21  loss:  0.0005624627810902894
Batch  31  loss:  0.0006261734524741769
Batch  41  loss:  0.0003827455220744014
Batch  51  loss:  0.00031316850800067186
Batch  61  loss:  0.00038793322164565325
Batch  71  loss:  0.000530289311427623
Batch  81  loss:  0.0005118893459439278
Batch  91  loss:  0.00033029206679202616
Batch  101  loss:  0.00046422527520917356
Batch  111  loss:  0.00024834327632561326
Batch  121  loss:  0.00048478617100045085
Batch  131  loss:  0.0004892038996331394
Batch  141  loss:  0.00039316824404522777
Batch  151  loss:  0.0008929800824262202
Batch  161  loss:  0.0006651879521086812
Batch  171  loss:  0.0003828864137176424
Batch  181  loss:  0.0002902635606005788
Batch  191  loss:  0.00039039424154907465
Validation on real data: 
LOSS supervised-train 0.0004214448142738547, valid 0.0005972487269900739
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00048744073137640953
Batch  11  loss:  0.00040376291144639254
Batch  21  loss:  0.0004983802791684866
Batch  31  loss:  0.0004897367325611413
Batch  41  loss:  0.0004672326904255897
Batch  51  loss:  0.00032553699566051364
Batch  61  loss:  0.0003909392689820379
Batch  71  loss:  0.0004892719443887472
Batch  81  loss:  0.0004987024003639817
Batch  91  loss:  0.00036974894464947283
Batch  101  loss:  0.00040185131365433335
Batch  111  loss:  0.000306985602946952
Batch  121  loss:  0.00043797766556963325
Batch  131  loss:  0.0004149906162638217
Batch  141  loss:  0.0003424007445573807
Batch  151  loss:  0.0008062862907536328
Batch  161  loss:  0.0005666264914907515
Batch  171  loss:  0.0003157580504193902
Batch  181  loss:  0.0003625360841397196
Batch  191  loss:  0.0002827087591867894
Validation on real data: 
LOSS supervised-train 0.0004197738973016385, valid 0.00035178716643713415
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0004889447591267526
Batch  11  loss:  0.0003201807849109173
Batch  21  loss:  0.0004167346633039415
Batch  31  loss:  0.0004475701425690204
Batch  41  loss:  0.0003350289771333337
Batch  51  loss:  0.00028669877792708576
Batch  61  loss:  0.00036750538856722414
Batch  71  loss:  0.00038300114101730287
Batch  81  loss:  0.0004589822783600539
Batch  91  loss:  0.00036295305471867323
Batch  101  loss:  0.00047993226326070726
Batch  111  loss:  0.00028123531956225634
Batch  121  loss:  0.00043977631139568985
Batch  131  loss:  0.0005602375022135675
Batch  141  loss:  0.0003377573157195002
Batch  151  loss:  0.0007934710010886192
Batch  161  loss:  0.0005399223300628364
Batch  171  loss:  0.00038267593481577933
Batch  181  loss:  0.00033078319393098354
Batch  191  loss:  0.0003149422991555184
Validation on real data: 
LOSS supervised-train 0.00040540225934819317, valid 0.00037012211396358907
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0005033505731262267
Batch  11  loss:  0.0003431797958910465
Batch  21  loss:  0.0005161832086741924
Batch  31  loss:  0.0005664858617819846
Batch  41  loss:  0.00040027411887422204
Batch  51  loss:  0.0003010352957062423
Batch  61  loss:  0.0003862812591250986
Batch  71  loss:  0.0004401425539981574
Batch  81  loss:  0.0005798662314191461
Batch  91  loss:  0.00038469501305371523
Batch  101  loss:  0.00046923226909711957
Batch  111  loss:  0.0002417374780634418
Batch  121  loss:  0.0003862963931169361
Batch  131  loss:  0.0003680500667542219
Batch  141  loss:  0.0004395736032165587
Batch  151  loss:  0.0006272582686506212
Batch  161  loss:  0.00044564902782440186
Batch  171  loss:  0.00022194083430804312
Batch  181  loss:  0.0003685058618430048
Batch  191  loss:  0.00036651320988312364
Validation on real data: 
LOSS supervised-train 0.0004055779714690289, valid 0.0003304407582618296
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0005305386730469763
Batch  11  loss:  0.00038963014958426356
Batch  21  loss:  0.0004994163755327463
Batch  31  loss:  0.0004892611177638173
Batch  41  loss:  0.0003603543736971915
Batch  51  loss:  0.0003070307138841599
Batch  61  loss:  0.00024103547912091017
Batch  71  loss:  0.0004670596681535244
Batch  81  loss:  0.00034385710023343563
Batch  91  loss:  0.00038916844641789794
Batch  101  loss:  0.00040536432061344385
Batch  111  loss:  0.00027050316566601396
Batch  121  loss:  0.0004821160400751978
Batch  131  loss:  0.0004226502787787467
Batch  141  loss:  0.0003566497762221843
Batch  151  loss:  0.0007093566237017512
Batch  161  loss:  0.00047042075311765075
Batch  171  loss:  0.00036259079934097826
Batch  181  loss:  0.000298478378681466
Batch  191  loss:  0.00023829481506254524
Validation on real data: 
LOSS supervised-train 0.0003919665401917882, valid 0.00033080606954172254
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0005051062908023596
Batch  11  loss:  0.0003349015023559332
Batch  21  loss:  0.0004903730005025864
Batch  31  loss:  0.0004950645379722118
Batch  41  loss:  0.000536968931555748
Batch  51  loss:  0.0002449191524647176
Batch  61  loss:  0.00041407992830500007
Batch  71  loss:  0.000417366303736344
Batch  81  loss:  0.0004948886344209313
Batch  91  loss:  0.0003118953900411725
Batch  101  loss:  0.00041567362495698035
Batch  111  loss:  0.00022421774337999523
Batch  121  loss:  0.0003800068225245923
Batch  131  loss:  0.00046686403220519423
Batch  141  loss:  0.00035386389936320484
Batch  151  loss:  0.0008130192290991545
Batch  161  loss:  0.00038641729042865336
Batch  171  loss:  0.0003177225880790502
Batch  181  loss:  0.00033230832195840776
Batch  191  loss:  0.00029005762189626694
Validation on real data: 
LOSS supervised-train 0.00038090535177616404, valid 0.00026456586783751845
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0005308463587425649
Batch  11  loss:  0.0004069213755428791
Batch  21  loss:  0.0006416619289666414
Batch  31  loss:  0.0005012359470129013
Batch  41  loss:  0.0004256547545082867
Batch  51  loss:  0.000297502992907539
Batch  61  loss:  0.0002710043336264789
Batch  71  loss:  0.0003440254076849669
Batch  81  loss:  0.00041537178913131356
Batch  91  loss:  0.00031863650656305254
Batch  101  loss:  0.00039812392788007855
Batch  111  loss:  0.0002832852187566459
Batch  121  loss:  0.0003524317580740899
Batch  131  loss:  0.00033928340417332947
Batch  141  loss:  0.00032327629742212594
Batch  151  loss:  0.000948027940467
Batch  161  loss:  0.00039793772157281637
Batch  171  loss:  0.0003159471962135285
Batch  181  loss:  0.00035032795858569443
Batch  191  loss:  0.00029474205803126097
Validation on real data: 
LOSS supervised-train 0.0003775375441182405, valid 0.0002335972385481
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0004479764320421964
Batch  11  loss:  0.00031871124519966543
Batch  21  loss:  0.0004968060529790819
Batch  31  loss:  0.0004864155489485711
Batch  41  loss:  0.00040192363667301834
Batch  51  loss:  0.00034679865348152816
Batch  61  loss:  0.000303166190860793
Batch  71  loss:  0.0003831695066764951
Batch  81  loss:  0.000494428735692054
Batch  91  loss:  0.0003247969434596598
Batch  101  loss:  0.0005376414046622813
Batch  111  loss:  0.00024035673413891345
Batch  121  loss:  0.00045880136894993484
Batch  131  loss:  0.0003977184242103249
Batch  141  loss:  0.0003003606398124248
Batch  151  loss:  0.000573713390622288
Batch  161  loss:  0.0003630049468483776
Batch  171  loss:  0.0003058651345781982
Batch  181  loss:  0.0003456701524555683
Batch  191  loss:  0.0003233853785786778
Validation on real data: 
LOSS supervised-train 0.00036879254817904437, valid 0.0002183753822464496
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00045677495654672384
Batch  11  loss:  0.0002954134833998978
Batch  21  loss:  0.0004729359061457217
Batch  31  loss:  0.00036332887248136103
Batch  41  loss:  0.00042923365253955126
Batch  51  loss:  0.000291023519821465
Batch  61  loss:  0.0003372312930878252
Batch  71  loss:  0.00041802614578045905
Batch  81  loss:  0.0004402413032948971
Batch  91  loss:  0.00037941630580462515
Batch  101  loss:  0.0005088267498649657
Batch  111  loss:  0.000254915765253827
Batch  121  loss:  0.0004123426624573767
Batch  131  loss:  0.00035340432077646255
Batch  141  loss:  0.0003295947681181133
Batch  151  loss:  0.0007155684288591146
Batch  161  loss:  0.0005505446461029351
Batch  171  loss:  0.00036193703999742866
Batch  181  loss:  0.0002631632669363171
Batch  191  loss:  0.00024981031310744584
Validation on real data: 
LOSS supervised-train 0.0003711951109289657, valid 0.00032468163408339024
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0004751996311824769
Batch  11  loss:  0.000327015615766868
Batch  21  loss:  0.00041268140194006264
Batch  31  loss:  0.00041296682320535183
Batch  41  loss:  0.0003569427935872227
Batch  51  loss:  0.00029521985561586916
Batch  61  loss:  0.00033081223955377936
Batch  71  loss:  0.0004334693949203938
Batch  81  loss:  0.0004371740506030619
Batch  91  loss:  0.0003052670799661428
Batch  101  loss:  0.0004370150272734463
Batch  111  loss:  0.0002764393575489521
Batch  121  loss:  0.0003604287630878389
Batch  131  loss:  0.00042073734221048653
Batch  141  loss:  0.0003664112009573728
Batch  151  loss:  0.0006885401089675725
Batch  161  loss:  0.0004359824233688414
Batch  171  loss:  0.0003555304720066488
Batch  181  loss:  0.0003377703542355448
Batch  191  loss:  0.00030627925298176706
Validation on real data: 
LOSS supervised-train 0.0003550931623612996, valid 0.0001941004884429276
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00042109022615477443
Batch  11  loss:  0.00033049078774638474
Batch  21  loss:  0.0003826027677860111
Batch  31  loss:  0.0004390072717797011
Batch  41  loss:  0.0003398507251404226
Batch  51  loss:  0.0002454725909046829
Batch  61  loss:  0.0002620863087940961
Batch  71  loss:  0.0003447672934271395
Batch  81  loss:  0.00037084557698108256
Batch  91  loss:  0.0002916005323641002
Batch  101  loss:  0.00037675356725230813
Batch  111  loss:  0.00022890600666869432
Batch  121  loss:  0.00034581616637296975
Batch  131  loss:  0.00036830708268098533
Batch  141  loss:  0.00033908357727341354
Batch  151  loss:  0.0005573782254941761
Batch  161  loss:  0.00043298277887515724
Batch  171  loss:  0.00022859165619593114
Batch  181  loss:  0.0003747548907995224
Batch  191  loss:  0.0002678522723726928
Validation on real data: 
LOSS supervised-train 0.00034623045168700626, valid 0.0002746584650594741
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0003432236553635448
Batch  11  loss:  0.00031900484464131296
Batch  21  loss:  0.00039827919681556523
Batch  31  loss:  0.00039036167436279356
Batch  41  loss:  0.0004464103258214891
Batch  51  loss:  0.00026548432651907206
Batch  61  loss:  0.00028525659581646323
Batch  71  loss:  0.0004250597849022597
Batch  81  loss:  0.0004294002428650856
Batch  91  loss:  0.00028102166834287345
Batch  101  loss:  0.00037661087117157876
Batch  111  loss:  0.0003612946020439267
Batch  121  loss:  0.0003591693239286542
Batch  131  loss:  0.00041391898412257433
Batch  141  loss:  0.00036468569305725396
Batch  151  loss:  0.0006855590618215501
Batch  161  loss:  0.0004456265305634588
Batch  171  loss:  0.0004075162287335843
Batch  181  loss:  0.00028484081849455833
Batch  191  loss:  0.00025817591813392937
Validation on real data: 
LOSS supervised-train 0.0003563214693713235, valid 0.00031540333293378353
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00047798058949410915
Batch  11  loss:  0.0003113801940344274
Batch  21  loss:  0.0004249808844178915
Batch  31  loss:  0.0004077216435689479
Batch  41  loss:  0.0003524197672959417
Batch  51  loss:  0.0002685151412151754
Batch  61  loss:  0.0003571185516193509
Batch  71  loss:  0.0004802182666026056
Batch  81  loss:  0.000388805492548272
Batch  91  loss:  0.0003167324757669121
Batch  101  loss:  0.00032304637716151774
Batch  111  loss:  0.00024192890850827098
Batch  121  loss:  0.00034242888796143234
Batch  131  loss:  0.00040861143497750163
Batch  141  loss:  0.0003792241623159498
Batch  151  loss:  0.0006324109272100031
Batch  161  loss:  0.0003893069224432111
Batch  171  loss:  0.0002608027425594628
Batch  181  loss:  0.0003011852386407554
Batch  191  loss:  0.0002588201023172587
Validation on real data: 
LOSS supervised-train 0.00034096614705049434, valid 0.0002027390873990953
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0003786485467571765
Batch  11  loss:  0.0003713898768182844
Batch  21  loss:  0.00040698156226426363
Batch  31  loss:  0.00043698970694094896
Batch  41  loss:  0.0003376280365046114
Batch  51  loss:  0.0002106000465573743
Batch  61  loss:  0.00026122384588234127
Batch  71  loss:  0.0003740119864232838
Batch  81  loss:  0.0004053958400618285
Batch  91  loss:  0.000275636266451329
Batch  101  loss:  0.00038478433270938694
Batch  111  loss:  0.00025421942700631917
Batch  121  loss:  0.00030452400096692145
Batch  131  loss:  0.00039556692354381084
Batch  141  loss:  0.0003352553758304566
Batch  151  loss:  0.000694537884555757
Batch  161  loss:  0.0003279985685367137
Batch  171  loss:  0.00025415781419724226
Batch  181  loss:  0.0003026145277544856
Batch  191  loss:  0.00030670673004351556
Validation on real data: 
LOSS supervised-train 0.00033207546723133417, valid 0.0002596929552964866
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0004245263698976487
Batch  11  loss:  0.00027707312256097794
Batch  21  loss:  0.0004549833247438073
Batch  31  loss:  0.00036302246735431254
Batch  41  loss:  0.00028941285563632846
Batch  51  loss:  0.00028007171931676567
Batch  61  loss:  0.0002705484221223742
Batch  71  loss:  0.0003253894974477589
Batch  81  loss:  0.00040652378811500967
Batch  91  loss:  0.0002822859096340835
Batch  101  loss:  0.00036947589251212776
Batch  111  loss:  0.00028593119350261986
Batch  121  loss:  0.0003407598997000605
Batch  131  loss:  0.00036813956103287637
Batch  141  loss:  0.00028785510221496224
Batch  151  loss:  0.0004961928352713585
Batch  161  loss:  0.0003599292831495404
Batch  171  loss:  0.0002629056398291141
Batch  181  loss:  0.00024368839513044804
Batch  191  loss:  0.00020888536528218538
Validation on real data: 
LOSS supervised-train 0.0003194764898944413, valid 0.0003063182230107486
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00036743542295880616
Batch  11  loss:  0.0003130659752059728
Batch  21  loss:  0.0004030842974316329
Batch  31  loss:  0.0003742692933883518
Batch  41  loss:  0.0003441566077526659
Batch  51  loss:  0.00027031428180634975
Batch  61  loss:  0.00025653099874034524
Batch  71  loss:  0.00031836674315854907
Batch  81  loss:  0.0004080264479853213
Batch  91  loss:  0.0002301489148521796
Batch  101  loss:  0.00041808761307038367
Batch  111  loss:  0.00026489741867408156
Batch  121  loss:  0.0003245666157454252
Batch  131  loss:  0.00036714220186695457
Batch  141  loss:  0.0002667093649506569
Batch  151  loss:  0.000594012439250946
Batch  161  loss:  0.00041394075378775597
Batch  171  loss:  0.0002860507811419666
Batch  181  loss:  0.00030058599077165127
Batch  191  loss:  0.00027904834132641554
Validation on real data: 
LOSS supervised-train 0.00033023875825165304, valid 0.0002303759683854878
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00042568775825202465
Batch  11  loss:  0.00033757081837393343
Batch  21  loss:  0.00037669079029001296
Batch  31  loss:  0.00038171850610524416
Batch  41  loss:  0.0003033530374523252
Batch  51  loss:  0.00023692303511779755
Batch  61  loss:  0.00032922590617090464
Batch  71  loss:  0.00037090215482749045
Batch  81  loss:  0.00037875084672123194
Batch  91  loss:  0.00022500913473777473
Batch  101  loss:  0.00035846876562573016
Batch  111  loss:  0.00027387760928831995
Batch  121  loss:  0.0003352711792103946
Batch  131  loss:  0.00033438915852457285
Batch  141  loss:  0.0002637142897583544
Batch  151  loss:  0.0008457477670162916
Batch  161  loss:  0.0003338954411447048
Batch  171  loss:  0.0002529313787817955
Batch  181  loss:  0.00030896166572347283
Batch  191  loss:  0.0003011730732396245
Validation on real data: 
LOSS supervised-train 0.0003183846768661169, valid 0.0002845607523340732
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.00042294568265788257
Batch  11  loss:  0.000262993125943467
Batch  21  loss:  0.0003631008439697325
Batch  31  loss:  0.00033737681224010885
Batch  41  loss:  0.00033730140421539545
Batch  51  loss:  0.0002651714894454926
Batch  61  loss:  0.0002932992356363684
Batch  71  loss:  0.00036696335882879794
Batch  81  loss:  0.00032218112028203905
Batch  91  loss:  0.00026671256637200713
Batch  101  loss:  0.00038174260407686234
Batch  111  loss:  0.00027977555873803794
Batch  121  loss:  0.00035097423824481666
Batch  131  loss:  0.00043963739881291986
Batch  141  loss:  0.00032099528471007943
Batch  151  loss:  0.00056363147450611
Batch  161  loss:  0.00033431051997467875
Batch  171  loss:  0.00024726841365918517
Batch  181  loss:  0.0002825866686180234
Batch  191  loss:  0.00037272615008987486
Validation on real data: 
LOSS supervised-train 0.00031384725669340695, valid 0.0003629794518928975
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00036788007128052413
Batch  11  loss:  0.00027016823878511786
Batch  21  loss:  0.0003335037035867572
Batch  31  loss:  0.0003733461198862642
Batch  41  loss:  0.0002831783494912088
Batch  51  loss:  0.0002564631577115506
Batch  61  loss:  0.00030258955666795373
Batch  71  loss:  0.0003417590050958097
Batch  81  loss:  0.00040054006967693567
Batch  91  loss:  0.00022162000823300332
Batch  101  loss:  0.00028371665393933654
Batch  111  loss:  0.00023656974371988326
Batch  121  loss:  0.0003836146788671613
Batch  131  loss:  0.00033061980502679944
Batch  141  loss:  0.00032308936351910233
Batch  151  loss:  0.0005444127018563449
Batch  161  loss:  0.0003446017799433321
Batch  171  loss:  0.00031399374711327255
Batch  181  loss:  0.0003287243889644742
Batch  191  loss:  0.0002601522137410939
Validation on real data: 
LOSS supervised-train 0.0003119769736804301, valid 0.00031974600278772414
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.000355483905877918
Batch  11  loss:  0.000284752924926579
Batch  21  loss:  0.0003629581187851727
Batch  31  loss:  0.00033758016070351005
Batch  41  loss:  0.0002798064087983221
Batch  51  loss:  0.0002555962128099054
Batch  61  loss:  0.00035467935958877206
Batch  71  loss:  0.00035884216777049005
Batch  81  loss:  0.0003547830565366894
Batch  91  loss:  0.00028401261079125106
Batch  101  loss:  0.0003952400293201208
Batch  111  loss:  0.00026663541211746633
Batch  121  loss:  0.00040569668635725975
Batch  131  loss:  0.0003245779371354729
Batch  141  loss:  0.00029732193797826767
Batch  151  loss:  0.000747490965295583
Batch  161  loss:  0.00035379669861868024
Batch  171  loss:  0.00019352379604242742
Batch  181  loss:  0.0002767834812402725
Batch  191  loss:  0.00027234305161982775
Validation on real data: 
LOSS supervised-train 0.0003068849989358569, valid 0.0003512857365421951
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00045481816050596535
Batch  11  loss:  0.0002694777795113623
Batch  21  loss:  0.00040298703243024647
Batch  31  loss:  0.00033990381052717566
Batch  41  loss:  0.00030865712324157357
Batch  51  loss:  0.00025530156563036144
Batch  61  loss:  0.0002711800334509462
Batch  71  loss:  0.0003153739671688527
Batch  81  loss:  0.00043928271043114364
Batch  91  loss:  0.0002530087367631495
Batch  101  loss:  0.0003098852757830173
Batch  111  loss:  0.00024021152057684958
Batch  121  loss:  0.0004132304748054594
Batch  131  loss:  0.00033828517189249396
Batch  141  loss:  0.00029801588971167803
Batch  151  loss:  0.00043682067189365625
Batch  161  loss:  0.0003413820231799036
Batch  171  loss:  0.00019951665308326483
Batch  181  loss:  0.00029221276054158807
Batch  191  loss:  0.0002899868704844266
Validation on real data: 
LOSS supervised-train 0.00030185298943251836, valid 0.00024177395971491933
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00031524969381280243
Batch  11  loss:  0.0002945309679489583
Batch  21  loss:  0.00039515356183983386
Batch  31  loss:  0.00036566751077771187
Batch  41  loss:  0.0003345893928781152
Batch  51  loss:  0.0002868576266337186
Batch  61  loss:  0.00031838653376325965
Batch  71  loss:  0.00033612147672101855
Batch  81  loss:  0.0003428889613132924
Batch  91  loss:  0.00020710467651952058
Batch  101  loss:  0.0002710775297600776
Batch  111  loss:  0.00024168282106984407
Batch  121  loss:  0.0002493123756721616
Batch  131  loss:  0.0003624291275627911
Batch  141  loss:  0.00025892563280649483
Batch  151  loss:  0.00042945798486471176
Batch  161  loss:  0.00038559280801564455
Batch  171  loss:  0.0003332856867928058
Batch  181  loss:  0.00027295967447571456
Batch  191  loss:  0.000287007016595453
Validation on real data: 
LOSS supervised-train 0.0002941892509988975, valid 0.000223402283154428
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0004803702759090811
Batch  11  loss:  0.00023219829017762095
Batch  21  loss:  0.0004290480865165591
Batch  31  loss:  0.0003717919753398746
Batch  41  loss:  0.00024031865177676082
Batch  51  loss:  0.0003000056021846831
Batch  61  loss:  0.0002804700343403965
Batch  71  loss:  0.0003250661538913846
Batch  81  loss:  0.0003223232924938202
Batch  91  loss:  0.00021180078329052776
Batch  101  loss:  0.0002857577637769282
Batch  111  loss:  0.0003054737753700465
Batch  121  loss:  0.00023747920931782573
Batch  131  loss:  0.00026632557273842394
Batch  141  loss:  0.00023825895914342254
Batch  151  loss:  0.0005710124969482422
Batch  161  loss:  0.00031751912320032716
Batch  171  loss:  0.00025792623637244105
Batch  181  loss:  0.0002905071305576712
Batch  191  loss:  0.00020865430997218937
Validation on real data: 
LOSS supervised-train 0.0002917052797420183, valid 0.00018320464005228132
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00041529734153300524
Batch  11  loss:  0.00032512846519239247
Batch  21  loss:  0.00042210641549900174
Batch  31  loss:  0.00033753292518667877
Batch  41  loss:  0.0002679046301636845
Batch  51  loss:  0.0002526536991354078
Batch  61  loss:  0.0002626737696118653
Batch  71  loss:  0.0003378387482371181
Batch  81  loss:  0.0004047394322697073
Batch  91  loss:  0.00022139563225209713
Batch  101  loss:  0.0003001989680342376
Batch  111  loss:  0.0001862870849436149
Batch  121  loss:  0.000297226884867996
Batch  131  loss:  0.000325193686876446
Batch  141  loss:  0.00031710154144093394
Batch  151  loss:  0.00043738639215007424
Batch  161  loss:  0.00045197352301329374
Batch  171  loss:  0.0002692501002456993
Batch  181  loss:  0.0003227558045182377
Batch  191  loss:  0.0003597126924432814
Validation on real data: 
LOSS supervised-train 0.00029825640391209165, valid 0.00022654799977317452
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00031086005037650466
Batch  11  loss:  0.0002475689398124814
Batch  21  loss:  0.0003469540970399976
Batch  31  loss:  0.00035708051291294396
Batch  41  loss:  0.00027109135407954454
Batch  51  loss:  0.00024683846277184784
Batch  61  loss:  0.0002604628971312195
Batch  71  loss:  0.00037334865191951394
Batch  81  loss:  0.0003788029425777495
Batch  91  loss:  0.00026115734362974763
Batch  101  loss:  0.000304064858937636
Batch  111  loss:  0.0002940933045465499
Batch  121  loss:  0.0002488340833224356
Batch  131  loss:  0.0003236813354305923
Batch  141  loss:  0.0002488268364686519
Batch  151  loss:  0.0005425221170298755
Batch  161  loss:  0.0002735871821641922
Batch  171  loss:  0.0002347492700209841
Batch  181  loss:  0.000308100541587919
Batch  191  loss:  0.0002532604557927698
Validation on real data: 
LOSS supervised-train 0.0002862752384680789, valid 0.0003236362535972148
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0003843314480036497
Batch  11  loss:  0.00030420886469073594
Batch  21  loss:  0.00037960446206852794
Batch  31  loss:  0.00037567655090242624
Batch  41  loss:  0.00029305799398571253
Batch  51  loss:  0.00029249602812342346
Batch  61  loss:  0.0002692055713851005
Batch  71  loss:  0.00031447704532183707
Batch  81  loss:  0.0003416344407014549
Batch  91  loss:  0.00025187438586726785
Batch  101  loss:  0.0003463839238975197
Batch  111  loss:  0.00021976136486046016
Batch  121  loss:  0.0002651607501320541
Batch  131  loss:  0.000285878631984815
Batch  141  loss:  0.000274728488875553
Batch  151  loss:  0.0005169070209376514
Batch  161  loss:  0.00027664730441756546
Batch  171  loss:  0.00022901984630152583
Batch  181  loss:  0.0002489151374902576
Batch  191  loss:  0.0002790871949400753
Validation on real data: 
LOSS supervised-train 0.0002835777252767002, valid 0.00019315569079481065
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0003607861581258476
Batch  11  loss:  0.00026465958217158914
Batch  21  loss:  0.00035994237987324595
Batch  31  loss:  0.00026826042449101806
Batch  41  loss:  0.0002974419330712408
Batch  51  loss:  0.00021663289226125926
Batch  61  loss:  0.00023153847723733634
Batch  71  loss:  0.00031814276007935405
Batch  81  loss:  0.0004184307181276381
Batch  91  loss:  0.00022899574832990766
Batch  101  loss:  0.0003558815806172788
Batch  111  loss:  0.0002124581951647997
Batch  121  loss:  0.0002354033204028383
Batch  131  loss:  0.00031521599157713354
Batch  141  loss:  0.0002732789725996554
Batch  151  loss:  0.00036905199522152543
Batch  161  loss:  0.00033300090581178665
Batch  171  loss:  0.00024146802024915814
Batch  181  loss:  0.00028301094425842166
Batch  191  loss:  0.0002714921429287642
Validation on real data: 
LOSS supervised-train 0.0002825619930808898, valid 0.0003211208386346698
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0004034736193716526
Batch  11  loss:  0.00021956273121759295
Batch  21  loss:  0.0003785832959692925
Batch  31  loss:  0.00032935626222752035
Batch  41  loss:  0.00028681932599283755
Batch  51  loss:  0.0003043872711714357
Batch  61  loss:  0.0002534429950173944
Batch  71  loss:  0.0002804990508593619
Batch  81  loss:  0.0003936854191124439
Batch  91  loss:  0.00021785363787785172
Batch  101  loss:  0.00030065610189922154
Batch  111  loss:  0.00017983076395466924
Batch  121  loss:  0.00027816329384222627
Batch  131  loss:  0.0002353963936911896
Batch  141  loss:  0.000258483923971653
Batch  151  loss:  0.0004481719224713743
Batch  161  loss:  0.00030744302785024047
Batch  171  loss:  0.00023225165205076337
Batch  181  loss:  0.00025705579901114106
Batch  191  loss:  0.0002221779286628589
Validation on real data: 
LOSS supervised-train 0.0002791583622456528, valid 0.0002168151258956641
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00036242269561626017
Batch  11  loss:  0.00018822528363671154
Batch  21  loss:  0.00044605074799619615
Batch  31  loss:  0.0003086948418058455
Batch  41  loss:  0.0002716626040637493
Batch  51  loss:  0.0003576402668841183
Batch  61  loss:  0.0002327261317986995
Batch  71  loss:  0.00030479772249236703
Batch  81  loss:  0.000429558742325753
Batch  91  loss:  0.00023105453874450177
Batch  101  loss:  0.00033678687759675086
Batch  111  loss:  0.0002596383565105498
Batch  121  loss:  0.0002552252262830734
Batch  131  loss:  0.0002768610720522702
Batch  141  loss:  0.0003120217297691852
Batch  151  loss:  0.0006585810915566981
Batch  161  loss:  0.00022259763500187546
Batch  171  loss:  0.00023819203488528728
Batch  181  loss:  0.0002715700538828969
Batch  191  loss:  0.0002439542004140094
Validation on real data: 
LOSS supervised-train 0.0002760674360615667, valid 0.0002771356957964599
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.000357890035957098
Batch  11  loss:  0.00021171983098611236
Batch  21  loss:  0.0003186753310728818
Batch  31  loss:  0.00032865445245988667
Batch  41  loss:  0.0002762200019787997
Batch  51  loss:  0.00024700656649656594
Batch  61  loss:  0.0002335316239623353
Batch  71  loss:  0.00035575503716245294
Batch  81  loss:  0.00045942040742374957
Batch  91  loss:  0.00025281275156885386
Batch  101  loss:  0.00030397382215596735
Batch  111  loss:  0.00024852415663190186
Batch  121  loss:  0.0003429114003665745
Batch  131  loss:  0.0003378900873940438
Batch  141  loss:  0.00021379374084062874
Batch  151  loss:  0.0004485779209062457
Batch  161  loss:  0.00029663159511983395
Batch  171  loss:  0.0002683484635781497
Batch  181  loss:  0.00021161287440918386
Batch  191  loss:  0.0001949449215317145
Validation on real data: 
LOSS supervised-train 0.0002678571098658722, valid 0.0002340024511795491
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0003983361821155995
Batch  11  loss:  0.00026711999089457095
Batch  21  loss:  0.0003675178450066596
Batch  31  loss:  0.0003170838754158467
Batch  41  loss:  0.00021477510745171458
Batch  51  loss:  0.0003121522313449532
Batch  61  loss:  0.0003166066890116781
Batch  71  loss:  0.00023449644504580647
Batch  81  loss:  0.0004033608711324632
Batch  91  loss:  0.0001988985313801095
Batch  101  loss:  0.00027144464547745883
Batch  111  loss:  0.0002310721465619281
Batch  121  loss:  0.00021652325813192874
Batch  131  loss:  0.00031345034949481487
Batch  141  loss:  0.0003157196915708482
Batch  151  loss:  0.0005143480375409126
Batch  161  loss:  0.00040372140938416123
Batch  171  loss:  0.00027713956660591066
Batch  181  loss:  0.0002788328274618834
Batch  191  loss:  0.0003293989284429699
Validation on real data: 
LOSS supervised-train 0.0002708817092207028, valid 0.0003002960002049804
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0003132446145173162
Batch  11  loss:  0.00020101753761991858
Batch  21  loss:  0.0004366994253359735
Batch  31  loss:  0.00027012077043764293
Batch  41  loss:  0.00038407501415349543
Batch  51  loss:  0.00020585629681590945
Batch  61  loss:  0.00023641041480004787
Batch  71  loss:  0.00034190030419267714
Batch  81  loss:  0.0002919469552580267
Batch  91  loss:  0.0002069207839667797
Batch  101  loss:  0.0003300812095403671
Batch  111  loss:  0.0002305044181412086
Batch  121  loss:  0.00030238082399591804
Batch  131  loss:  0.0002806483244057745
Batch  141  loss:  0.00029764274950139225
Batch  151  loss:  0.0003532574337441474
Batch  161  loss:  0.00033038886613212526
Batch  171  loss:  0.0001783409243216738
Batch  181  loss:  0.00023682140454184264
Batch  191  loss:  0.00020136748207733035
Validation on real data: 
LOSS supervised-train 0.0002673266595957102, valid 0.0001739558792905882
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0003415230894461274
Batch  11  loss:  0.00024157795996870846
Batch  21  loss:  0.0003147876122966409
Batch  31  loss:  0.0003578117466531694
Batch  41  loss:  0.000282859691651538
Batch  51  loss:  0.00019228208111599088
Batch  61  loss:  0.0002514238585717976
Batch  71  loss:  0.00025448761880397797
Batch  81  loss:  0.00029922713292762637
Batch  91  loss:  0.00014785866369493306
Batch  101  loss:  0.00025943494983948767
Batch  111  loss:  0.000269452080829069
Batch  121  loss:  0.00026211977819912136
Batch  131  loss:  0.00025338464183732867
Batch  141  loss:  0.000272002158453688
Batch  151  loss:  0.00044795346911996603
Batch  161  loss:  0.00029017473571002483
Batch  171  loss:  0.00016426420188508928
Batch  181  loss:  0.00026572414208203554
Batch  191  loss:  0.0002348434936720878
Validation on real data: 
LOSS supervised-train 0.00025948664857423866, valid 0.00025335949612781405
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00034152038278989494
Batch  11  loss:  0.0002301607746630907
Batch  21  loss:  0.0003522206679917872
Batch  31  loss:  0.000300020124996081
Batch  41  loss:  0.0002437066286802292
Batch  51  loss:  0.0002099631237797439
Batch  61  loss:  0.00023296104336623102
Batch  71  loss:  0.00025487702805548906
Batch  81  loss:  0.0003810445487033576
Batch  91  loss:  0.0001828405074775219
Batch  101  loss:  0.0003029383369721472
Batch  111  loss:  0.0002614202967379242
Batch  121  loss:  0.0002741460921242833
Batch  131  loss:  0.0002153050882043317
Batch  141  loss:  0.00023459266230929643
Batch  151  loss:  0.00043722998816519976
Batch  161  loss:  0.00030846541631035507
Batch  171  loss:  0.00021885782189201564
Batch  181  loss:  0.000244649505475536
Batch  191  loss:  0.00024085740733426064
Validation on real data: 
LOSS supervised-train 0.0002635847563215066, valid 0.0002475363144185394
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0003182003565598279
Batch  11  loss:  0.00019407746731303632
Batch  21  loss:  0.0003888020582962781
Batch  31  loss:  0.0003127652744296938
Batch  41  loss:  0.00021435994131024927
Batch  51  loss:  0.00018530248780734837
Batch  61  loss:  0.00021325353009160608
Batch  71  loss:  0.0002622390165925026
Batch  81  loss:  0.00037033954868093133
Batch  91  loss:  0.00019585230620577931
Batch  101  loss:  0.00025929277762770653
Batch  111  loss:  0.00021869369084015489
Batch  121  loss:  0.00027734512696042657
Batch  131  loss:  0.000237084852415137
Batch  141  loss:  0.00022905541118234396
Batch  151  loss:  0.000442326272604987
Batch  161  loss:  0.0002820300869643688
Batch  171  loss:  0.00019731343491002917
Batch  181  loss:  0.00027779460651800036
Batch  191  loss:  0.00018843285215552896
Validation on real data: 
LOSS supervised-train 0.0002523196952097351, valid 0.00016036257147789001
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00028742861468344927
Batch  11  loss:  0.00023250064987223595
Batch  21  loss:  0.0003249813453294337
Batch  31  loss:  0.000282598368357867
Batch  41  loss:  0.00025143183302134275
Batch  51  loss:  0.00025587796699255705
Batch  61  loss:  0.00023513640917371958
Batch  71  loss:  0.0002492677012924105
Batch  81  loss:  0.00043238751823082566
Batch  91  loss:  0.0002634358825162053
Batch  101  loss:  0.0002977894910145551
Batch  111  loss:  0.00021106895292177796
Batch  121  loss:  0.0002381815284024924
Batch  131  loss:  0.0002644410415086895
Batch  141  loss:  0.0002604602777864784
Batch  151  loss:  0.00042035296792164445
Batch  161  loss:  0.00025315527454949915
Batch  171  loss:  0.00020899937953799963
Batch  181  loss:  0.00022078103211242706
Batch  191  loss:  0.0002789903664961457
Validation on real data: 
LOSS supervised-train 0.00026043057048809716, valid 0.00021744580590166152
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0003635484608821571
Batch  11  loss:  0.00020393688464537263
Batch  21  loss:  0.00024932590895332396
Batch  31  loss:  0.00026094733038917184
Batch  41  loss:  0.0002517958637326956
Batch  51  loss:  0.00022447148512583226
Batch  61  loss:  0.0002239070163341239
Batch  71  loss:  0.00029642373556271195
Batch  81  loss:  0.0002874586789403111
Batch  91  loss:  0.00021221696806605905
Batch  101  loss:  0.0002922426792792976
Batch  111  loss:  0.00018885632744058967
Batch  121  loss:  0.0001923970994539559
Batch  131  loss:  0.00027929869247600436
Batch  141  loss:  0.0002215703862020746
Batch  151  loss:  0.0007569707231596112
Batch  161  loss:  0.0002462754782754928
Batch  171  loss:  0.000226062853471376
Batch  181  loss:  0.00021778111113235354
Batch  191  loss:  0.00024520346778444946
Validation on real data: 
LOSS supervised-train 0.0002529832295840606, valid 0.00022113343584351242
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0003792807983700186
Batch  11  loss:  0.00020860150107182562
Batch  21  loss:  0.0003580590127967298
Batch  31  loss:  0.0002806904958561063
Batch  41  loss:  0.0002234121348010376
Batch  51  loss:  0.00023527060693595558
Batch  61  loss:  0.00022610093583352864
Batch  71  loss:  0.00030584578053094447
Batch  81  loss:  0.0003429219068493694
Batch  91  loss:  0.0002681161859072745
Batch  101  loss:  0.0002536074898671359
Batch  111  loss:  0.00024270392896141857
Batch  121  loss:  0.00022088458354119211
Batch  131  loss:  0.0003088516241405159
Batch  141  loss:  0.0002635091950651258
Batch  151  loss:  0.00037353267543949187
Batch  161  loss:  0.0002399283548584208
Batch  171  loss:  0.000188459234777838
Batch  181  loss:  0.00019403091573622078
Batch  191  loss:  0.000261673703789711
Validation on real data: 
LOSS supervised-train 0.00024755495378485647, valid 0.00018347299192100763
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00030581894679926336
Batch  11  loss:  0.00023489426530431956
Batch  21  loss:  0.000333279138430953
Batch  31  loss:  0.000323160202242434
Batch  41  loss:  0.00019777928537223488
Batch  51  loss:  0.0001993209880311042
Batch  61  loss:  0.00022276517120189965
Batch  71  loss:  0.0003303160483483225
Batch  81  loss:  0.0003589701955206692
Batch  91  loss:  0.00019029038958251476
Batch  101  loss:  0.0002922589483205229
Batch  111  loss:  0.0001974776532733813
Batch  121  loss:  0.0002776958863250911
Batch  131  loss:  0.00023038993822410703
Batch  141  loss:  0.00024154044513124973
Batch  151  loss:  0.0004003987123724073
Batch  161  loss:  0.0002320172352483496
Batch  171  loss:  0.0001821283804019913
Batch  181  loss:  0.00022467132657766342
Batch  191  loss:  0.0001932046579895541
Validation on real data: 
LOSS supervised-train 0.00025125480686256197, valid 0.00020706112263724208
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00033887478639371693
Batch  11  loss:  0.00020951186888851225
Batch  21  loss:  0.00033235110458917916
Batch  31  loss:  0.00022863711637910455
Batch  41  loss:  0.0002500695700291544
Batch  51  loss:  0.0001996420614887029
Batch  61  loss:  0.0002662474871613085
Batch  71  loss:  0.0002355652250116691
Batch  81  loss:  0.0002591949887573719
Batch  91  loss:  0.00019475325825624168
Batch  101  loss:  0.00024845212465152144
Batch  111  loss:  0.00023639603750780225
Batch  121  loss:  0.00027458625845611095
Batch  131  loss:  0.00026981550035998225
Batch  141  loss:  0.00026071909815073013
Batch  151  loss:  0.00038108541048131883
Batch  161  loss:  0.00029125463333912194
Batch  171  loss:  0.00015107265789993107
Batch  181  loss:  0.00023817097826395184
Batch  191  loss:  0.00022156178602017462
Validation on real data: 
LOSS supervised-train 0.0002446882777439896, valid 0.00021818384993821383
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0003686848212964833
Batch  11  loss:  0.00019573706958908588
Batch  21  loss:  0.00033569789957255125
Batch  31  loss:  0.0003326685691718012
Batch  41  loss:  0.00029005721444264054
Batch  51  loss:  0.00023347701062448323
Batch  61  loss:  0.0003037465794477612
Batch  71  loss:  0.00026098877424374223
Batch  81  loss:  0.0003433088422752917
Batch  91  loss:  0.00022776196419727057
Batch  101  loss:  0.00028226684662513435
Batch  111  loss:  0.000230935329454951
Batch  121  loss:  0.0002599280560389161
Batch  131  loss:  0.00025059396284632385
Batch  141  loss:  0.0002739894262049347
Batch  151  loss:  0.0004138804506510496
Batch  161  loss:  0.0002812230377458036
Batch  171  loss:  0.00021774043852929026
Batch  181  loss:  0.00019687753228936344
Batch  191  loss:  0.00022443680791184306
Validation on real data: 
LOSS supervised-train 0.000245914760089363, valid 0.00022164701658766717
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00039820405072532594
Batch  11  loss:  0.000186521778232418
Batch  21  loss:  0.0003682390961330384
Batch  31  loss:  0.0003126981027889997
Batch  41  loss:  0.0002185904886573553
Batch  51  loss:  0.00015297645586542785
Batch  61  loss:  0.0002464822318870574
Batch  71  loss:  0.00026367823011241853
Batch  81  loss:  0.00031456901342608035
Batch  91  loss:  0.00018486104090698063
Batch  101  loss:  0.0003041769959963858
Batch  111  loss:  0.00026365407393313944
Batch  121  loss:  0.00023860290821176022
Batch  131  loss:  0.0002945030282717198
Batch  141  loss:  0.00025260369875468314
Batch  151  loss:  0.00039390663732774556
Batch  161  loss:  0.0002690828696358949
Batch  171  loss:  0.00023541074187960476
Batch  181  loss:  0.00019868278468493372
Batch  191  loss:  0.00021135959832463413
Validation on real data: 
LOSS supervised-train 0.00024394533247686922, valid 0.00026484846603125334
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00026137311942875385
Batch  11  loss:  0.00023242518363986164
Batch  21  loss:  0.000332822761265561
Batch  31  loss:  0.0002398038632236421
Batch  41  loss:  0.00028184414259158075
Batch  51  loss:  0.00018698944768402725
Batch  61  loss:  0.00022330203501041979
Batch  71  loss:  0.00020473377662710845
Batch  81  loss:  0.00029494400951080024
Batch  91  loss:  0.0002223668561782688
Batch  101  loss:  0.00028397596906870604
Batch  111  loss:  0.00022981356596574187
Batch  121  loss:  0.0002072103670798242
Batch  131  loss:  0.00022695919324178249
Batch  141  loss:  0.00019897165475413203
Batch  151  loss:  0.0003800120030064136
Batch  161  loss:  0.0002537323161959648
Batch  171  loss:  0.0001678278058534488
Batch  181  loss:  0.00018758595979306847
Batch  191  loss:  0.00020118414249736816
Validation on real data: 
LOSS supervised-train 0.00024031490240304266, valid 0.00019590326701290905
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0003284433332737535
Batch  11  loss:  0.00020633245003409684
Batch  21  loss:  0.0003552135603968054
Batch  31  loss:  0.0002506398595869541
Batch  41  loss:  0.00025976254255510867
Batch  51  loss:  0.0001995113561861217
Batch  61  loss:  0.00020286327344365418
Batch  71  loss:  0.00027402627165429294
Batch  81  loss:  0.0003264984115958214
Batch  91  loss:  0.00018492460367269814
Batch  101  loss:  0.0002724149380810559
Batch  111  loss:  0.00022325744794216007
Batch  121  loss:  0.00024014487280510366
Batch  131  loss:  0.0002876642975024879
Batch  141  loss:  0.00019359584257472306
Batch  151  loss:  0.00038239965215325356
Batch  161  loss:  0.00020060903625562787
Batch  171  loss:  0.0002349996502744034
Batch  181  loss:  0.0002722595236264169
Batch  191  loss:  0.0002584589528851211
Validation on real data: 
LOSS supervised-train 0.00024224728775152472, valid 0.0001746675989124924
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.000364501029253006
Batch  11  loss:  0.0002364430547459051
Batch  21  loss:  0.0003380205889698118
Batch  31  loss:  0.0002941374550573528
Batch  41  loss:  0.00022132358571980149
Batch  51  loss:  0.00022647518198937178
Batch  61  loss:  0.00016337599663529545
Batch  71  loss:  0.000260413798969239
Batch  81  loss:  0.0003088060184381902
Batch  91  loss:  0.0001954366161953658
Batch  101  loss:  0.00029476539930328727
Batch  111  loss:  0.00020984470029361546
Batch  121  loss:  0.00024172193661797792
Batch  131  loss:  0.0002759514027275145
Batch  141  loss:  0.00020927384321112186
Batch  151  loss:  0.0005361716612242162
Batch  161  loss:  0.000229576529818587
Batch  171  loss:  0.00025387652567587793
Batch  181  loss:  0.00025721106794662774
Batch  191  loss:  0.00018665255629457533
Validation on real data: 
LOSS supervised-train 0.00023662340332521127, valid 0.0001775672717485577
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0003783778811339289
Batch  11  loss:  0.00022530811838805676
Batch  21  loss:  0.00026647280901670456
Batch  31  loss:  0.0002458204689901322
Batch  41  loss:  0.0002132331719622016
Batch  51  loss:  0.0001920164650073275
Batch  61  loss:  0.00019503093790262938
Batch  71  loss:  0.0003155764134135097
Batch  81  loss:  0.000290894036879763
Batch  91  loss:  0.00022559343778993934
Batch  101  loss:  0.00028707797173410654
Batch  111  loss:  0.00016915825835894793
Batch  121  loss:  0.0002385452826274559
Batch  131  loss:  0.0003133352438453585
Batch  141  loss:  0.00022745522437617183
Batch  151  loss:  0.0003931173705495894
Batch  161  loss:  0.00021307950373739004
Batch  171  loss:  0.0001697308907750994
Batch  181  loss:  0.00020218564895913005
Batch  191  loss:  0.00019813302787952125
Validation on real data: 
LOSS supervised-train 0.0002338945663359482, valid 0.0002564013411756605
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00025962889776565135
Batch  11  loss:  0.00019355725089553744
Batch  21  loss:  0.0003256326599512249
Batch  31  loss:  0.00028682660195045173
Batch  41  loss:  0.0002234779967693612
Batch  51  loss:  0.00017756811575964093
Batch  61  loss:  0.0002079606056213379
Batch  71  loss:  0.00024030980421230197
Batch  81  loss:  0.0003837435506284237
Batch  91  loss:  0.00021445807942654938
Batch  101  loss:  0.0002245510258944705
Batch  111  loss:  0.00023970677284523845
Batch  121  loss:  0.00020205411419738084
Batch  131  loss:  0.00023106361913960427
Batch  141  loss:  0.00022322328004520386
Batch  151  loss:  0.0003745951398741454
Batch  161  loss:  0.0002999660209752619
Batch  171  loss:  0.00021208210091572255
Batch  181  loss:  0.00018728220311459154
Batch  191  loss:  0.0002013880293816328
Validation on real data: 
LOSS supervised-train 0.0002381641224565101, valid 0.00018454375094734132
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0002665141655597836
Batch  11  loss:  0.00020400511857587844
Batch  21  loss:  0.00028759593260474503
Batch  31  loss:  0.00027371523901820183
Batch  41  loss:  0.0002799781213980168
Batch  51  loss:  0.00017435844347346574
Batch  61  loss:  0.0001923853560583666
Batch  71  loss:  0.00022589138825424016
Batch  81  loss:  0.0003155579324811697
Batch  91  loss:  0.00017768908583093435
Batch  101  loss:  0.0002548052871134132
Batch  111  loss:  0.00019824615446850657
Batch  121  loss:  0.00021504740288946778
Batch  131  loss:  0.00021604432549793273
Batch  141  loss:  0.00027480488643050194
Batch  151  loss:  0.000685848412103951
Batch  161  loss:  0.00019890342082362622
Batch  171  loss:  0.00017722524353303015
Batch  181  loss:  0.00019998954667244107
Batch  191  loss:  0.00017877999925985932
Validation on real data: 
LOSS supervised-train 0.00023149616215960124, valid 0.00019890280964318663
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0003809714107774198
Batch  11  loss:  0.00022101006470620632
Batch  21  loss:  0.0002808138378895819
Batch  31  loss:  0.0002631840470712632
Batch  41  loss:  0.00019061419880017638
Batch  51  loss:  0.00020509898604359478
Batch  61  loss:  0.00019695665105246007
Batch  71  loss:  0.0002592721430119127
Batch  81  loss:  0.00037487136432901025
Batch  91  loss:  0.0001971616584341973
Batch  101  loss:  0.00031922810012474656
Batch  111  loss:  0.0002248312084702775
Batch  121  loss:  0.00023414011229760945
Batch  131  loss:  0.0002457385417073965
Batch  141  loss:  0.00020857900381088257
Batch  151  loss:  0.0003557910386007279
Batch  161  loss:  0.0002122989681083709
Batch  171  loss:  0.00019632888142950833
Batch  181  loss:  0.00020249950466677547
Batch  191  loss:  0.0001829880493460223
Validation on real data: 
LOSS supervised-train 0.00023634855417185464, valid 0.0002889238821808249
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0003510233946144581
Batch  11  loss:  0.00021765010023955256
Batch  21  loss:  0.0002952516661025584
Batch  31  loss:  0.00029344361973926425
Batch  41  loss:  0.00022000107856001705
Batch  51  loss:  0.0002249868557555601
Batch  61  loss:  0.0001367086370009929
Batch  71  loss:  0.0002771944855339825
Batch  81  loss:  0.00026866240659728646
Batch  91  loss:  0.00017005667905323207
Batch  101  loss:  0.00024725202820263803
Batch  111  loss:  0.00023324854555539787
Batch  121  loss:  0.0002558502310421318
Batch  131  loss:  0.00027143789338879287
Batch  141  loss:  0.00022392980463337153
Batch  151  loss:  0.00041141314432024956
Batch  161  loss:  0.00017057708464562893
Batch  171  loss:  0.00023062934633344412
Batch  181  loss:  0.00019707564206328243
Batch  191  loss:  0.00016707321628928185
Validation on real data: 
LOSS supervised-train 0.00023419374203513145, valid 0.00018658884800970554
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00026983607676811516
Batch  11  loss:  0.0002247431402793154
Batch  21  loss:  0.00024309133004862815
Batch  31  loss:  0.0002547572657931596
Batch  41  loss:  0.00020972469064872712
Batch  51  loss:  0.00019179093942511827
Batch  61  loss:  0.00021181524789426476
Batch  71  loss:  0.00025700172409415245
Batch  81  loss:  0.0002701198682188988
Batch  91  loss:  0.0001903130323626101
Batch  101  loss:  0.00022452532721217722
Batch  111  loss:  0.00018855370581150055
Batch  121  loss:  0.00024235478485934436
Batch  131  loss:  0.00023789313854649663
Batch  141  loss:  0.0001972604513866827
Batch  151  loss:  0.00048710181727074087
Batch  161  loss:  0.00030264229280874133
Batch  171  loss:  0.00018193642608821392
Batch  181  loss:  0.00021651946008205414
Batch  191  loss:  0.00024000744451768696
Validation on real data: 
LOSS supervised-train 0.00022571367255295628, valid 0.00017102251877076924
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00031758955447003245
Batch  11  loss:  0.00017626656335778534
Batch  21  loss:  0.0003056472633033991
Batch  31  loss:  0.00024062502779997885
Batch  41  loss:  0.00019346147018950433
Batch  51  loss:  0.00020229943038430065
Batch  61  loss:  0.00017331486742477864
Batch  71  loss:  0.0002496283268555999
Batch  81  loss:  0.0003051871317438781
Batch  91  loss:  0.00024350886815227568
Batch  101  loss:  0.00023129339388106018
Batch  111  loss:  0.00016630845493637025
Batch  121  loss:  0.00021668583212886006
Batch  131  loss:  0.00022725810413248837
Batch  141  loss:  0.00019413657719269395
Batch  151  loss:  0.00035804256913252175
Batch  161  loss:  0.00024562986800447106
Batch  171  loss:  0.00017615534306969494
Batch  181  loss:  0.0002419774536974728
Batch  191  loss:  0.00020156856044195592
Validation on real data: 
LOSS supervised-train 0.0002261753116181353, valid 0.0002311885473318398
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00026145181618630886
Batch  11  loss:  0.00017938912787940353
Batch  21  loss:  0.0003082266775891185
Batch  31  loss:  0.00022132050071377307
Batch  41  loss:  0.0002563601592555642
Batch  51  loss:  0.00020402326481416821
Batch  61  loss:  0.000192118706763722
Batch  71  loss:  0.00026304059429094195
Batch  81  loss:  0.00023237451387103647
Batch  91  loss:  0.00017787951219361275
Batch  101  loss:  0.00021088066569063812
Batch  111  loss:  0.000257249892456457
Batch  121  loss:  0.0001888787664938718
Batch  131  loss:  0.0002477596281096339
Batch  141  loss:  0.00021782185649499297
Batch  151  loss:  0.000376131443772465
Batch  161  loss:  0.00017147883772850037
Batch  171  loss:  0.00023319714819081128
Batch  181  loss:  0.00021802567061968148
Batch  191  loss:  0.00018765905406326056
Validation on real data: 
LOSS supervised-train 0.00022496772449812853, valid 0.00016808256623335183
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00038816596497781575
Batch  11  loss:  0.0002118582633556798
Batch  21  loss:  0.00033830077154561877
Batch  31  loss:  0.0002885022549889982
Batch  41  loss:  0.00021722455858252943
Batch  51  loss:  0.00018883930169977248
Batch  61  loss:  0.00023575182422064245
Batch  71  loss:  0.0003015284310095012
Batch  81  loss:  0.0003215162723790854
Batch  91  loss:  0.00022974418243393302
Batch  101  loss:  0.00023085321299731731
Batch  111  loss:  0.00020310189574956894
Batch  121  loss:  0.00015372197958640754
Batch  131  loss:  0.00021657647448591888
Batch  141  loss:  0.00023891420278232545
Batch  151  loss:  0.00032428334816358984
Batch  161  loss:  0.00026372139109298587
Batch  171  loss:  0.00015828032337594777
Batch  181  loss:  0.00022087381512392312
Batch  191  loss:  0.0002326062967767939
Validation on real data: 
LOSS supervised-train 0.00022159284420922632, valid 0.0003361775306984782
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00033362797694280744
Batch  11  loss:  0.0001770411618053913
Batch  21  loss:  0.0003482744505163282
Batch  31  loss:  0.00029071670724079013
Batch  41  loss:  0.0001749627263052389
Batch  51  loss:  0.0001989359298022464
Batch  61  loss:  0.00017032116011250764
Batch  71  loss:  0.00019223189156036824
Batch  81  loss:  0.000245544157223776
Batch  91  loss:  0.00015986847574822605
Batch  101  loss:  0.00023225642507895827
Batch  111  loss:  0.00019106695253867656
Batch  121  loss:  0.0002710703411139548
Batch  131  loss:  0.0002596129197627306
Batch  141  loss:  0.00018376371008343995
Batch  151  loss:  0.00032053925679065287
Batch  161  loss:  0.00031417940044775605
Batch  171  loss:  0.00017878998187370598
Batch  181  loss:  0.0002499604597687721
Batch  191  loss:  0.00021531998936552554
Validation on real data: 
LOSS supervised-train 0.00022118642613349948, valid 0.00018742747488431633
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0002841033274307847
Batch  11  loss:  0.00021577646839432418
Batch  21  loss:  0.000310278293909505
Batch  31  loss:  0.0002570389478933066
Batch  41  loss:  0.00021854336955584586
Batch  51  loss:  0.0001886336103780195
Batch  61  loss:  0.00021270825527608395
Batch  71  loss:  0.00025480083422735333
Batch  81  loss:  0.0002526268071960658
Batch  91  loss:  0.00017208507051691413
Batch  101  loss:  0.00022780205472372472
Batch  111  loss:  0.00020646279153879732
Batch  121  loss:  0.0002509659971110523
Batch  131  loss:  0.0002120990102412179
Batch  141  loss:  0.00020455452613532543
Batch  151  loss:  0.00033946568146348
Batch  161  loss:  0.00022144806280266494
Batch  171  loss:  0.00015954530681483448
Batch  181  loss:  0.0002605110639706254
Batch  191  loss:  0.00020009532454423606
Validation on real data: 
LOSS supervised-train 0.00022287982144916896, valid 0.0002067332243314013
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.00034284451976418495
Batch  11  loss:  0.00026035524206236005
Batch  21  loss:  0.0002997258852701634
Batch  31  loss:  0.00027443605358712375
Batch  41  loss:  0.00020861155644524843
Batch  51  loss:  0.0002349022397538647
Batch  61  loss:  0.0002452605403959751
Batch  71  loss:  0.0002471584302838892
Batch  81  loss:  0.00027362609398551285
Batch  91  loss:  0.0001865189551608637
Batch  101  loss:  0.00023164742742665112
Batch  111  loss:  0.00019428640371188521
Batch  121  loss:  0.0001994689810089767
Batch  131  loss:  0.00020185513130854815
Batch  141  loss:  0.0002515646629035473
Batch  151  loss:  0.00036125394399277866
Batch  161  loss:  0.0002315720048500225
Batch  171  loss:  0.00021199844195507467
Batch  181  loss:  0.0001846027880674228
Batch  191  loss:  0.0001660223788348958
Validation on real data: 
LOSS supervised-train 0.0002181145379290683, valid 0.00019931525457650423
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00031189125729724765
Batch  11  loss:  0.0001753291580826044
Batch  21  loss:  0.00022737572726327926
Batch  31  loss:  0.00021594163263216615
Batch  41  loss:  0.00018974475096911192
Batch  51  loss:  0.00018477272533345968
Batch  61  loss:  0.0002344746026210487
Batch  71  loss:  0.0002526880125515163
Batch  81  loss:  0.000295132165774703
Batch  91  loss:  0.00022702898422721773
Batch  101  loss:  0.00019828040967695415
Batch  111  loss:  0.00020994273654650897
Batch  121  loss:  0.00017583584121894091
Batch  131  loss:  0.00021370587637647986
Batch  141  loss:  0.00021300585649441928
Batch  151  loss:  0.00031600979855284095
Batch  161  loss:  0.00018794457719195634
Batch  171  loss:  0.0001784732739906758
Batch  181  loss:  0.00023517470981460065
Batch  191  loss:  0.0002701816556509584
Validation on real data: 
LOSS supervised-train 0.0002146155080117751, valid 0.00022778884158469737
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0003120458568446338
Batch  11  loss:  0.00015680413343943655
Batch  21  loss:  0.00029290426755324006
Batch  31  loss:  0.0002468652091920376
Batch  41  loss:  0.0002254170540254563
Batch  51  loss:  0.00023513838823419064
Batch  61  loss:  0.0002226751676062122
Batch  71  loss:  0.00024681127979420125
Batch  81  loss:  0.00024247898545581847
Batch  91  loss:  0.00023052193864714354
Batch  101  loss:  0.00027713397867046297
Batch  111  loss:  0.0002162004093406722
Batch  121  loss:  0.00019790254009421915
Batch  131  loss:  0.00024103495525196195
Batch  141  loss:  0.00018799639656208456
Batch  151  loss:  0.0002964080194942653
Batch  161  loss:  0.0001936472108354792
Batch  171  loss:  0.00022804457694292068
Batch  181  loss:  0.00021666449902113527
Batch  191  loss:  0.00015503010945394635
Validation on real data: 
LOSS supervised-train 0.0002187819671962643, valid 0.00020839381613768637
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00029294315027073026
Batch  11  loss:  0.00019473776046652347
Batch  21  loss:  0.0002639574813656509
Batch  31  loss:  0.00024866682360880077
Batch  41  loss:  0.00020941614639014006
Batch  51  loss:  0.00023986748419702053
Batch  61  loss:  0.00019393980619497597
Batch  71  loss:  0.0002461960248183459
Batch  81  loss:  0.00028449343517422676
Batch  91  loss:  0.00021253552404232323
Batch  101  loss:  0.0002650618553161621
Batch  111  loss:  0.00022108275152277201
Batch  121  loss:  0.0001422332861693576
Batch  131  loss:  0.0002462739357724786
Batch  141  loss:  0.00020054250489920378
Batch  151  loss:  0.00035575125366449356
Batch  161  loss:  0.00022720242850482464
Batch  171  loss:  0.0002668721426744014
Batch  181  loss:  0.00020220047736074775
Batch  191  loss:  0.00014496315270662308
Validation on real data: 
LOSS supervised-train 0.00021987722589983605, valid 0.00018490718503016979
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0003296214563306421
Batch  11  loss:  0.00020067274454049766
Batch  21  loss:  0.00022505212109535933
Batch  31  loss:  0.00023302080808207393
Batch  41  loss:  0.00020325492369011045
Batch  51  loss:  0.00020878032955806702
Batch  61  loss:  0.00017611235671211034
Batch  71  loss:  0.00022833151160739362
Batch  81  loss:  0.0002526388270780444
Batch  91  loss:  0.0002009902091231197
Batch  101  loss:  0.0002917521051131189
Batch  111  loss:  0.0002617585123516619
Batch  121  loss:  0.00019735305977519602
Batch  131  loss:  0.0002643587940838188
Batch  141  loss:  0.00023346896341536194
Batch  151  loss:  0.0003292055334895849
Batch  161  loss:  0.0001811680558603257
Batch  171  loss:  0.00022231410548556596
Batch  181  loss:  0.00021009240299463272
Batch  191  loss:  0.00015888805501163006
Validation on real data: 
LOSS supervised-train 0.0002143618018453708, valid 0.00019785514450632036
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0003073883999604732
Batch  11  loss:  0.0001827291416702792
Batch  21  loss:  0.0002636308781802654
Batch  31  loss:  0.00028095219749957323
Batch  41  loss:  0.00019493531726766378
Batch  51  loss:  0.00025911201373673975
Batch  61  loss:  0.00014027940051164478
Batch  71  loss:  0.00026030020671896636
Batch  81  loss:  0.0002641947939991951
Batch  91  loss:  0.00021500559523701668
Batch  101  loss:  0.00025284916046075523
Batch  111  loss:  0.00028358938288874924
Batch  121  loss:  0.00023258668079506606
Batch  131  loss:  0.0002874073397833854
Batch  141  loss:  0.00022352307860273868
Batch  151  loss:  0.00026128359604626894
Batch  161  loss:  0.00020444214169401675
Batch  171  loss:  0.00017287112132180482
Batch  181  loss:  0.0001691282814135775
Batch  191  loss:  0.0001702953304629773
Validation on real data: 
LOSS supervised-train 0.00021570334196439943, valid 0.00020548254542518407
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0002657056611496955
Batch  11  loss:  0.00020265667990315706
Batch  21  loss:  0.00025498346076346934
Batch  31  loss:  0.0002376805932726711
Batch  41  loss:  0.00021668504632543772
Batch  51  loss:  0.00018977727449964732
Batch  61  loss:  0.00017741677584126592
Batch  71  loss:  0.0002815889602061361
Batch  81  loss:  0.00031913601560518146
Batch  91  loss:  0.00017697250586934388
Batch  101  loss:  0.0002521269198041409
Batch  111  loss:  0.00021029810886830091
Batch  121  loss:  0.00025064044166356325
Batch  131  loss:  0.00019756950496230274
Batch  141  loss:  0.0002046200243057683
Batch  151  loss:  0.0003972068370785564
Batch  161  loss:  0.0004706823965534568
Batch  171  loss:  0.00021356798242777586
Batch  181  loss:  0.00021871659555472434
Batch  191  loss:  0.00019890502153430134
Validation on real data: 
LOSS supervised-train 0.00021019928375608286, valid 0.00021294929319992661
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0003307888109702617
Batch  11  loss:  0.00016711272473912686
Batch  21  loss:  0.00022162121604196727
Batch  31  loss:  0.00022310651547741145
Batch  41  loss:  0.00019504791998770088
Batch  51  loss:  0.00018185583758167922
Batch  61  loss:  0.00021102266327943653
Batch  71  loss:  0.00028044695500284433
Batch  81  loss:  0.0002604301262181252
Batch  91  loss:  0.00018025303143076599
Batch  101  loss:  0.0002585573529358953
Batch  111  loss:  0.00018066720804199576
Batch  121  loss:  0.00022740876011084765
Batch  131  loss:  0.00021503872994799167
Batch  141  loss:  0.0002084967854898423
Batch  151  loss:  0.0003294696507509798
Batch  161  loss:  0.00019609459559433162
Batch  171  loss:  0.00022448509116657078
Batch  181  loss:  0.00020970743207726628
Batch  191  loss:  0.00019690861518029124
Validation on real data: 
LOSS supervised-train 0.0002116238750750199, valid 0.0002170433581341058
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0002512295905034989
Batch  11  loss:  0.00014205928891897202
Batch  21  loss:  0.00019247818272560835
Batch  31  loss:  0.00018228199041914195
Batch  41  loss:  0.00019316698308102787
Batch  51  loss:  0.00023820559727028012
Batch  61  loss:  0.0001898266636999324
Batch  71  loss:  0.0002031427138717845
Batch  81  loss:  0.00027095971745438874
Batch  91  loss:  0.00025418016593903303
Batch  101  loss:  0.0002185226621804759
Batch  111  loss:  0.00019115082977805287
Batch  121  loss:  0.00015313370386138558
Batch  131  loss:  0.00018438092956785113
Batch  141  loss:  0.0001518654462415725
Batch  151  loss:  0.00031590991420671344
Batch  161  loss:  0.00021606168593280017
Batch  171  loss:  0.00016205805877689272
Batch  181  loss:  0.00023726712970528752
Batch  191  loss:  0.00032954150810837746
Validation on real data: 
LOSS supervised-train 0.00020804313026019373, valid 0.0001638565881876275
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0003410794597584754
Batch  11  loss:  0.00023468567815143615
Batch  21  loss:  0.0002472705382388085
Batch  31  loss:  0.0002315901219844818
Batch  41  loss:  0.0002454946225043386
Batch  51  loss:  0.0001886350946733728
Batch  61  loss:  0.00014564117009285837
Batch  71  loss:  0.0002936449891421944
Batch  81  loss:  0.00024592550471425056
Batch  91  loss:  0.00016751847579143941
Batch  101  loss:  0.00022895015717949718
Batch  111  loss:  0.00020622339798137546
Batch  121  loss:  0.00022236361110117286
Batch  131  loss:  0.00019776202680077404
Batch  141  loss:  0.00019344933389220387
Batch  151  loss:  0.0003375709638930857
Batch  161  loss:  0.0002567236952017993
Batch  171  loss:  0.00022929231636226177
Batch  181  loss:  0.00018276050104759634
Batch  191  loss:  0.0002083430445054546
Validation on real data: 
LOSS supervised-train 0.00020610214214684676, valid 0.00014894959167577326
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0002169970393879339
Batch  11  loss:  0.00017941583064384758
Batch  21  loss:  0.00026853973395191133
Batch  31  loss:  0.0002169587096432224
Batch  41  loss:  0.00018416873353999108
Batch  51  loss:  0.00022770378564018756
Batch  61  loss:  0.0002093080256599933
Batch  71  loss:  0.0002253102429676801
Batch  81  loss:  0.0002781983057502657
Batch  91  loss:  0.0002552572113927454
Batch  101  loss:  0.00024451460922136903
Batch  111  loss:  0.00017961535195354372
Batch  121  loss:  0.00021536211716011167
Batch  131  loss:  0.00018028781050816178
Batch  141  loss:  0.00024667949764989316
Batch  151  loss:  0.0003146815288346261
Batch  161  loss:  0.00021400688274297863
Batch  171  loss:  0.00017114337242674083
Batch  181  loss:  0.00019749932107515633
Batch  191  loss:  0.000194670632481575
Validation on real data: 
LOSS supervised-train 0.00020771045677975053, valid 0.00019092412549071014
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00023228794452734292
Batch  11  loss:  0.00021682748047169298
Batch  21  loss:  0.0002838268701452762
Batch  31  loss:  0.0002324650704395026
Batch  41  loss:  0.0002480307302903384
Batch  51  loss:  0.00017749959079083055
Batch  61  loss:  0.00018954696133732796
Batch  71  loss:  0.00019362611055839807
Batch  81  loss:  0.000231903453823179
Batch  91  loss:  0.00016555229376535863
Batch  101  loss:  0.00023386019165627658
Batch  111  loss:  0.00017959321849048138
Batch  121  loss:  0.00023668493668083102
Batch  131  loss:  0.00020282596233300865
Batch  141  loss:  0.00020799905178137124
Batch  151  loss:  0.00024048221530392766
Batch  161  loss:  0.0002054342912742868
Batch  171  loss:  0.0001999812084250152
Batch  181  loss:  0.00020628543279599398
Batch  191  loss:  0.00019969529239460826
Validation on real data: 
LOSS supervised-train 0.00020400986180902692, valid 0.00022058287868276238
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00029746448853984475
Batch  11  loss:  0.00015908933710306883
Batch  21  loss:  0.0002826997952070087
Batch  31  loss:  0.00017278238374274224
Batch  41  loss:  0.00017656745330896229
Batch  51  loss:  0.0002263595088152215
Batch  61  loss:  0.00019884217181243002
Batch  71  loss:  0.00023300792963709682
Batch  81  loss:  0.00021377565280999988
Batch  91  loss:  0.00021675888274330646
Batch  101  loss:  0.000244221359025687
Batch  111  loss:  0.00022295104281511158
Batch  121  loss:  0.00017189938807860017
Batch  131  loss:  0.00019393990805838257
Batch  141  loss:  0.0002413661713944748
Batch  151  loss:  0.00030774594051763415
Batch  161  loss:  0.0004971437738277018
Batch  171  loss:  0.000128615865833126
Batch  181  loss:  0.00018019576964434236
Batch  191  loss:  0.0001688031479716301
Validation on real data: 
LOSS supervised-train 0.00020497341767622857, valid 0.0002668878878466785
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.00025256205117329955
Batch  11  loss:  0.00017366134852636606
Batch  21  loss:  0.00022992040612734854
Batch  31  loss:  0.0001980053202714771
Batch  41  loss:  0.00019997426716145128
Batch  51  loss:  0.0001545534178148955
Batch  61  loss:  0.00019182033429387957
Batch  71  loss:  0.00023341238556895405
Batch  81  loss:  0.000207361183129251
Batch  91  loss:  0.00016610737657174468
Batch  101  loss:  0.00016936585598159581
Batch  111  loss:  0.0002029489987762645
Batch  121  loss:  0.00018628640100359917
Batch  131  loss:  0.00019555581093300134
Batch  141  loss:  0.00019096741743851453
Batch  151  loss:  0.00041335748392157257
Batch  161  loss:  0.00018116705177817494
Batch  171  loss:  0.0002055761869996786
Batch  181  loss:  0.00020731776021420956
Batch  191  loss:  0.0001520764926681295
Validation on real data: 
LOSS supervised-train 0.00020327036454546033, valid 0.00018312456086277962
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00024875366943888366
Batch  11  loss:  0.00016560530639253557
Batch  21  loss:  0.00024114857660606503
Batch  31  loss:  0.00025294345687143505
Batch  41  loss:  0.0001822080957936123
Batch  51  loss:  0.00018118371372111142
Batch  61  loss:  0.00019854135462082922
Batch  71  loss:  0.00017515085346531123
Batch  81  loss:  0.0002478094247635454
Batch  91  loss:  0.0001464687375118956
Batch  101  loss:  0.00019381196761969477
Batch  111  loss:  0.0001815206924220547
Batch  121  loss:  0.0001627764868317172
Batch  131  loss:  0.0002303838264197111
Batch  141  loss:  0.00017489056335762143
Batch  151  loss:  0.0004570844175759703
Batch  161  loss:  0.00019753738888539374
Batch  171  loss:  0.00016396428691223264
Batch  181  loss:  0.0001819407189032063
Batch  191  loss:  0.00016697065439075232
Validation on real data: 
LOSS supervised-train 0.00019849951269861776, valid 0.00018072474631480873
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  bed ; Model ID: 7c8eb4ab1f2c8bfa2fb46fb8b9b1ac9f
--------------------
Training baseline regression model:  2022-03-29 21:50:26.373532
Detector:  point_transformer
Object:  bed
--------------------
device is  cuda
--------------------
Number of trainable parameters:  894622
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.2401980608701706
Batch  11  loss:  0.07897671312093735
Batch  21  loss:  0.04747982323169708
Batch  31  loss:  0.02666405588388443
Batch  41  loss:  0.030403079465031624
Batch  51  loss:  0.037160277366638184
Batch  61  loss:  0.037165652960538864
Batch  71  loss:  0.022953113541007042
Batch  81  loss:  0.014743451960384846
Batch  91  loss:  0.03776437044143677
Batch  101  loss:  0.01027242187410593
Batch  111  loss:  0.015948519110679626
Batch  121  loss:  0.006567943375557661
Batch  131  loss:  0.020126085728406906
Batch  141  loss:  0.00746793020516634
Batch  151  loss:  0.01687101274728775
Batch  161  loss:  0.009153335355222225
Batch  171  loss:  0.012435867451131344
Batch  181  loss:  0.012459887191653252
Batch  191  loss:  0.0049492716789245605
Validation on real data: 
LOSS supervised-train 0.0282822501193732, valid 0.0030904910527169704
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.002976594492793083
Batch  11  loss:  0.0026362126227468252
Batch  21  loss:  0.0057680257596075535
Batch  31  loss:  0.008052152581512928
Batch  41  loss:  0.009732279926538467
Batch  51  loss:  0.008682338520884514
Batch  61  loss:  0.009880506433546543
Batch  71  loss:  0.004912226926535368
Batch  81  loss:  0.004543749615550041
Batch  91  loss:  0.017197176814079285
Batch  101  loss:  0.004246007651090622
Batch  111  loss:  0.004360696766525507
Batch  121  loss:  0.002388299209997058
Batch  131  loss:  0.006785048637539148
Batch  141  loss:  0.0034058319870382547
Batch  151  loss:  0.009360573254525661
Batch  161  loss:  0.0036487930919975042
Batch  171  loss:  0.0037847093772143126
Batch  181  loss:  0.005140126682817936
Batch  191  loss:  0.0033570616506040096
Validation on real data: 
LOSS supervised-train 0.006323966536438092, valid 0.0018196887103840709
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.002291050273925066
Batch  11  loss:  0.0017172934021800756
Batch  21  loss:  0.0039000846445560455
Batch  31  loss:  0.003720022737979889
Batch  41  loss:  0.005081909243017435
Batch  51  loss:  0.004724664147943258
Batch  61  loss:  0.005461880937218666
Batch  71  loss:  0.003680229652673006
Batch  81  loss:  0.002640142571181059
Batch  91  loss:  0.009146135300397873
Batch  101  loss:  0.0021828445605933666
Batch  111  loss:  0.0028496133163571358
Batch  121  loss:  0.0020839846692979336
Batch  131  loss:  0.0045563289895653725
Batch  141  loss:  0.002152461325749755
Batch  151  loss:  0.005894853267818689
Batch  161  loss:  0.0025108973495662212
Batch  171  loss:  0.003051185514777899
Batch  181  loss:  0.0025572096928954124
Batch  191  loss:  0.0024862191639840603
Validation on real data: 
LOSS supervised-train 0.0037869778997264803, valid 0.0016611912287771702
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0019258602987974882
Batch  11  loss:  0.0010918878251686692
Batch  21  loss:  0.002459952374920249
Batch  31  loss:  0.002404816448688507
Batch  41  loss:  0.002988885622471571
Batch  51  loss:  0.002850983291864395
Batch  61  loss:  0.0026875233743339777
Batch  71  loss:  0.0020469028968364
Batch  81  loss:  0.002158382209017873
Batch  91  loss:  0.004394803661853075
Batch  101  loss:  0.0016635933425277472
Batch  111  loss:  0.00243588094599545
Batch  121  loss:  0.0013417848385870457
Batch  131  loss:  0.0032459376379847527
Batch  141  loss:  0.0014295385917648673
Batch  151  loss:  0.005088347010314465
Batch  161  loss:  0.0018602537456899881
Batch  171  loss:  0.0023244027979671955
Batch  181  loss:  0.0022023033816367388
Batch  191  loss:  0.002040429273620248
Validation on real data: 
LOSS supervised-train 0.002625300491927192, valid 0.0013992090243846178
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0018422731664031744
Batch  11  loss:  0.0010949523421004415
Batch  21  loss:  0.0018252581357955933
Batch  31  loss:  0.0018681496148929
Batch  41  loss:  0.002352075418457389
Batch  51  loss:  0.0019335956312716007
Batch  61  loss:  0.0017874327022582293
Batch  71  loss:  0.0020230012014508247
Batch  81  loss:  0.0013672786299139261
Batch  91  loss:  0.0034694906789809465
Batch  101  loss:  0.0010661721462383866
Batch  111  loss:  0.0015798197127878666
Batch  121  loss:  0.0012099992018193007
Batch  131  loss:  0.002387002110481262
Batch  141  loss:  0.001178257749415934
Batch  151  loss:  0.003201837418600917
Batch  161  loss:  0.0011564576998353004
Batch  171  loss:  0.0015050756046548486
Batch  181  loss:  0.0017665562918409705
Batch  191  loss:  0.001780648366548121
Validation on real data: 
LOSS supervised-train 0.001984499122481793, valid 0.0011582383885979652
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0015192779246717691
Batch  11  loss:  0.0008762697107158601
Batch  21  loss:  0.0015062245074659586
Batch  31  loss:  0.0013794128317385912
Batch  41  loss:  0.001414608908817172
Batch  51  loss:  0.0012095050187781453
Batch  61  loss:  0.0009775712387636304
Batch  71  loss:  0.0010576389031484723
Batch  81  loss:  0.0011893449118360877
Batch  91  loss:  0.003051002277061343
Batch  101  loss:  0.0011676863068714738
Batch  111  loss:  0.001483115484006703
Batch  121  loss:  0.0011567031033337116
Batch  131  loss:  0.002008239272981882
Batch  141  loss:  0.0011969257611781359
Batch  151  loss:  0.0027366087306290865
Batch  161  loss:  0.0012457692064344883
Batch  171  loss:  0.0015413400251418352
Batch  181  loss:  0.001732638105750084
Batch  191  loss:  0.0014390742871910334
Validation on real data: 
LOSS supervised-train 0.0016376777039840817, valid 0.0010800892487168312
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0012906144838780165
Batch  11  loss:  0.0008335199672728777
Batch  21  loss:  0.001207693014293909
Batch  31  loss:  0.001335023669525981
Batch  41  loss:  0.0011676839785650373
Batch  51  loss:  0.0012418725527822971
Batch  61  loss:  0.0009489489020779729
Batch  71  loss:  0.0015729042934253812
Batch  81  loss:  0.0011915405048057437
Batch  91  loss:  0.002243979601189494
Batch  101  loss:  0.0010088226990774274
Batch  111  loss:  0.001324361888691783
Batch  121  loss:  0.0011593769304454327
Batch  131  loss:  0.0017779680201783776
Batch  141  loss:  0.0009038710268214345
Batch  151  loss:  0.002193579450249672
Batch  161  loss:  0.0008904863498173654
Batch  171  loss:  0.0012821747222915292
Batch  181  loss:  0.0013445698423311114
Batch  191  loss:  0.0014199005672708154
Validation on real data: 
LOSS supervised-train 0.0014024105999851598, valid 0.0009507723152637482
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.001230262452736497
Batch  11  loss:  0.0009934775298461318
Batch  21  loss:  0.0007751138182356954
Batch  31  loss:  0.001124534523114562
Batch  41  loss:  0.0012019070563837886
Batch  51  loss:  0.0011502456618472934
Batch  61  loss:  0.0008680752362124622
Batch  71  loss:  0.0010296003893017769
Batch  81  loss:  0.0009725079289637506
Batch  91  loss:  0.0019212302286177874
Batch  101  loss:  0.0009070505038835108
Batch  111  loss:  0.0010654504876583815
Batch  121  loss:  0.0009507122449576855
Batch  131  loss:  0.0015911284135654569
Batch  141  loss:  0.00099374505225569
Batch  151  loss:  0.001724899047985673
Batch  161  loss:  0.0007209770265035331
Batch  171  loss:  0.0012241920921951532
Batch  181  loss:  0.0012178038014099002
Batch  191  loss:  0.001119375228881836
Validation on real data: 
LOSS supervised-train 0.0012562301382422448, valid 0.000848755706101656
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0011533091310411692
Batch  11  loss:  0.000845039845444262
Batch  21  loss:  0.0007594175403937697
Batch  31  loss:  0.0010431011905893683
Batch  41  loss:  0.0011613951064646244
Batch  51  loss:  0.0008941614651121199
Batch  61  loss:  0.0006350668845698237
Batch  71  loss:  0.0011203540489077568
Batch  81  loss:  0.0009521485189907253
Batch  91  loss:  0.0020820454228669405
Batch  101  loss:  0.0010121222585439682
Batch  111  loss:  0.0009062553872354329
Batch  121  loss:  0.0007658497779630125
Batch  131  loss:  0.0015747809084132314
Batch  141  loss:  0.0008734776056371629
Batch  151  loss:  0.001576139940880239
Batch  161  loss:  0.0007122089737094939
Batch  171  loss:  0.0012967662187293172
Batch  181  loss:  0.0014162686420604587
Batch  191  loss:  0.0013604597188532352
Validation on real data: 
LOSS supervised-train 0.001132909412845038, valid 0.0007014672737568617
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0009188746917061508
Batch  11  loss:  0.0005329513223841786
Batch  21  loss:  0.0006841265712864697
Batch  31  loss:  0.0010339334839954972
Batch  41  loss:  0.0009296124917455018
Batch  51  loss:  0.0009858953999355435
Batch  61  loss:  0.0006869182107038796
Batch  71  loss:  0.0009805000154301524
Batch  81  loss:  0.0009079062729142606
Batch  91  loss:  0.0014657769352197647
Batch  101  loss:  0.0010208740131929517
Batch  111  loss:  0.0008802571101114154
Batch  121  loss:  0.0007615825161337852
Batch  131  loss:  0.0015227687545120716
Batch  141  loss:  0.0008235831628553569
Batch  151  loss:  0.001601874246262014
Batch  161  loss:  0.0005908534512855113
Batch  171  loss:  0.0010272556683048606
Batch  181  loss:  0.0010614139027893543
Batch  191  loss:  0.000890399853233248
Validation on real data: 
LOSS supervised-train 0.0010402004563366063, valid 0.0007636919617652893
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0009672571322880685
Batch  11  loss:  0.0006587696843780577
Batch  21  loss:  0.0008498100796714425
Batch  31  loss:  0.0010185684077441692
Batch  41  loss:  0.001002017525024712
Batch  51  loss:  0.000759677030146122
Batch  61  loss:  0.0006718282238580287
Batch  71  loss:  0.0007879279437474906
Batch  81  loss:  0.0007451565470546484
Batch  91  loss:  0.0013695850502699614
Batch  101  loss:  0.0008231973624788225
Batch  111  loss:  0.0011179803404957056
Batch  121  loss:  0.0007393754785880446
Batch  131  loss:  0.001083850278519094
Batch  141  loss:  0.0009189547854475677
Batch  151  loss:  0.0015542241744697094
Batch  161  loss:  0.000538732623681426
Batch  171  loss:  0.0010764178587123752
Batch  181  loss:  0.0010109966387972236
Batch  191  loss:  0.0012356535298749804
Validation on real data: 
LOSS supervised-train 0.001001587612554431, valid 0.0008032115874812007
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0012485507177188993
Batch  11  loss:  0.0005555450334213674
Batch  21  loss:  0.0007353229448199272
Batch  31  loss:  0.0008057263330556452
Batch  41  loss:  0.0009850020287558436
Batch  51  loss:  0.0007587885484099388
Batch  61  loss:  0.0007144030532799661
Batch  71  loss:  0.0007505488465540111
Batch  81  loss:  0.0007028989493846893
Batch  91  loss:  0.0012585357762873173
Batch  101  loss:  0.0007036338793113828
Batch  111  loss:  0.0009897020645439625
Batch  121  loss:  0.000700739212334156
Batch  131  loss:  0.0012034091632813215
Batch  141  loss:  0.0007622763514518738
Batch  151  loss:  0.001395877916365862
Batch  161  loss:  0.0005415399209596217
Batch  171  loss:  0.0011617399286478758
Batch  181  loss:  0.0012385561130940914
Batch  191  loss:  0.0010150513844564557
Validation on real data: 
LOSS supervised-train 0.0009264832040935289, valid 0.0006662978557869792
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0009639827185310423
Batch  11  loss:  0.0005856172065250576
Batch  21  loss:  0.000603895983658731
Batch  31  loss:  0.0007336289854720235
Batch  41  loss:  0.0007207343587651849
Batch  51  loss:  0.0008280511246994138
Batch  61  loss:  0.0006634630262851715
Batch  71  loss:  0.0007209380273707211
Batch  81  loss:  0.000666632957290858
Batch  91  loss:  0.0013441002229228616
Batch  101  loss:  0.0008569213678129017
Batch  111  loss:  0.0008991834474727511
Batch  121  loss:  0.0007009097607806325
Batch  131  loss:  0.0011230482487007976
Batch  141  loss:  0.000717818969860673
Batch  151  loss:  0.0013010089751332998
Batch  161  loss:  0.0004721737641375512
Batch  171  loss:  0.0008233549888245761
Batch  181  loss:  0.0008917820523492992
Batch  191  loss:  0.0008918977109715343
Validation on real data: 
LOSS supervised-train 0.0008656537579372525, valid 0.00062392937252298
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0008200420415960252
Batch  11  loss:  0.00048233772395178676
Batch  21  loss:  0.0006365725421346724
Batch  31  loss:  0.0007012889836914837
Batch  41  loss:  0.0005621743621304631
Batch  51  loss:  0.0005682178889401257
Batch  61  loss:  0.0005637759459204972
Batch  71  loss:  0.0006554083665832877
Batch  81  loss:  0.0007049954729154706
Batch  91  loss:  0.00152601208537817
Batch  101  loss:  0.0007331284577958286
Batch  111  loss:  0.0009094093693420291
Batch  121  loss:  0.0006466281483881176
Batch  131  loss:  0.0014041768154129386
Batch  141  loss:  0.0006341115804389119
Batch  151  loss:  0.001405412214808166
Batch  161  loss:  0.0005971625214442611
Batch  171  loss:  0.0008781288634054363
Batch  181  loss:  0.0009734815685078502
Batch  191  loss:  0.0009237619815394282
Validation on real data: 
LOSS supervised-train 0.0008101766201434657, valid 0.0007288199849426746
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0008581937872804701
Batch  11  loss:  0.0004739296273328364
Batch  21  loss:  0.0005094049265608191
Batch  31  loss:  0.0008006441639736295
Batch  41  loss:  0.000637309392914176
Batch  51  loss:  0.000598716433160007
Batch  61  loss:  0.0005990950157865882
Batch  71  loss:  0.0008268345845863223
Batch  81  loss:  0.0007202572305686772
Batch  91  loss:  0.001137260696850717
Batch  101  loss:  0.0006906609050929546
Batch  111  loss:  0.000688191328663379
Batch  121  loss:  0.0005792766460217535
Batch  131  loss:  0.0010895427549257874
Batch  141  loss:  0.0006510386592708528
Batch  151  loss:  0.001236348762176931
Batch  161  loss:  0.00051785638788715
Batch  171  loss:  0.0006859382847324014
Batch  181  loss:  0.0010207260493189096
Batch  191  loss:  0.0008405876578763127
Validation on real data: 
LOSS supervised-train 0.0007904430074268021, valid 0.0005609802319668233
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0008211685344576836
Batch  11  loss:  0.00048360167420469224
Batch  21  loss:  0.0006095873541198671
Batch  31  loss:  0.0005879913223907351
Batch  41  loss:  0.0006874485407024622
Batch  51  loss:  0.0006434755632653832
Batch  61  loss:  0.0005302463541738689
Batch  71  loss:  0.000450970750534907
Batch  81  loss:  0.0005123340524733067
Batch  91  loss:  0.0008293659193441272
Batch  101  loss:  0.0006478251307271421
Batch  111  loss:  0.000703122525010258
Batch  121  loss:  0.0006762060220353305
Batch  131  loss:  0.0011315710144117475
Batch  141  loss:  0.0006197575130499899
Batch  151  loss:  0.0009064028272405267
Batch  161  loss:  0.0005019272794015706
Batch  171  loss:  0.0006894163088873029
Batch  181  loss:  0.0008306796662509441
Batch  191  loss:  0.0007418852765113115
Validation on real data: 
LOSS supervised-train 0.0007518528403306846, valid 0.0005374126485548913
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0007422443595714867
Batch  11  loss:  0.0003850661450996995
Batch  21  loss:  0.00047229789197444916
Batch  31  loss:  0.0005959435948170722
Batch  41  loss:  0.0007224689470604062
Batch  51  loss:  0.0005075549124740064
Batch  61  loss:  0.0006020445725880563
Batch  71  loss:  0.0008706111111678183
Batch  81  loss:  0.0005456565413624048
Batch  91  loss:  0.0010135489283129573
Batch  101  loss:  0.0008154299575835466
Batch  111  loss:  0.0006835891399532557
Batch  121  loss:  0.0005816097254864872
Batch  131  loss:  0.0010044315131381154
Batch  141  loss:  0.0007398999878205359
Batch  151  loss:  0.0013069637352600694
Batch  161  loss:  0.000504536903463304
Batch  171  loss:  0.0007055986789055169
Batch  181  loss:  0.0008480139076709747
Batch  191  loss:  0.0009556270088069141
Validation on real data: 
LOSS supervised-train 0.0007511888288718183, valid 0.0005198337603360415
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0005578414420597255
Batch  11  loss:  0.000403237936552614
Batch  21  loss:  0.00044863304356113076
Batch  31  loss:  0.0006796152447350323
Batch  41  loss:  0.0006871312507428229
Batch  51  loss:  0.0005803199019283056
Batch  61  loss:  0.000563734385650605
Batch  71  loss:  0.0004673750954680145
Batch  81  loss:  0.0005712641286663711
Batch  91  loss:  0.000787644530646503
Batch  101  loss:  0.000607948808465153
Batch  111  loss:  0.0007253082003444433
Batch  121  loss:  0.0005291753332130611
Batch  131  loss:  0.000995434122160077
Batch  141  loss:  0.0005316993920132518
Batch  151  loss:  0.001008927240036428
Batch  161  loss:  0.0004993036272935569
Batch  171  loss:  0.0005325327510945499
Batch  181  loss:  0.0008650112431496382
Batch  191  loss:  0.0008180145523510873
Validation on real data: 
LOSS supervised-train 0.0007351917175401468, valid 0.0005206995992921293
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0007941632647998631
Batch  11  loss:  0.00038572566700167954
Batch  21  loss:  0.0005122413276694715
Batch  31  loss:  0.0005561906727962196
Batch  41  loss:  0.0007375719142146409
Batch  51  loss:  0.0005620169686153531
Batch  61  loss:  0.0005197090213187039
Batch  71  loss:  0.0007609015447087586
Batch  81  loss:  0.0005912506603635848
Batch  91  loss:  0.0009634603629820049
Batch  101  loss:  0.0007752525852993131
Batch  111  loss:  0.0006231744191609323
Batch  121  loss:  0.000434449000749737
Batch  131  loss:  0.000830694567412138
Batch  141  loss:  0.0005363228847272694
Batch  151  loss:  0.001149025629274547
Batch  161  loss:  0.0004925850662402809
Batch  171  loss:  0.0006199969793669879
Batch  181  loss:  0.000794548075646162
Batch  191  loss:  0.0006531506078317761
Validation on real data: 
LOSS supervised-train 0.00070357014235924, valid 0.0005415290361270308
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0006617150502279401
Batch  11  loss:  0.00039612874388694763
Batch  21  loss:  0.0005392233142629266
Batch  31  loss:  0.0005468144663609564
Batch  41  loss:  0.000550457218196243
Batch  51  loss:  0.0004436236631590873
Batch  61  loss:  0.0005267824744805694
Batch  71  loss:  0.00038030018913559616
Batch  81  loss:  0.0006424067541956902
Batch  91  loss:  0.0009438110864721239
Batch  101  loss:  0.00064996094442904
Batch  111  loss:  0.0006093528936617076
Batch  121  loss:  0.0005602897726930678
Batch  131  loss:  0.0007973870960995555
Batch  141  loss:  0.0005512854550033808
Batch  151  loss:  0.0009313237969763577
Batch  161  loss:  0.0004497270565479994
Batch  171  loss:  0.0006011644727550447
Batch  181  loss:  0.0009962444892153144
Batch  191  loss:  0.000866061425767839
Validation on real data: 
LOSS supervised-train 0.000684899034386035, valid 0.0006672481540590525
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0008223246550187469
Batch  11  loss:  0.00035212963121011853
Batch  21  loss:  0.0005119674024172127
Batch  31  loss:  0.0004523010575212538
Batch  41  loss:  0.0005710924160666764
Batch  51  loss:  0.00045075564412400126
Batch  61  loss:  0.0005619078292511404
Batch  71  loss:  0.0005020984099246562
Batch  81  loss:  0.0004767477512359619
Batch  91  loss:  0.0007515475153923035
Batch  101  loss:  0.0005784256500191987
Batch  111  loss:  0.0006317960796877742
Batch  121  loss:  0.0004505318065639585
Batch  131  loss:  0.000782688963226974
Batch  141  loss:  0.0005340793868526816
Batch  151  loss:  0.0009541246690787375
Batch  161  loss:  0.00038048086571507156
Batch  171  loss:  0.0007658159011043608
Batch  181  loss:  0.0006997984601184726
Batch  191  loss:  0.0007263445877470076
Validation on real data: 
LOSS supervised-train 0.0006838144475477748, valid 0.0005372600862756371
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0006698679644614458
Batch  11  loss:  0.0003487338835839182
Batch  21  loss:  0.0004235335218254477
Batch  31  loss:  0.0005894492496736348
Batch  41  loss:  0.0005762837827205658
Batch  51  loss:  0.00048498084652237594
Batch  61  loss:  0.0006106429500505328
Batch  71  loss:  0.000493635656312108
Batch  81  loss:  0.00045774877071380615
Batch  91  loss:  0.0007524381508119404
Batch  101  loss:  0.0005639326991513371
Batch  111  loss:  0.0005898663075640798
Batch  121  loss:  0.0004934536991640925
Batch  131  loss:  0.0008400605292990804
Batch  141  loss:  0.0005964188603684306
Batch  151  loss:  0.0010247314348816872
Batch  161  loss:  0.0004154926573392004
Batch  171  loss:  0.0006615443271584809
Batch  181  loss:  0.0008158213458955288
Batch  191  loss:  0.0005238053854554892
Validation on real data: 
LOSS supervised-train 0.0006502265277958941, valid 0.000482748233480379
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0004832972481381148
Batch  11  loss:  0.0003055543056689203
Batch  21  loss:  0.0003679571091197431
Batch  31  loss:  0.00043736628140322864
Batch  41  loss:  0.0005027288571000099
Batch  51  loss:  0.0005458644591271877
Batch  61  loss:  0.00044920487562194467
Batch  71  loss:  0.0004835246072616428
Batch  81  loss:  0.00039735203608870506
Batch  91  loss:  0.0005913514760322869
Batch  101  loss:  0.000537813815753907
Batch  111  loss:  0.0005168032948859036
Batch  121  loss:  0.0004957785131409764
Batch  131  loss:  0.0007904717931523919
Batch  141  loss:  0.0005429634475149214
Batch  151  loss:  0.0006856495165266097
Batch  161  loss:  0.0004742542514577508
Batch  171  loss:  0.0005127657204866409
Batch  181  loss:  0.0006443153251893818
Batch  191  loss:  0.0006462110904976726
Validation on real data: 
LOSS supervised-train 0.0006232493466814048, valid 0.00043004797771573067
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0006309633026830852
Batch  11  loss:  0.0003073546104133129
Batch  21  loss:  0.000543555011972785
Batch  31  loss:  0.0004892849829047918
Batch  41  loss:  0.0005993226659484208
Batch  51  loss:  0.00047209381591528654
Batch  61  loss:  0.00045421492541208863
Batch  71  loss:  0.0004929720307700336
Batch  81  loss:  0.0005581306759268045
Batch  91  loss:  0.0006415846291929483
Batch  101  loss:  0.0004902207874692976
Batch  111  loss:  0.0007774965488351882
Batch  121  loss:  0.00045218272134661674
Batch  131  loss:  0.0007729671779088676
Batch  141  loss:  0.0005441170069389045
Batch  151  loss:  0.0007295127143152058
Batch  161  loss:  0.0003403244772925973
Batch  171  loss:  0.00048057857202365994
Batch  181  loss:  0.0006916634738445282
Batch  191  loss:  0.0005980873829685152
Validation on real data: 
LOSS supervised-train 0.0006197110735229217, valid 0.0005004421691410244
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.00045070305350236595
Batch  11  loss:  0.00036352654569782317
Batch  21  loss:  0.00033287209225818515
Batch  31  loss:  0.0004402922641020268
Batch  41  loss:  0.000530686229467392
Batch  51  loss:  0.0003971139376517385
Batch  61  loss:  0.00046015105908736587
Batch  71  loss:  0.0004495980101637542
Batch  81  loss:  0.00039341780939139426
Batch  91  loss:  0.0006765644066035748
Batch  101  loss:  0.0006391422357410192
Batch  111  loss:  0.0005570463836193085
Batch  121  loss:  0.0005072103231213987
Batch  131  loss:  0.0006993673741817474
Batch  141  loss:  0.0005146922776475549
Batch  151  loss:  0.0009861814323812723
Batch  161  loss:  0.00037554200389422476
Batch  171  loss:  0.0005096058011986315
Batch  181  loss:  0.0006716524949297309
Batch  191  loss:  0.0007538424106314778
Validation on real data: 
LOSS supervised-train 0.0005956899300508667, valid 0.0004140398814342916
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0005002949037589133
Batch  11  loss:  0.00032484810799360275
Batch  21  loss:  0.0004845359071623534
Batch  31  loss:  0.00044221317511983216
Batch  41  loss:  0.0004100149089936167
Batch  51  loss:  0.00043263498810119927
Batch  61  loss:  0.0004123605031054467
Batch  71  loss:  0.0004481904034037143
Batch  81  loss:  0.0003416533290874213
Batch  91  loss:  0.0006978544406592846
Batch  101  loss:  0.0005371892475523055
Batch  111  loss:  0.0005454697529785335
Batch  121  loss:  0.0003855317772831768
Batch  131  loss:  0.0006476874696090817
Batch  141  loss:  0.0004719842108897865
Batch  151  loss:  0.0008190343505702913
Batch  161  loss:  0.00037941482150927186
Batch  171  loss:  0.00043500488391146064
Batch  181  loss:  0.0008934071520343423
Batch  191  loss:  0.0005533010698854923
Validation on real data: 
LOSS supervised-train 0.0005874601623509079, valid 0.00042953461525030434
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0005005574785172939
Batch  11  loss:  0.0003285658312961459
Batch  21  loss:  0.00040135186281986535
Batch  31  loss:  0.0005138959386385977
Batch  41  loss:  0.0004983336548320949
Batch  51  loss:  0.00043455755803734064
Batch  61  loss:  0.0004587963630910963
Batch  71  loss:  0.000399169628508389
Batch  81  loss:  0.0003452343516983092
Batch  91  loss:  0.0004884695517830551
Batch  101  loss:  0.0005689679528586566
Batch  111  loss:  0.0005675535066984594
Batch  121  loss:  0.00041646609315648675
Batch  131  loss:  0.0007553707691840827
Batch  141  loss:  0.0004216392117086798
Batch  151  loss:  0.0008875808562152088
Batch  161  loss:  0.0003245587577112019
Batch  171  loss:  0.0005789304850623012
Batch  181  loss:  0.0005357777699828148
Batch  191  loss:  0.0005614366964437068
Validation on real data: 
LOSS supervised-train 0.0005718202068237588, valid 0.00041781156323850155
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0005956293316558003
Batch  11  loss:  0.00035942296381108463
Batch  21  loss:  0.0003549116663634777
Batch  31  loss:  0.0004582951369229704
Batch  41  loss:  0.00045704463263973594
Batch  51  loss:  0.00040764285949990153
Batch  61  loss:  0.0003520039317663759
Batch  71  loss:  0.0003948357480112463
Batch  81  loss:  0.0004053726443089545
Batch  91  loss:  0.00045526219764724374
Batch  101  loss:  0.00039170769741758704
Batch  111  loss:  0.0005237310542725027
Batch  121  loss:  0.0004277766274753958
Batch  131  loss:  0.0007665650337003171
Batch  141  loss:  0.0004350501112639904
Batch  151  loss:  0.0008994772797450423
Batch  161  loss:  0.0003805191081482917
Batch  171  loss:  0.0005370814469642937
Batch  181  loss:  0.0007578012300655246
Batch  191  loss:  0.00048358418280258775
Validation on real data: 
LOSS supervised-train 0.0005486452726472634, valid 0.0003666655393317342
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00046225590631365776
Batch  11  loss:  0.00028204708360135555
Batch  21  loss:  0.00035436428152024746
Batch  31  loss:  0.0003739607345778495
Batch  41  loss:  0.0004350301460362971
Batch  51  loss:  0.0003908274229615927
Batch  61  loss:  0.0004151748144067824
Batch  71  loss:  0.00029965004068799317
Batch  81  loss:  0.0003253452305216342
Batch  91  loss:  0.0005671121180057526
Batch  101  loss:  0.0005716837476938963
Batch  111  loss:  0.0006477402639575303
Batch  121  loss:  0.0004463086952455342
Batch  131  loss:  0.0005921300617046654
Batch  141  loss:  0.0005525154410861433
Batch  151  loss:  0.0007782296743243933
Batch  161  loss:  0.00039936540997587144
Batch  171  loss:  0.0005217629950493574
Batch  181  loss:  0.0006795970839448273
Batch  191  loss:  0.0004649067996069789
Validation on real data: 
LOSS supervised-train 0.0005537280236603692, valid 0.0003901351010426879
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.00059710000641644
Batch  11  loss:  0.00034342872095294297
Batch  21  loss:  0.0003695604973472655
Batch  31  loss:  0.00041131730540655553
Batch  41  loss:  0.0005115426611155272
Batch  51  loss:  0.00044404089567251503
Batch  61  loss:  0.0003535586583893746
Batch  71  loss:  0.0003388036566320807
Batch  81  loss:  0.0003245406551286578
Batch  91  loss:  0.000618729623965919
Batch  101  loss:  0.0005513773648999631
Batch  111  loss:  0.00044669199269264936
Batch  121  loss:  0.0005095373489893973
Batch  131  loss:  0.0005811620503664017
Batch  141  loss:  0.0004739510186482221
Batch  151  loss:  0.0007098303758539259
Batch  161  loss:  0.00048012560000643134
Batch  171  loss:  0.00036354950862005353
Batch  181  loss:  0.0006618144107051194
Batch  191  loss:  0.0005538024706766009
Validation on real data: 
LOSS supervised-train 0.0005399885948281735, valid 0.0003806596214417368
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0005669342936016619
Batch  11  loss:  0.0003156626771669835
Batch  21  loss:  0.0003824438899755478
Batch  31  loss:  0.00047017677570693195
Batch  41  loss:  0.00046712480252608657
Batch  51  loss:  0.0003547541855368763
Batch  61  loss:  0.0004471615538932383
Batch  71  loss:  0.0002938107936643064
Batch  81  loss:  0.0003236730699427426
Batch  91  loss:  0.0004368749214336276
Batch  101  loss:  0.0004976417985744774
Batch  111  loss:  0.0006228681304492056
Batch  121  loss:  0.0004187621525488794
Batch  131  loss:  0.0006956703145988286
Batch  141  loss:  0.00041375443106517196
Batch  151  loss:  0.0008224535267800093
Batch  161  loss:  0.0004474437446333468
Batch  171  loss:  0.00048513541696593165
Batch  181  loss:  0.0006702984101139009
Batch  191  loss:  0.0006305257556959987
Validation on real data: 
LOSS supervised-train 0.0005433506191184278, valid 0.00031865929486230016
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0004631398478522897
Batch  11  loss:  0.0003021768934559077
Batch  21  loss:  0.0003782518906518817
Batch  31  loss:  0.00041592944762669504
Batch  41  loss:  0.0005074897198937833
Batch  51  loss:  0.0003688391298055649
Batch  61  loss:  0.0003746385336853564
Batch  71  loss:  0.0003071814717259258
Batch  81  loss:  0.00034190595033578575
Batch  91  loss:  0.00048215468996204436
Batch  101  loss:  0.0005073846550658345
Batch  111  loss:  0.0005202868487685919
Batch  121  loss:  0.00040106935193762183
Batch  131  loss:  0.0005872929468750954
Batch  141  loss:  0.00038719401345588267
Batch  151  loss:  0.0009992410195991397
Batch  161  loss:  0.00036579984589479864
Batch  171  loss:  0.0005399402580223978
Batch  181  loss:  0.0007630789768882096
Batch  191  loss:  0.0005616507260128856
Validation on real data: 
LOSS supervised-train 0.0005358755607448984, valid 0.00040625949623063207
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0004753350222017616
Batch  11  loss:  0.0003252911556046456
Batch  21  loss:  0.00035916766501031816
Batch  31  loss:  0.0003642177034635097
Batch  41  loss:  0.0004764030745718628
Batch  51  loss:  0.00040973216528072953
Batch  61  loss:  0.00043919257586821914
Batch  71  loss:  0.00036881017149426043
Batch  81  loss:  0.00038330178358592093
Batch  91  loss:  0.0004645527806133032
Batch  101  loss:  0.0005032033077441156
Batch  111  loss:  0.0005950094200670719
Batch  121  loss:  0.0003893691464327276
Batch  131  loss:  0.0005837918724864721
Batch  141  loss:  0.0004411747504491359
Batch  151  loss:  0.0007783879991620779
Batch  161  loss:  0.00032896094489842653
Batch  171  loss:  0.00046194903552532196
Batch  181  loss:  0.0006362221902236342
Batch  191  loss:  0.0005305332015268505
Validation on real data: 
LOSS supervised-train 0.0005449555697850883, valid 0.0003449931973591447
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.000438204180682078
Batch  11  loss:  0.0002946221502497792
Batch  21  loss:  0.0004515939799603075
Batch  31  loss:  0.00040069277747534215
Batch  41  loss:  0.00038801375194452703
Batch  51  loss:  0.0003234422765672207
Batch  61  loss:  0.0003512637340463698
Batch  71  loss:  0.00034436225541867316
Batch  81  loss:  0.00035286336787976325
Batch  91  loss:  0.0006078456062823534
Batch  101  loss:  0.0004350336967036128
Batch  111  loss:  0.00047179110697470605
Batch  121  loss:  0.0005449312156997621
Batch  131  loss:  0.0005821188678964972
Batch  141  loss:  0.0004608333401847631
Batch  151  loss:  0.0009874355746433139
Batch  161  loss:  0.0003876182599924505
Batch  171  loss:  0.0004514687170740217
Batch  181  loss:  0.0005692768027074635
Batch  191  loss:  0.0006215044995769858
Validation on real data: 
LOSS supervised-train 0.0005062150681624189, valid 0.0004414537106640637
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0004232774081174284
Batch  11  loss:  0.0002652685798238963
Batch  21  loss:  0.0004094416799489409
Batch  31  loss:  0.00040379606070928276
Batch  41  loss:  0.00046206454862840474
Batch  51  loss:  0.0003180648200213909
Batch  61  loss:  0.0003407949989195913
Batch  71  loss:  0.0003249233413953334
Batch  81  loss:  0.0004464197554625571
Batch  91  loss:  0.0003739824169315398
Batch  101  loss:  0.0004105044936295599
Batch  111  loss:  0.00036549902870319784
Batch  121  loss:  0.0005085760494694114
Batch  131  loss:  0.0005390557926148176
Batch  141  loss:  0.0004863507056143135
Batch  151  loss:  0.0009072867105714977
Batch  161  loss:  0.00035438028862699866
Batch  171  loss:  0.0003060897870454937
Batch  181  loss:  0.0005476668011397123
Batch  191  loss:  0.0003720764070749283
Validation on real data: 
LOSS supervised-train 0.0004938689032860566, valid 0.0003390962374396622
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0004060838255099952
Batch  11  loss:  0.00027735019102692604
Batch  21  loss:  0.0003178016049787402
Batch  31  loss:  0.00034325162414461374
Batch  41  loss:  0.00044405716471374035
Batch  51  loss:  0.0003141220659017563
Batch  61  loss:  0.0004078027850482613
Batch  71  loss:  0.00038564030546694994
Batch  81  loss:  0.0003741817199625075
Batch  91  loss:  0.0006227571866475046
Batch  101  loss:  0.00047682400327175856
Batch  111  loss:  0.0004280353896319866
Batch  121  loss:  0.0004880882042925805
Batch  131  loss:  0.0005439333035610616
Batch  141  loss:  0.0004566574061755091
Batch  151  loss:  0.0008322741487063468
Batch  161  loss:  0.0003967217344325036
Batch  171  loss:  0.00041406459058634937
Batch  181  loss:  0.0007376532885245979
Batch  191  loss:  0.0004278603882994503
Validation on real data: 
LOSS supervised-train 0.0005001220155099873, valid 0.00034189209691248834
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.00039940609713084996
Batch  11  loss:  0.00026082643307745457
Batch  21  loss:  0.00040922663174569607
Batch  31  loss:  0.0003198968479409814
Batch  41  loss:  0.00044803047785535455
Batch  51  loss:  0.0003701915265992284
Batch  61  loss:  0.00040071504190564156
Batch  71  loss:  0.0003110974794253707
Batch  81  loss:  0.00034428786602802575
Batch  91  loss:  0.00044104442349635065
Batch  101  loss:  0.0004118042706977576
Batch  111  loss:  0.0005072837229818106
Batch  121  loss:  0.0004280350694898516
Batch  131  loss:  0.0007124328403733671
Batch  141  loss:  0.0004815055290237069
Batch  151  loss:  0.0008485454018227756
Batch  161  loss:  0.0004896707832813263
Batch  171  loss:  0.00046517752343788743
Batch  181  loss:  0.0006463135359808803
Batch  191  loss:  0.0004413368587847799
Validation on real data: 
LOSS supervised-train 0.00047563589745550414, valid 0.0004125010163988918
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0003493318217806518
Batch  11  loss:  0.0002818105276674032
Batch  21  loss:  0.0003233470779377967
Batch  31  loss:  0.0003788831818383187
Batch  41  loss:  0.0004163193516433239
Batch  51  loss:  0.00036370771704241633
Batch  61  loss:  0.0003183328954037279
Batch  71  loss:  0.00028217185172252357
Batch  81  loss:  0.0003190235001966357
Batch  91  loss:  0.0003168532275594771
Batch  101  loss:  0.00038630160270258784
Batch  111  loss:  0.0005629351362586021
Batch  121  loss:  0.00047346038627438247
Batch  131  loss:  0.0005779661587439477
Batch  141  loss:  0.0004713327216450125
Batch  151  loss:  0.0007489220588468015
Batch  161  loss:  0.0004749581858050078
Batch  171  loss:  0.00033395757782272995
Batch  181  loss:  0.0005309677799232304
Batch  191  loss:  0.00039824951090849936
Validation on real data: 
LOSS supervised-train 0.0004674957551469561, valid 0.00028104669763706625
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.00035843014484271407
Batch  11  loss:  0.00034952283021993935
Batch  21  loss:  0.00031884669442661107
Batch  31  loss:  0.0003941634495276958
Batch  41  loss:  0.000418997835367918
Batch  51  loss:  0.00031457646400667727
Batch  61  loss:  0.00030993256950750947
Batch  71  loss:  0.0003430465585552156
Batch  81  loss:  0.00039926348836161196
Batch  91  loss:  0.0004305007169023156
Batch  101  loss:  0.0005046862061135471
Batch  111  loss:  0.0004511033184826374
Batch  121  loss:  0.00037310889456421137
Batch  131  loss:  0.0006528638768941164
Batch  141  loss:  0.00048463657731190324
Batch  151  loss:  0.0008744515362195671
Batch  161  loss:  0.00036878479295410216
Batch  171  loss:  0.00036262147477827966
Batch  181  loss:  0.0005123212467879057
Batch  191  loss:  0.00045303223305381835
Validation on real data: 
LOSS supervised-train 0.000450775006756885, valid 0.00034373364178463817
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00043856684351339936
Batch  11  loss:  0.0002941636194009334
Batch  21  loss:  0.00031185633270069957
Batch  31  loss:  0.0004124671104364097
Batch  41  loss:  0.00039177818689495325
Batch  51  loss:  0.00034957699244841933
Batch  61  loss:  0.00036565656773746014
Batch  71  loss:  0.00027898073312826455
Batch  81  loss:  0.00035322210169397295
Batch  91  loss:  0.0004497168119996786
Batch  101  loss:  0.00036236594314686954
Batch  111  loss:  0.0005404071998782456
Batch  121  loss:  0.0004613756900653243
Batch  131  loss:  0.0005985659663565457
Batch  141  loss:  0.00041482484084554017
Batch  151  loss:  0.0006547109805978835
Batch  161  loss:  0.00037354647065512836
Batch  171  loss:  0.00041364392382092774
Batch  181  loss:  0.0005960453418083489
Batch  191  loss:  0.0004003759822808206
Validation on real data: 
LOSS supervised-train 0.00047275729011744263, valid 0.0003844345919787884
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0004936445620842278
Batch  11  loss:  0.00027688711998052895
Batch  21  loss:  0.00034298840910196304
Batch  31  loss:  0.0003793533833231777
Batch  41  loss:  0.000423040910391137
Batch  51  loss:  0.00032472237944602966
Batch  61  loss:  0.00028102187206968665
Batch  71  loss:  0.000246478826738894
Batch  81  loss:  0.00037934412830509245
Batch  91  loss:  0.0005897340597584844
Batch  101  loss:  0.0004169326857663691
Batch  111  loss:  0.00044677560799755156
Batch  121  loss:  0.0004334607510827482
Batch  131  loss:  0.0006115933647379279
Batch  141  loss:  0.0004806324723176658
Batch  151  loss:  0.0009485439513809979
Batch  161  loss:  0.0004185835423413664
Batch  171  loss:  0.00039630630635656416
Batch  181  loss:  0.000547902483958751
Batch  191  loss:  0.00047871944843791425
Validation on real data: 
LOSS supervised-train 0.00045554906348115764, valid 0.00039675208972766995
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00039669833495281637
Batch  11  loss:  0.00022970463032834232
Batch  21  loss:  0.00032355933217331767
Batch  31  loss:  0.00035945980926044285
Batch  41  loss:  0.0004812639963347465
Batch  51  loss:  0.000302384200040251
Batch  61  loss:  0.00033760862424969673
Batch  71  loss:  0.0003258291690144688
Batch  81  loss:  0.00037449266528710723
Batch  91  loss:  0.0003432908561080694
Batch  101  loss:  0.00048046521260403097
Batch  111  loss:  0.00045820034574717283
Batch  121  loss:  0.0003996203886345029
Batch  131  loss:  0.0005785998073406518
Batch  141  loss:  0.00047983683180063963
Batch  151  loss:  0.0009782795095816255
Batch  161  loss:  0.0004049159470014274
Batch  171  loss:  0.00037296503433026373
Batch  181  loss:  0.000539693864993751
Batch  191  loss:  0.00046857481356710196
Validation on real data: 
LOSS supervised-train 0.0004507810442009941, valid 0.0003461110172793269
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00038183125434443355
Batch  11  loss:  0.0003210946451872587
Batch  21  loss:  0.00038643094012513757
Batch  31  loss:  0.0003121194604318589
Batch  41  loss:  0.00034313573269173503
Batch  51  loss:  0.00031598928035236895
Batch  61  loss:  0.00024403742281720042
Batch  71  loss:  0.00030970940133556724
Batch  81  loss:  0.00036970412475056946
Batch  91  loss:  0.0004279669956304133
Batch  101  loss:  0.00036564416950568557
Batch  111  loss:  0.0004417449526954442
Batch  121  loss:  0.0004296999250072986
Batch  131  loss:  0.0005449892487376928
Batch  141  loss:  0.000526587653439492
Batch  151  loss:  0.0008612601086497307
Batch  161  loss:  0.000510001671500504
Batch  171  loss:  0.0004527838318608701
Batch  181  loss:  0.0005330400890670717
Batch  191  loss:  0.00033560561132617295
Validation on real data: 
LOSS supervised-train 0.00044094489145209083, valid 0.00033674429869279265
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0004046698159072548
Batch  11  loss:  0.0003467403585091233
Batch  21  loss:  0.00039150757947936654
Batch  31  loss:  0.00040173224988393486
Batch  41  loss:  0.00037613886524923146
Batch  51  loss:  0.00027284843963570893
Batch  61  loss:  0.00025036788429133594
Batch  71  loss:  0.00026345482910983264
Batch  81  loss:  0.00041543145198374987
Batch  91  loss:  0.0004207820456940681
Batch  101  loss:  0.0004241029382683337
Batch  111  loss:  0.0004693508381024003
Batch  121  loss:  0.00046969307004474103
Batch  131  loss:  0.0006631772848777473
Batch  141  loss:  0.00046171402209438384
Batch  151  loss:  0.0008961157873272896
Batch  161  loss:  0.00045938859693706036
Batch  171  loss:  0.0004437109746504575
Batch  181  loss:  0.00047350607928819954
Batch  191  loss:  0.00047547361464239657
Validation on real data: 
LOSS supervised-train 0.00044058717583538964, valid 0.00032592000206932425
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0004060239880345762
Batch  11  loss:  0.00023259092995431274
Batch  21  loss:  0.00036771371378563344
Batch  31  loss:  0.0003588740946725011
Batch  41  loss:  0.00036351519520394504
Batch  51  loss:  0.0002675010764505714
Batch  61  loss:  0.0003076115681324154
Batch  71  loss:  0.00030429268372245133
Batch  81  loss:  0.0003320365212857723
Batch  91  loss:  0.0004652095085475594
Batch  101  loss:  0.0005096743698231876
Batch  111  loss:  0.00040778383845463395
Batch  121  loss:  0.000444442848674953
Batch  131  loss:  0.000582023523747921
Batch  141  loss:  0.000553285819478333
Batch  151  loss:  0.0008026507566682994
Batch  161  loss:  0.0005138153210282326
Batch  171  loss:  0.0004217087698634714
Batch  181  loss:  0.0004711572255473584
Batch  191  loss:  0.00045592637616209686
Validation on real data: 
LOSS supervised-train 0.00043915774498600515, valid 0.0003586498205550015
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00036331292358227074
Batch  11  loss:  0.00022647199511993676
Batch  21  loss:  0.000325608387356624
Batch  31  loss:  0.00032462633680552244
Batch  41  loss:  0.00028221693355590105
Batch  51  loss:  0.00030534196412190795
Batch  61  loss:  0.00025067589012905955
Batch  71  loss:  0.0003697544161695987
Batch  81  loss:  0.0004068875568918884
Batch  91  loss:  0.0005036268848925829
Batch  101  loss:  0.0003967777593061328
Batch  111  loss:  0.0004195640212856233
Batch  121  loss:  0.0003933250845875591
Batch  131  loss:  0.0006926323403604329
Batch  141  loss:  0.0005142140435054898
Batch  151  loss:  0.0007218156824819744
Batch  161  loss:  0.00045670761028304696
Batch  171  loss:  0.00039322226075455546
Batch  181  loss:  0.000519532011821866
Batch  191  loss:  0.0003833856317214668
Validation on real data: 
LOSS supervised-train 0.00042597458115778864, valid 0.00023297304869629443
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.00037836364936083555
Batch  11  loss:  0.0003121093614026904
Batch  21  loss:  0.0003224202955607325
Batch  31  loss:  0.0003790466289501637
Batch  41  loss:  0.00036360815283842385
Batch  51  loss:  0.00036139963776804507
Batch  61  loss:  0.0003383552248124033
Batch  71  loss:  0.00031867704819887877
Batch  81  loss:  0.00037096705636940897
Batch  91  loss:  0.00048347291885875165
Batch  101  loss:  0.0003770170151256025
Batch  111  loss:  0.00043892316170968115
Batch  121  loss:  0.00036938017001375556
Batch  131  loss:  0.0006029693759046495
Batch  141  loss:  0.0005196870770305395
Batch  151  loss:  0.0007178989471867681
Batch  161  loss:  0.0005267045344226062
Batch  171  loss:  0.00040085133514367044
Batch  181  loss:  0.0005588317872025073
Batch  191  loss:  0.0004271075886208564
Validation on real data: 
LOSS supervised-train 0.00042557556211249905, valid 0.00028029605164192617
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00032315991120412946
Batch  11  loss:  0.00027648956165648997
Batch  21  loss:  0.0003027831553481519
Batch  31  loss:  0.0003448871721047908
Batch  41  loss:  0.00028637185459956527
Batch  51  loss:  0.000379692210117355
Batch  61  loss:  0.00028324101003818214
Batch  71  loss:  0.00032578676473349333
Batch  81  loss:  0.0003099505847785622
Batch  91  loss:  0.0003435923426877707
Batch  101  loss:  0.0004003196954727173
Batch  111  loss:  0.0004061186918988824
Batch  121  loss:  0.0003972382110077888
Batch  131  loss:  0.0005889267777092755
Batch  141  loss:  0.000546150840818882
Batch  151  loss:  0.0005794232129119337
Batch  161  loss:  0.0004145353159401566
Batch  171  loss:  0.00043300396646372974
Batch  181  loss:  0.0004156130598857999
Batch  191  loss:  0.00025707989698275924
Validation on real data: 
LOSS supervised-train 0.0004004433078080183, valid 0.00026819706545211375
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0004175688955001533
Batch  11  loss:  0.0002853947808034718
Batch  21  loss:  0.000354759453330189
Batch  31  loss:  0.00037759021506644785
Batch  41  loss:  0.00029611759237013757
Batch  51  loss:  0.00033660075860098004
Batch  61  loss:  0.0002696265873964876
Batch  71  loss:  0.00038910217699594796
Batch  81  loss:  0.0003965126525145024
Batch  91  loss:  0.0004046499961987138
Batch  101  loss:  0.00038072734605520964
Batch  111  loss:  0.000486074888613075
Batch  121  loss:  0.0004250359197612852
Batch  131  loss:  0.0005425114068202674
Batch  141  loss:  0.00045680537004955113
Batch  151  loss:  0.0007072464795783162
Batch  161  loss:  0.00044706440530717373
Batch  171  loss:  0.00039425818249583244
Batch  181  loss:  0.0005300923949107528
Batch  191  loss:  0.00045440575922839344
Validation on real data: 
LOSS supervised-train 0.00040911092481110245, valid 0.00026945839636027813
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00028620086959563196
Batch  11  loss:  0.00024057773407548666
Batch  21  loss:  0.00035440988722257316
Batch  31  loss:  0.00028235066565684974
Batch  41  loss:  0.0004038358456455171
Batch  51  loss:  0.0003093397244811058
Batch  61  loss:  0.00025884879869408906
Batch  71  loss:  0.00030740321381017566
Batch  81  loss:  0.00032473745523020625
Batch  91  loss:  0.00045669637620449066
Batch  101  loss:  0.0004152343317400664
Batch  111  loss:  0.0004535636107902974
Batch  121  loss:  0.0004368536756373942
Batch  131  loss:  0.0006171106942929327
Batch  141  loss:  0.0003642254159785807
Batch  151  loss:  0.0007276262040250003
Batch  161  loss:  0.000504847033880651
Batch  171  loss:  0.0003805731248576194
Batch  181  loss:  0.0005479957908391953
Batch  191  loss:  0.000343613006407395
Validation on real data: 
LOSS supervised-train 0.00040133272392267827, valid 0.00032619485864415765
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00042355916230008006
Batch  11  loss:  0.000271594530204311
Batch  21  loss:  0.0003210080321878195
Batch  31  loss:  0.00035092339385300875
Batch  41  loss:  0.0003619752824306488
Batch  51  loss:  0.00031726888846606016
Batch  61  loss:  0.00035263795871287584
Batch  71  loss:  0.0003580816846806556
Batch  81  loss:  0.00025971789727918804
Batch  91  loss:  0.00028521561762318015
Batch  101  loss:  0.00033220212208107114
Batch  111  loss:  0.0005301673081703484
Batch  121  loss:  0.00041069588041864336
Batch  131  loss:  0.0005823435494676232
Batch  141  loss:  0.0003666022385004908
Batch  151  loss:  0.0006137765594758093
Batch  161  loss:  0.000516472733579576
Batch  171  loss:  0.0004887566319666803
Batch  181  loss:  0.0005419879453256726
Batch  191  loss:  0.00040997625910677016
Validation on real data: 
LOSS supervised-train 0.000402855861466378, valid 0.0002508221659809351
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00025825054035522044
Batch  11  loss:  0.00023613506346009672
Batch  21  loss:  0.00027518393471837044
Batch  31  loss:  0.0003537462616804987
Batch  41  loss:  0.0002549613709561527
Batch  51  loss:  0.0003027370839845389
Batch  61  loss:  0.0003081461472902447
Batch  71  loss:  0.00032691130763851106
Batch  81  loss:  0.000359428784577176
Batch  91  loss:  0.0008148456690832973
Batch  101  loss:  0.0004103108076378703
Batch  111  loss:  0.00038872819277457893
Batch  121  loss:  0.00044450495624914765
Batch  131  loss:  0.0004594665952026844
Batch  141  loss:  0.0004144783306401223
Batch  151  loss:  0.0008146802429109812
Batch  161  loss:  0.000380660523660481
Batch  171  loss:  0.0003962236805818975
Batch  181  loss:  0.00048800400691106915
Batch  191  loss:  0.0003840780118480325
Validation on real data: 
LOSS supervised-train 0.00039835042145568877, valid 0.0003153722209390253
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00034443268668837845
Batch  11  loss:  0.00019905272347386926
Batch  21  loss:  0.0003130639670416713
Batch  31  loss:  0.0002893247874453664
Batch  41  loss:  0.0003232620656490326
Batch  51  loss:  0.00030247808899730444
Batch  61  loss:  0.0002605455229058862
Batch  71  loss:  0.00026526497094891965
Batch  81  loss:  0.000386649597203359
Batch  91  loss:  0.0003576483577489853
Batch  101  loss:  0.00038446320104412735
Batch  111  loss:  0.00036762605304829776
Batch  121  loss:  0.00040635603363625705
Batch  131  loss:  0.000579258194193244
Batch  141  loss:  0.0004938539932481945
Batch  151  loss:  0.0006175338057801127
Batch  161  loss:  0.0004801871255040169
Batch  171  loss:  0.0004084478714503348
Batch  181  loss:  0.000439326191553846
Batch  191  loss:  0.000358029268682003
Validation on real data: 
LOSS supervised-train 0.00038904477376490833, valid 0.00029534983332268894
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00028375533293001354
Batch  11  loss:  0.00027605422656051815
Batch  21  loss:  0.0003008972853422165
Batch  31  loss:  0.0002730715204961598
Batch  41  loss:  0.0002462544944137335
Batch  51  loss:  0.00032993173226714134
Batch  61  loss:  0.0002767862461041659
Batch  71  loss:  0.0002797665656544268
Batch  81  loss:  0.0003554587601684034
Batch  91  loss:  0.00040952389826998115
Batch  101  loss:  0.0004017261671833694
Batch  111  loss:  0.0004583248810376972
Batch  121  loss:  0.00040102729690261185
Batch  131  loss:  0.0005186817143112421
Batch  141  loss:  0.0005575210670940578
Batch  151  loss:  0.0008196692215278745
Batch  161  loss:  0.0003996476298198104
Batch  171  loss:  0.0004193115164525807
Batch  181  loss:  0.00045473655336536467
Batch  191  loss:  0.0003546435327734798
Validation on real data: 
LOSS supervised-train 0.00039712698860967067, valid 0.00031194963958114386
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00024504406610503793
Batch  11  loss:  0.0002709598047658801
Batch  21  loss:  0.00030532016535289586
Batch  31  loss:  0.0003543714410625398
Batch  41  loss:  0.00030555023113265634
Batch  51  loss:  0.00030466713360510767
Batch  61  loss:  0.00028868805384263396
Batch  71  loss:  0.0003302961413282901
Batch  81  loss:  0.0003807473985943943
Batch  91  loss:  0.000375505187548697
Batch  101  loss:  0.0004000540648121387
Batch  111  loss:  0.00036053324583917856
Batch  121  loss:  0.00030699220951646566
Batch  131  loss:  0.00048708199756219983
Batch  141  loss:  0.0004991890746168792
Batch  151  loss:  0.0007934368913993239
Batch  161  loss:  0.00037022976903244853
Batch  171  loss:  0.0003027353377547115
Batch  181  loss:  0.0003685229457914829
Batch  191  loss:  0.0003054143162444234
Validation on real data: 
LOSS supervised-train 0.00037262292702507695, valid 0.0002271819394081831
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00029475256451405585
Batch  11  loss:  0.00027329445583745837
Batch  21  loss:  0.00031703474815003574
Batch  31  loss:  0.0002363581588724628
Batch  41  loss:  0.0003101521579083055
Batch  51  loss:  0.00026669164071790874
Batch  61  loss:  0.000286623602733016
Batch  71  loss:  0.0003573528665583581
Batch  81  loss:  0.00027553061954677105
Batch  91  loss:  0.0003446896735113114
Batch  101  loss:  0.000295830745017156
Batch  111  loss:  0.0004446280363481492
Batch  121  loss:  0.00043325297883711755
Batch  131  loss:  0.00043120025657117367
Batch  141  loss:  0.00048154813703149557
Batch  151  loss:  0.0008018782828003168
Batch  161  loss:  0.000418325507780537
Batch  171  loss:  0.00036720596835948527
Batch  181  loss:  0.00042693482828326523
Batch  191  loss:  0.0003641426737885922
Validation on real data: 
LOSS supervised-train 0.000378579265307053, valid 0.00025908430689014494
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.00030793261248618364
Batch  11  loss:  0.00027993202093057334
Batch  21  loss:  0.00031404479523189366
Batch  31  loss:  0.0004126456624362618
Batch  41  loss:  0.0003087506047450006
Batch  51  loss:  0.0003026895283255726
Batch  61  loss:  0.000264412141405046
Batch  71  loss:  0.00027605853392742574
Batch  81  loss:  0.0002891738258767873
Batch  91  loss:  0.00039246882079169154
Batch  101  loss:  0.00035565425059758127
Batch  111  loss:  0.00045409766607917845
Batch  121  loss:  0.00032556435326114297
Batch  131  loss:  0.00048393523320555687
Batch  141  loss:  0.0003696046187542379
Batch  151  loss:  0.0006295216735452414
Batch  161  loss:  0.00041860475903376937
Batch  171  loss:  0.0003612283617258072
Batch  181  loss:  0.0005062186392024159
Batch  191  loss:  0.00035052598104812205
Validation on real data: 
LOSS supervised-train 0.00036746116486028766, valid 0.00025216458016075194
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00028132545412518084
Batch  11  loss:  0.00024521470186300576
Batch  21  loss:  0.0002523074217606336
Batch  31  loss:  0.00032676741830073297
Batch  41  loss:  0.00027761526871472597
Batch  51  loss:  0.0002984290767926723
Batch  61  loss:  0.0002131993678631261
Batch  71  loss:  0.00019768279162235558
Batch  81  loss:  0.0003759578394237906
Batch  91  loss:  0.0003503227198962122
Batch  101  loss:  0.000448505423264578
Batch  111  loss:  0.00041618398972786963
Batch  121  loss:  0.0004294905811548233
Batch  131  loss:  0.0004599844687618315
Batch  141  loss:  0.0005145318573340774
Batch  151  loss:  0.0008065496222116053
Batch  161  loss:  0.00039481240673922
Batch  171  loss:  0.0003201182116754353
Batch  181  loss:  0.00032718112925067544
Batch  191  loss:  0.000282469904050231
Validation on real data: 
LOSS supervised-train 0.0003668914833542658, valid 0.0003204292443115264
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.00033210936817340553
Batch  11  loss:  0.0002733812725637108
Batch  21  loss:  0.0003934447013307363
Batch  31  loss:  0.00032977148657664657
Batch  41  loss:  0.0003261140373069793
Batch  51  loss:  0.00028855481650680304
Batch  61  loss:  0.0002848943113349378
Batch  71  loss:  0.00026669990620575845
Batch  81  loss:  0.0002858439402189106
Batch  91  loss:  0.0003310543252155185
Batch  101  loss:  0.0003259490476921201
Batch  111  loss:  0.00030526265618391335
Batch  121  loss:  0.0003618735645432025
Batch  131  loss:  0.0004948543501086533
Batch  141  loss:  0.00038212272920645773
Batch  151  loss:  0.0007001349003985524
Batch  161  loss:  0.00029139945399947464
Batch  171  loss:  0.00024429152836091816
Batch  181  loss:  0.0003512095718178898
Batch  191  loss:  0.0003292753535788506
Validation on real data: 
LOSS supervised-train 0.0003667988715460524, valid 0.00024274541647173464
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00024559139274060726
Batch  11  loss:  0.00027146533830091357
Batch  21  loss:  0.000258256564848125
Batch  31  loss:  0.00030731273000128567
Batch  41  loss:  0.00030943876481615007
Batch  51  loss:  0.0002553135564085096
Batch  61  loss:  0.0002189157239627093
Batch  71  loss:  0.00022478005848824978
Batch  81  loss:  0.0002780152717605233
Batch  91  loss:  0.0004802668699994683
Batch  101  loss:  0.00036914614611305296
Batch  111  loss:  0.00046695515629835427
Batch  121  loss:  0.00035649671917781234
Batch  131  loss:  0.00047437965986318886
Batch  141  loss:  0.0004314407706260681
Batch  151  loss:  0.0006730985478498042
Batch  161  loss:  0.0003636878391262144
Batch  171  loss:  0.00027817877708002925
Batch  181  loss:  0.0003861608274746686
Batch  191  loss:  0.0002985377795994282
Validation on real data: 
LOSS supervised-train 0.00036827575255301783, valid 0.00025855586864054203
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0003022223536390811
Batch  11  loss:  0.0002504672738723457
Batch  21  loss:  0.0003374719526618719
Batch  31  loss:  0.0002676461299415678
Batch  41  loss:  0.0002508651523385197
Batch  51  loss:  0.0002657225413713604
Batch  61  loss:  0.0002937279350589961
Batch  71  loss:  0.00025393947726115584
Batch  81  loss:  0.00035125602153129876
Batch  91  loss:  0.00034654978662729263
Batch  101  loss:  0.00035808118991553783
Batch  111  loss:  0.0003707323921844363
Batch  121  loss:  0.000360860547516495
Batch  131  loss:  0.0005465818685479462
Batch  141  loss:  0.00042173019028268754
Batch  151  loss:  0.000604558561462909
Batch  161  loss:  0.0003752317279577255
Batch  171  loss:  0.00034049086389131844
Batch  181  loss:  0.0004295257094781846
Batch  191  loss:  0.00029848821577616036
Validation on real data: 
LOSS supervised-train 0.00035449441900709645, valid 0.0002703606151044369
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00025083517539314926
Batch  11  loss:  0.00030332704773172736
Batch  21  loss:  0.0002781551156658679
Batch  31  loss:  0.00029026344418525696
Batch  41  loss:  0.0002811248414218426
Batch  51  loss:  0.00032202890724875033
Batch  61  loss:  0.0002287495299242437
Batch  71  loss:  0.00027143623447045684
Batch  81  loss:  0.00032717082649469376
Batch  91  loss:  0.0003756189253181219
Batch  101  loss:  0.00038544388371519744
Batch  111  loss:  0.00045428957673721015
Batch  121  loss:  0.0003783299762289971
Batch  131  loss:  0.000447165803052485
Batch  141  loss:  0.00039476624806411564
Batch  151  loss:  0.0006504194461740553
Batch  161  loss:  0.00040625795372761786
Batch  171  loss:  0.0002699804899748415
Batch  181  loss:  0.0003497265570331365
Batch  191  loss:  0.00035455040051601827
Validation on real data: 
LOSS supervised-train 0.00035349154262803495, valid 0.0001977559004444629
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00024183183268178254
Batch  11  loss:  0.00023104134015738964
Batch  21  loss:  0.0002641219471115619
Batch  31  loss:  0.00032498035579919815
Batch  41  loss:  0.00026557184173725545
Batch  51  loss:  0.00025485141668468714
Batch  61  loss:  0.0002585606707725674
Batch  71  loss:  0.0002134135866072029
Batch  81  loss:  0.00025908093084581196
Batch  91  loss:  0.0003776756639126688
Batch  101  loss:  0.0004623674903996289
Batch  111  loss:  0.0005394063773564994
Batch  121  loss:  0.000403871905291453
Batch  131  loss:  0.0005222106701694429
Batch  141  loss:  0.0004860929329879582
Batch  151  loss:  0.000632666633464396
Batch  161  loss:  0.0003216710756532848
Batch  171  loss:  0.0002796933113131672
Batch  181  loss:  0.00034469165257178247
Batch  191  loss:  0.00022275600349530578
Validation on real data: 
LOSS supervised-train 0.00035803919527097603, valid 0.000209980207728222
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0002343010128242895
Batch  11  loss:  0.00029503152472898364
Batch  21  loss:  0.0002630797680467367
Batch  31  loss:  0.00032745383214205503
Batch  41  loss:  0.000276869599474594
Batch  51  loss:  0.0002683096390683204
Batch  61  loss:  0.00021369750902522355
Batch  71  loss:  0.00027391916955821216
Batch  81  loss:  0.00027835529181174934
Batch  91  loss:  0.0003074975102208555
Batch  101  loss:  0.0003909040242433548
Batch  111  loss:  0.00033114029793068767
Batch  121  loss:  0.0003247001732233912
Batch  131  loss:  0.0005889874300919473
Batch  141  loss:  0.0003549011016730219
Batch  151  loss:  0.0007340708980336785
Batch  161  loss:  0.0003750796604435891
Batch  171  loss:  0.0003541849728208035
Batch  181  loss:  0.000273625657428056
Batch  191  loss:  0.000258811516687274
Validation on real data: 
LOSS supervised-train 0.0003531658978317864, valid 0.0002480045077390969
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00023089769820217043
Batch  11  loss:  0.0002596711565274745
Batch  21  loss:  0.00025406639906577766
Batch  31  loss:  0.00028245587600395083
Batch  41  loss:  0.00022419232118409127
Batch  51  loss:  0.0003371487546246499
Batch  61  loss:  0.0002233532432001084
Batch  71  loss:  0.00024451626813970506
Batch  81  loss:  0.00030326348496600986
Batch  91  loss:  0.00029619826818816364
Batch  101  loss:  0.0004168672894593328
Batch  111  loss:  0.00035599624970927835
Batch  121  loss:  0.0003539047611411661
Batch  131  loss:  0.00046525790821760893
Batch  141  loss:  0.0004612170741893351
Batch  151  loss:  0.0005687237135134637
Batch  161  loss:  0.00039191782707348466
Batch  171  loss:  0.0003131120465695858
Batch  181  loss:  0.0003291545726824552
Batch  191  loss:  0.00027157741715200245
Validation on real data: 
LOSS supervised-train 0.00035514310759026557, valid 0.00024310339358635247
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00022462978085968643
Batch  11  loss:  0.0002881154650822282
Batch  21  loss:  0.0003070867678616196
Batch  31  loss:  0.0002378608623985201
Batch  41  loss:  0.0002751944412011653
Batch  51  loss:  0.0002683354541659355
Batch  61  loss:  0.0002503229188732803
Batch  71  loss:  0.0002503658761270344
Batch  81  loss:  0.00023320979380514473
Batch  91  loss:  0.0002353331947233528
Batch  101  loss:  0.00035172002390027046
Batch  111  loss:  0.00033215555595234036
Batch  121  loss:  0.00029364958754740655
Batch  131  loss:  0.00045522788423113525
Batch  141  loss:  0.00038544824928976595
Batch  151  loss:  0.0004770848318003118
Batch  161  loss:  0.0003223923849873245
Batch  171  loss:  0.00032522305264137685
Batch  181  loss:  0.00028354901587590575
Batch  191  loss:  0.0002694803988561034
Validation on real data: 
LOSS supervised-train 0.0003297427275538212, valid 0.0002456112706568092
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00022854376584291458
Batch  11  loss:  0.0003020856238435954
Batch  21  loss:  0.0002784378593787551
Batch  31  loss:  0.0002644396445248276
Batch  41  loss:  0.0002876162761822343
Batch  51  loss:  0.00022521259961649776
Batch  61  loss:  0.00023688803776167333
Batch  71  loss:  0.00027239194605499506
Batch  81  loss:  0.00029898309730924666
Batch  91  loss:  0.0003381595015525818
Batch  101  loss:  0.00032848931732587516
Batch  111  loss:  0.00038164816214703023
Batch  121  loss:  0.00033459978294558823
Batch  131  loss:  0.0005689899553544819
Batch  141  loss:  0.0003425347385928035
Batch  151  loss:  0.0006519427988678217
Batch  161  loss:  0.00038343213964253664
Batch  171  loss:  0.0002915401419159025
Batch  181  loss:  0.00031310453778132796
Batch  191  loss:  0.00026950251776725054
Validation on real data: 
LOSS supervised-train 0.00033944491442525757, valid 0.00025715609081089497
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00022620442905463278
Batch  11  loss:  0.0002803076640702784
Batch  21  loss:  0.00034732770291157067
Batch  31  loss:  0.00026497902581468225
Batch  41  loss:  0.0002461901749484241
Batch  51  loss:  0.00025794695829972625
Batch  61  loss:  0.00023463448451366276
Batch  71  loss:  0.00019780259754043072
Batch  81  loss:  0.00034200347727164626
Batch  91  loss:  0.00026922771940007806
Batch  101  loss:  0.0003900851879734546
Batch  111  loss:  0.00039862049743533134
Batch  121  loss:  0.0003371714847162366
Batch  131  loss:  0.0005360107170417905
Batch  141  loss:  0.00038385047810152173
Batch  151  loss:  0.0007354269619099796
Batch  161  loss:  0.0004727992636617273
Batch  171  loss:  0.00028300980920903385
Batch  181  loss:  0.00038197956746444106
Batch  191  loss:  0.00025429308880120516
Validation on real data: 
LOSS supervised-train 0.00034546023853181395, valid 0.00024236436001956463
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0002233380073448643
Batch  11  loss:  0.00022385947522707283
Batch  21  loss:  0.0002918653190135956
Batch  31  loss:  0.00023399751808028668
Batch  41  loss:  0.0002083516592392698
Batch  51  loss:  0.00030675524612888694
Batch  61  loss:  0.00023737765150144696
Batch  71  loss:  0.00028979958733543754
Batch  81  loss:  0.00023650474031455815
Batch  91  loss:  0.00041444538510404527
Batch  101  loss:  0.00044522961252368987
Batch  111  loss:  0.00037214666372165084
Batch  121  loss:  0.0003964913194067776
Batch  131  loss:  0.0005500700790435076
Batch  141  loss:  0.0003623442316893488
Batch  151  loss:  0.0006275095511227846
Batch  161  loss:  0.00035276784910820425
Batch  171  loss:  0.000340916943969205
Batch  181  loss:  0.00032466306583955884
Batch  191  loss:  0.0002152427041437477
Validation on real data: 
LOSS supervised-train 0.00033772005117498337, valid 0.0002084540610667318
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0002254585560876876
Batch  11  loss:  0.0002452791668474674
Batch  21  loss:  0.00021526991622522473
Batch  31  loss:  0.0003177442995365709
Batch  41  loss:  0.00027762269019149244
Batch  51  loss:  0.00032053663744591177
Batch  61  loss:  0.00023238209541887045
Batch  71  loss:  0.0002591672237031162
Batch  81  loss:  0.00027510797372087836
Batch  91  loss:  0.0004432238929439336
Batch  101  loss:  0.0003630319843068719
Batch  111  loss:  0.00030443500145338476
Batch  121  loss:  0.00039788888534530997
Batch  131  loss:  0.00039599640876986086
Batch  141  loss:  0.0003556302981451154
Batch  151  loss:  0.0006189626874402165
Batch  161  loss:  0.00034829071955755353
Batch  171  loss:  0.0002844691916834563
Batch  181  loss:  0.0003483643231447786
Batch  191  loss:  0.00024694818421266973
Validation on real data: 
LOSS supervised-train 0.000329622061617556, valid 0.0002535441308282316
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00024353295157197863
Batch  11  loss:  0.0002540703280828893
Batch  21  loss:  0.00030354136833921075
Batch  31  loss:  0.00034307263558730483
Batch  41  loss:  0.00027397804660722613
Batch  51  loss:  0.00023115605290513486
Batch  61  loss:  0.00020327612583059818
Batch  71  loss:  0.00022199870727490634
Batch  81  loss:  0.00019337981939315796
Batch  91  loss:  0.0002666647487785667
Batch  101  loss:  0.0003161456261295825
Batch  111  loss:  0.00034943080390803516
Batch  121  loss:  0.0003610671847127378
Batch  131  loss:  0.0004417717282194644
Batch  141  loss:  0.000334705546265468
Batch  151  loss:  0.0007003565551713109
Batch  161  loss:  0.00035505974665284157
Batch  171  loss:  0.0003630442952271551
Batch  181  loss:  0.0003126403607893735
Batch  191  loss:  0.0002671007241588086
Validation on real data: 
LOSS supervised-train 0.00032796393126773184, valid 0.0003056508721783757
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0003181304200552404
Batch  11  loss:  0.00031411900999955833
Batch  21  loss:  0.0002541510621085763
Batch  31  loss:  0.00032571874908171594
Batch  41  loss:  0.0002740439958870411
Batch  51  loss:  0.0002414204936940223
Batch  61  loss:  0.00024561744066886604
Batch  71  loss:  0.0002623574691824615
Batch  81  loss:  0.0002611186937429011
Batch  91  loss:  0.0002669129171408713
Batch  101  loss:  0.00038262063753791153
Batch  111  loss:  0.00037452703691087663
Batch  121  loss:  0.0003202570660505444
Batch  131  loss:  0.0003833988157566637
Batch  141  loss:  0.0003521811740938574
Batch  151  loss:  0.0005398247158154845
Batch  161  loss:  0.0003162196953780949
Batch  171  loss:  0.000331470015225932
Batch  181  loss:  0.0003783730207942426
Batch  191  loss:  0.00025738077238202095
Validation on real data: 
LOSS supervised-train 0.00032451057973958085, valid 0.00021137659496162087
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0002578066778369248
Batch  11  loss:  0.00028526753885671496
Batch  21  loss:  0.00026696056011132896
Batch  31  loss:  0.0002694547874853015
Batch  41  loss:  0.00027192142442800105
Batch  51  loss:  0.00026172964135184884
Batch  61  loss:  0.00023930164752528071
Batch  71  loss:  0.00024619471514597535
Batch  81  loss:  0.00020708615193143487
Batch  91  loss:  0.0002644808264449239
Batch  101  loss:  0.00041618512477725744
Batch  111  loss:  0.000367565342457965
Batch  121  loss:  0.00032665676553733647
Batch  131  loss:  0.0005597741692326963
Batch  141  loss:  0.00033808976877480745
Batch  151  loss:  0.00044706452172249556
Batch  161  loss:  0.00032582864514552057
Batch  171  loss:  0.0002828372234944254
Batch  181  loss:  0.00023751116532366723
Batch  191  loss:  0.0002195084816776216
Validation on real data: 
LOSS supervised-train 0.00032429177204903683, valid 0.00025560619542375207
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0002874899946618825
Batch  11  loss:  0.00033722087391652167
Batch  21  loss:  0.0002715146110858768
Batch  31  loss:  0.00027001361013390124
Batch  41  loss:  0.000295964942779392
Batch  51  loss:  0.00024494726676493883
Batch  61  loss:  0.00021644662774633616
Batch  71  loss:  0.0002137460687663406
Batch  81  loss:  0.0002642897597979754
Batch  91  loss:  0.0003311032778583467
Batch  101  loss:  0.0003726001305039972
Batch  111  loss:  0.0003762402920983732
Batch  121  loss:  0.00034027002402581275
Batch  131  loss:  0.0005087503814138472
Batch  141  loss:  0.00036864448338747025
Batch  151  loss:  0.0005444840062409639
Batch  161  loss:  0.0003402054135221988
Batch  171  loss:  0.0002780563081614673
Batch  181  loss:  0.0003404988965485245
Batch  191  loss:  0.00030655047157779336
Validation on real data: 
LOSS supervised-train 0.00033573489854461515, valid 0.00023755789152346551
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00027064172900281847
Batch  11  loss:  0.0002466017031110823
Batch  21  loss:  0.00024686160031706095
Batch  31  loss:  0.0003181149368174374
Batch  41  loss:  0.0002507321769371629
Batch  51  loss:  0.00022820862068329006
Batch  61  loss:  0.00023238128051161766
Batch  71  loss:  0.00021270030993036926
Batch  81  loss:  0.0002261418994748965
Batch  91  loss:  0.0004206816665828228
Batch  101  loss:  0.0003751800977624953
Batch  111  loss:  0.0004032453871332109
Batch  121  loss:  0.00036351679591462016
Batch  131  loss:  0.0004488991398829967
Batch  141  loss:  0.00036268727853894234
Batch  151  loss:  0.0006059127044863999
Batch  161  loss:  0.0003044531913474202
Batch  171  loss:  0.00029576229280792177
Batch  181  loss:  0.0002310498821316287
Batch  191  loss:  0.0002634328557178378
Validation on real data: 
LOSS supervised-train 0.00032880913699045777, valid 0.0002570062060840428
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00030182383488863707
Batch  11  loss:  0.0002616128767840564
Batch  21  loss:  0.00025517563335597515
Batch  31  loss:  0.0003791285853367299
Batch  41  loss:  0.00031475769355893135
Batch  51  loss:  0.00023007782874628901
Batch  61  loss:  0.00019717318355105817
Batch  71  loss:  0.00020102593407500535
Batch  81  loss:  0.0003252009628340602
Batch  91  loss:  0.00043219083454459906
Batch  101  loss:  0.0004501917865127325
Batch  111  loss:  0.00046448330977000296
Batch  121  loss:  0.0003278598887845874
Batch  131  loss:  0.00047211669152602553
Batch  141  loss:  0.0002767832193057984
Batch  151  loss:  0.0005059835966676474
Batch  161  loss:  0.0003606239624787122
Batch  171  loss:  0.0003160956548526883
Batch  181  loss:  0.0002471767656970769
Batch  191  loss:  0.00024848172324709594
Validation on real data: 
LOSS supervised-train 0.0003235718893847661, valid 0.00026497896760702133
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0002774659951683134
Batch  11  loss:  0.00025483701028861105
Batch  21  loss:  0.0002494754153303802
Batch  31  loss:  0.00023217628768179566
Batch  41  loss:  0.0002882256230805069
Batch  51  loss:  0.0002389890287304297
Batch  61  loss:  0.0002380969381192699
Batch  71  loss:  0.00024708054843358696
Batch  81  loss:  0.0002479187387507409
Batch  91  loss:  0.0003122504276689142
Batch  101  loss:  0.00040310973417945206
Batch  111  loss:  0.00031934393336996436
Batch  121  loss:  0.00027452988433651626
Batch  131  loss:  0.0005381107330322266
Batch  141  loss:  0.0003264624101575464
Batch  151  loss:  0.0005752157885581255
Batch  161  loss:  0.00040918809827417135
Batch  171  loss:  0.00023713624977972358
Batch  181  loss:  0.0003344475117046386
Batch  191  loss:  0.00023756145674269646
Validation on real data: 
LOSS supervised-train 0.00032233642239589246, valid 0.00022450379037763923
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.000249641016125679
Batch  11  loss:  0.0002951285568997264
Batch  21  loss:  0.00024875006056390703
Batch  31  loss:  0.0002767367986962199
Batch  41  loss:  0.00021448022744152695
Batch  51  loss:  0.00027877523098140955
Batch  61  loss:  0.00018748697766568512
Batch  71  loss:  0.00018995821301359683
Batch  81  loss:  0.0002679361787158996
Batch  91  loss:  0.00031307755853049457
Batch  101  loss:  0.0004103008541278541
Batch  111  loss:  0.00046274231863208115
Batch  121  loss:  0.0003138216270599514
Batch  131  loss:  0.0004301228327676654
Batch  141  loss:  0.0002921267587225884
Batch  151  loss:  0.0003999781620223075
Batch  161  loss:  0.0002633892581798136
Batch  171  loss:  0.00024494543322362006
Batch  181  loss:  0.00022999357315711677
Batch  191  loss:  0.0002442136756144464
Validation on real data: 
LOSS supervised-train 0.0003096579742850736, valid 0.00025925016961991787
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00031839177245274186
Batch  11  loss:  0.00029063134570606053
Batch  21  loss:  0.00029027581331320107
Batch  31  loss:  0.00025119000929407775
Batch  41  loss:  0.00026387133402749896
Batch  51  loss:  0.0002118442498613149
Batch  61  loss:  0.000177870606421493
Batch  71  loss:  0.00020845547260250896
Batch  81  loss:  0.00019780446018557996
Batch  91  loss:  0.0002661327598616481
Batch  101  loss:  0.0003936414432246238
Batch  111  loss:  0.00027421695995144546
Batch  121  loss:  0.0002947534085251391
Batch  131  loss:  0.000395677488995716
Batch  141  loss:  0.0003395361127331853
Batch  151  loss:  0.0005462318076752126
Batch  161  loss:  0.0003370191843714565
Batch  171  loss:  0.0003116564475931227
Batch  181  loss:  0.0002198020665673539
Batch  191  loss:  0.0002723105426412076
Validation on real data: 
LOSS supervised-train 0.00030800563436059745, valid 0.0002612120006233454
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0002488560858182609
Batch  11  loss:  0.00028508686227723956
Batch  21  loss:  0.0001887504186015576
Batch  31  loss:  0.0003575238515622914
Batch  41  loss:  0.00026706448988988996
Batch  51  loss:  0.00028187301359139383
Batch  61  loss:  0.00022406272182706743
Batch  71  loss:  0.00021183292847126722
Batch  81  loss:  0.00026528589660301805
Batch  91  loss:  0.0002828769793268293
Batch  101  loss:  0.0003434634127188474
Batch  111  loss:  0.000301067455438897
Batch  121  loss:  0.0003601259959395975
Batch  131  loss:  0.00041768100345507264
Batch  141  loss:  0.0003426463808864355
Batch  151  loss:  0.0004193198692519218
Batch  161  loss:  0.00027424152358435094
Batch  171  loss:  0.00032032220042310655
Batch  181  loss:  0.0002491413033567369
Batch  191  loss:  0.0002209492668043822
Validation on real data: 
LOSS supervised-train 0.00030812190838332756, valid 0.00026943295961245894
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00026811897987499833
Batch  11  loss:  0.0003432379162404686
Batch  21  loss:  0.0002707029052544385
Batch  31  loss:  0.00029770153923891485
Batch  41  loss:  0.00029221875593066216
Batch  51  loss:  0.00023991914349608123
Batch  61  loss:  0.0002104240847984329
Batch  71  loss:  0.0002340414357604459
Batch  81  loss:  0.00029941744287498295
Batch  91  loss:  0.0004077788325957954
Batch  101  loss:  0.0003650123253464699
Batch  111  loss:  0.00036230607656762004
Batch  121  loss:  0.00034998488263227046
Batch  131  loss:  0.0004314036632422358
Batch  141  loss:  0.0002837024803739041
Batch  151  loss:  0.0004543436225503683
Batch  161  loss:  0.00030192307895049453
Batch  171  loss:  0.00027833753847517073
Batch  181  loss:  0.0002678317541722208
Batch  191  loss:  0.0002580687578301877
Validation on real data: 
LOSS supervised-train 0.00032051836722530425, valid 0.00024782796390354633
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00025644645211286843
Batch  11  loss:  0.00025975483004003763
Batch  21  loss:  0.00029469115543179214
Batch  31  loss:  0.0002468288003001362
Batch  41  loss:  0.0003112612175755203
Batch  51  loss:  0.0002821740636136383
Batch  61  loss:  0.00020507279259618372
Batch  71  loss:  0.00022713211365044117
Batch  81  loss:  0.00024341329117305577
Batch  91  loss:  0.000305609981296584
Batch  101  loss:  0.00037446676287800074
Batch  111  loss:  0.0003589460684452206
Batch  121  loss:  0.0002644399064593017
Batch  131  loss:  0.00043308618478477
Batch  141  loss:  0.00038634910015389323
Batch  151  loss:  0.00045567183406092227
Batch  161  loss:  0.00028708684840239584
Batch  171  loss:  0.00024197871971409768
Batch  181  loss:  0.00026303966296836734
Batch  191  loss:  0.0002708670508582145
Validation on real data: 
LOSS supervised-train 0.00030721676186658444, valid 0.00022049419931136072
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00028463045600801706
Batch  11  loss:  0.00035645460593514144
Batch  21  loss:  0.00032040305086411536
Batch  31  loss:  0.00028335952083580196
Batch  41  loss:  0.0002849104057531804
Batch  51  loss:  0.00025989217101596296
Batch  61  loss:  0.00023557299573440105
Batch  71  loss:  0.0002128155465470627
Batch  81  loss:  0.00022182789689395577
Batch  91  loss:  0.0003247970307711512
Batch  101  loss:  0.00036670788540504873
Batch  111  loss:  0.00032195207313634455
Batch  121  loss:  0.00030216199229471385
Batch  131  loss:  0.0004971951711922884
Batch  141  loss:  0.0002725713711697608
Batch  151  loss:  0.0003527207882143557
Batch  161  loss:  0.00025662127882242203
Batch  171  loss:  0.000243896953179501
Batch  181  loss:  0.00027874860097654164
Batch  191  loss:  0.00024268649576697499
Validation on real data: 
LOSS supervised-train 0.0003126501904625911, valid 0.00031019310699775815
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0002677932207006961
Batch  11  loss:  0.0002673006383702159
Batch  21  loss:  0.00022504822118207812
Batch  31  loss:  0.00033828316372819245
Batch  41  loss:  0.000315287325065583
Batch  51  loss:  0.0002461681142449379
Batch  61  loss:  0.00019187778525520116
Batch  71  loss:  0.00022850505774840713
Batch  81  loss:  0.00025500456104055047
Batch  91  loss:  0.00035278021823614836
Batch  101  loss:  0.0003246291307732463
Batch  111  loss:  0.0003900516021531075
Batch  121  loss:  0.00033160470775328577
Batch  131  loss:  0.00032996974186971784
Batch  141  loss:  0.0002629258669912815
Batch  151  loss:  0.0005006705760024488
Batch  161  loss:  0.0002619882288854569
Batch  171  loss:  0.0002704281941987574
Batch  181  loss:  0.00024309860600624233
Batch  191  loss:  0.00023264971969183534
Validation on real data: 
LOSS supervised-train 0.00030863201725878753, valid 0.00023683284234721214
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0002364667598158121
Batch  11  loss:  0.0002760099887382239
Batch  21  loss:  0.000283628236502409
Batch  31  loss:  0.00024917745031416416
Batch  41  loss:  0.00022955721942707896
Batch  51  loss:  0.00026089235325343907
Batch  61  loss:  0.00021187803940847516
Batch  71  loss:  0.0002368547284277156
Batch  81  loss:  0.0002813177416101098
Batch  91  loss:  0.00030510485521517694
Batch  101  loss:  0.00040322018321603537
Batch  111  loss:  0.0003861026489175856
Batch  121  loss:  0.00034723232965916395
Batch  131  loss:  0.00034014679840765893
Batch  141  loss:  0.0003094550920650363
Batch  151  loss:  0.0004576042992994189
Batch  161  loss:  0.00031539122574031353
Batch  171  loss:  0.00023377228353638202
Batch  181  loss:  0.00025758412084542215
Batch  191  loss:  0.0003166962706018239
Validation on real data: 
LOSS supervised-train 0.0003103072291560238, valid 0.0002621931489557028
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0002810224541462958
Batch  11  loss:  0.0003083033370785415
Batch  21  loss:  0.0003794570220634341
Batch  31  loss:  0.00036552990786731243
Batch  41  loss:  0.0002725904050748795
Batch  51  loss:  0.0002292580174980685
Batch  61  loss:  0.00022419045853894204
Batch  71  loss:  0.0002691219560801983
Batch  81  loss:  0.00023757673625368625
Batch  91  loss:  0.00033205628278665245
Batch  101  loss:  0.00029826993704773486
Batch  111  loss:  0.0003035723348148167
Batch  121  loss:  0.00029297725996002555
Batch  131  loss:  0.00040109839756041765
Batch  141  loss:  0.00033494835952296853
Batch  151  loss:  0.0005495964433066547
Batch  161  loss:  0.0003130217082798481
Batch  171  loss:  0.0002725117083173245
Batch  181  loss:  0.00022461681510321796
Batch  191  loss:  0.0002726272214204073
Validation on real data: 
LOSS supervised-train 0.00030420933020650407, valid 0.00024569244123995304
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00021096671116538346
Batch  11  loss:  0.0002571130753494799
Batch  21  loss:  0.00018640536291059107
Batch  31  loss:  0.0002500245755072683
Batch  41  loss:  0.00029442415689118207
Batch  51  loss:  0.0003003384917974472
Batch  61  loss:  0.00021349522285163403
Batch  71  loss:  0.00021962336904834956
Batch  81  loss:  0.0002987316402141005
Batch  91  loss:  0.000284107169136405
Batch  101  loss:  0.0003986311494372785
Batch  111  loss:  0.00037479697493836284
Batch  121  loss:  0.00035491949529387057
Batch  131  loss:  0.00042141840094700456
Batch  141  loss:  0.00037493216223083436
Batch  151  loss:  0.0005776036414317787
Batch  161  loss:  0.0002518602996133268
Batch  171  loss:  0.0002630713570397347
Batch  181  loss:  0.00019464264914859086
Batch  191  loss:  0.0002695947769097984
Validation on real data: 
LOSS supervised-train 0.0003076928124210099, valid 0.0002427513391012326
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.000272899866104126
Batch  11  loss:  0.00021064961038064212
Batch  21  loss:  0.00022454647114500403
Batch  31  loss:  0.0002368955611018464
Batch  41  loss:  0.00019689236069098115
Batch  51  loss:  0.00018940653535537422
Batch  61  loss:  0.00023342037457041442
Batch  71  loss:  0.00017718940216582268
Batch  81  loss:  0.00024894470698200166
Batch  91  loss:  0.0002600231091491878
Batch  101  loss:  0.0003763404383789748
Batch  111  loss:  0.0003675500920508057
Batch  121  loss:  0.00028821208979934454
Batch  131  loss:  0.0003356027591507882
Batch  141  loss:  0.00034004758344963193
Batch  151  loss:  0.0003985690709669143
Batch  161  loss:  0.0002848258300218731
Batch  171  loss:  0.0002461609255988151
Batch  181  loss:  0.00026210572104901075
Batch  191  loss:  0.0002546726609580219
Validation on real data: 
LOSS supervised-train 0.0002929655264597386, valid 0.00025913171702995896
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00023270481324288994
Batch  11  loss:  0.00028579559875652194
Batch  21  loss:  0.00021177539019845426
Batch  31  loss:  0.00033013909705914557
Batch  41  loss:  0.0002733797300606966
Batch  51  loss:  0.00020276413124520332
Batch  61  loss:  0.00017723377095535398
Batch  71  loss:  0.00021399377146735787
Batch  81  loss:  0.0002461036783643067
Batch  91  loss:  0.0002805055119097233
Batch  101  loss:  0.0004143144760746509
Batch  111  loss:  0.0003384439623914659
Batch  121  loss:  0.00021813734201714396
Batch  131  loss:  0.0004250451165717095
Batch  141  loss:  0.00032789818942546844
Batch  151  loss:  0.0004404527135193348
Batch  161  loss:  0.0002983614685945213
Batch  171  loss:  0.00034946235246025026
Batch  181  loss:  0.00027752219466492534
Batch  191  loss:  0.00024775322526693344
Validation on real data: 
LOSS supervised-train 0.00030571286544727627, valid 0.00020449404837563634
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00024717985070310533
Batch  11  loss:  0.0002595026744529605
Batch  21  loss:  0.00020905713608954102
Batch  31  loss:  0.0002127920015482232
Batch  41  loss:  0.000257616164162755
Batch  51  loss:  0.00026148383039981127
Batch  61  loss:  0.00017766172823030502
Batch  71  loss:  0.00019689882174134254
Batch  81  loss:  0.0002430592867312953
Batch  91  loss:  0.0003127097734250128
Batch  101  loss:  0.0003825209860224277
Batch  111  loss:  0.0003429918142501265
Batch  121  loss:  0.00032411611755378544
Batch  131  loss:  0.000380969257093966
Batch  141  loss:  0.00035504664992913604
Batch  151  loss:  0.0004366057401057333
Batch  161  loss:  0.0002097848046105355
Batch  171  loss:  0.00032550940522924066
Batch  181  loss:  0.00021493017266038805
Batch  191  loss:  0.00031600496731698513
Validation on real data: 
LOSS supervised-train 0.00030422570860537233, valid 0.0002934852964244783
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00024506362387910485
Batch  11  loss:  0.00023220281582325697
Batch  21  loss:  0.00024109501100610942
Batch  31  loss:  0.00026875498588196933
Batch  41  loss:  0.0002513194631319493
Batch  51  loss:  0.00021039991406723857
Batch  61  loss:  0.0001922513620229438
Batch  71  loss:  0.0002153265377273783
Batch  81  loss:  0.0002478237438481301
Batch  91  loss:  0.0003369381302036345
Batch  101  loss:  0.00041899687494151294
Batch  111  loss:  0.00022594827169086784
Batch  121  loss:  0.00031523502548225224
Batch  131  loss:  0.00039485591696575284
Batch  141  loss:  0.00031912565464153886
Batch  151  loss:  0.0004509389400482178
Batch  161  loss:  0.0003437366394791752
Batch  171  loss:  0.0003242935927119106
Batch  181  loss:  0.00020671531092375517
Batch  191  loss:  0.00025351980002596974
Validation on real data: 
LOSS supervised-train 0.00029043642294709573, valid 0.00026161267305724323
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.000291259988443926
Batch  11  loss:  0.000278117397101596
Batch  21  loss:  0.0002637049474287778
Batch  31  loss:  0.00028210500022396445
Batch  41  loss:  0.00022451566474046558
Batch  51  loss:  0.0002404110855422914
Batch  61  loss:  0.00018179052858613431
Batch  71  loss:  0.0002205493947258219
Batch  81  loss:  0.0002766353718470782
Batch  91  loss:  0.0005093454383313656
Batch  101  loss:  0.00034448548103682697
Batch  111  loss:  0.00031031889375299215
Batch  121  loss:  0.00027674008742906153
Batch  131  loss:  0.00045666558435186744
Batch  141  loss:  0.0003696410567499697
Batch  151  loss:  0.00046609994024038315
Batch  161  loss:  0.00025913416175171733
Batch  171  loss:  0.00029109182651154697
Batch  181  loss:  0.00022811084636487067
Batch  191  loss:  0.00022925998200662434
Validation on real data: 
LOSS supervised-train 0.00029830193205270916, valid 0.00024795683566480875
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0002745066594798118
Batch  11  loss:  0.0002442520344629884
Batch  21  loss:  0.00023891810269560665
Batch  31  loss:  0.0002652884868439287
Batch  41  loss:  0.00022560841171070933
Batch  51  loss:  0.0002207063080277294
Batch  61  loss:  0.00019617335055954754
Batch  71  loss:  0.00021778813970740885
Batch  81  loss:  0.00028153040329925716
Batch  91  loss:  0.0002652850525919348
Batch  101  loss:  0.00032047799322754145
Batch  111  loss:  0.0003498746664263308
Batch  121  loss:  0.00032418532646261156
Batch  131  loss:  0.0004146602295804769
Batch  141  loss:  0.0002863311965484172
Batch  151  loss:  0.00047879168414510787
Batch  161  loss:  0.0002810962323565036
Batch  171  loss:  0.00027153268456459045
Batch  181  loss:  0.00019679820979945362
Batch  191  loss:  0.0002652085095178336
Validation on real data: 
LOSS supervised-train 0.00029252911830553787, valid 0.00023585581220686436
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00025379942962899804
Batch  11  loss:  0.0002541519934311509
Batch  21  loss:  0.00020155419770162553
Batch  31  loss:  0.00023512534971814603
Batch  41  loss:  0.0002385904808761552
Batch  51  loss:  0.00023306289222091436
Batch  61  loss:  0.00022047858510632068
Batch  71  loss:  0.00019767343474086374
Batch  81  loss:  0.0003013739478774369
Batch  91  loss:  0.000349786743754521
Batch  101  loss:  0.0004116111667826772
Batch  111  loss:  0.0002478880051057786
Batch  121  loss:  0.0002757379843387753
Batch  131  loss:  0.0003213410673197359
Batch  141  loss:  0.0002663281047716737
Batch  151  loss:  0.0003718126390594989
Batch  161  loss:  0.0002225484640803188
Batch  171  loss:  0.0003254213952459395
Batch  181  loss:  0.00020730908727273345
Batch  191  loss:  0.00028682619449682534
Validation on real data: 
LOSS supervised-train 0.0002897548799228389, valid 0.000265843904344365
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0002892602060455829
Batch  11  loss:  0.0002531024510972202
Batch  21  loss:  0.0002610054798424244
Batch  31  loss:  0.0002742852084338665
Batch  41  loss:  0.00025359101709909737
Batch  51  loss:  0.00026610339409671724
Batch  61  loss:  0.00020759236940648407
Batch  71  loss:  0.0002495397930033505
Batch  81  loss:  0.00036598375299945474
Batch  91  loss:  0.00040706960135139525
Batch  101  loss:  0.00043367885518819094
Batch  111  loss:  0.0003264248662162572
Batch  121  loss:  0.00027515474357642233
Batch  131  loss:  0.0004833275743294507
Batch  141  loss:  0.00031343428418040276
Batch  151  loss:  0.0003701082896441221
Batch  161  loss:  0.0002454651694279164
Batch  171  loss:  0.000264647213043645
Batch  181  loss:  0.0002819328219629824
Batch  191  loss:  0.0002876164799090475
Validation on real data: 
LOSS supervised-train 0.0002909001158695901, valid 0.00028696723165921867
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0003221261431463063
Batch  11  loss:  0.00025340341380797327
Batch  21  loss:  0.00018682699010241777
Batch  31  loss:  0.00028210709569975734
Batch  41  loss:  0.00024542154278606176
Batch  51  loss:  0.00022656991495750844
Batch  61  loss:  0.00019255485676694661
Batch  71  loss:  0.00029046827694401145
Batch  81  loss:  0.0002939845435321331
Batch  91  loss:  0.00034025305649265647
Batch  101  loss:  0.00028873220435343683
Batch  111  loss:  0.0003000035649165511
Batch  121  loss:  0.000335640215780586
Batch  131  loss:  0.00032416998874396086
Batch  141  loss:  0.0003194360469933599
Batch  151  loss:  0.0003834839735645801
Batch  161  loss:  0.0002525172312743962
Batch  171  loss:  0.00030448107281699777
Batch  181  loss:  0.00018302976968698204
Batch  191  loss:  0.00023009155120234936
Validation on real data: 
LOSS supervised-train 0.0002855096162238624, valid 0.00031974713783711195
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00023712567053735256
Batch  11  loss:  0.00020165603200439364
Batch  21  loss:  0.00018062841263599694
Batch  31  loss:  0.00023395057360175997
Batch  41  loss:  0.00022681306290905923
Batch  51  loss:  0.0002550134086050093
Batch  61  loss:  0.00023401093494612724
Batch  71  loss:  0.0002721605997066945
Batch  81  loss:  0.0002625433844514191
Batch  91  loss:  0.00019468188111204654
Batch  101  loss:  0.0002934887888841331
Batch  111  loss:  0.00023806429817341268
Batch  121  loss:  0.00020599045092239976
Batch  131  loss:  0.00025369724608026445
Batch  141  loss:  0.00023713517293799669
Batch  151  loss:  0.0003507385263219476
Batch  161  loss:  0.00022866173821967095
Batch  171  loss:  0.00031404467881657183
Batch  181  loss:  0.0002448789600748569
Batch  191  loss:  0.0002861800021491945
Validation on real data: 
LOSS supervised-train 0.0002808710253884783, valid 0.00024298109929077327
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.000242100257310085
Batch  11  loss:  0.0002164737816201523
Batch  21  loss:  0.0001837502932175994
Batch  31  loss:  0.00026893633184954524
Batch  41  loss:  0.00022493931464850903
Batch  51  loss:  0.00024052828666754067
Batch  61  loss:  0.00020037588546983898
Batch  71  loss:  0.0002592561941128224
Batch  81  loss:  0.0002777249610517174
Batch  91  loss:  0.000291245523840189
Batch  101  loss:  0.00043156504398211837
Batch  111  loss:  0.0002645394706632942
Batch  121  loss:  0.00024637021124362946
Batch  131  loss:  0.0002627872454468161
Batch  141  loss:  0.00026073941262438893
Batch  151  loss:  0.0003503963234834373
Batch  161  loss:  0.0002527913893572986
Batch  171  loss:  0.00030377611983567476
Batch  181  loss:  0.00024580227909609675
Batch  191  loss:  0.0002931722265202552
Validation on real data: 
LOSS supervised-train 0.0002813299711851869, valid 0.0002636281424202025
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.00025943509535863996
Batch  11  loss:  0.00024027966719586402
Batch  21  loss:  0.0001998926600208506
Batch  31  loss:  0.0002223139745183289
Batch  41  loss:  0.00027937281993217766
Batch  51  loss:  0.00019504719239193946
Batch  61  loss:  0.00020067587320227176
Batch  71  loss:  0.0002478186215739697
Batch  81  loss:  0.0002544748713262379
Batch  91  loss:  0.00025713752256706357
Batch  101  loss:  0.00033829931635409594
Batch  111  loss:  0.0003880862204823643
Batch  121  loss:  0.00027880407287739217
Batch  131  loss:  0.0003567876701708883
Batch  141  loss:  0.00027268347912468016
Batch  151  loss:  0.0003835909010376781
Batch  161  loss:  0.00024802880943752825
Batch  171  loss:  0.00028437376022338867
Batch  181  loss:  0.00025104189990088344
Batch  191  loss:  0.00026053664623759687
Validation on real data: 
LOSS supervised-train 0.00027622451256320346, valid 0.0002456390648148954
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00024875259259715676
Batch  11  loss:  0.00021801493130624294
Batch  21  loss:  0.00024529380607418716
Batch  31  loss:  0.0002827909484039992
Batch  41  loss:  0.00024449589545838535
Batch  51  loss:  0.00026333381538279355
Batch  61  loss:  0.00021489134815055877
Batch  71  loss:  0.00022368210193235427
Batch  81  loss:  0.0002541106950957328
Batch  91  loss:  0.00023319615866057575
Batch  101  loss:  0.00031419662991538644
Batch  111  loss:  0.000251405785093084
Batch  121  loss:  0.00018646969692781568
Batch  131  loss:  0.0003369466576259583
Batch  141  loss:  0.0003106802178081125
Batch  151  loss:  0.0003886381455231458
Batch  161  loss:  0.00022023844940122217
Batch  171  loss:  0.0002151459048036486
Batch  181  loss:  0.00022657157387584448
Batch  191  loss:  0.00023542731651104987
Validation on real data: 
LOSS supervised-train 0.0002737066922418308, valid 0.0002847512951120734
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  bottle ; Model ID: 41a2005b595ae783be1868124d5ddbcb
--------------------
Training baseline regression model:  2022-03-29 22:53:53.574488
Detector:  point_transformer
Object:  bottle
--------------------
device is  cuda
--------------------
Number of trainable parameters:  905395
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.22701719403266907
Batch  11  loss:  0.015145949088037014
Batch  21  loss:  0.010664794594049454
Batch  31  loss:  0.015731878578662872
Batch  41  loss:  0.009777677245438099
Batch  51  loss:  0.008003327995538712
Batch  61  loss:  0.007738804444670677
Batch  71  loss:  0.007503450848162174
Batch  81  loss:  0.007844371721148491
Batch  91  loss:  0.007538897916674614
Batch  101  loss:  0.007730225101113319
Batch  111  loss:  0.007147655356675386
Batch  121  loss:  0.0070918831042945385
Batch  131  loss:  0.006843622773885727
Batch  141  loss:  0.006980189122259617
Batch  151  loss:  0.00706153130158782
Batch  161  loss:  0.006537817418575287
Batch  171  loss:  0.0065782140009105206
Batch  181  loss:  0.0069648269563913345
Batch  191  loss:  0.007006869651377201
Validation on real data: 
LOSS supervised-train 0.011966440866235644, valid 0.00672873854637146
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.006740691140294075
Batch  11  loss:  0.006801560986787081
Batch  21  loss:  0.0068003092892467976
Batch  31  loss:  0.006907725241035223
Batch  41  loss:  0.006920479238033295
Batch  51  loss:  0.006792762782424688
Batch  61  loss:  0.006465919315814972
Batch  71  loss:  0.006724317092448473
Batch  81  loss:  0.007621563505381346
Batch  91  loss:  0.007112838327884674
Batch  101  loss:  0.007227867841720581
Batch  111  loss:  0.0067515247501432896
Batch  121  loss:  0.0066241491585969925
Batch  131  loss:  0.006674362812191248
Batch  141  loss:  0.0067821079865098
Batch  151  loss:  0.006929970812052488
Batch  161  loss:  0.006426158826798201
Batch  171  loss:  0.006639879196882248
Batch  181  loss:  0.006805401761084795
Batch  191  loss:  0.006736821960657835
Validation on real data: 
LOSS supervised-train 0.006773507636971772, valid 0.006627342663705349
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.006567241623997688
Batch  11  loss:  0.006698635872453451
Batch  21  loss:  0.006661117542535067
Batch  31  loss:  0.00670311925932765
Batch  41  loss:  0.006814992055296898
Batch  51  loss:  0.006857750471681356
Batch  61  loss:  0.006499496754258871
Batch  71  loss:  0.006795770954340696
Batch  81  loss:  0.00752075994387269
Batch  91  loss:  0.006911136209964752
Batch  101  loss:  0.007151117082685232
Batch  111  loss:  0.006699127610772848
Batch  121  loss:  0.006636476144194603
Batch  131  loss:  0.006634426768869162
Batch  141  loss:  0.006610661745071411
Batch  151  loss:  0.006749632302671671
Batch  161  loss:  0.0063902148976922035
Batch  171  loss:  0.006449754815548658
Batch  181  loss:  0.006708779372274876
Batch  191  loss:  0.006757216528058052
Validation on real data: 
LOSS supervised-train 0.0066925792209804056, valid 0.006475429981946945
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.006593687459826469
Batch  11  loss:  0.006580948829650879
Batch  21  loss:  0.006646682042628527
Batch  31  loss:  0.006661897525191307
Batch  41  loss:  0.006655390840023756
Batch  51  loss:  0.006702568382024765
Batch  61  loss:  0.006450190208852291
Batch  71  loss:  0.006660921964794397
Batch  81  loss:  0.007416927255690098
Batch  91  loss:  0.0068496414460241795
Batch  101  loss:  0.006983344908803701
Batch  111  loss:  0.006684181746095419
Batch  121  loss:  0.006567270494997501
Batch  131  loss:  0.006507583893835545
Batch  141  loss:  0.00663014268502593
Batch  151  loss:  0.006624085828661919
Batch  161  loss:  0.006350027862936258
Batch  171  loss:  0.006326425354927778
Batch  181  loss:  0.006666583940386772
Batch  191  loss:  0.006680031772702932
Validation on real data: 
LOSS supervised-train 0.006616308393422515, valid 0.006525383796542883
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.006448086351156235
Batch  11  loss:  0.006572806276381016
Batch  21  loss:  0.006412323564291
Batch  31  loss:  0.006569773890078068
Batch  41  loss:  0.006636349949985743
Batch  51  loss:  0.006651212461292744
Batch  61  loss:  0.006334718782454729
Batch  71  loss:  0.006681280676275492
Batch  81  loss:  0.007418291177600622
Batch  91  loss:  0.006696880329400301
Batch  101  loss:  0.006842123344540596
Batch  111  loss:  0.006477985065430403
Batch  121  loss:  0.006361585110425949
Batch  131  loss:  0.006415023468434811
Batch  141  loss:  0.00635838508605957
Batch  151  loss:  0.0065901195630431175
Batch  161  loss:  0.006228789687156677
Batch  171  loss:  0.006026966497302055
Batch  181  loss:  0.006505547557026148
Batch  191  loss:  0.006467954255640507
Validation on real data: 
LOSS supervised-train 0.0064971005381084974, valid 0.007678884547203779
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.006300636567175388
Batch  11  loss:  0.006325170863419771
Batch  21  loss:  0.0062139383517205715
Batch  31  loss:  0.006350149866193533
Batch  41  loss:  0.0062312716618180275
Batch  51  loss:  0.0062924898229539394
Batch  61  loss:  0.006167503539472818
Batch  71  loss:  0.006132752168923616
Batch  81  loss:  0.006500791292637587
Batch  91  loss:  0.006489637307822704
Batch  101  loss:  0.006145492661744356
Batch  111  loss:  0.006398587021976709
Batch  121  loss:  0.005804120562970638
Batch  131  loss:  0.005883258301764727
Batch  141  loss:  0.0061577605083584785
Batch  151  loss:  0.005567765329033136
Batch  161  loss:  0.006066314876079559
Batch  171  loss:  0.005656159948557615
Batch  181  loss:  0.006361501291394234
Batch  191  loss:  0.005681365728378296
Validation on real data: 
LOSS supervised-train 0.0062152425944805145, valid 0.00734753767028451
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.005846228450536728
Batch  11  loss:  0.006243058480322361
Batch  21  loss:  0.005612658802419901
Batch  31  loss:  0.006372782867401838
Batch  41  loss:  0.005433178972452879
Batch  51  loss:  0.005818797275424004
Batch  61  loss:  0.005638680886477232
Batch  71  loss:  0.005195317789912224
Batch  81  loss:  0.005397622473537922
Batch  91  loss:  0.005064904689788818
Batch  101  loss:  0.00622777221724391
Batch  111  loss:  0.004646822810173035
Batch  121  loss:  0.004113303497433662
Batch  131  loss:  0.0037510793190449476
Batch  141  loss:  0.005076645407825708
Batch  151  loss:  0.0045023635029792786
Batch  161  loss:  0.004079391714185476
Batch  171  loss:  0.003023645142093301
Batch  181  loss:  0.00482031749561429
Batch  191  loss:  0.004054814577102661
Validation on real data: 
LOSS supervised-train 0.0050772292050533, valid 0.03226620703935623
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.004151474218815565
Batch  11  loss:  0.004808645695447922
Batch  21  loss:  0.003574648406356573
Batch  31  loss:  0.005581978242844343
Batch  41  loss:  0.004042050335556269
Batch  51  loss:  0.003991169389337301
Batch  61  loss:  0.004110566806048155
Batch  71  loss:  0.0046024746261537075
Batch  81  loss:  0.004517875611782074
Batch  91  loss:  0.004242272116243839
Batch  101  loss:  0.005030596163123846
Batch  111  loss:  0.004023981746286154
Batch  121  loss:  0.0034046433866024017
Batch  131  loss:  0.0031043034978210926
Batch  141  loss:  0.00410305242985487
Batch  151  loss:  0.003628999460488558
Batch  161  loss:  0.0032576285302639008
Batch  171  loss:  0.002506356220692396
Batch  181  loss:  0.00327015807852149
Batch  191  loss:  0.003335881745442748
Validation on real data: 
LOSS supervised-train 0.004024799226317555, valid 0.009969212114810944
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0032228368800133467
Batch  11  loss:  0.0039045209996402264
Batch  21  loss:  0.0034575711470097303
Batch  31  loss:  0.0034335320815443993
Batch  41  loss:  0.0035053964238613844
Batch  51  loss:  0.003420744789764285
Batch  61  loss:  0.0027918689884245396
Batch  71  loss:  0.0039052197244018316
Batch  81  loss:  0.0037196523044258356
Batch  91  loss:  0.003223414532840252
Batch  101  loss:  0.005149962846189737
Batch  111  loss:  0.003183086169883609
Batch  121  loss:  0.0026556900702416897
Batch  131  loss:  0.002603460568934679
Batch  141  loss:  0.004373736679553986
Batch  151  loss:  0.0034250030294060707
Batch  161  loss:  0.0025848057121038437
Batch  171  loss:  0.0020009444560855627
Batch  181  loss:  0.0019467162201181054
Batch  191  loss:  0.0025676055811345577
Validation on real data: 
LOSS supervised-train 0.003347195108071901, valid 0.005738609936088324
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0024968625511974096
Batch  11  loss:  0.0030681234784424305
Batch  21  loss:  0.0026553438510745764
Batch  31  loss:  0.002842616755515337
Batch  41  loss:  0.0020039852242916822
Batch  51  loss:  0.0027861809358000755
Batch  61  loss:  0.0018206029199063778
Batch  71  loss:  0.002163133816793561
Batch  81  loss:  0.002822120441123843
Batch  91  loss:  0.00416203448548913
Batch  101  loss:  0.003835623851045966
Batch  111  loss:  0.0033577685244381428
Batch  121  loss:  0.0021069692447781563
Batch  131  loss:  0.004033310804516077
Batch  141  loss:  0.0032819968182593584
Batch  151  loss:  0.0027466355822980404
Batch  161  loss:  0.0020849963184446096
Batch  171  loss:  0.0022061977069824934
Batch  181  loss:  0.0018831562483683228
Batch  191  loss:  0.0013804323971271515
Validation on real data: 
LOSS supervised-train 0.0026260655105579643, valid 0.02096385881304741
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0014779844786971807
Batch  11  loss:  0.001841978868469596
Batch  21  loss:  0.00183510419446975
Batch  31  loss:  0.0030230130068957806
Batch  41  loss:  0.0015746738063171506
Batch  51  loss:  0.0026583278086036444
Batch  61  loss:  0.0019522805232554674
Batch  71  loss:  0.002177121816202998
Batch  81  loss:  0.00218657567165792
Batch  91  loss:  0.002028870163485408
Batch  101  loss:  0.0034062950871884823
Batch  111  loss:  0.002410651883110404
Batch  121  loss:  0.0023322657216340303
Batch  131  loss:  0.0029610153287649155
Batch  141  loss:  0.0028987133409827948
Batch  151  loss:  0.003096889704465866
Batch  161  loss:  0.002533507999032736
Batch  171  loss:  0.0013018762692809105
Batch  181  loss:  0.0014797606272622943
Batch  191  loss:  0.0017462600953876972
Validation on real data: 
LOSS supervised-train 0.002283203098340891, valid 0.01995553821325302
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0018687047995626926
Batch  11  loss:  0.002637899713590741
Batch  21  loss:  0.0019928403198719025
Batch  31  loss:  0.002182571915909648
Batch  41  loss:  0.0014851205050945282
Batch  51  loss:  0.0017682097386568785
Batch  61  loss:  0.0017186416080221534
Batch  71  loss:  0.0017809250857681036
Batch  81  loss:  0.002517897402867675
Batch  91  loss:  0.0022139938082545996
Batch  101  loss:  0.0025690512266010046
Batch  111  loss:  0.002386565785855055
Batch  121  loss:  0.002775958040729165
Batch  131  loss:  0.0014070769539102912
Batch  141  loss:  0.0026837685145437717
Batch  151  loss:  0.001762821921147406
Batch  161  loss:  0.0015506903873756528
Batch  171  loss:  0.0014215335249900818
Batch  181  loss:  0.0013547515263780951
Batch  191  loss:  0.0015451093204319477
Validation on real data: 
LOSS supervised-train 0.0018901598156662658, valid 0.008708996698260307
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0012612891150638461
Batch  11  loss:  0.0017725634388625622
Batch  21  loss:  0.0016164311673492193
Batch  31  loss:  0.001721903681755066
Batch  41  loss:  0.0012439715210348368
Batch  51  loss:  0.0023560761474072933
Batch  61  loss:  0.0013652386842295527
Batch  71  loss:  0.0011491631157696247
Batch  81  loss:  0.0020309542305767536
Batch  91  loss:  0.0014986576279625297
Batch  101  loss:  0.002884417772293091
Batch  111  loss:  0.001977472333237529
Batch  121  loss:  0.002261717803776264
Batch  131  loss:  0.0020382015500217676
Batch  141  loss:  0.001988545060157776
Batch  151  loss:  0.0018661129288375378
Batch  161  loss:  0.000838486710563302
Batch  171  loss:  0.0012738328659906983
Batch  181  loss:  0.001167883863672614
Batch  191  loss:  0.001646831282414496
Validation on real data: 
LOSS supervised-train 0.001645534676790703, valid 0.0025786878541111946
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0014035900821909308
Batch  11  loss:  0.0020312536507844925
Batch  21  loss:  0.0013509681448340416
Batch  31  loss:  0.0013451805571094155
Batch  41  loss:  0.001023689517751336
Batch  51  loss:  0.0015760055975988507
Batch  61  loss:  0.0014070839388296008
Batch  71  loss:  0.0018102048197761178
Batch  81  loss:  0.0018818354001268744
Batch  91  loss:  0.001277659204788506
Batch  101  loss:  0.001995474100112915
Batch  111  loss:  0.001670526573434472
Batch  121  loss:  0.001973158912733197
Batch  131  loss:  0.0011682268232107162
Batch  141  loss:  0.0016182672698050737
Batch  151  loss:  0.001082194154150784
Batch  161  loss:  0.0007706385804340243
Batch  171  loss:  0.0012696891790255904
Batch  181  loss:  0.0011494155041873455
Batch  191  loss:  0.0008527439786121249
Validation on real data: 
LOSS supervised-train 0.0014341838969266973, valid 0.00781315565109253
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0013870178954675794
Batch  11  loss:  0.0014496478252112865
Batch  21  loss:  0.0011571758659556508
Batch  31  loss:  0.0009852921357378364
Batch  41  loss:  0.001108232419937849
Batch  51  loss:  0.001370651531033218
Batch  61  loss:  0.0007975113112479448
Batch  71  loss:  0.0012012390652671456
Batch  81  loss:  0.0018168184906244278
Batch  91  loss:  0.0009731390746310353
Batch  101  loss:  0.0018244808306917548
Batch  111  loss:  0.0016233385540544987
Batch  121  loss:  0.0013063999358564615
Batch  131  loss:  0.001499260077252984
Batch  141  loss:  0.0017621666193008423
Batch  151  loss:  0.0012284598778933287
Batch  161  loss:  0.0006057705031707883
Batch  171  loss:  0.0014570631319656968
Batch  181  loss:  0.0013068532571196556
Batch  191  loss:  0.0010969516588374972
Validation on real data: 
LOSS supervised-train 0.0013720189267769456, valid 0.011444881558418274
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0009750181925483048
Batch  11  loss:  0.0015956488205119967
Batch  21  loss:  0.001033852226100862
Batch  31  loss:  0.0014052727492526174
Batch  41  loss:  0.0011265946086496115
Batch  51  loss:  0.0014564463635906577
Batch  61  loss:  0.0014060946414247155
Batch  71  loss:  0.0011906371219083667
Batch  81  loss:  0.0020145713351666927
Batch  91  loss:  0.0010574222542345524
Batch  101  loss:  0.0015154150314629078
Batch  111  loss:  0.0015892075607553124
Batch  121  loss:  0.0016498970799148083
Batch  131  loss:  0.0008613406680524349
Batch  141  loss:  0.001570971915498376
Batch  151  loss:  0.0010098092025145888
Batch  161  loss:  0.0008775602909736335
Batch  171  loss:  0.0013498780317604542
Batch  181  loss:  0.0010345408227294683
Batch  191  loss:  0.0011383970268070698
Validation on real data: 
LOSS supervised-train 0.0012607605630182662, valid 0.005663513205945492
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0010839382885023952
Batch  11  loss:  0.0013657176168635488
Batch  21  loss:  0.0009109620586968958
Batch  31  loss:  0.00139513926114887
Batch  41  loss:  0.0011417591013014317
Batch  51  loss:  0.0009325857390649617
Batch  61  loss:  0.001005039201118052
Batch  71  loss:  0.0011119971750304103
Batch  81  loss:  0.0017861080123111606
Batch  91  loss:  0.0008430379675701261
Batch  101  loss:  0.0014063060516491532
Batch  111  loss:  0.0010732586961239576
Batch  121  loss:  0.0012670459691435099
Batch  131  loss:  0.0010265869786962867
Batch  141  loss:  0.0011752302525565028
Batch  151  loss:  0.0008557486580684781
Batch  161  loss:  0.0007176766521297395
Batch  171  loss:  0.001229956978932023
Batch  181  loss:  0.0011103285942226648
Batch  191  loss:  0.0009675654582679272
Validation on real data: 
LOSS supervised-train 0.001134009230590891, valid 0.007493059150874615
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0009830346098169684
Batch  11  loss:  0.00163513224106282
Batch  21  loss:  0.0008204369805753231
Batch  31  loss:  0.001466263085603714
Batch  41  loss:  0.0008624595357105136
Batch  51  loss:  0.0011061945697292686
Batch  61  loss:  0.0009974978165701032
Batch  71  loss:  0.0012476840056478977
Batch  81  loss:  0.0017688935622572899
Batch  91  loss:  0.0009701774688437581
Batch  101  loss:  0.0012863209703937173
Batch  111  loss:  0.0008775541209615767
Batch  121  loss:  0.0011099927360191941
Batch  131  loss:  0.001094599487259984
Batch  141  loss:  0.001300427014939487
Batch  151  loss:  0.0006119570462033153
Batch  161  loss:  0.0006558724562637508
Batch  171  loss:  0.001139664906077087
Batch  181  loss:  0.0008513473439961672
Batch  191  loss:  0.0011303372448310256
Validation on real data: 
LOSS supervised-train 0.0011033972207223997, valid 0.013197332620620728
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0009349687024950981
Batch  11  loss:  0.001001847325824201
Batch  21  loss:  0.0008690246031619608
Batch  31  loss:  0.001146330963820219
Batch  41  loss:  0.0008758994517847896
Batch  51  loss:  0.0009598658652976155
Batch  61  loss:  0.000823774840682745
Batch  71  loss:  0.0012194307055324316
Batch  81  loss:  0.0017946002772077918
Batch  91  loss:  0.0008321095956489444
Batch  101  loss:  0.0012129150563850999
Batch  111  loss:  0.0008916668593883514
Batch  121  loss:  0.001531228655949235
Batch  131  loss:  0.000791058293543756
Batch  141  loss:  0.0014123950386419892
Batch  151  loss:  0.0009368653409183025
Batch  161  loss:  0.0005634077242575586
Batch  171  loss:  0.0008653679396957159
Batch  181  loss:  0.0008692587143741548
Batch  191  loss:  0.0007222540443763137
Validation on real data: 
LOSS supervised-train 0.0010739347830531188, valid 0.01386241801083088
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0007878392934799194
Batch  11  loss:  0.0006007688934914768
Batch  21  loss:  0.000882039254065603
Batch  31  loss:  0.0010477255564182997
Batch  41  loss:  0.0009913796093314886
Batch  51  loss:  0.0011740330373868346
Batch  61  loss:  0.0010184310376644135
Batch  71  loss:  0.0011612102389335632
Batch  81  loss:  0.0015806753654032946
Batch  91  loss:  0.0008266488439403474
Batch  101  loss:  0.0012584462529048324
Batch  111  loss:  0.0007297400152310729
Batch  121  loss:  0.001301088952459395
Batch  131  loss:  0.0007904189405962825
Batch  141  loss:  0.0012762093683704734
Batch  151  loss:  0.0009614065056666732
Batch  161  loss:  0.0005788899143226445
Batch  171  loss:  0.001052120584063232
Batch  181  loss:  0.0008250944665633142
Batch  191  loss:  0.000750784995034337
Validation on real data: 
LOSS supervised-train 0.0010174653737340122, valid 0.0020300571341067553
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0008420278900302947
Batch  11  loss:  0.0007935303146950901
Batch  21  loss:  0.0009452396188862622
Batch  31  loss:  0.0009766563307493925
Batch  41  loss:  0.0007026717648841441
Batch  51  loss:  0.0010832541156560183
Batch  61  loss:  0.0008579216664656997
Batch  71  loss:  0.0011528799077495933
Batch  81  loss:  0.0017392090521752834
Batch  91  loss:  0.0008876743959262967
Batch  101  loss:  0.0009902770398184657
Batch  111  loss:  0.0006543376366607845
Batch  121  loss:  0.0012445354368537664
Batch  131  loss:  0.0008775304304435849
Batch  141  loss:  0.001060866517946124
Batch  151  loss:  0.0008343608351424336
Batch  161  loss:  0.0004894441808573902
Batch  171  loss:  0.0009661652729846537
Batch  181  loss:  0.0009027938358485699
Batch  191  loss:  0.0007336976123042405
Validation on real data: 
LOSS supervised-train 0.0009525623019726482, valid 0.007996597327291965
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0007759214495308697
Batch  11  loss:  0.0007614209898747504
Batch  21  loss:  0.001025786274112761
Batch  31  loss:  0.0010973330354318023
Batch  41  loss:  0.0008293234859593213
Batch  51  loss:  0.0011425327975302935
Batch  61  loss:  0.0007403198978863657
Batch  71  loss:  0.0012571957195177674
Batch  81  loss:  0.0013822170440107584
Batch  91  loss:  0.0008150410722009838
Batch  101  loss:  0.0013857887824997306
Batch  111  loss:  0.0006914016557857394
Batch  121  loss:  0.001261445228010416
Batch  131  loss:  0.0007911702850833535
Batch  141  loss:  0.000990500906482339
Batch  151  loss:  0.0007259770063683391
Batch  161  loss:  0.0004681314458139241
Batch  171  loss:  0.0010914179729297757
Batch  181  loss:  0.000771344464737922
Batch  191  loss:  0.0006133493734523654
Validation on real data: 
LOSS supervised-train 0.0009515887490124441, valid 0.0009412884246557951
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0006236484041437507
Batch  11  loss:  0.0009739916422404349
Batch  21  loss:  0.0008324579102918506
Batch  31  loss:  0.0009357110247947276
Batch  41  loss:  0.0006194234592840075
Batch  51  loss:  0.0009446684853173792
Batch  61  loss:  0.0005925284349359572
Batch  71  loss:  0.0010157227516174316
Batch  81  loss:  0.001536842086352408
Batch  91  loss:  0.0008248890517279506
Batch  101  loss:  0.0009055199334397912
Batch  111  loss:  0.0008863832335919142
Batch  121  loss:  0.001294125453568995
Batch  131  loss:  0.0007187098381109536
Batch  141  loss:  0.0008603909518569708
Batch  151  loss:  0.0007324474863708019
Batch  161  loss:  0.0005305969389155507
Batch  171  loss:  0.0008435361087322235
Batch  181  loss:  0.0008463725098408759
Batch  191  loss:  0.0007361367461271584
Validation on real data: 
LOSS supervised-train 0.0009144719179312233, valid 0.015565826557576656
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0007359571754932404
Batch  11  loss:  0.000639544625300914
Batch  21  loss:  0.0008895309874787927
Batch  31  loss:  0.000992630491964519
Batch  41  loss:  0.0008630511583760381
Batch  51  loss:  0.0008685365319252014
Batch  61  loss:  0.0008694588323123753
Batch  71  loss:  0.001204739324748516
Batch  81  loss:  0.0013473203871399164
Batch  91  loss:  0.0007460358319804072
Batch  101  loss:  0.0010253478540107608
Batch  111  loss:  0.0006134270806796849
Batch  121  loss:  0.0014001341769471765
Batch  131  loss:  0.0008563176379539073
Batch  141  loss:  0.0009290014277212322
Batch  151  loss:  0.0006029866635799408
Batch  161  loss:  0.00040025971247814596
Batch  171  loss:  0.0008514681831002235
Batch  181  loss:  0.0006725962157361209
Batch  191  loss:  0.0005310876294970512
Validation on real data: 
LOSS supervised-train 0.0008838013863714878, valid 0.005658128298819065
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.00043940931209363043
Batch  11  loss:  0.0008224014891311526
Batch  21  loss:  0.001036850386299193
Batch  31  loss:  0.0008876423817127943
Batch  41  loss:  0.0006943456828594208
Batch  51  loss:  0.000734879809897393
Batch  61  loss:  0.0008795036119408906
Batch  71  loss:  0.0010223772842437029
Batch  81  loss:  0.0014623778406530619
Batch  91  loss:  0.0007738011772744358
Batch  101  loss:  0.0011809822171926498
Batch  111  loss:  0.00072306371293962
Batch  121  loss:  0.0014378917403519154
Batch  131  loss:  0.0006778854876756668
Batch  141  loss:  0.0009301174432039261
Batch  151  loss:  0.000532207079231739
Batch  161  loss:  0.00044067358248867095
Batch  171  loss:  0.0008836165652610362
Batch  181  loss:  0.0006595168961212039
Batch  191  loss:  0.0007650696788914502
Validation on real data: 
LOSS supervised-train 0.0008620586979668588, valid 0.003681664587929845
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0005669267848134041
Batch  11  loss:  0.000537946296390146
Batch  21  loss:  0.0009164249640889466
Batch  31  loss:  0.0008225265191867948
Batch  41  loss:  0.0008227206417359412
Batch  51  loss:  0.0008172406232915819
Batch  61  loss:  0.0009653332526795566
Batch  71  loss:  0.0012570417020469904
Batch  81  loss:  0.0012663236120715737
Batch  91  loss:  0.0009300857782363892
Batch  101  loss:  0.0009403543663211167
Batch  111  loss:  0.0007989007281139493
Batch  121  loss:  0.0015250392025336623
Batch  131  loss:  0.0009604048682376742
Batch  141  loss:  0.0009639554773457348
Batch  151  loss:  0.0006608013063669205
Batch  161  loss:  0.0005423251423053443
Batch  171  loss:  0.0008387878187932074
Batch  181  loss:  0.0008022964466363192
Batch  191  loss:  0.0005897693336009979
Validation on real data: 
LOSS supervised-train 0.0008421664412890096, valid 0.01964845508337021
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0005671569379046559
Batch  11  loss:  0.0007783673354424536
Batch  21  loss:  0.0009064755868166685
Batch  31  loss:  0.0007461035856977105
Batch  41  loss:  0.0006303424015641212
Batch  51  loss:  0.0006429888308048248
Batch  61  loss:  0.0008246526122093201
Batch  71  loss:  0.0009935245616361499
Batch  81  loss:  0.0013873305870220065
Batch  91  loss:  0.0008372699958272278
Batch  101  loss:  0.0009463776950724423
Batch  111  loss:  0.000579517160076648
Batch  121  loss:  0.0010712813818827271
Batch  131  loss:  0.0007213243516162038
Batch  141  loss:  0.000793653423897922
Batch  151  loss:  0.0005441212560981512
Batch  161  loss:  0.0005419280496425927
Batch  171  loss:  0.0007047453545965254
Batch  181  loss:  0.0005509870243258774
Batch  191  loss:  0.0006051489035598934
Validation on real data: 
LOSS supervised-train 0.0008010906731942669, valid 0.00677376938983798
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0006978358142077923
Batch  11  loss:  0.0008858859073370695
Batch  21  loss:  0.0008877810905687511
Batch  31  loss:  0.0007584576378576458
Batch  41  loss:  0.0006317762890830636
Batch  51  loss:  0.0006701305392198265
Batch  61  loss:  0.0008811639272607863
Batch  71  loss:  0.0012174821458756924
Batch  81  loss:  0.001310640713199973
Batch  91  loss:  0.0007690618513152003
Batch  101  loss:  0.0009500994347035885
Batch  111  loss:  0.000538741413038224
Batch  121  loss:  0.0013450294500216842
Batch  131  loss:  0.000731044274289161
Batch  141  loss:  0.0008713583229109645
Batch  151  loss:  0.0006129861576482654
Batch  161  loss:  0.0004156975483056158
Batch  171  loss:  0.0005796608747914433
Batch  181  loss:  0.0006513410480692983
Batch  191  loss:  0.0004815129213966429
Validation on real data: 
LOSS supervised-train 0.0008078127953922376, valid 0.008134135976433754
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0005907785962335765
Batch  11  loss:  0.000625220884103328
Batch  21  loss:  0.0007386112120002508
Batch  31  loss:  0.000636590993963182
Batch  41  loss:  0.0007677206303924322
Batch  51  loss:  0.000714785885065794
Batch  61  loss:  0.0008309289696626365
Batch  71  loss:  0.0009744823328219354
Batch  81  loss:  0.0011767888208851218
Batch  91  loss:  0.0005671567632816732
Batch  101  loss:  0.0009752903133630753
Batch  111  loss:  0.0005007796571590006
Batch  121  loss:  0.001218321267515421
Batch  131  loss:  0.0006181732169352472
Batch  141  loss:  0.0009369661565870047
Batch  151  loss:  0.000725948775652796
Batch  161  loss:  0.00043481640750542283
Batch  171  loss:  0.0006137820309959352
Batch  181  loss:  0.0008813212625682354
Batch  191  loss:  0.0005129429628141224
Validation on real data: 
LOSS supervised-train 0.000763480557652656, valid 0.005172467790544033
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0005063736462034285
Batch  11  loss:  0.0006146614323370159
Batch  21  loss:  0.0008189982618205249
Batch  31  loss:  0.0007448024116456509
Batch  41  loss:  0.0007586300489492714
Batch  51  loss:  0.0005929562030360103
Batch  61  loss:  0.0007592120091430843
Batch  71  loss:  0.0010957602644339204
Batch  81  loss:  0.0011680565075948834
Batch  91  loss:  0.0006098076701164246
Batch  101  loss:  0.0008656689315102994
Batch  111  loss:  0.0004488248669076711
Batch  121  loss:  0.001459128106944263
Batch  131  loss:  0.0005381643422879279
Batch  141  loss:  0.0006699924124404788
Batch  151  loss:  0.0005881841061636806
Batch  161  loss:  0.0003337716334499419
Batch  171  loss:  0.0006434684619307518
Batch  181  loss:  0.000833774683997035
Batch  191  loss:  0.0004601535911206156
Validation on real data: 
LOSS supervised-train 0.0007447584441979415, valid 0.0019509457051753998
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0005912907654419541
Batch  11  loss:  0.0006737038493156433
Batch  21  loss:  0.000655696727335453
Batch  31  loss:  0.0006027354393154383
Batch  41  loss:  0.0008022869005799294
Batch  51  loss:  0.0004958491772413254
Batch  61  loss:  0.00083780731074512
Batch  71  loss:  0.0008636683924123645
Batch  81  loss:  0.0010264043230563402
Batch  91  loss:  0.0005738858017139137
Batch  101  loss:  0.000861556560266763
Batch  111  loss:  0.0005506313755176961
Batch  121  loss:  0.0010558342328295112
Batch  131  loss:  0.0006486994680017233
Batch  141  loss:  0.0010112379677593708
Batch  151  loss:  0.0005831747548654675
Batch  161  loss:  0.0003142273344565183
Batch  171  loss:  0.0006052213138900697
Batch  181  loss:  0.0008054362260736525
Batch  191  loss:  0.0005020925891585648
Validation on real data: 
LOSS supervised-train 0.0007109415723243729, valid 0.00520677212625742
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0004110455047339201
Batch  11  loss:  0.0006128375534899533
Batch  21  loss:  0.0006559066823683679
Batch  31  loss:  0.0006941086030565202
Batch  41  loss:  0.0005900104879401624
Batch  51  loss:  0.0005078900721855462
Batch  61  loss:  0.0007522080559283495
Batch  71  loss:  0.0007678217370994389
Batch  81  loss:  0.001066279481165111
Batch  91  loss:  0.0006679895450361073
Batch  101  loss:  0.0006485434714704752
Batch  111  loss:  0.00041489917202852666
Batch  121  loss:  0.0009719340014271438
Batch  131  loss:  0.0005195171106606722
Batch  141  loss:  0.0008997436962090433
Batch  151  loss:  0.0005386258126236498
Batch  161  loss:  0.00034010683884844184
Batch  171  loss:  0.0004908046103082597
Batch  181  loss:  0.0008928845636546612
Batch  191  loss:  0.00046991946874186397
Validation on real data: 
LOSS supervised-train 0.0006849694388802164, valid 0.0021028220653533936
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0004229808400850743
Batch  11  loss:  0.0007494103629142046
Batch  21  loss:  0.00044526680721901357
Batch  31  loss:  0.0005104357842355967
Batch  41  loss:  0.0006250167498365045
Batch  51  loss:  0.00048290699487552047
Batch  61  loss:  0.0005885800928808749
Batch  71  loss:  0.0007786357309669256
Batch  81  loss:  0.0009381694835610688
Batch  91  loss:  0.0005619925796054304
Batch  101  loss:  0.0007055808673612773
Batch  111  loss:  0.00043576149619184434
Batch  121  loss:  0.0011918218806385994
Batch  131  loss:  0.0007220677216537297
Batch  141  loss:  0.0007760852458886802
Batch  151  loss:  0.00039290677523240447
Batch  161  loss:  0.0004133305628784001
Batch  171  loss:  0.0006159284384921193
Batch  181  loss:  0.0006634984747506678
Batch  191  loss:  0.00040173702291212976
Validation on real data: 
LOSS supervised-train 0.0006724568900244776, valid 0.0046797338873147964
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00046764136641286314
Batch  11  loss:  0.0006986108492128551
Batch  21  loss:  0.0006518345326185226
Batch  31  loss:  0.0005239899619482458
Batch  41  loss:  0.0007147076539695263
Batch  51  loss:  0.0005187406786717474
Batch  61  loss:  0.0006103716441430151
Batch  71  loss:  0.0008249153033830225
Batch  81  loss:  0.00102722248993814
Batch  91  loss:  0.000520210771355778
Batch  101  loss:  0.000814112601801753
Batch  111  loss:  0.00047079406795091927
Batch  121  loss:  0.0009619509801268578
Batch  131  loss:  0.00045823765685781837
Batch  141  loss:  0.0009330959292128682
Batch  151  loss:  0.0005840850644744933
Batch  161  loss:  0.00030646531376987696
Batch  171  loss:  0.0005611730739474297
Batch  181  loss:  0.0006185039528645575
Batch  191  loss:  0.00038121998659335077
Validation on real data: 
LOSS supervised-train 0.000653025007341057, valid 0.005207152105867863
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0004210328625049442
Batch  11  loss:  0.0006219478091225028
Batch  21  loss:  0.000592399388551712
Batch  31  loss:  0.0005549669731408358
Batch  41  loss:  0.0007058470509946346
Batch  51  loss:  0.0005048636812716722
Batch  61  loss:  0.0005231155082583427
Batch  71  loss:  0.0008803711389191449
Batch  81  loss:  0.0010776104172691703
Batch  91  loss:  0.0005882942932657897
Batch  101  loss:  0.000668150489218533
Batch  111  loss:  0.0004410857509355992
Batch  121  loss:  0.0010970566654577851
Batch  131  loss:  0.0006459398427978158
Batch  141  loss:  0.0007944751996546984
Batch  151  loss:  0.00043709660531021655
Batch  161  loss:  0.00031111485441215336
Batch  171  loss:  0.00045112433144822717
Batch  181  loss:  0.0006471346714533865
Batch  191  loss:  0.0003757925296667963
Validation on real data: 
LOSS supervised-train 0.0006559680715145078, valid 0.002938100602477789
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00041701606824062765
Batch  11  loss:  0.0005528297042474151
Batch  21  loss:  0.000663381943013519
Batch  31  loss:  0.0005621936288662255
Batch  41  loss:  0.000767469231504947
Batch  51  loss:  0.0005028174491599202
Batch  61  loss:  0.0005148979835212231
Batch  71  loss:  0.0006959539605304599
Batch  81  loss:  0.0008381269872188568
Batch  91  loss:  0.0006952596013434231
Batch  101  loss:  0.0007000158075243235
Batch  111  loss:  0.00038131041219457984
Batch  121  loss:  0.0008590545039623976
Batch  131  loss:  0.00048029093886725605
Batch  141  loss:  0.0007739958236925304
Batch  151  loss:  0.00045307897380553186
Batch  161  loss:  0.00040537139284424484
Batch  171  loss:  0.0003508113732095808
Batch  181  loss:  0.0007317871204577386
Batch  191  loss:  0.0004186358710285276
Validation on real data: 
LOSS supervised-train 0.0006337231823999901, valid 0.005369286052882671
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0003564432554412633
Batch  11  loss:  0.0005332034779712558
Batch  21  loss:  0.0006511419196613133
Batch  31  loss:  0.0006566905067302287
Batch  41  loss:  0.0006135932635515928
Batch  51  loss:  0.0005377986817620695
Batch  61  loss:  0.0005779342027381063
Batch  71  loss:  0.0008979722042568028
Batch  81  loss:  0.0008943916764110327
Batch  91  loss:  0.0006024845060892403
Batch  101  loss:  0.0008493380155414343
Batch  111  loss:  0.0003913318505510688
Batch  121  loss:  0.0010558048961684108
Batch  131  loss:  0.0004772942920681089
Batch  141  loss:  0.0007197069353424013
Batch  151  loss:  0.0004494345048442483
Batch  161  loss:  0.0003012539236806333
Batch  171  loss:  0.00046743120765313506
Batch  181  loss:  0.0006265396950766444
Batch  191  loss:  0.00041205305024050176
Validation on real data: 
LOSS supervised-train 0.000606966987834312, valid 0.002465003402903676
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00042668916285037994
Batch  11  loss:  0.0003914348199032247
Batch  21  loss:  0.0005922790733166039
Batch  31  loss:  0.00044088735012337565
Batch  41  loss:  0.0006707805441692472
Batch  51  loss:  0.0003379768750164658
Batch  61  loss:  0.0005373067106120288
Batch  71  loss:  0.0008746848325245082
Batch  81  loss:  0.0009681513765826821
Batch  91  loss:  0.0005731002311222255
Batch  101  loss:  0.0006083883927203715
Batch  111  loss:  0.0004458129988051951
Batch  121  loss:  0.0007782191387377679
Batch  131  loss:  0.0005123213632032275
Batch  141  loss:  0.0006977464072406292
Batch  151  loss:  0.0005057639209553599
Batch  161  loss:  0.0002959506236948073
Batch  171  loss:  0.0005788026028312743
Batch  181  loss:  0.0006526221986860037
Batch  191  loss:  0.00037237367359921336
Validation on real data: 
LOSS supervised-train 0.0006109250424196944, valid 0.005074074491858482
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0004305728361941874
Batch  11  loss:  0.0003958566812798381
Batch  21  loss:  0.0005191134405322373
Batch  31  loss:  0.0007772773969918489
Batch  41  loss:  0.0005533084040507674
Batch  51  loss:  0.0004835578438360244
Batch  61  loss:  0.0005937727983109653
Batch  71  loss:  0.0006946945213712752
Batch  81  loss:  0.0007877214229665697
Batch  91  loss:  0.0005300987977534533
Batch  101  loss:  0.0006843770970590413
Batch  111  loss:  0.00037250565947033465
Batch  121  loss:  0.0008174541871994734
Batch  131  loss:  0.00043606548570096493
Batch  141  loss:  0.0008917658706195652
Batch  151  loss:  0.00034193010651506484
Batch  161  loss:  0.0003268904983997345
Batch  171  loss:  0.0004836009466089308
Batch  181  loss:  0.0006250092410482466
Batch  191  loss:  0.0004432035202626139
Validation on real data: 
LOSS supervised-train 0.0005784224388480652, valid 0.007648286875337362
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00042758334893733263
Batch  11  loss:  0.000449353305157274
Batch  21  loss:  0.0005728265969082713
Batch  31  loss:  0.00045401183888316154
Batch  41  loss:  0.0005430124583654106
Batch  51  loss:  0.0004227252211421728
Batch  61  loss:  0.0006196515751071274
Batch  71  loss:  0.0006013567326590419
Batch  81  loss:  0.0008399710059165955
Batch  91  loss:  0.0006696496857330203
Batch  101  loss:  0.0005741444765590131
Batch  111  loss:  0.0004415246658027172
Batch  121  loss:  0.0009567634551785886
Batch  131  loss:  0.0004747753555420786
Batch  141  loss:  0.0006123799830675125
Batch  151  loss:  0.00045663121272809803
Batch  161  loss:  0.00027809536550194025
Batch  171  loss:  0.0004258911940269172
Batch  181  loss:  0.0006763276178389788
Batch  191  loss:  0.00030675396556034684
Validation on real data: 
LOSS supervised-train 0.0005742065722006373, valid 0.003133803606033325
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.00045400275848805904
Batch  11  loss:  0.000438092858530581
Batch  21  loss:  0.00041413193685002625
Batch  31  loss:  0.00046280259266495705
Batch  41  loss:  0.0005346964462660253
Batch  51  loss:  0.0005425363779067993
Batch  61  loss:  0.0005317573086358607
Batch  71  loss:  0.0007218153332360089
Batch  81  loss:  0.0008751988061703742
Batch  91  loss:  0.0006421497091650963
Batch  101  loss:  0.0007976244087330997
Batch  111  loss:  0.0004572714678943157
Batch  121  loss:  0.0009054691763594747
Batch  131  loss:  0.00033065315801650286
Batch  141  loss:  0.000868564355187118
Batch  151  loss:  0.0005890676402486861
Batch  161  loss:  0.0002470231556799263
Batch  171  loss:  0.0004268862830940634
Batch  181  loss:  0.0005863792030140758
Batch  191  loss:  0.0003775657678488642
Validation on real data: 
LOSS supervised-train 0.0005841521758702584, valid 0.004073534160852432
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00031737424433231354
Batch  11  loss:  0.00046097664744593203
Batch  21  loss:  0.0004932900192216039
Batch  31  loss:  0.0004548940632957965
Batch  41  loss:  0.0006424298626370728
Batch  51  loss:  0.00040651074959896505
Batch  61  loss:  0.0005392038729041815
Batch  71  loss:  0.000791700673289597
Batch  81  loss:  0.0008239515009336174
Batch  91  loss:  0.0005300628254190087
Batch  101  loss:  0.0005540034617297351
Batch  111  loss:  0.00038004378438927233
Batch  121  loss:  0.0007834809948690236
Batch  131  loss:  0.00033804282429628074
Batch  141  loss:  0.0007035690359771252
Batch  151  loss:  0.00043841145816259086
Batch  161  loss:  0.00023229759244713932
Batch  171  loss:  0.00038372716517187655
Batch  181  loss:  0.0005734501755796373
Batch  191  loss:  0.00035084731644019485
Validation on real data: 
LOSS supervised-train 0.0005587538795225555, valid 0.0015182485803961754
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00039996186387725174
Batch  11  loss:  0.00037393372622318566
Batch  21  loss:  0.0005856622010469437
Batch  31  loss:  0.0005446693976409733
Batch  41  loss:  0.0004899172345176339
Batch  51  loss:  0.0003624579985626042
Batch  61  loss:  0.0005358445923775434
Batch  71  loss:  0.0006728869047947228
Batch  81  loss:  0.0008350936695933342
Batch  91  loss:  0.0005573629750870168
Batch  101  loss:  0.0005920521216467023
Batch  111  loss:  0.0002607779169920832
Batch  121  loss:  0.0008182617020793259
Batch  131  loss:  0.00038117155781947076
Batch  141  loss:  0.0006362283602356911
Batch  151  loss:  0.000541997083928436
Batch  161  loss:  0.0003743004344869405
Batch  171  loss:  0.0003979786124546081
Batch  181  loss:  0.00047338916920125484
Batch  191  loss:  0.0003137061430606991
Validation on real data: 
LOSS supervised-train 0.0005521694920025766, valid 0.004851647652685642
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00041208340553566813
Batch  11  loss:  0.00040088474634103477
Batch  21  loss:  0.0005843476392328739
Batch  31  loss:  0.0006109729874879122
Batch  41  loss:  0.0005843808175995946
Batch  51  loss:  0.0003359726688358933
Batch  61  loss:  0.0005306699895299971
Batch  71  loss:  0.0006073784898035228
Batch  81  loss:  0.0008322631474584341
Batch  91  loss:  0.0004339739680290222
Batch  101  loss:  0.0005336751928552985
Batch  111  loss:  0.0003318900999147445
Batch  121  loss:  0.0010821804171428084
Batch  131  loss:  0.00032449106220155954
Batch  141  loss:  0.0006533178384415805
Batch  151  loss:  0.0004403338534757495
Batch  161  loss:  0.00028354732785373926
Batch  171  loss:  0.0004550229641608894
Batch  181  loss:  0.00046872542588971555
Batch  191  loss:  0.0003517706354614347
Validation on real data: 
LOSS supervised-train 0.0005284988601488294, valid 0.0017715502763167024
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00040696829091757536
Batch  11  loss:  0.00036327747511677444
Batch  21  loss:  0.0006013691890984774
Batch  31  loss:  0.0004397022712510079
Batch  41  loss:  0.0006605696398764849
Batch  51  loss:  0.0004163933335803449
Batch  61  loss:  0.00054175965487957
Batch  71  loss:  0.000661536818370223
Batch  81  loss:  0.0007981277303770185
Batch  91  loss:  0.0005650906823575497
Batch  101  loss:  0.0006168955587781966
Batch  111  loss:  0.00035613909130916
Batch  121  loss:  0.0008813662570901215
Batch  131  loss:  0.00034420687006786466
Batch  141  loss:  0.0006039214204065502
Batch  151  loss:  0.00043052627006545663
Batch  161  loss:  0.0003660247311927378
Batch  171  loss:  0.00043791381176561117
Batch  181  loss:  0.0005351778818294406
Batch  191  loss:  0.0003517149016261101
Validation on real data: 
LOSS supervised-train 0.0005351265480567235, valid 0.0022938773036003113
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00035801000194624066
Batch  11  loss:  0.0003374435764271766
Batch  21  loss:  0.0004527551936917007
Batch  31  loss:  0.0004896036698482931
Batch  41  loss:  0.0004220130213070661
Batch  51  loss:  0.00048085922026075423
Batch  61  loss:  0.00039802759420126677
Batch  71  loss:  0.0005627015489153564
Batch  81  loss:  0.000723200268112123
Batch  91  loss:  0.0005138129345141351
Batch  101  loss:  0.0008503225981257856
Batch  111  loss:  0.0003147242241539061
Batch  121  loss:  0.0008459105156362057
Batch  131  loss:  0.0003114896244369447
Batch  141  loss:  0.0006416699034161866
Batch  151  loss:  0.0004917806363664567
Batch  161  loss:  0.00027206516824662685
Batch  171  loss:  0.00041108953882940114
Batch  181  loss:  0.0004078346537426114
Batch  191  loss:  0.00028271120390854776
Validation on real data: 
LOSS supervised-train 0.0005162071462109452, valid 0.0013976308982819319
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0004973053582943976
Batch  11  loss:  0.00045342204975895584
Batch  21  loss:  0.0004466199898160994
Batch  31  loss:  0.0004036200698465109
Batch  41  loss:  0.0004671805363614112
Batch  51  loss:  0.0003714858612511307
Batch  61  loss:  0.00047849712427705526
Batch  71  loss:  0.0005892388289794326
Batch  81  loss:  0.0008400113438256085
Batch  91  loss:  0.0005097312387079
Batch  101  loss:  0.0005955257220193744
Batch  111  loss:  0.0002378554199822247
Batch  121  loss:  0.0007854856085032225
Batch  131  loss:  0.000397389754652977
Batch  141  loss:  0.0005263350903987885
Batch  151  loss:  0.000392866728361696
Batch  161  loss:  0.00022908893879503012
Batch  171  loss:  0.0004056112375110388
Batch  181  loss:  0.00046106550144031644
Batch  191  loss:  0.0002870552125386894
Validation on real data: 
LOSS supervised-train 0.0005060335199959809, valid 0.0043968865647912025
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00039422919508069754
Batch  11  loss:  0.0004171933978796005
Batch  21  loss:  0.0004611538315657526
Batch  31  loss:  0.0004421684134285897
Batch  41  loss:  0.00047692967928014696
Batch  51  loss:  0.00030415839864872396
Batch  61  loss:  0.000535778293851763
Batch  71  loss:  0.0006466723862104118
Batch  81  loss:  0.0007661853451281786
Batch  91  loss:  0.0006384365260601044
Batch  101  loss:  0.0005476695368997753
Batch  111  loss:  0.0003314038913231343
Batch  121  loss:  0.0008530648774467409
Batch  131  loss:  0.0003510250826366246
Batch  141  loss:  0.0006565594812855124
Batch  151  loss:  0.00040416730917058885
Batch  161  loss:  0.000242554466240108
Batch  171  loss:  0.00036759948125109076
Batch  181  loss:  0.000574534060433507
Batch  191  loss:  0.0002614372060634196
Validation on real data: 
LOSS supervised-train 0.000514301402727142, valid 0.0010167515138164163
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0003892575914505869
Batch  11  loss:  0.0003926178615074605
Batch  21  loss:  0.00047486575203947723
Batch  31  loss:  0.00036155199632048607
Batch  41  loss:  0.00042512660729698837
Batch  51  loss:  0.00025708990870043635
Batch  61  loss:  0.0005886791041120887
Batch  71  loss:  0.0005605148617178202
Batch  81  loss:  0.0008433180046267807
Batch  91  loss:  0.00044612452620640397
Batch  101  loss:  0.0006011981167830527
Batch  111  loss:  0.0003242353559471667
Batch  121  loss:  0.0008590139332227409
Batch  131  loss:  0.0004199261893518269
Batch  141  loss:  0.0005299312761053443
Batch  151  loss:  0.00043103922507725656
Batch  161  loss:  0.00024098098219837993
Batch  171  loss:  0.000402285746531561
Batch  181  loss:  0.0004547979624476284
Batch  191  loss:  0.0002431419416097924
Validation on real data: 
LOSS supervised-train 0.0004970395585405641, valid 0.0003836990217678249
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00030321881058625877
Batch  11  loss:  0.0003982258203905076
Batch  21  loss:  0.0004599002713803202
Batch  31  loss:  0.000439636962255463
Batch  41  loss:  0.0006570348050445318
Batch  51  loss:  0.000278867402812466
Batch  61  loss:  0.0004926703404635191
Batch  71  loss:  0.0005831269081681967
Batch  81  loss:  0.0007811607210896909
Batch  91  loss:  0.0005001007812097669
Batch  101  loss:  0.0007235100492835045
Batch  111  loss:  0.00034459828748367727
Batch  121  loss:  0.0007531538722105324
Batch  131  loss:  0.0003585711820051074
Batch  141  loss:  0.0005694026476703584
Batch  151  loss:  0.0003894010151270777
Batch  161  loss:  0.0002811287413351238
Batch  171  loss:  0.00039525923784822226
Batch  181  loss:  0.0005775259342044592
Batch  191  loss:  0.0002808492281474173
Validation on real data: 
LOSS supervised-train 0.0004863715650571976, valid 0.0020270729437470436
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00039415876381099224
Batch  11  loss:  0.0004846245574299246
Batch  21  loss:  0.00044638532563112676
Batch  31  loss:  0.0005347946425899863
Batch  41  loss:  0.00047320520388893783
Batch  51  loss:  0.0003094030253123492
Batch  61  loss:  0.0004521526279859245
Batch  71  loss:  0.0006120754405856133
Batch  81  loss:  0.0007747350609861314
Batch  91  loss:  0.0005233934498392045
Batch  101  loss:  0.000513718812726438
Batch  111  loss:  0.00029507948784157634
Batch  121  loss:  0.0008010024903342128
Batch  131  loss:  0.00032614119118079543
Batch  141  loss:  0.0007045610109344125
Batch  151  loss:  0.00033574618282727897
Batch  161  loss:  0.00028809564537368715
Batch  171  loss:  0.0003643978270702064
Batch  181  loss:  0.00044714074465446174
Batch  191  loss:  0.0002829779696185142
Validation on real data: 
LOSS supervised-train 0.0004845244351599831, valid 0.0004359057638794184
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00037654052721336484
Batch  11  loss:  0.0004650781920645386
Batch  21  loss:  0.00037713421625085175
Batch  31  loss:  0.0004649216716643423
Batch  41  loss:  0.0004769164079334587
Batch  51  loss:  0.0003427965857554227
Batch  61  loss:  0.00040876693674363196
Batch  71  loss:  0.0007016339222900569
Batch  81  loss:  0.0007052370929159224
Batch  91  loss:  0.000446880905656144
Batch  101  loss:  0.0005559705314226449
Batch  111  loss:  0.00033746447297744453
Batch  121  loss:  0.0008701616316102445
Batch  131  loss:  0.0003367964527569711
Batch  141  loss:  0.0005059666582383215
Batch  151  loss:  0.0003730439639184624
Batch  161  loss:  0.00019172541215084493
Batch  171  loss:  0.0004655615193769336
Batch  181  loss:  0.00045431902981363237
Batch  191  loss:  0.00025809562066569924
Validation on real data: 
LOSS supervised-train 0.0004746109837287804, valid 0.0014872717438265681
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00032891309820115566
Batch  11  loss:  0.00046949548413977027
Batch  21  loss:  0.00039586599450558424
Batch  31  loss:  0.00043770112097263336
Batch  41  loss:  0.0003635560569819063
Batch  51  loss:  0.000352341536199674
Batch  61  loss:  0.0005235587595961988
Batch  71  loss:  0.0005672391853295267
Batch  81  loss:  0.0007716008694842458
Batch  91  loss:  0.0005211550160311162
Batch  101  loss:  0.0005369618302211165
Batch  111  loss:  0.0003168365219607949
Batch  121  loss:  0.0006918152794241905
Batch  131  loss:  0.00022071863349992782
Batch  141  loss:  0.0005554851959459484
Batch  151  loss:  0.00034549087285995483
Batch  161  loss:  0.00023052665346767753
Batch  171  loss:  0.0003314808418508619
Batch  181  loss:  0.0005132595542818308
Batch  191  loss:  0.00029124916181899607
Validation on real data: 
LOSS supervised-train 0.0004606904368847609, valid 0.0018697320483624935
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00032316186116077006
Batch  11  loss:  0.00031315474188886583
Batch  21  loss:  0.0004262636066414416
Batch  31  loss:  0.00042748352279886603
Batch  41  loss:  0.0004967302666045725
Batch  51  loss:  0.0003724271082319319
Batch  61  loss:  0.00041524614789523184
Batch  71  loss:  0.000622051244135946
Batch  81  loss:  0.0007099775830283761
Batch  91  loss:  0.0005063088028691709
Batch  101  loss:  0.0006000928697176278
Batch  111  loss:  0.00030088654602877796
Batch  121  loss:  0.0006451464141719043
Batch  131  loss:  0.0002758151094894856
Batch  141  loss:  0.0004433821886777878
Batch  151  loss:  0.0003497052239254117
Batch  161  loss:  0.0002782916708383709
Batch  171  loss:  0.00043262718827463686
Batch  181  loss:  0.0004652765055652708
Batch  191  loss:  0.00022314602392725646
Validation on real data: 
LOSS supervised-train 0.00046427231980487703, valid 0.0012184514198452234
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0003266546700615436
Batch  11  loss:  0.00038807917735539377
Batch  21  loss:  0.00040651558083482087
Batch  31  loss:  0.0003650776343420148
Batch  41  loss:  0.0004818302986677736
Batch  51  loss:  0.0002857643994502723
Batch  61  loss:  0.00045649692765437067
Batch  71  loss:  0.00060990487691015
Batch  81  loss:  0.000715052243322134
Batch  91  loss:  0.00041950299055315554
Batch  101  loss:  0.0005544510786421597
Batch  111  loss:  0.00031101712374947965
Batch  121  loss:  0.0007924212259240448
Batch  131  loss:  0.00028817541897296906
Batch  141  loss:  0.000408158142818138
Batch  151  loss:  0.00037851801607757807
Batch  161  loss:  0.0002483874268364161
Batch  171  loss:  0.0003656946064438671
Batch  181  loss:  0.00040155163151212037
Batch  191  loss:  0.0001950738369487226
Validation on real data: 
LOSS supervised-train 0.00044583850023627747, valid 0.002067655324935913
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00040618175989948213
Batch  11  loss:  0.0004258710832800716
Batch  21  loss:  0.0003820126294158399
Batch  31  loss:  0.000372587819583714
Batch  41  loss:  0.00047998662921600044
Batch  51  loss:  0.00026823949883691967
Batch  61  loss:  0.0004575414059218019
Batch  71  loss:  0.0006740627577528358
Batch  81  loss:  0.0006889082142151892
Batch  91  loss:  0.000399260432459414
Batch  101  loss:  0.00047540199011564255
Batch  111  loss:  0.00029334163991734385
Batch  121  loss:  0.0008246633806265891
Batch  131  loss:  0.00026152568170800805
Batch  141  loss:  0.0004557636275421828
Batch  151  loss:  0.0003453758545219898
Batch  161  loss:  0.00025083665968850255
Batch  171  loss:  0.00039050955092534423
Batch  181  loss:  0.00047688139602541924
Batch  191  loss:  0.00022492810967378318
Validation on real data: 
LOSS supervised-train 0.00045392932697723153, valid 0.000719538307748735
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0003037190472241491
Batch  11  loss:  0.00032511542667634785
Batch  21  loss:  0.0003919461159966886
Batch  31  loss:  0.00033573838300071657
Batch  41  loss:  0.0005286074592731893
Batch  51  loss:  0.0002892754855565727
Batch  61  loss:  0.0005417533684521914
Batch  71  loss:  0.0006446995539590716
Batch  81  loss:  0.0007054792367853224
Batch  91  loss:  0.00033261938369832933
Batch  101  loss:  0.0005024357233196497
Batch  111  loss:  0.0002933354116976261
Batch  121  loss:  0.0008082624990493059
Batch  131  loss:  0.0003553007554728538
Batch  141  loss:  0.000463318545371294
Batch  151  loss:  0.0003484557382762432
Batch  161  loss:  0.00018751407333184034
Batch  171  loss:  0.0003476125712040812
Batch  181  loss:  0.0005133711965754628
Batch  191  loss:  0.0002184007316827774
Validation on real data: 
LOSS supervised-train 0.00044353105156915264, valid 0.0012753428891301155
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0003450110962148756
Batch  11  loss:  0.0002934591320808977
Batch  21  loss:  0.00038744258927181363
Batch  31  loss:  0.00040255876956507564
Batch  41  loss:  0.0004505123943090439
Batch  51  loss:  0.00038545142160728574
Batch  61  loss:  0.0004778411821462214
Batch  71  loss:  0.00039735346217639744
Batch  81  loss:  0.0007203678833320737
Batch  91  loss:  0.0004922300577163696
Batch  101  loss:  0.000554789206944406
Batch  111  loss:  0.0002732081920839846
Batch  121  loss:  0.0007142400718294084
Batch  131  loss:  0.00031026051146909595
Batch  141  loss:  0.0004879646294284612
Batch  151  loss:  0.00033054358209483325
Batch  161  loss:  0.00022536440519616008
Batch  171  loss:  0.0003718780353665352
Batch  181  loss:  0.0003845909086521715
Batch  191  loss:  0.00026957460795529187
Validation on real data: 
LOSS supervised-train 0.0004285647890355904, valid 0.0005872279871255159
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0003094254934694618
Batch  11  loss:  0.0003595158050302416
Batch  21  loss:  0.0004219311522319913
Batch  31  loss:  0.0003926584031432867
Batch  41  loss:  0.0004765150952152908
Batch  51  loss:  0.0002966668689623475
Batch  61  loss:  0.0004550837620627135
Batch  71  loss:  0.0005656018620356917
Batch  81  loss:  0.0006354257347993553
Batch  91  loss:  0.00041261722799390554
Batch  101  loss:  0.0005175555124878883
Batch  111  loss:  0.00027990745729766786
Batch  121  loss:  0.0008495602523908019
Batch  131  loss:  0.0002447586739435792
Batch  141  loss:  0.0004051463329233229
Batch  151  loss:  0.0002921896521002054
Batch  161  loss:  0.0002819590736180544
Batch  171  loss:  0.0004230029881000519
Batch  181  loss:  0.0003769787435885519
Batch  191  loss:  0.00019975053146481514
Validation on real data: 
LOSS supervised-train 0.0004234574038127903, valid 0.0009496421553194523
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0003293846966698766
Batch  11  loss:  0.00031127105467021465
Batch  21  loss:  0.00036930065834894776
Batch  31  loss:  0.0004631367919500917
Batch  41  loss:  0.00046469076187349856
Batch  51  loss:  0.00028076759190298617
Batch  61  loss:  0.0004803119518328458
Batch  71  loss:  0.0006493232795037329
Batch  81  loss:  0.0007211854681372643
Batch  91  loss:  0.00047866374370642006
Batch  101  loss:  0.00043945020297542214
Batch  111  loss:  0.000303441658616066
Batch  121  loss:  0.0007546754204668105
Batch  131  loss:  0.0002979818673338741
Batch  141  loss:  0.0004705604806076735
Batch  151  loss:  0.0004071386356372386
Batch  161  loss:  0.00026178473490290344
Batch  171  loss:  0.0003817593678832054
Batch  181  loss:  0.00042471580673009157
Batch  191  loss:  0.0002174642140744254
Validation on real data: 
LOSS supervised-train 0.00043597229072474877, valid 0.0005287689855322242
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0002636066637933254
Batch  11  loss:  0.0003265115665271878
Batch  21  loss:  0.00037331387284211814
Batch  31  loss:  0.000413970003137365
Batch  41  loss:  0.0004967893473803997
Batch  51  loss:  0.00033523980528116226
Batch  61  loss:  0.00048342806985601783
Batch  71  loss:  0.0004784609191119671
Batch  81  loss:  0.0006303098052740097
Batch  91  loss:  0.0003413927333895117
Batch  101  loss:  0.0005264020292088389
Batch  111  loss:  0.00029358454048633575
Batch  121  loss:  0.0006483506876975298
Batch  131  loss:  0.0003304379351902753
Batch  141  loss:  0.00041296074050478637
Batch  151  loss:  0.0003864837344735861
Batch  161  loss:  0.00015033282397780567
Batch  171  loss:  0.0004062993684783578
Batch  181  loss:  0.0003909817896783352
Batch  191  loss:  0.00020116209634579718
Validation on real data: 
LOSS supervised-train 0.0004229709807259496, valid 0.0005376673652790487
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0002827468852046877
Batch  11  loss:  0.00042771914741024375
Batch  21  loss:  0.000363581464625895
Batch  31  loss:  0.00036723120138049126
Batch  41  loss:  0.0003854894603136927
Batch  51  loss:  0.00030357169453054667
Batch  61  loss:  0.00040925180655904114
Batch  71  loss:  0.0005579707794822752
Batch  81  loss:  0.0005716680898331106
Batch  91  loss:  0.00036719918716698885
Batch  101  loss:  0.00042169319931417704
Batch  111  loss:  0.0002652286784723401
Batch  121  loss:  0.0006508739315904677
Batch  131  loss:  0.00029357694438658655
Batch  141  loss:  0.00044282464659772813
Batch  151  loss:  0.0002806269330903888
Batch  161  loss:  0.0002099488629028201
Batch  171  loss:  0.00039682595524936914
Batch  181  loss:  0.00042294940794818103
Batch  191  loss:  0.00017268008377868682
Validation on real data: 
LOSS supervised-train 0.00041201443928002847, valid 0.001050720689818263
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0002275939186802134
Batch  11  loss:  0.0002959148841910064
Batch  21  loss:  0.0003429602656979114
Batch  31  loss:  0.00040681398240849376
Batch  41  loss:  0.00031659609521739185
Batch  51  loss:  0.0002473136701155454
Batch  61  loss:  0.00041257968405261636
Batch  71  loss:  0.0005335442838259041
Batch  81  loss:  0.0006778951501473784
Batch  91  loss:  0.0003839923010673374
Batch  101  loss:  0.0005535119562409818
Batch  111  loss:  0.0002951023925561458
Batch  121  loss:  0.0005456996732391417
Batch  131  loss:  0.00026370916748419404
Batch  141  loss:  0.0004985919804312289
Batch  151  loss:  0.0003247915010433644
Batch  161  loss:  0.0002765860699582845
Batch  171  loss:  0.00042448609019629657
Batch  181  loss:  0.0004429747932590544
Batch  191  loss:  0.0002455037320032716
Validation on real data: 
LOSS supervised-train 0.0004146015881269705, valid 0.0006486726342700422
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00026397875626571476
Batch  11  loss:  0.00037894825800321996
Batch  21  loss:  0.00034801935544237494
Batch  31  loss:  0.0004165621066931635
Batch  41  loss:  0.000360700098099187
Batch  51  loss:  0.0002952513750642538
Batch  61  loss:  0.0003530676185619086
Batch  71  loss:  0.0003995810984633863
Batch  81  loss:  0.0007099861977621913
Batch  91  loss:  0.0003551193221937865
Batch  101  loss:  0.0005096680833958089
Batch  111  loss:  0.0002610779774840921
Batch  121  loss:  0.0007645238656550646
Batch  131  loss:  0.00021862605353817344
Batch  141  loss:  0.00043404256575740874
Batch  151  loss:  0.00037125899689272046
Batch  161  loss:  0.00027041370049118996
Batch  171  loss:  0.00036325561814010143
Batch  181  loss:  0.00038115066126920283
Batch  191  loss:  0.00015236830222420394
Validation on real data: 
LOSS supervised-train 0.0004061590771743795, valid 0.0003774109063670039
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00029836155590601265
Batch  11  loss:  0.0002944465377368033
Batch  21  loss:  0.0003460062143858522
Batch  31  loss:  0.0003560764016583562
Batch  41  loss:  0.00036629504757001996
Batch  51  loss:  0.00026741885812953115
Batch  61  loss:  0.00041157787200063467
Batch  71  loss:  0.0004299593565519899
Batch  81  loss:  0.0006285308045335114
Batch  91  loss:  0.00038053831667639315
Batch  101  loss:  0.0004809507227037102
Batch  111  loss:  0.00024241670325864106
Batch  121  loss:  0.0006237527704797685
Batch  131  loss:  0.00026914189220406115
Batch  141  loss:  0.00043283135164529085
Batch  151  loss:  0.00044746731873601675
Batch  161  loss:  0.00021659123012796044
Batch  171  loss:  0.00040201155934482813
Batch  181  loss:  0.0004482893564272672
Batch  191  loss:  0.00018648836703505367
Validation on real data: 
LOSS supervised-train 0.00040352470030484254, valid 0.00044795687426812947
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0002933541836682707
Batch  11  loss:  0.000388662883779034
Batch  21  loss:  0.00031441933242604136
Batch  31  loss:  0.0004472056170925498
Batch  41  loss:  0.00035606970777735114
Batch  51  loss:  0.0002763276279438287
Batch  61  loss:  0.00037264468846842647
Batch  71  loss:  0.000432827539043501
Batch  81  loss:  0.0007131167221814394
Batch  91  loss:  0.0003611466090660542
Batch  101  loss:  0.0004080593353137374
Batch  111  loss:  0.00029911709134466946
Batch  121  loss:  0.0005316153983585536
Batch  131  loss:  0.0002444367855787277
Batch  141  loss:  0.00042858667438849807
Batch  151  loss:  0.00044679848360829055
Batch  161  loss:  0.00021848169853910804
Batch  171  loss:  0.0003735850041266531
Batch  181  loss:  0.0003683005925267935
Batch  191  loss:  0.0002170622901758179
Validation on real data: 
LOSS supervised-train 0.0004012753728602547, valid 0.00113201723434031
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.000395187787944451
Batch  11  loss:  0.0003609016421250999
Batch  21  loss:  0.0003190315328538418
Batch  31  loss:  0.00033522030571475625
Batch  41  loss:  0.00041819846956059337
Batch  51  loss:  0.00028469631797634065
Batch  61  loss:  0.00048472039634361863
Batch  71  loss:  0.0005976365646347404
Batch  81  loss:  0.0006823766161687672
Batch  91  loss:  0.00041151358163915575
Batch  101  loss:  0.0004368367954157293
Batch  111  loss:  0.00033171422546729445
Batch  121  loss:  0.0005570068024098873
Batch  131  loss:  0.00022921714116819203
Batch  141  loss:  0.0005213167169131339
Batch  151  loss:  0.00035913355532102287
Batch  161  loss:  0.0002613393298815936
Batch  171  loss:  0.00036289729177951813
Batch  181  loss:  0.0003959718160331249
Batch  191  loss:  0.00020773831056430936
Validation on real data: 
LOSS supervised-train 0.0004075758752878755, valid 0.0012303926050662994
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0003678880457300693
Batch  11  loss:  0.0002548424818087369
Batch  21  loss:  0.00037992565194144845
Batch  31  loss:  0.0003781904233619571
Batch  41  loss:  0.00048556571709923446
Batch  51  loss:  0.00023563871218357235
Batch  61  loss:  0.00045266179949976504
Batch  71  loss:  0.0005952910287305713
Batch  81  loss:  0.0007360743475146592
Batch  91  loss:  0.00040674908086657524
Batch  101  loss:  0.000471646839287132
Batch  111  loss:  0.0002781372459139675
Batch  121  loss:  0.0006518829613924026
Batch  131  loss:  0.000221819500438869
Batch  141  loss:  0.00041190325282514095
Batch  151  loss:  0.0003493785916361958
Batch  161  loss:  0.00020559226686600596
Batch  171  loss:  0.0004674494848586619
Batch  181  loss:  0.00034148996928706765
Batch  191  loss:  0.00014460849342867732
Validation on real data: 
LOSS supervised-train 0.00040137776522897184, valid 0.0032691785600036383
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0003203474043402821
Batch  11  loss:  0.00030847167363390326
Batch  21  loss:  0.00026473900652490556
Batch  31  loss:  0.0003300205571576953
Batch  41  loss:  0.00038352602859959006
Batch  51  loss:  0.00032404903322458267
Batch  61  loss:  0.0003881111624650657
Batch  71  loss:  0.0005938983522355556
Batch  81  loss:  0.00055015116231516
Batch  91  loss:  0.00032264977926388383
Batch  101  loss:  0.00039874017238616943
Batch  111  loss:  0.00025724212173372507
Batch  121  loss:  0.000717493356205523
Batch  131  loss:  0.00027061544824391603
Batch  141  loss:  0.00041134568164125085
Batch  151  loss:  0.0003355539811309427
Batch  161  loss:  0.00026659376453608274
Batch  171  loss:  0.0004350266535766423
Batch  181  loss:  0.000415515765780583
Batch  191  loss:  0.00015699744108133018
Validation on real data: 
LOSS supervised-train 0.0003849127941793995, valid 0.0005612716777250171
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00035247750929556787
Batch  11  loss:  0.0002879624080378562
Batch  21  loss:  0.0003696406492963433
Batch  31  loss:  0.0003860107681248337
Batch  41  loss:  0.00033484783489257097
Batch  51  loss:  0.0003255166520830244
Batch  61  loss:  0.000496693595778197
Batch  71  loss:  0.0005387385608628392
Batch  81  loss:  0.0006313093472272158
Batch  91  loss:  0.00035115875652991235
Batch  101  loss:  0.0004274437960702926
Batch  111  loss:  0.0002512707142159343
Batch  121  loss:  0.0005909341271035373
Batch  131  loss:  0.00023844938550610095
Batch  141  loss:  0.0004718177660834044
Batch  151  loss:  0.0002916690718848258
Batch  161  loss:  0.00025774858659133315
Batch  171  loss:  0.0004859000036958605
Batch  181  loss:  0.0002709630352910608
Batch  191  loss:  0.00019454247376415879
Validation on real data: 
LOSS supervised-train 0.0003808123149065068, valid 0.002589975483715534
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00032559814280830324
Batch  11  loss:  0.0003824351297225803
Batch  21  loss:  0.00028804774046875536
Batch  31  loss:  0.00028710352489724755
Batch  41  loss:  0.0003353069187141955
Batch  51  loss:  0.0003109714307356626
Batch  61  loss:  0.0004046305257361382
Batch  71  loss:  0.00048031104961410165
Batch  81  loss:  0.0007748327334411442
Batch  91  loss:  0.0004976252093911171
Batch  101  loss:  0.0003751888871192932
Batch  111  loss:  0.0002541986759752035
Batch  121  loss:  0.0005218959995545447
Batch  131  loss:  0.00018481015285942703
Batch  141  loss:  0.00034637071075849235
Batch  151  loss:  0.00025092335999943316
Batch  161  loss:  0.00022340049326885492
Batch  171  loss:  0.0004281722940504551
Batch  181  loss:  0.00036090239882469177
Batch  191  loss:  0.00020951626356691122
Validation on real data: 
LOSS supervised-train 0.00038447459213784895, valid 0.0005996408872306347
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00032351151457987726
Batch  11  loss:  0.00029314763378351927
Batch  21  loss:  0.0002978160046041012
Batch  31  loss:  0.00037084921495988965
Batch  41  loss:  0.000354576128302142
Batch  51  loss:  0.0002475743240211159
Batch  61  loss:  0.00036980200093239546
Batch  71  loss:  0.0004411086847539991
Batch  81  loss:  0.0006416770629584789
Batch  91  loss:  0.00034301975392736495
Batch  101  loss:  0.0003613430308178067
Batch  111  loss:  0.00022286013700067997
Batch  121  loss:  0.0005292445421218872
Batch  131  loss:  0.0002813232713378966
Batch  141  loss:  0.0003245873376727104
Batch  151  loss:  0.0002708876854740083
Batch  161  loss:  0.0002941073034889996
Batch  171  loss:  0.0004126749699935317
Batch  181  loss:  0.00041270075598731637
Batch  191  loss:  0.0001378981105517596
Validation on real data: 
LOSS supervised-train 0.0003752280921617057, valid 0.001108280150219798
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00029498778167180717
Batch  11  loss:  0.0003542004560586065
Batch  21  loss:  0.00034620947553776205
Batch  31  loss:  0.00034261433756910264
Batch  41  loss:  0.0003029171493835747
Batch  51  loss:  0.0002953237562905997
Batch  61  loss:  0.00047114654444158077
Batch  71  loss:  0.00038727192441001534
Batch  81  loss:  0.000628867419436574
Batch  91  loss:  0.0003783112042583525
Batch  101  loss:  0.0004263636365067214
Batch  111  loss:  0.00021197165187913924
Batch  121  loss:  0.0005777754704467952
Batch  131  loss:  0.00033888063626363873
Batch  141  loss:  0.0003761284751817584
Batch  151  loss:  0.0003041886375285685
Batch  161  loss:  0.00025441485922783613
Batch  171  loss:  0.000336755852913484
Batch  181  loss:  0.0003834235540125519
Batch  191  loss:  0.0001982708927243948
Validation on real data: 
LOSS supervised-train 0.00036889520757540597, valid 0.0008981632417999208
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0003050919040106237
Batch  11  loss:  0.0002726954990066588
Batch  21  loss:  0.00032262440072372556
Batch  31  loss:  0.00032838559127412736
Batch  41  loss:  0.0002709454856812954
Batch  51  loss:  0.00022262960555963218
Batch  61  loss:  0.0004077011253684759
Batch  71  loss:  0.0004977722419425845
Batch  81  loss:  0.0006603187066502869
Batch  91  loss:  0.0003180834464728832
Batch  101  loss:  0.0003389596240594983
Batch  111  loss:  0.00022622707183472812
Batch  121  loss:  0.0006580164190381765
Batch  131  loss:  0.00035050336737185717
Batch  141  loss:  0.00041849887929856777
Batch  151  loss:  0.0003160852356813848
Batch  161  loss:  0.0002158439892809838
Batch  171  loss:  0.0003392521175555885
Batch  181  loss:  0.00036084026214666665
Batch  191  loss:  0.00013548487913794816
Validation on real data: 
LOSS supervised-train 0.00037839533950318583, valid 0.0006876381230540574
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0002756499743554741
Batch  11  loss:  0.0002756294561550021
Batch  21  loss:  0.0003707275027409196
Batch  31  loss:  0.0003063298645429313
Batch  41  loss:  0.0003731268516276032
Batch  51  loss:  0.00023066667199600488
Batch  61  loss:  0.00037240993697196245
Batch  71  loss:  0.0005229142843745649
Batch  81  loss:  0.0006590382545255125
Batch  91  loss:  0.0003142482601106167
Batch  101  loss:  0.0003203898959327489
Batch  111  loss:  0.00021724590624216944
Batch  121  loss:  0.00047122372779995203
Batch  131  loss:  0.00023655036056879908
Batch  141  loss:  0.0003078890440519899
Batch  151  loss:  0.0002829003205988556
Batch  161  loss:  0.00021884996385779232
Batch  171  loss:  0.00037979541230015457
Batch  181  loss:  0.00038574973586946726
Batch  191  loss:  0.0001942767557920888
Validation on real data: 
LOSS supervised-train 0.00036196170447510667, valid 0.0008447119034826756
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00031226585269905627
Batch  11  loss:  0.0004035617457702756
Batch  21  loss:  0.00041322430479340255
Batch  31  loss:  0.0002917642705142498
Batch  41  loss:  0.000290771946310997
Batch  51  loss:  0.0002653102274052799
Batch  61  loss:  0.00037496566073969007
Batch  71  loss:  0.0005253833369351923
Batch  81  loss:  0.000593216042034328
Batch  91  loss:  0.00040280676330439746
Batch  101  loss:  0.00033847635495476425
Batch  111  loss:  0.00022205471759662032
Batch  121  loss:  0.0007860318874008954
Batch  131  loss:  0.00022587379498872906
Batch  141  loss:  0.0004023922374472022
Batch  151  loss:  0.00031522702192887664
Batch  161  loss:  0.0002579244610387832
Batch  171  loss:  0.0003673015453387052
Batch  181  loss:  0.000335718912538141
Batch  191  loss:  0.0001613726926734671
Validation on real data: 
LOSS supervised-train 0.00036140891337709034, valid 0.00025703368009999394
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00028449995443224907
Batch  11  loss:  0.0002856014762073755
Batch  21  loss:  0.0002620157029014081
Batch  31  loss:  0.00033008281025104225
Batch  41  loss:  0.00030528102070093155
Batch  51  loss:  0.00018350586469750851
Batch  61  loss:  0.00043452082900330424
Batch  71  loss:  0.0004641769337467849
Batch  81  loss:  0.0004994477494619787
Batch  91  loss:  0.00029083964182063937
Batch  101  loss:  0.00031593217863701284
Batch  111  loss:  0.00025110936257988214
Batch  121  loss:  0.0005713755381293595
Batch  131  loss:  0.0001981449022423476
Batch  141  loss:  0.0004269385535735637
Batch  151  loss:  0.0002966849133372307
Batch  161  loss:  0.0002800297224894166
Batch  171  loss:  0.00032565888250246644
Batch  181  loss:  0.00029636890394613147
Batch  191  loss:  0.00016387055802624673
Validation on real data: 
LOSS supervised-train 0.0003604158681991976, valid 0.0010660478146746755
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00029025322874076664
Batch  11  loss:  0.0003879251307807863
Batch  21  loss:  0.0003596115275286138
Batch  31  loss:  0.00038694796967320144
Batch  41  loss:  0.0003382688737474382
Batch  51  loss:  0.0002518336405046284
Batch  61  loss:  0.00040043334593065083
Batch  71  loss:  0.00046556725283153355
Batch  81  loss:  0.000545365153811872
Batch  91  loss:  0.0002968165499623865
Batch  101  loss:  0.0003301007964182645
Batch  111  loss:  0.00025775033282116055
Batch  121  loss:  0.0005712084239348769
Batch  131  loss:  0.00022745713067706674
Batch  141  loss:  0.00045270068221725523
Batch  151  loss:  0.0003385610762052238
Batch  161  loss:  0.0002484729920979589
Batch  171  loss:  0.0003796032688114792
Batch  181  loss:  0.0003474417608231306
Batch  191  loss:  0.0001659827248658985
Validation on real data: 
LOSS supervised-train 0.0003540722655452555, valid 0.00037689259625039995
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00029298316803760827
Batch  11  loss:  0.00032321474282070994
Batch  21  loss:  0.00024199782637879252
Batch  31  loss:  0.00033359057852067053
Batch  41  loss:  0.00031162440427578986
Batch  51  loss:  0.00024991441750898957
Batch  61  loss:  0.0003225257096346468
Batch  71  loss:  0.0005729037220589817
Batch  81  loss:  0.0006016289116814733
Batch  91  loss:  0.0003188908158335835
Batch  101  loss:  0.00039010110776871443
Batch  111  loss:  0.00023637685808353126
Batch  121  loss:  0.0005378625937737525
Batch  131  loss:  0.0002505875308997929
Batch  141  loss:  0.0004770884697791189
Batch  151  loss:  0.0003038033319171518
Batch  161  loss:  0.00026171523495577276
Batch  171  loss:  0.0003638311754912138
Batch  181  loss:  0.0003701139066834003
Batch  191  loss:  0.0002670024405233562
Validation on real data: 
LOSS supervised-train 0.00036076554853934793, valid 0.00048660929314792156
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00028221969841979444
Batch  11  loss:  0.00032743747578933835
Batch  21  loss:  0.00026958336820825934
Batch  31  loss:  0.00026881947997026145
Batch  41  loss:  0.0002917170349974185
Batch  51  loss:  0.00021105674386490136
Batch  61  loss:  0.00043839102727361023
Batch  71  loss:  0.0004258213157299906
Batch  81  loss:  0.0006144294166006148
Batch  91  loss:  0.00044665945461019874
Batch  101  loss:  0.0002915511140599847
Batch  111  loss:  0.00022613233886659145
Batch  121  loss:  0.0005637772264890373
Batch  131  loss:  0.00022618418734055012
Batch  141  loss:  0.00048310100100934505
Batch  151  loss:  0.00021639189799316227
Batch  161  loss:  0.0002035303768934682
Batch  171  loss:  0.0003450447111390531
Batch  181  loss:  0.00041629880433902144
Batch  191  loss:  0.00017643037426751107
Validation on real data: 
LOSS supervised-train 0.0003554114102735184, valid 0.0010700327111408114
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00023332280397880822
Batch  11  loss:  0.00032690141233615577
Batch  21  loss:  0.00029926514253020287
Batch  31  loss:  0.0002656015276443213
Batch  41  loss:  0.0002964678278658539
Batch  51  loss:  0.0002787548291962594
Batch  61  loss:  0.0004173534398432821
Batch  71  loss:  0.00042844717972911894
Batch  81  loss:  0.0006186010432429612
Batch  91  loss:  0.00033487213659100235
Batch  101  loss:  0.0002702432684600353
Batch  111  loss:  0.00021130175446160138
Batch  121  loss:  0.0005089337937533855
Batch  131  loss:  0.00022701946727465838
Batch  141  loss:  0.0003643218951765448
Batch  151  loss:  0.0003127349482383579
Batch  161  loss:  0.00021077989367768168
Batch  171  loss:  0.0003409224736969918
Batch  181  loss:  0.00032490870216861367
Batch  191  loss:  0.0001535645715193823
Validation on real data: 
LOSS supervised-train 0.0003544364204572048, valid 0.0004875031008850783
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0002865353017114103
Batch  11  loss:  0.0002849565935321152
Batch  21  loss:  0.00032205632305704057
Batch  31  loss:  0.0003622553776949644
Batch  41  loss:  0.00032789664692245424
Batch  51  loss:  0.00023967820743564516
Batch  61  loss:  0.0004137186915613711
Batch  71  loss:  0.00039589969674125314
Batch  81  loss:  0.0006015636608935893
Batch  91  loss:  0.0003777527599595487
Batch  101  loss:  0.00048662148765288293
Batch  111  loss:  0.0002414625632809475
Batch  121  loss:  0.0004275664105080068
Batch  131  loss:  0.0001996952632907778
Batch  141  loss:  0.0002696834271773696
Batch  151  loss:  0.0003061530878767371
Batch  161  loss:  0.0002153536188416183
Batch  171  loss:  0.0003454202087596059
Batch  181  loss:  0.0002988100459333509
Batch  191  loss:  0.00014326928067021072
Validation on real data: 
LOSS supervised-train 0.000345786303805653, valid 0.0008826245320960879
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00020375271560624242
Batch  11  loss:  0.00026746775256469846
Batch  21  loss:  0.00030359142692759633
Batch  31  loss:  0.0003065908094868064
Batch  41  loss:  0.0003065461933147162
Batch  51  loss:  0.0002438211813569069
Batch  61  loss:  0.00034725797013379633
Batch  71  loss:  0.0004147861327510327
Batch  81  loss:  0.0006806600140407681
Batch  91  loss:  0.0003508185618557036
Batch  101  loss:  0.0003457670100033283
Batch  111  loss:  0.00020589448104146868
Batch  121  loss:  0.0005524278967641294
Batch  131  loss:  0.00019774213433265686
Batch  141  loss:  0.00030983006581664085
Batch  151  loss:  0.0002869669406209141
Batch  161  loss:  0.0001814818097045645
Batch  171  loss:  0.0003836097603198141
Batch  181  loss:  0.0002845168928615749
Batch  191  loss:  0.00014365838433150202
Validation on real data: 
LOSS supervised-train 0.00033972096651268657, valid 0.0010246920865029097
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0002830286684911698
Batch  11  loss:  0.000313376949634403
Batch  21  loss:  0.00028620241209864616
Batch  31  loss:  0.0002971102949231863
Batch  41  loss:  0.00032204980379901826
Batch  51  loss:  0.00024509374634362757
Batch  61  loss:  0.0003715534112416208
Batch  71  loss:  0.00047452436410821974
Batch  81  loss:  0.0005713613354600966
Batch  91  loss:  0.00043123780051246285
Batch  101  loss:  0.00032140305847860873
Batch  111  loss:  0.0002790718572214246
Batch  121  loss:  0.0005110592464916408
Batch  131  loss:  0.00020132896315772086
Batch  141  loss:  0.00030685667297802866
Batch  151  loss:  0.00026178520056419075
Batch  161  loss:  0.00022930819250177592
Batch  171  loss:  0.0003902106545865536
Batch  181  loss:  0.00032054114853963256
Batch  191  loss:  0.0001865944650489837
Validation on real data: 
LOSS supervised-train 0.0003348630810069153, valid 0.0004959840443916619
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00024815264623612165
Batch  11  loss:  0.0003624655364546925
Batch  21  loss:  0.00029262909083627164
Batch  31  loss:  0.0002708463871385902
Batch  41  loss:  0.00026019627694040537
Batch  51  loss:  0.00023824253003112972
Batch  61  loss:  0.00040575338061898947
Batch  71  loss:  0.00039337901398539543
Batch  81  loss:  0.0006295116618275642
Batch  91  loss:  0.0003357195819262415
Batch  101  loss:  0.00038293495890684426
Batch  111  loss:  0.00027481463621370494
Batch  121  loss:  0.0005098311230540276
Batch  131  loss:  0.0001749464136082679
Batch  141  loss:  0.0002507273748051375
Batch  151  loss:  0.00025062027270905674
Batch  161  loss:  0.00020347870304249227
Batch  171  loss:  0.00030922656878829
Batch  181  loss:  0.0003717371728271246
Batch  191  loss:  0.00015210226410999894
Validation on real data: 
LOSS supervised-train 0.0003342717672785511, valid 0.0010726102627813816
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0002184159093303606
Batch  11  loss:  0.0002467820013407618
Batch  21  loss:  0.00038298737490549684
Batch  31  loss:  0.0002876449725590646
Batch  41  loss:  0.0002804299001581967
Batch  51  loss:  0.00022238181554712355
Batch  61  loss:  0.0004154774360358715
Batch  71  loss:  0.00041752588003873825
Batch  81  loss:  0.0005175128462724388
Batch  91  loss:  0.0003142131317872554
Batch  101  loss:  0.00035638088593259454
Batch  111  loss:  0.0002480257535353303
Batch  121  loss:  0.0006262819515541196
Batch  131  loss:  0.000194724794710055
Batch  141  loss:  0.0003699289227370173
Batch  151  loss:  0.00032781370100565255
Batch  161  loss:  0.00024758995277807117
Batch  171  loss:  0.0004622602427843958
Batch  181  loss:  0.00030316447373479605
Batch  191  loss:  0.0001430740230716765
Validation on real data: 
LOSS supervised-train 0.0003364246007549809, valid 0.0006411781068891287
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00023425272956956178
Batch  11  loss:  0.00034535222221165895
Batch  21  loss:  0.0004083892563357949
Batch  31  loss:  0.0002555761602707207
Batch  41  loss:  0.00018804427236318588
Batch  51  loss:  0.00023708048684056848
Batch  61  loss:  0.00036196972359903157
Batch  71  loss:  0.00032579779508523643
Batch  81  loss:  0.0005258952151052654
Batch  91  loss:  0.0002677671436686069
Batch  101  loss:  0.00037761995918117464
Batch  111  loss:  0.0002096869720844552
Batch  121  loss:  0.00040043494664132595
Batch  131  loss:  0.00019994199101347476
Batch  141  loss:  0.0003320598916616291
Batch  151  loss:  0.00030570424860343337
Batch  161  loss:  0.0002289120020577684
Batch  171  loss:  0.00031176552874967456
Batch  181  loss:  0.00025270358310081065
Batch  191  loss:  0.00020355168089736253
Validation on real data: 
LOSS supervised-train 0.00032302958687068894, valid 0.002202932257205248
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0002742198121268302
Batch  11  loss:  0.00030449655605480075
Batch  21  loss:  0.00027785421116277575
Batch  31  loss:  0.00032402752549387515
Batch  41  loss:  0.00023514259373769164
Batch  51  loss:  0.00019812531536445022
Batch  61  loss:  0.0003856029361486435
Batch  71  loss:  0.000436551112215966
Batch  81  loss:  0.0006048229988664389
Batch  91  loss:  0.00039934884989634156
Batch  101  loss:  0.00040410086512565613
Batch  111  loss:  0.0002464571443852037
Batch  121  loss:  0.0006210803985595703
Batch  131  loss:  0.00019730828353203833
Batch  141  loss:  0.00039884186116978526
Batch  151  loss:  0.00031463350751437247
Batch  161  loss:  0.00021761056268587708
Batch  171  loss:  0.0004405151994433254
Batch  181  loss:  0.0002605076879262924
Batch  191  loss:  0.00015939272998366505
Validation on real data: 
LOSS supervised-train 0.00033013701569871044, valid 0.0013200140092521906
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0002687929954845458
Batch  11  loss:  0.00026469462318345904
Batch  21  loss:  0.00026954044005833566
Batch  31  loss:  0.00033844506833702326
Batch  41  loss:  0.000229613870033063
Batch  51  loss:  0.00018319976516067982
Batch  61  loss:  0.00032133780769072473
Batch  71  loss:  0.0005905700381845236
Batch  81  loss:  0.0005800368962809443
Batch  91  loss:  0.0002956815587822348
Batch  101  loss:  0.0003502632607705891
Batch  111  loss:  0.00020703376503661275
Batch  121  loss:  0.0006223645177669823
Batch  131  loss:  0.0002004642301471904
Batch  141  loss:  0.00033135476405732334
Batch  151  loss:  0.00034846251946873963
Batch  161  loss:  0.0001704208116279915
Batch  171  loss:  0.0003079781890846789
Batch  181  loss:  0.000310117204207927
Batch  191  loss:  0.00011799548519775271
Validation on real data: 
LOSS supervised-train 0.00032746331409725824, valid 0.000408557360060513
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00031123575172387064
Batch  11  loss:  0.0003271174500696361
Batch  21  loss:  0.00025314491358585656
Batch  31  loss:  0.0002498517860658467
Batch  41  loss:  0.00022550004359800369
Batch  51  loss:  0.00022463589266408235
Batch  61  loss:  0.00035479330108501017
Batch  71  loss:  0.000391516019590199
Batch  81  loss:  0.0005726755480282009
Batch  91  loss:  0.0003282881807535887
Batch  101  loss:  0.00028365530306473374
Batch  111  loss:  0.0002455776266288012
Batch  121  loss:  0.0005845379200764
Batch  131  loss:  0.000302666041534394
Batch  141  loss:  0.0003067223879043013
Batch  151  loss:  0.0002418227813905105
Batch  161  loss:  0.00023083116684574634
Batch  171  loss:  0.00035696281702257693
Batch  181  loss:  0.0003188800474163145
Batch  191  loss:  0.00012884974421467632
Validation on real data: 
LOSS supervised-train 0.0003180865187459858, valid 0.0005651961546391249
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0002574876125436276
Batch  11  loss:  0.00036593087133951485
Batch  21  loss:  0.00024029778433032334
Batch  31  loss:  0.00022615246416535228
Batch  41  loss:  0.00022822893515694886
Batch  51  loss:  0.00019330918439663947
Batch  61  loss:  0.0003920808376278728
Batch  71  loss:  0.0003800542326644063
Batch  81  loss:  0.0005141362198628485
Batch  91  loss:  0.0003654994652606547
Batch  101  loss:  0.00030271048308350146
Batch  111  loss:  0.00023421442892868072
Batch  121  loss:  0.0006333604105748236
Batch  131  loss:  0.0002050960174528882
Batch  141  loss:  0.00037733977660536766
Batch  151  loss:  0.00023775588488206267
Batch  161  loss:  0.00021366577129811049
Batch  171  loss:  0.0003390929487068206
Batch  181  loss:  0.00023491274623665959
Batch  191  loss:  0.00013431560364551842
Validation on real data: 
LOSS supervised-train 0.00031878802546998487, valid 0.0008156240219250321
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00023123669961933047
Batch  11  loss:  0.0002819187648128718
Batch  21  loss:  0.00020403091912157834
Batch  31  loss:  0.0002648387453518808
Batch  41  loss:  0.00029773954884149134
Batch  51  loss:  0.00018163886852562428
Batch  61  loss:  0.00030741014052182436
Batch  71  loss:  0.00041948899161070585
Batch  81  loss:  0.000513796228915453
Batch  91  loss:  0.00029302467009983957
Batch  101  loss:  0.0002463593555148691
Batch  111  loss:  0.0002713438880164176
Batch  121  loss:  0.0004545979027170688
Batch  131  loss:  0.00021009240299463272
Batch  141  loss:  0.00030722058727405965
Batch  151  loss:  0.00024409072648268193
Batch  161  loss:  0.0002479830291122198
Batch  171  loss:  0.0003539541212376207
Batch  181  loss:  0.0003273879992775619
Batch  191  loss:  0.00015196643653325737
Validation on real data: 
LOSS supervised-train 0.00031358537169580815, valid 0.0003982231137342751
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00020907043654005975
Batch  11  loss:  0.0003645399119704962
Batch  21  loss:  0.00032902881503105164
Batch  31  loss:  0.0003490968083497137
Batch  41  loss:  0.0002588813076727092
Batch  51  loss:  0.00023677924764342606
Batch  61  loss:  0.0003178115002810955
Batch  71  loss:  0.0005154657992534339
Batch  81  loss:  0.0005701800109818578
Batch  91  loss:  0.0003660774964373559
Batch  101  loss:  0.00033271938445977867
Batch  111  loss:  0.00019416818395256996
Batch  121  loss:  0.0004875856393482536
Batch  131  loss:  0.00024963420582935214
Batch  141  loss:  0.0003438224957790226
Batch  151  loss:  0.0002892049087677151
Batch  161  loss:  0.0001790854730643332
Batch  171  loss:  0.00030483928276225924
Batch  181  loss:  0.0002704457438085228
Batch  191  loss:  0.0001535076298750937
Validation on real data: 
LOSS supervised-train 0.0003156275184301194, valid 0.00035348584060557187
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00025623050169087946
Batch  11  loss:  0.0002965295861940831
Batch  21  loss:  0.00023223436437547207
Batch  31  loss:  0.0003098967717960477
Batch  41  loss:  0.0002141015575034544
Batch  51  loss:  0.00017133099026978016
Batch  61  loss:  0.0003503455955069512
Batch  71  loss:  0.000485708296764642
Batch  81  loss:  0.0005863516707904637
Batch  91  loss:  0.00029928586445748806
Batch  101  loss:  0.00027100651641376317
Batch  111  loss:  0.00020137004321441054
Batch  121  loss:  0.00046600267523899674
Batch  131  loss:  0.00024893623776733875
Batch  141  loss:  0.0003171986900269985
Batch  151  loss:  0.00024397547531407326
Batch  161  loss:  0.00019815344421658665
Batch  171  loss:  0.00029658651328645647
Batch  181  loss:  0.00026592810172587633
Batch  191  loss:  0.00013028689136262983
Validation on real data: 
LOSS supervised-train 0.0003108835985767655, valid 0.0012759442906826735
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00022541440557688475
Batch  11  loss:  0.0002243817289127037
Batch  21  loss:  0.0002540588902775198
Batch  31  loss:  0.0002714569855015725
Batch  41  loss:  0.0002838990476448089
Batch  51  loss:  0.00021096429554745555
Batch  61  loss:  0.00033594659180380404
Batch  71  loss:  0.0003593855944927782
Batch  81  loss:  0.000560178654268384
Batch  91  loss:  0.0002833468315657228
Batch  101  loss:  0.0002687025989871472
Batch  111  loss:  0.0002456369111314416
Batch  121  loss:  0.0004579679516609758
Batch  131  loss:  0.0001883703371277079
Batch  141  loss:  0.0002935515658464283
Batch  151  loss:  0.0002365976688452065
Batch  161  loss:  0.0001931767474161461
Batch  171  loss:  0.00033046735916286707
Batch  181  loss:  0.0002706191153265536
Batch  191  loss:  0.00012071161472704262
Validation on real data: 
LOSS supervised-train 0.0003094243330997415, valid 0.000421199481934309
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00021612267300952226
Batch  11  loss:  0.00027204531943425536
Batch  21  loss:  0.00026618747506290674
Batch  31  loss:  0.00024790538009256124
Batch  41  loss:  0.00020373363804537803
Batch  51  loss:  0.00023949112801346928
Batch  61  loss:  0.00025212770560756326
Batch  71  loss:  0.0004291022487450391
Batch  81  loss:  0.00044059386709704995
Batch  91  loss:  0.00031706312438473105
Batch  101  loss:  0.00027181513723917305
Batch  111  loss:  0.00025741150602698326
Batch  121  loss:  0.0005589960492216051
Batch  131  loss:  0.00022940084454603493
Batch  141  loss:  0.00020688633958343416
Batch  151  loss:  0.0002984800667036325
Batch  161  loss:  0.0002052220661425963
Batch  171  loss:  0.0004306788614485413
Batch  181  loss:  0.00034160620998591185
Batch  191  loss:  0.00014332235150504857
Validation on real data: 
LOSS supervised-train 0.00030710031649505253, valid 0.0006336222868412733
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00023056473582983017
Batch  11  loss:  0.00027506251353770494
Batch  21  loss:  0.0002851478639058769
Batch  31  loss:  0.0003546814259607345
Batch  41  loss:  0.00026613203226588666
Batch  51  loss:  0.00020061503164470196
Batch  61  loss:  0.0003800586564466357
Batch  71  loss:  0.0004348944057710469
Batch  81  loss:  0.0004776550631504506
Batch  91  loss:  0.00026034755865111947
Batch  101  loss:  0.0003193678567185998
Batch  111  loss:  0.0002121759462170303
Batch  121  loss:  0.0004476798349060118
Batch  131  loss:  0.00021492320229299366
Batch  141  loss:  0.00035958518856205046
Batch  151  loss:  0.00026810579583980143
Batch  161  loss:  0.00021267797274049371
Batch  171  loss:  0.00032039431971497834
Batch  181  loss:  0.0003230372676625848
Batch  191  loss:  0.00013154675252735615
Validation on real data: 
LOSS supervised-train 0.00030526581685990094, valid 0.002053705742582679
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00022248619643505663
Batch  11  loss:  0.0003541300829965621
Batch  21  loss:  0.0003636715409811586
Batch  31  loss:  0.00020460500672925264
Batch  41  loss:  0.0003332618798594922
Batch  51  loss:  0.00021954529802314937
Batch  61  loss:  0.00030039853299967945
Batch  71  loss:  0.00026566721498966217
Batch  81  loss:  0.0004181306285317987
Batch  91  loss:  0.0002942970022559166
Batch  101  loss:  0.00029889229335822165
Batch  111  loss:  0.00024089081853162497
Batch  121  loss:  0.0004028175608254969
Batch  131  loss:  0.0002525087329559028
Batch  141  loss:  0.0002996948023792356
Batch  151  loss:  0.00030789696029387414
Batch  161  loss:  0.00018515477131586522
Batch  171  loss:  0.0003956510918214917
Batch  181  loss:  0.00030846268055029213
Batch  191  loss:  0.00017333227151539177
Validation on real data: 
LOSS supervised-train 0.00030430115381022913, valid 0.0003244069521315396
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0002075779775623232
Batch  11  loss:  0.0002642364415805787
Batch  21  loss:  0.00026497934595681727
Batch  31  loss:  0.00031983733060769737
Batch  41  loss:  0.00032826897222548723
Batch  51  loss:  0.00014913427003193647
Batch  61  loss:  0.0003676643827930093
Batch  71  loss:  0.0003494111297186464
Batch  81  loss:  0.0003862754092551768
Batch  91  loss:  0.00031630881130695343
Batch  101  loss:  0.0002921904670074582
Batch  111  loss:  0.00022249636822380126
Batch  121  loss:  0.0004207307065371424
Batch  131  loss:  0.0002059137332253158
Batch  141  loss:  0.00030136987334117293
Batch  151  loss:  0.0002569620555732399
Batch  161  loss:  0.00014152027142699808
Batch  171  loss:  0.0003120721084997058
Batch  181  loss:  0.000289796240394935
Batch  191  loss:  0.00014338376058731228
Validation on real data: 
LOSS supervised-train 0.0002961276629503118, valid 0.00035124647547490895
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00026187693583779037
Batch  11  loss:  0.00029342688503675163
Batch  21  loss:  0.0002306077949469909
Batch  31  loss:  0.0003210963332094252
Batch  41  loss:  0.00019252680067438632
Batch  51  loss:  0.00018132064724341035
Batch  61  loss:  0.00039392689359374344
Batch  71  loss:  0.0004102559178136289
Batch  81  loss:  0.0004907657857984304
Batch  91  loss:  0.0003297613584436476
Batch  101  loss:  0.00034537017927505076
Batch  111  loss:  0.0002808673307299614
Batch  121  loss:  0.000536288833245635
Batch  131  loss:  0.00026392677682451904
Batch  141  loss:  0.00026788614923134446
Batch  151  loss:  0.0002885204739868641
Batch  161  loss:  0.0002279290638398379
Batch  171  loss:  0.0002980554709210992
Batch  181  loss:  0.00025607238058000803
Batch  191  loss:  0.00015798493404872715
Validation on real data: 
LOSS supervised-train 0.0003042104701307835, valid 0.000297525868518278
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  cap ; Model ID: 3dec0d851cba045fbf444790f25ea3db
--------------------
Training baseline regression model:  2022-03-29 23:57:56.559865
Detector:  point_transformer
Object:  cap
--------------------
device is  cuda
--------------------
Number of trainable parameters:  888466
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.11338668316602707
Batch  11  loss:  0.027651246637105942
Batch  21  loss:  0.013113822788000107
Batch  31  loss:  0.0101951714605093
Batch  41  loss:  0.009582476690411568
Batch  51  loss:  0.004469776060432196
Batch  61  loss:  0.006251798942685127
Batch  71  loss:  0.013937320560216904
Batch  81  loss:  0.005062835291028023
Batch  91  loss:  0.012310093268752098
Batch  101  loss:  0.005852091126143932
Batch  111  loss:  0.0023865492548793554
Batch  121  loss:  0.00531424256041646
Batch  131  loss:  0.01874525286257267
Batch  141  loss:  0.0021469590719789267
Batch  151  loss:  0.0028769236523658037
Batch  161  loss:  0.006066972855478525
Batch  171  loss:  0.002604777691885829
Batch  181  loss:  0.0017715098802000284
Batch  191  loss:  0.002417908515781164
Validation on real data: 
LOSS supervised-train 0.011553305511479267, valid 0.00197562831453979
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.002897451864555478
Batch  11  loss:  0.0009560311445966363
Batch  21  loss:  0.002757537644356489
Batch  31  loss:  0.0028840675950050354
Batch  41  loss:  0.0021708679851144552
Batch  51  loss:  0.0011352323926985264
Batch  61  loss:  0.0011680192546918988
Batch  71  loss:  0.002671475987881422
Batch  81  loss:  0.0014520033728331327
Batch  91  loss:  0.001528613269329071
Batch  101  loss:  0.0018894517561420798
Batch  111  loss:  0.0010524139506742358
Batch  121  loss:  0.002225444419309497
Batch  131  loss:  0.004557877313345671
Batch  141  loss:  0.0009434231906197965
Batch  151  loss:  0.0017213273094967008
Batch  161  loss:  0.0024173313286155462
Batch  171  loss:  0.0011504224967211485
Batch  181  loss:  0.0011578478151932359
Batch  191  loss:  0.0016398392617702484
Validation on real data: 
LOSS supervised-train 0.0017883605646784417, valid 0.0019444931531324983
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0008720075129531324
Batch  11  loss:  0.00063567073084414
Batch  21  loss:  0.0011812328593805432
Batch  31  loss:  0.0014948602765798569
Batch  41  loss:  0.0010549015132710338
Batch  51  loss:  0.0007766708731651306
Batch  61  loss:  0.0006559700123034418
Batch  71  loss:  0.0014889115700498223
Batch  81  loss:  0.0009864767780527472
Batch  91  loss:  0.0011214512633159757
Batch  101  loss:  0.0013028039829805493
Batch  111  loss:  0.0008555041276849806
Batch  121  loss:  0.0009736549691297114
Batch  131  loss:  0.0026101695839315653
Batch  141  loss:  0.0008098840480670333
Batch  151  loss:  0.0009558106539770961
Batch  161  loss:  0.0012710749870166183
Batch  171  loss:  0.0007280529825948179
Batch  181  loss:  0.0009780816035345197
Batch  191  loss:  0.0008958171238191426
Validation on real data: 
LOSS supervised-train 0.0010933249769732355, valid 0.0009620566270314157
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0006519436719827354
Batch  11  loss:  0.0006541768088936806
Batch  21  loss:  0.0011925443541258574
Batch  31  loss:  0.0012018666602671146
Batch  41  loss:  0.0007307088235393167
Batch  51  loss:  0.0007136404165066779
Batch  61  loss:  0.0007324166363105178
Batch  71  loss:  0.0009707199060358107
Batch  81  loss:  0.000860002008266747
Batch  91  loss:  0.0006886287592351437
Batch  101  loss:  0.0009842559229582548
Batch  111  loss:  0.0007377780857495964
Batch  121  loss:  0.0009569193352945149
Batch  131  loss:  0.002301965607330203
Batch  141  loss:  0.0007627950399182737
Batch  151  loss:  0.0010472266003489494
Batch  161  loss:  0.0009538233280181885
Batch  171  loss:  0.0006509521626867354
Batch  181  loss:  0.0007638668175786734
Batch  191  loss:  0.0008525018347427249
Validation on real data: 
LOSS supervised-train 0.0008755847379507031, valid 0.0006961701437830925
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0003957431763410568
Batch  11  loss:  0.0005721203051507473
Batch  21  loss:  0.0009950778912752867
Batch  31  loss:  0.0010187439620494843
Batch  41  loss:  0.0005906586302444339
Batch  51  loss:  0.0006067758076824248
Batch  61  loss:  0.0004970792797394097
Batch  71  loss:  0.0006877000560052693
Batch  81  loss:  0.0008721519261598587
Batch  91  loss:  0.0008579131681472063
Batch  101  loss:  0.0006693979958072305
Batch  111  loss:  0.0007589627639390528
Batch  121  loss:  0.0006848230841569602
Batch  131  loss:  0.0014882230898365378
Batch  141  loss:  0.0008076751255430281
Batch  151  loss:  0.0008723097271285951
Batch  161  loss:  0.0009807918686419725
Batch  171  loss:  0.0005648063961416483
Batch  181  loss:  0.0005883745034225285
Batch  191  loss:  0.0008999198325909674
Validation on real data: 
LOSS supervised-train 0.0007420131689286791, valid 0.00119929073844105
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0005385330296121538
Batch  11  loss:  0.0004056582984048873
Batch  21  loss:  0.0008033765479922295
Batch  31  loss:  0.0007547427667304873
Batch  41  loss:  0.0006004227907396853
Batch  51  loss:  0.0005238528246991336
Batch  61  loss:  0.000562526227440685
Batch  71  loss:  0.0007254470838233829
Batch  81  loss:  0.0008679635357111692
Batch  91  loss:  0.000496925029437989
Batch  101  loss:  0.0006932595861144364
Batch  111  loss:  0.0005659501766785979
Batch  121  loss:  0.0005141649744473398
Batch  131  loss:  0.0012879626592621207
Batch  141  loss:  0.0004568818840198219
Batch  151  loss:  0.0006931208772584796
Batch  161  loss:  0.0007923080120235682
Batch  171  loss:  0.00045607989886775613
Batch  181  loss:  0.0005879831151105464
Batch  191  loss:  0.0006790399202145636
Validation on real data: 
LOSS supervised-train 0.0006253308047598693, valid 0.0007713189115747809
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0004440922930371016
Batch  11  loss:  0.00043246179120615125
Batch  21  loss:  0.0006855296087451279
Batch  31  loss:  0.0005916596273891628
Batch  41  loss:  0.00045809708535671234
Batch  51  loss:  0.0005023367120884359
Batch  61  loss:  0.0003521608014125377
Batch  71  loss:  0.0005211099050939083
Batch  81  loss:  0.0005137026309967041
Batch  91  loss:  0.0005015427013859153
Batch  101  loss:  0.0005408094148151577
Batch  111  loss:  0.0007388484082184732
Batch  121  loss:  0.0005751302815042436
Batch  131  loss:  0.00138873013202101
Batch  141  loss:  0.0006185935344547033
Batch  151  loss:  0.0007847232045605779
Batch  161  loss:  0.0006420761928893626
Batch  171  loss:  0.00040338910184800625
Batch  181  loss:  0.00046410225331783295
Batch  191  loss:  0.0007543687243014574
Validation on real data: 
LOSS supervised-train 0.0005558315663074609, valid 0.0006165116792544723
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0003292843757662922
Batch  11  loss:  0.0003107606607954949
Batch  21  loss:  0.0005524828447960317
Batch  31  loss:  0.0005722643109038472
Batch  41  loss:  0.00032803299836814404
Batch  51  loss:  0.0003466535417828709
Batch  61  loss:  0.0003611080173868686
Batch  71  loss:  0.00048774719471111894
Batch  81  loss:  0.000633514835499227
Batch  91  loss:  0.0006258424837142229
Batch  101  loss:  0.0003938502341043204
Batch  111  loss:  0.0004529270518105477
Batch  121  loss:  0.0005300003685988486
Batch  131  loss:  0.0010701162973418832
Batch  141  loss:  0.0004696612013503909
Batch  151  loss:  0.0006985832587815821
Batch  161  loss:  0.0003564923827070743
Batch  171  loss:  0.00038551047327928245
Batch  181  loss:  0.0005749174160882831
Batch  191  loss:  0.0006890638032928109
Validation on real data: 
LOSS supervised-train 0.0005128423260612181, valid 0.0005706685478799045
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.00035909810685552657
Batch  11  loss:  0.00039115501567721367
Batch  21  loss:  0.0005029999883845448
Batch  31  loss:  0.0005124723538756371
Batch  41  loss:  0.0004917547921650112
Batch  51  loss:  0.0004368521040305495
Batch  61  loss:  0.0003426410839892924
Batch  71  loss:  0.00040086006629280746
Batch  81  loss:  0.000534702674485743
Batch  91  loss:  0.0004582963010761887
Batch  101  loss:  0.000490637612529099
Batch  111  loss:  0.00033749983413144946
Batch  121  loss:  0.0004408505337778479
Batch  131  loss:  0.0009135599248111248
Batch  141  loss:  0.0005388262798078358
Batch  151  loss:  0.0006156263407319784
Batch  161  loss:  0.0006187674007378519
Batch  171  loss:  0.0003549016546458006
Batch  181  loss:  0.0005500905099324882
Batch  191  loss:  0.0007190205506049097
Validation on real data: 
LOSS supervised-train 0.00047465399955399335, valid 0.0005260773468762636
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0003325659781694412
Batch  11  loss:  0.00023541873088106513
Batch  21  loss:  0.0005905578145757318
Batch  31  loss:  0.0005309171974658966
Batch  41  loss:  0.0003397272957954556
Batch  51  loss:  0.0004916063626296818
Batch  61  loss:  0.00030989080551080406
Batch  71  loss:  0.000388263986678794
Batch  81  loss:  0.00039297761395573616
Batch  91  loss:  0.0003557046875357628
Batch  101  loss:  0.0004133248294238001
Batch  111  loss:  0.00037261471152305603
Batch  121  loss:  0.0003971396363340318
Batch  131  loss:  0.0006916679558344185
Batch  141  loss:  0.00032872764859348536
Batch  151  loss:  0.0005662153707817197
Batch  161  loss:  0.0004945052205584943
Batch  171  loss:  0.00025344567256979644
Batch  181  loss:  0.0005174899706616998
Batch  191  loss:  0.0006059143343009055
Validation on real data: 
LOSS supervised-train 0.00044497002752905243, valid 0.0006277745706029236
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0002719444746617228
Batch  11  loss:  0.0003189587441738695
Batch  21  loss:  0.0005804007523693144
Batch  31  loss:  0.0004171762557234615
Batch  41  loss:  0.0003918374131899327
Batch  51  loss:  0.0003497537400107831
Batch  61  loss:  0.0003660740330815315
Batch  71  loss:  0.0003224603133276105
Batch  81  loss:  0.00034653395414352417
Batch  91  loss:  0.00023538153618574142
Batch  101  loss:  0.00045138856512494385
Batch  111  loss:  0.0003317050286568701
Batch  121  loss:  0.00045302207581698895
Batch  131  loss:  0.0006500156596302986
Batch  141  loss:  0.00035238306736573577
Batch  151  loss:  0.0004329713701736182
Batch  161  loss:  0.000450034043751657
Batch  171  loss:  0.00025948078837245703
Batch  181  loss:  0.0004336844722274691
Batch  191  loss:  0.0005794093594886363
Validation on real data: 
LOSS supervised-train 0.0004059346307622036, valid 0.00042028247844427824
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0003150640695821494
Batch  11  loss:  0.0002755690657068044
Batch  21  loss:  0.0004392497939988971
Batch  31  loss:  0.00039319772622548044
Batch  41  loss:  0.00022025941871106625
Batch  51  loss:  0.00038396462332457304
Batch  61  loss:  0.0003113361308351159
Batch  71  loss:  0.0003031923552043736
Batch  81  loss:  0.0003150302800349891
Batch  91  loss:  0.0004664510488510132
Batch  101  loss:  0.00045121851144358516
Batch  111  loss:  0.00031531407148577273
Batch  121  loss:  0.00024407505406998098
Batch  131  loss:  0.0005357422633096576
Batch  141  loss:  0.0003046584315598011
Batch  151  loss:  0.0004231605271343142
Batch  161  loss:  0.000459764851257205
Batch  171  loss:  0.0003366175515111536
Batch  181  loss:  0.00040335903759114444
Batch  191  loss:  0.0005316337337717414
Validation on real data: 
LOSS supervised-train 0.000369783100541099, valid 0.00041408921242691576
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0002538757398724556
Batch  11  loss:  0.00026244763284921646
Batch  21  loss:  0.0003148244577459991
Batch  31  loss:  0.00040318057290278375
Batch  41  loss:  0.0003486345231067389
Batch  51  loss:  0.0003522399056237191
Batch  61  loss:  0.00024226288951467723
Batch  71  loss:  0.00043622314115054905
Batch  81  loss:  0.00036770544829778373
Batch  91  loss:  0.00033266207901760936
Batch  101  loss:  0.0004115555202588439
Batch  111  loss:  0.00040896161226555705
Batch  121  loss:  0.0003198983904439956
Batch  131  loss:  0.0005589937791228294
Batch  141  loss:  0.00033205337240360677
Batch  151  loss:  0.0005631547537632287
Batch  161  loss:  0.00035270187072455883
Batch  171  loss:  0.00031881924951449037
Batch  181  loss:  0.00041725538903847337
Batch  191  loss:  0.0005262608756311238
Validation on real data: 
LOSS supervised-train 0.00036688675747427625, valid 0.0004251502105034888
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00024141353787854314
Batch  11  loss:  0.000282799155684188
Batch  21  loss:  0.00038899958599358797
Batch  31  loss:  0.0004519313806667924
Batch  41  loss:  0.0003308666928205639
Batch  51  loss:  0.0003372725914232433
Batch  61  loss:  0.0002778494090307504
Batch  71  loss:  0.00033260195050388575
Batch  81  loss:  0.0002896262158174068
Batch  91  loss:  0.0003339257091283798
Batch  101  loss:  0.0003318420785944909
Batch  111  loss:  0.00035891018342226744
Batch  121  loss:  0.00033491081558167934
Batch  131  loss:  0.0005951198982074857
Batch  141  loss:  0.0003281555254943669
Batch  151  loss:  0.00042615592246875167
Batch  161  loss:  0.00040665449341759086
Batch  171  loss:  0.0003032666281796992
Batch  181  loss:  0.00038743458571843803
Batch  191  loss:  0.0005069951293990016
Validation on real data: 
LOSS supervised-train 0.0003545428176585119, valid 0.0004849641409236938
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0002639557933434844
Batch  11  loss:  0.0003016041882801801
Batch  21  loss:  0.0004605047288350761
Batch  31  loss:  0.0002992556255776435
Batch  41  loss:  0.00021590145479422063
Batch  51  loss:  0.00031494369613938034
Batch  61  loss:  0.0002502812712918967
Batch  71  loss:  0.00028166393167339265
Batch  81  loss:  0.00030986531055532396
Batch  91  loss:  0.00035295242560096085
Batch  101  loss:  0.000394569244235754
Batch  111  loss:  0.00036413970519788563
Batch  121  loss:  0.00029992058989591897
Batch  131  loss:  0.0006626578397117555
Batch  141  loss:  0.0003479381266515702
Batch  151  loss:  0.0003976610896643251
Batch  161  loss:  0.00035802333150058985
Batch  171  loss:  0.00026429109857417643
Batch  181  loss:  0.0003790423215832561
Batch  191  loss:  0.0004299411957617849
Validation on real data: 
LOSS supervised-train 0.00034063682331179733, valid 0.0004980651428923011
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.00022904567595105618
Batch  11  loss:  0.00020696337742265314
Batch  21  loss:  0.00033636673470027745
Batch  31  loss:  0.00030301924562081695
Batch  41  loss:  0.00027197712915949523
Batch  51  loss:  0.00032736704451963305
Batch  61  loss:  0.00020108588796574622
Batch  71  loss:  0.0002854610502254218
Batch  81  loss:  0.0003152125282213092
Batch  91  loss:  0.00029134153737686574
Batch  101  loss:  0.00038558017695322633
Batch  111  loss:  0.0003663739189505577
Batch  121  loss:  0.0002913871721830219
Batch  131  loss:  0.0005227841902524233
Batch  141  loss:  0.0002796150802168995
Batch  151  loss:  0.0005968243349343538
Batch  161  loss:  0.0004112754249945283
Batch  171  loss:  0.00032086647115647793
Batch  181  loss:  0.00032554991776123643
Batch  191  loss:  0.0005654768901877105
Validation on real data: 
LOSS supervised-train 0.0003247429197654128, valid 0.0003518693265505135
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0002411698515061289
Batch  11  loss:  0.0002062811836367473
Batch  21  loss:  0.00035965361166745424
Batch  31  loss:  0.000331571907736361
Batch  41  loss:  0.00023561868874821812
Batch  51  loss:  0.0002881296968553215
Batch  61  loss:  0.0002162193413823843
Batch  71  loss:  0.0002575747494120151
Batch  81  loss:  0.0002811129088513553
Batch  91  loss:  0.0002832406316883862
Batch  101  loss:  0.0002622563042677939
Batch  111  loss:  0.0002152415254386142
Batch  121  loss:  0.00030164248892106116
Batch  131  loss:  0.0005355516914278269
Batch  141  loss:  0.00024070951621979475
Batch  151  loss:  0.0004081713268533349
Batch  161  loss:  0.0003537734446581453
Batch  171  loss:  0.0002651922404766083
Batch  181  loss:  0.0002547542389947921
Batch  191  loss:  0.0003917644207831472
Validation on real data: 
LOSS supervised-train 0.0003043820965103805, valid 0.0004467069520615041
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0003034171531908214
Batch  11  loss:  0.0001720484724501148
Batch  21  loss:  0.0003077480650972575
Batch  31  loss:  0.00032888902933336794
Batch  41  loss:  0.0003695162304211408
Batch  51  loss:  0.00028845990891568363
Batch  61  loss:  0.0002128830528818071
Batch  71  loss:  0.0002811575250234455
Batch  81  loss:  0.00030282424995675683
Batch  91  loss:  0.00033779165823943913
Batch  101  loss:  0.00028052242123521864
Batch  111  loss:  0.0003031666565220803
Batch  121  loss:  0.0002275858714710921
Batch  131  loss:  0.0005259945173747838
Batch  141  loss:  0.0002253645652672276
Batch  151  loss:  0.00024381217372138053
Batch  161  loss:  0.00028786342591047287
Batch  171  loss:  0.00027943431632593274
Batch  181  loss:  0.00035892269806936383
Batch  191  loss:  0.0004302918678149581
Validation on real data: 
LOSS supervised-train 0.00031767160435265397, valid 0.00047578991507180035
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.00027897401014342904
Batch  11  loss:  0.00022181074018590152
Batch  21  loss:  0.0003008512721862644
Batch  31  loss:  0.0002538827829994261
Batch  41  loss:  0.0002856008941307664
Batch  51  loss:  0.00021841700072400272
Batch  61  loss:  0.0002084262960124761
Batch  71  loss:  0.00023816507018636912
Batch  81  loss:  0.0002736753085628152
Batch  91  loss:  0.0002950351045001298
Batch  101  loss:  0.00030519545543938875
Batch  111  loss:  0.0002382834063610062
Batch  121  loss:  0.00026992554194293916
Batch  131  loss:  0.0006814847583882511
Batch  141  loss:  0.00021359778475016356
Batch  151  loss:  0.0003356998786330223
Batch  161  loss:  0.0004026966926176101
Batch  171  loss:  0.00023193744709715247
Batch  181  loss:  0.00030298734782263637
Batch  191  loss:  0.0003996334853582084
Validation on real data: 
LOSS supervised-train 0.0002950697174674133, valid 0.0003679774818010628
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0003297750954516232
Batch  11  loss:  0.00018368136079516262
Batch  21  loss:  0.00034084811341017485
Batch  31  loss:  0.00029080527019686997
Batch  41  loss:  0.0001987982541322708
Batch  51  loss:  0.00031838417635299265
Batch  61  loss:  0.000241080837440677
Batch  71  loss:  0.00021640949125867337
Batch  81  loss:  0.0003228837449569255
Batch  91  loss:  0.00024867133470252156
Batch  101  loss:  0.00028048348031006753
Batch  111  loss:  0.0003049710940103978
Batch  121  loss:  0.00028508936520665884
Batch  131  loss:  0.0006235279724933207
Batch  141  loss:  0.0003048615180887282
Batch  151  loss:  0.00036107219057157636
Batch  161  loss:  0.0002660222235135734
Batch  171  loss:  0.00020633495296351612
Batch  181  loss:  0.0003177336184307933
Batch  191  loss:  0.00036986812483519316
Validation on real data: 
LOSS supervised-train 0.00029260716677526944, valid 0.0003737724618986249
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.00023156094539444894
Batch  11  loss:  0.00023699049779679626
Batch  21  loss:  0.00031818693969398737
Batch  31  loss:  0.00026104736025445163
Batch  41  loss:  0.0002927381719928235
Batch  51  loss:  0.000217079883441329
Batch  61  loss:  0.00017818299238570035
Batch  71  loss:  0.0002559507847763598
Batch  81  loss:  0.0002745670499280095
Batch  91  loss:  0.000253575446549803
Batch  101  loss:  0.00030151655664667487
Batch  111  loss:  0.0002707340463530272
Batch  121  loss:  0.00031301527633331716
Batch  131  loss:  0.0004839944595005363
Batch  141  loss:  0.00017698245937936008
Batch  151  loss:  0.00037678563967347145
Batch  161  loss:  0.00035426372778601944
Batch  171  loss:  0.00022603200341109186
Batch  181  loss:  0.00031416662386618555
Batch  191  loss:  0.0004568654694594443
Validation on real data: 
LOSS supervised-train 0.00029576477172668094, valid 0.00036271641147322953
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.00026109229656867683
Batch  11  loss:  0.0001955820043804124
Batch  21  loss:  0.0001772372197592631
Batch  31  loss:  0.00034865763154812157
Batch  41  loss:  0.00020619489077944309
Batch  51  loss:  0.0002660405880305916
Batch  61  loss:  0.00018251682922709733
Batch  71  loss:  0.00022615003399550915
Batch  81  loss:  0.00021353257761802524
Batch  91  loss:  0.00020145448797848076
Batch  101  loss:  0.0002480088733136654
Batch  111  loss:  0.00027128623332828283
Batch  121  loss:  0.00026997539680451155
Batch  131  loss:  0.0004724082536995411
Batch  141  loss:  0.00020011008018627763
Batch  151  loss:  0.000385039224056527
Batch  161  loss:  0.0004089239810127765
Batch  171  loss:  0.0002745596575550735
Batch  181  loss:  0.0002948629262391478
Batch  191  loss:  0.00038177426904439926
Validation on real data: 
LOSS supervised-train 0.0002712169858568814, valid 0.000251618679612875
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0002331389405298978
Batch  11  loss:  0.00016819487791508436
Batch  21  loss:  0.00028480993933044374
Batch  31  loss:  0.00022967427503317595
Batch  41  loss:  0.00022135682229418308
Batch  51  loss:  0.00020657654386013746
Batch  61  loss:  0.00022973628074396402
Batch  71  loss:  0.00024197480524890125
Batch  81  loss:  0.0003232006565667689
Batch  91  loss:  0.00025040999753400683
Batch  101  loss:  0.00020815178868360817
Batch  111  loss:  0.0003631307336036116
Batch  121  loss:  0.00024871359346434474
Batch  131  loss:  0.0004265614552423358
Batch  141  loss:  0.00025610270677134395
Batch  151  loss:  0.00036672005080617964
Batch  161  loss:  0.0002705055521801114
Batch  171  loss:  0.0002928992034867406
Batch  181  loss:  0.0002867957518901676
Batch  191  loss:  0.0003022481396328658
Validation on real data: 
LOSS supervised-train 0.0002711273540990078, valid 0.0004102622624486685
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0002396773488726467
Batch  11  loss:  0.0001911437138915062
Batch  21  loss:  0.0002609897928778082
Batch  31  loss:  0.00021182365890126675
Batch  41  loss:  0.00023247703211382031
Batch  51  loss:  0.00023601103748660535
Batch  61  loss:  0.0002113219234161079
Batch  71  loss:  0.00015961870667524636
Batch  81  loss:  0.00024966467753984034
Batch  91  loss:  0.00030084524769335985
Batch  101  loss:  0.00021047711197752506
Batch  111  loss:  0.00033266018726862967
Batch  121  loss:  0.00014354712038766593
Batch  131  loss:  0.0004280582070350647
Batch  141  loss:  0.00019637861987575889
Batch  151  loss:  0.0003029520739801228
Batch  161  loss:  0.00019210332538932562
Batch  171  loss:  0.00021962459140922874
Batch  181  loss:  0.00027446725289337337
Batch  191  loss:  0.0002643611514940858
Validation on real data: 
LOSS supervised-train 0.00025758955111086833, valid 0.0002609516668599099
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0002666731015779078
Batch  11  loss:  0.00017162665608339012
Batch  21  loss:  0.0003267641586717218
Batch  31  loss:  0.00019878741295542568
Batch  41  loss:  0.00023122863785829395
Batch  51  loss:  0.0002622764150146395
Batch  61  loss:  0.00023103294370230287
Batch  71  loss:  0.00015224888920783997
Batch  81  loss:  0.00020671113452408463
Batch  91  loss:  0.00021947886853013188
Batch  101  loss:  0.00024364351702388376
Batch  111  loss:  0.00021513018873520195
Batch  121  loss:  0.00032792225829325616
Batch  131  loss:  0.0003868058556690812
Batch  141  loss:  0.00025892278063111007
Batch  151  loss:  0.00037674253690056503
Batch  161  loss:  0.0003146431117784232
Batch  171  loss:  0.0001895803725346923
Batch  181  loss:  0.00024987448705360293
Batch  191  loss:  0.00028599894721992314
Validation on real data: 
LOSS supervised-train 0.0002632562132930616, valid 0.00023090827744454145
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.00019813928520306945
Batch  11  loss:  0.00014677116996608675
Batch  21  loss:  0.0002455572539474815
Batch  31  loss:  0.00025995983742177486
Batch  41  loss:  0.00023455901828128844
Batch  51  loss:  0.00024959887377917767
Batch  61  loss:  0.00020201194274704903
Batch  71  loss:  0.00020925782155245543
Batch  81  loss:  0.00027888527256436646
Batch  91  loss:  0.00019403063924983144
Batch  101  loss:  0.0003755907528102398
Batch  111  loss:  0.0003338006208650768
Batch  121  loss:  0.00028705349541269243
Batch  131  loss:  0.0003220242215320468
Batch  141  loss:  0.0001750627561705187
Batch  151  loss:  0.00029253732645884156
Batch  161  loss:  0.0002943124563898891
Batch  171  loss:  0.00025819469010457397
Batch  181  loss:  0.00028181582456454635
Batch  191  loss:  0.00043054891284555197
Validation on real data: 
LOSS supervised-train 0.0002535548610467231, valid 0.0003156244056299329
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00023731010151095688
Batch  11  loss:  0.00019251588673796505
Batch  21  loss:  0.00024449985357932746
Batch  31  loss:  0.00025222290423698723
Batch  41  loss:  0.00024326026323251426
Batch  51  loss:  0.00016977354243863374
Batch  61  loss:  0.00016904597578104585
Batch  71  loss:  0.0002482520358171314
Batch  81  loss:  0.0002256773877888918
Batch  91  loss:  0.00026881994563154876
Batch  101  loss:  0.00025731706409715116
Batch  111  loss:  0.00023319113824982196
Batch  121  loss:  0.00024116042186506093
Batch  131  loss:  0.00035687623312696815
Batch  141  loss:  0.00022669139434583485
Batch  151  loss:  0.0003454756224527955
Batch  161  loss:  0.00020145371672697365
Batch  171  loss:  0.0001592691260157153
Batch  181  loss:  0.00025290061603300273
Batch  191  loss:  0.0002980382414534688
Validation on real data: 
LOSS supervised-train 0.00024234331995103274, valid 0.0003202641964890063
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0001337295980192721
Batch  11  loss:  0.00018609884136822075
Batch  21  loss:  0.0001924889802467078
Batch  31  loss:  0.00026147192693315446
Batch  41  loss:  0.00016990663425531238
Batch  51  loss:  0.00022629878367297351
Batch  61  loss:  0.00016447232337668538
Batch  71  loss:  0.00024219942861236632
Batch  81  loss:  0.00019882891501765698
Batch  91  loss:  0.00025291935889981687
Batch  101  loss:  0.00023650161165278405
Batch  111  loss:  0.0002745394594967365
Batch  121  loss:  0.00032675033435225487
Batch  131  loss:  0.000360696081770584
Batch  141  loss:  0.00018556251598056406
Batch  151  loss:  0.00023668970970902592
Batch  161  loss:  0.00032184869633056223
Batch  171  loss:  0.00021215483138803393
Batch  181  loss:  0.000342513551004231
Batch  191  loss:  0.00022241163242142648
Validation on real data: 
LOSS supervised-train 0.00024382406896620524, valid 0.00026026455452665687
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00023627634800504893
Batch  11  loss:  0.00012104547931812704
Batch  21  loss:  0.00024905294412747025
Batch  31  loss:  0.00024929430219344795
Batch  41  loss:  0.0002173601824324578
Batch  51  loss:  0.00023572426289319992
Batch  61  loss:  0.00017769236001186073
Batch  71  loss:  0.0002032226329902187
Batch  81  loss:  0.0001934336469275877
Batch  91  loss:  0.00015887558402027935
Batch  101  loss:  0.00028268073219805956
Batch  111  loss:  0.00017926777945831418
Batch  121  loss:  0.00020688695076387376
Batch  131  loss:  0.0003352296771481633
Batch  141  loss:  0.00017743557691574097
Batch  151  loss:  0.00037401309236884117
Batch  161  loss:  0.0003887814236804843
Batch  171  loss:  0.00021623923385050148
Batch  181  loss:  0.00025963003281503916
Batch  191  loss:  0.0003142093191854656
Validation on real data: 
LOSS supervised-train 0.00024022118155698992, valid 0.0002754341112449765
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0002015936333918944
Batch  11  loss:  0.00019440895994193852
Batch  21  loss:  0.00022958274348638952
Batch  31  loss:  0.0002432206820230931
Batch  41  loss:  0.00019151013111695647
Batch  51  loss:  0.00017645570915192366
Batch  61  loss:  0.00016975119069684297
Batch  71  loss:  0.0001963616523426026
Batch  81  loss:  0.00023520903778262436
Batch  91  loss:  0.00022591307060793042
Batch  101  loss:  0.00022937716857995838
Batch  111  loss:  0.00020686976495198905
Batch  121  loss:  0.0003112901176791638
Batch  131  loss:  0.00043361535063013434
Batch  141  loss:  0.00016127459821291268
Batch  151  loss:  0.0002339305356144905
Batch  161  loss:  0.00025546172400936484
Batch  171  loss:  0.00019975489703938365
Batch  181  loss:  0.00021458190167322755
Batch  191  loss:  0.0002921385457739234
Validation on real data: 
LOSS supervised-train 0.00023387004253891064, valid 0.000262639980064705
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0001342200703220442
Batch  11  loss:  0.00013868763926438987
Batch  21  loss:  0.0002950796333607286
Batch  31  loss:  0.0002594456891529262
Batch  41  loss:  0.00018051540246233344
Batch  51  loss:  0.0002113720402121544
Batch  61  loss:  0.0002073115756502375
Batch  71  loss:  0.00020693123224191368
Batch  81  loss:  0.0002494791115168482
Batch  91  loss:  0.00016007958038244396
Batch  101  loss:  0.00024540501181036234
Batch  111  loss:  0.000259070802712813
Batch  121  loss:  0.00022103740775492042
Batch  131  loss:  0.00035971342003904283
Batch  141  loss:  0.0001601932308403775
Batch  151  loss:  0.0002370494621573016
Batch  161  loss:  0.000307624286506325
Batch  171  loss:  0.00021177624876145273
Batch  181  loss:  0.00026068100123666227
Batch  191  loss:  0.00025093572912737727
Validation on real data: 
LOSS supervised-train 0.0002166694509651279, valid 0.0002429835731163621
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.00018837943207472563
Batch  11  loss:  0.00018788460874930024
Batch  21  loss:  0.00029583959258161485
Batch  31  loss:  0.00021797837689518929
Batch  41  loss:  0.0001413298596162349
Batch  51  loss:  0.00016361560847144574
Batch  61  loss:  0.00021386526350397617
Batch  71  loss:  0.00020372826838865876
Batch  81  loss:  0.00020836747717112303
Batch  91  loss:  0.00022878202435094863
Batch  101  loss:  0.00020900538947898895
Batch  111  loss:  0.0002354614407522604
Batch  121  loss:  0.0002105648600263521
Batch  131  loss:  0.0004374986165203154
Batch  141  loss:  0.00026881322264671326
Batch  151  loss:  0.00029373812139965594
Batch  161  loss:  0.00025278926477767527
Batch  171  loss:  0.00020942947594448924
Batch  181  loss:  0.00017186235345434397
Batch  191  loss:  0.00027728165150620043
Validation on real data: 
LOSS supervised-train 0.00022158995030622464, valid 0.0002958737313747406
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0001898586197057739
Batch  11  loss:  0.00014557524991687387
Batch  21  loss:  0.0002142049343092367
Batch  31  loss:  0.00019210220489185303
Batch  41  loss:  0.00021017629478592426
Batch  51  loss:  0.00021351284522097558
Batch  61  loss:  0.00016933493316173553
Batch  71  loss:  0.0001373774284729734
Batch  81  loss:  0.00023044545378070325
Batch  91  loss:  0.00013465374649967998
Batch  101  loss:  0.00018489648937247694
Batch  111  loss:  0.0002381721424171701
Batch  121  loss:  0.0002740725758485496
Batch  131  loss:  0.0003617613692767918
Batch  141  loss:  0.00016377147403545678
Batch  151  loss:  0.00024283640959765762
Batch  161  loss:  0.00025670448667369783
Batch  171  loss:  0.00021563010523095727
Batch  181  loss:  0.0001958732318598777
Batch  191  loss:  0.00024417706299573183
Validation on real data: 
LOSS supervised-train 0.00021981019635859412, valid 0.0002765957615338266
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00018342729890719056
Batch  11  loss:  0.0001427371462341398
Batch  21  loss:  0.00018475933757144958
Batch  31  loss:  0.000239052576944232
Batch  41  loss:  0.00021539273438975215
Batch  51  loss:  0.00023653946118429303
Batch  61  loss:  0.00018163718050345778
Batch  71  loss:  0.00022186897695064545
Batch  81  loss:  0.000219189198105596
Batch  91  loss:  0.0002020868269028142
Batch  101  loss:  0.00025490805273875594
Batch  111  loss:  0.00026199035346508026
Batch  121  loss:  0.00025221132091246545
Batch  131  loss:  0.00034093408612534404
Batch  141  loss:  0.00020976013911422342
Batch  151  loss:  0.0002487462479621172
Batch  161  loss:  0.00026120617985725403
Batch  171  loss:  0.000231011668802239
Batch  181  loss:  0.0002451522450428456
Batch  191  loss:  0.0003034606925211847
Validation on real data: 
LOSS supervised-train 0.00021969620254822075, valid 0.00025556216132827103
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0002136280672857538
Batch  11  loss:  0.00020640793081838638
Batch  21  loss:  0.00023646795307286084
Batch  31  loss:  0.00019701707060448825
Batch  41  loss:  0.0002188548824051395
Batch  51  loss:  0.000257457431871444
Batch  61  loss:  0.0001805421634344384
Batch  71  loss:  0.00017929931345861405
Batch  81  loss:  0.0001806980581022799
Batch  91  loss:  0.00019782486197073013
Batch  101  loss:  0.00028153808671049774
Batch  111  loss:  0.00018755729252006859
Batch  121  loss:  0.0002777791232801974
Batch  131  loss:  0.0003002703015226871
Batch  141  loss:  0.00019398062431719154
Batch  151  loss:  0.000218462897464633
Batch  161  loss:  0.00020456490165088326
Batch  171  loss:  0.00017911664326675236
Batch  181  loss:  0.00025805478799156845
Batch  191  loss:  0.00021643574291374534
Validation on real data: 
LOSS supervised-train 0.00021675943604350324, valid 0.0001919670612551272
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00021018905681557953
Batch  11  loss:  0.00013625803694594651
Batch  21  loss:  0.00017903471598401666
Batch  31  loss:  0.00017447650316171348
Batch  41  loss:  0.00018644201918505132
Batch  51  loss:  0.00020280020544305444
Batch  61  loss:  0.00013008700625505298
Batch  71  loss:  0.00015024181629996747
Batch  81  loss:  0.00024548708461225033
Batch  91  loss:  0.00019546538533177227
Batch  101  loss:  0.00023958466772455722
Batch  111  loss:  0.0001594898640178144
Batch  121  loss:  0.000191921106306836
Batch  131  loss:  0.00033060010173358023
Batch  141  loss:  0.00018374920182395726
Batch  151  loss:  0.000267548457486555
Batch  161  loss:  0.0002030822215601802
Batch  171  loss:  0.00022464121866505593
Batch  181  loss:  0.0002032699267147109
Batch  191  loss:  0.0002784263633657247
Validation on real data: 
LOSS supervised-train 0.00020456672496948158, valid 0.00021775445202365518
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0001448124967282638
Batch  11  loss:  0.00022267478925641626
Batch  21  loss:  0.0001725237088976428
Batch  31  loss:  0.00024421513080596924
Batch  41  loss:  0.0002195377164753154
Batch  51  loss:  0.0001772189134499058
Batch  61  loss:  0.0001363591873086989
Batch  71  loss:  0.00027121679158881307
Batch  81  loss:  0.00019725918537005782
Batch  91  loss:  0.00021241958893369883
Batch  101  loss:  0.00020703750487882644
Batch  111  loss:  0.00023895179037936032
Batch  121  loss:  0.00024317837960552424
Batch  131  loss:  0.00038276327541098
Batch  141  loss:  0.00018175534205511212
Batch  151  loss:  0.0002170366351492703
Batch  161  loss:  0.00019678422540891916
Batch  171  loss:  0.00025170837761834264
Batch  181  loss:  0.0002264014328829944
Batch  191  loss:  0.00028248655144125223
Validation on real data: 
LOSS supervised-train 0.00021338235026632901, valid 0.0003059080336242914
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00012745257117785513
Batch  11  loss:  0.0001697712141321972
Batch  21  loss:  0.00014576826652046293
Batch  31  loss:  0.00018085725605487823
Batch  41  loss:  0.00018739077495411038
Batch  51  loss:  0.00020015885820612311
Batch  61  loss:  0.00020932176266796887
Batch  71  loss:  0.00017859594663605094
Batch  81  loss:  0.0001827155938372016
Batch  91  loss:  0.00021638777980115265
Batch  101  loss:  0.00022002329933457077
Batch  111  loss:  0.0001780744205461815
Batch  121  loss:  0.00020777809550054371
Batch  131  loss:  0.0003158762992825359
Batch  141  loss:  0.00017533372738398612
Batch  151  loss:  0.00021924584871158004
Batch  161  loss:  0.0003379131085239351
Batch  171  loss:  0.00019950553542003036
Batch  181  loss:  0.0002774382592178881
Batch  191  loss:  0.0002748306142166257
Validation on real data: 
LOSS supervised-train 0.00020723931484099012, valid 0.00022841324971523136
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.00025189973530359566
Batch  11  loss:  0.00011984840966761112
Batch  21  loss:  0.00019871500262524933
Batch  31  loss:  0.0002970340137835592
Batch  41  loss:  0.00013007293455302715
Batch  51  loss:  0.00014842735254205763
Batch  61  loss:  0.00011868266301462427
Batch  71  loss:  0.00020600917923729867
Batch  81  loss:  0.00018720859952736646
Batch  91  loss:  0.00019695149967446923
Batch  101  loss:  0.0001417814492015168
Batch  111  loss:  0.0001982449903152883
Batch  121  loss:  0.000344758213032037
Batch  131  loss:  0.00030409236205741763
Batch  141  loss:  0.00021688635752070695
Batch  151  loss:  0.0002216708817286417
Batch  161  loss:  0.0002868224692065269
Batch  171  loss:  0.00020950761972926557
Batch  181  loss:  0.00024300170480273664
Batch  191  loss:  0.00022270403860602528
Validation on real data: 
LOSS supervised-train 0.00021619321356411092, valid 0.0002277835737913847
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0001924456300912425
Batch  11  loss:  0.00013989495346322656
Batch  21  loss:  0.00017393437155988067
Batch  31  loss:  0.00022279498807620257
Batch  41  loss:  0.00017543722060509026
Batch  51  loss:  0.00021731125889346004
Batch  61  loss:  0.00018050424114335328
Batch  71  loss:  0.00015854595403652638
Batch  81  loss:  0.00022592395544052124
Batch  91  loss:  0.00020401354413479567
Batch  101  loss:  0.00023407030676025897
Batch  111  loss:  0.00016451960254926234
Batch  121  loss:  0.00017713106353767216
Batch  131  loss:  0.00035320635652169585
Batch  141  loss:  0.00016618506924714893
Batch  151  loss:  0.0002264268259750679
Batch  161  loss:  0.0002480311959516257
Batch  171  loss:  0.0001919399219332263
Batch  181  loss:  0.0002574709360487759
Batch  191  loss:  0.000224154835450463
Validation on real data: 
LOSS supervised-train 0.0002034092600661097, valid 0.00030773665639571846
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0001634631335036829
Batch  11  loss:  0.00011903107224497944
Batch  21  loss:  0.0002054850774584338
Batch  31  loss:  0.0001932623126776889
Batch  41  loss:  0.00018543566693551838
Batch  51  loss:  0.00015611093840561807
Batch  61  loss:  0.00013799953740090132
Batch  71  loss:  0.00015931167581584305
Batch  81  loss:  0.00019628349400591105
Batch  91  loss:  0.00013743432646151632
Batch  101  loss:  0.00019421434262767434
Batch  111  loss:  0.00021673721494153142
Batch  121  loss:  0.00021668274712283164
Batch  131  loss:  0.0003334744833409786
Batch  141  loss:  0.0001581608084961772
Batch  151  loss:  0.00022762472508475184
Batch  161  loss:  0.00024741844390518963
Batch  171  loss:  0.00016982028319034725
Batch  181  loss:  0.00021327915601432323
Batch  191  loss:  0.0001878194889286533
Validation on real data: 
LOSS supervised-train 0.00019683486545545747, valid 0.00024091413069982082
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00016905380471143872
Batch  11  loss:  0.0001272715744562447
Batch  21  loss:  0.00026011629961431026
Batch  31  loss:  0.0001482699008192867
Batch  41  loss:  0.00017674466653261334
Batch  51  loss:  0.00019535422325134277
Batch  61  loss:  0.00014378511696122587
Batch  71  loss:  0.0001740103616612032
Batch  81  loss:  0.00014648860087618232
Batch  91  loss:  0.00020036959904246032
Batch  101  loss:  0.0001842124474933371
Batch  111  loss:  0.00014693236153107136
Batch  121  loss:  0.00017529905016999692
Batch  131  loss:  0.00035732192918658257
Batch  141  loss:  0.0001318219001404941
Batch  151  loss:  0.00019076955504715443
Batch  161  loss:  0.00022014752903487533
Batch  171  loss:  0.00017852876044344157
Batch  181  loss:  0.00021234460291452706
Batch  191  loss:  0.00020650883379857987
Validation on real data: 
LOSS supervised-train 0.00019202098974346882, valid 0.00022699983674101532
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00013081698853056878
Batch  11  loss:  0.00011128714686492458
Batch  21  loss:  0.00019361177692189813
Batch  31  loss:  0.0001856684684753418
Batch  41  loss:  0.0001498110214015469
Batch  51  loss:  0.00019893016724381596
Batch  61  loss:  0.00011701359471771866
Batch  71  loss:  0.00016627300647087395
Batch  81  loss:  0.00018782886036206037
Batch  91  loss:  0.00022193806944414973
Batch  101  loss:  0.00024052344087976962
Batch  111  loss:  0.0002225625212304294
Batch  121  loss:  0.0002765052777249366
Batch  131  loss:  0.0002799212816171348
Batch  141  loss:  0.00012801167031284422
Batch  151  loss:  0.0002364600804867223
Batch  161  loss:  0.00024924962781369686
Batch  171  loss:  0.00016255464288406074
Batch  181  loss:  0.00024718130589462817
Batch  191  loss:  0.00022066287056077272
Validation on real data: 
LOSS supervised-train 0.00018959906003146898, valid 0.00024273853341583163
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00014142254076432437
Batch  11  loss:  0.00015089054068084806
Batch  21  loss:  0.00018921488663181663
Batch  31  loss:  0.00022318416449707001
Batch  41  loss:  0.00016746300389058888
Batch  51  loss:  0.00019687542226165533
Batch  61  loss:  0.00015099353913683444
Batch  71  loss:  0.00014743738574907184
Batch  81  loss:  0.00019035431614611298
Batch  91  loss:  0.0002178417780669406
Batch  101  loss:  0.00023096214863471687
Batch  111  loss:  0.00020106375450268388
Batch  121  loss:  0.00030681901262141764
Batch  131  loss:  0.0002489703765604645
Batch  141  loss:  0.00014937376545276493
Batch  151  loss:  0.0002267891977680847
Batch  161  loss:  0.00023567963216919452
Batch  171  loss:  0.00018883819575421512
Batch  181  loss:  0.00018829635519068688
Batch  191  loss:  0.00023305205104406923
Validation on real data: 
LOSS supervised-train 0.0001942138645244995, valid 0.0002133095113094896
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00017947121523320675
Batch  11  loss:  0.0001683798764133826
Batch  21  loss:  0.00017143436707556248
Batch  31  loss:  0.00015549328236375004
Batch  41  loss:  0.00017295870929956436
Batch  51  loss:  0.00019380405137781054
Batch  61  loss:  0.00011262464977335185
Batch  71  loss:  0.0001124339978559874
Batch  81  loss:  0.00018160007311962545
Batch  91  loss:  0.00018316457862965763
Batch  101  loss:  0.00018560972239356488
Batch  111  loss:  0.0003100262547377497
Batch  121  loss:  0.0002672275877557695
Batch  131  loss:  0.0002809437573887408
Batch  141  loss:  0.00020377340842969716
Batch  151  loss:  0.0002433977642795071
Batch  161  loss:  0.0002692295820452273
Batch  171  loss:  0.00016853507258929312
Batch  181  loss:  0.0002080866979667917
Batch  191  loss:  0.00018603373609948903
Validation on real data: 
LOSS supervised-train 0.0001863986730677425, valid 0.0002645089407451451
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00016676101949997246
Batch  11  loss:  0.0001267239567823708
Batch  21  loss:  0.00018997456936631352
Batch  31  loss:  0.0001938749774126336
Batch  41  loss:  0.00018257061310578138
Batch  51  loss:  0.00015796969819348305
Batch  61  loss:  0.00015514048573095351
Batch  71  loss:  0.00019621173851191998
Batch  81  loss:  0.00016810129454825073
Batch  91  loss:  0.00019634817726910114
Batch  101  loss:  0.0002048941096290946
Batch  111  loss:  0.00017868376744445413
Batch  121  loss:  0.00018072056991513819
Batch  131  loss:  0.0002650349051691592
Batch  141  loss:  0.0001745209447108209
Batch  151  loss:  0.0002272736601298675
Batch  161  loss:  0.00019115082977805287
Batch  171  loss:  0.00017281342297792435
Batch  181  loss:  0.00020819708879571408
Batch  191  loss:  0.00016884614888112992
Validation on real data: 
LOSS supervised-train 0.00018644755469722442, valid 0.0002897112281061709
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0001955902698682621
Batch  11  loss:  0.00013272676733322442
Batch  21  loss:  0.00019466331286821514
Batch  31  loss:  0.00020526975276879966
Batch  41  loss:  0.000194086431292817
Batch  51  loss:  0.00022282652207650244
Batch  61  loss:  0.00013543515524361283
Batch  71  loss:  0.00014866514538880438
Batch  81  loss:  0.00022084268857724965
Batch  91  loss:  0.00020180991850793362
Batch  101  loss:  0.0001495806354796514
Batch  111  loss:  0.00017485713760834187
Batch  121  loss:  0.00024585743085481226
Batch  131  loss:  0.0003627564001362771
Batch  141  loss:  0.00013322253653313965
Batch  151  loss:  0.00023988130851648748
Batch  161  loss:  0.00020957808010280132
Batch  171  loss:  0.0001584367419127375
Batch  181  loss:  0.0001856321032391861
Batch  191  loss:  0.00021716856281273067
Validation on real data: 
LOSS supervised-train 0.00018499868299841183, valid 0.00021733834000770003
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00015855362289585173
Batch  11  loss:  0.00017725217912811786
Batch  21  loss:  0.0001860541815403849
Batch  31  loss:  0.00020850439614150673
Batch  41  loss:  0.0001397329760948196
Batch  51  loss:  0.00020126826711930335
Batch  61  loss:  0.00014008325524628162
Batch  71  loss:  0.00018862987053580582
Batch  81  loss:  0.00026594114024192095
Batch  91  loss:  0.000189557031262666
Batch  101  loss:  0.00021881726570427418
Batch  111  loss:  0.0002034900535363704
Batch  121  loss:  0.00017198982823174447
Batch  131  loss:  0.00021052430383861065
Batch  141  loss:  0.00017761181516107172
Batch  151  loss:  0.0001550865563331172
Batch  161  loss:  0.0002896306978072971
Batch  171  loss:  0.0001740552397677675
Batch  181  loss:  0.00010637208470143378
Batch  191  loss:  0.00025640573585405946
Validation on real data: 
LOSS supervised-train 0.00018809100467478856, valid 0.0002737313916441053
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.00014380743959918618
Batch  11  loss:  0.00012097757280571386
Batch  21  loss:  0.00017033636686392128
Batch  31  loss:  0.00014875443594064564
Batch  41  loss:  0.00013325311010703444
Batch  51  loss:  0.0002063742431346327
Batch  61  loss:  0.00016175105702131987
Batch  71  loss:  0.0001930371072376147
Batch  81  loss:  0.00019149288709741086
Batch  91  loss:  0.0002140957221854478
Batch  101  loss:  0.0001752293755998835
Batch  111  loss:  0.00018760452803689986
Batch  121  loss:  0.0001818819291656837
Batch  131  loss:  0.00024339450465049595
Batch  141  loss:  0.0002221129834651947
Batch  151  loss:  0.00020734407007694244
Batch  161  loss:  0.0002644662163220346
Batch  171  loss:  0.00015347261796705425
Batch  181  loss:  0.00024106416094582528
Batch  191  loss:  0.00022928677208255976
Validation on real data: 
LOSS supervised-train 0.00018309392522496636, valid 0.00023688439978286624
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0001214772419189103
Batch  11  loss:  0.00011653344699880108
Batch  21  loss:  0.00020098805543966591
Batch  31  loss:  0.00023344003420788795
Batch  41  loss:  0.00014691583055537194
Batch  51  loss:  0.0001925126853166148
Batch  61  loss:  0.00013107275299262255
Batch  71  loss:  0.00017116904200520366
Batch  81  loss:  0.00016591390885878354
Batch  91  loss:  0.00013098532508593053
Batch  101  loss:  0.00016387236246373504
Batch  111  loss:  0.00020481865794863552
Batch  121  loss:  0.00024275558826047927
Batch  131  loss:  0.0002563759917393327
Batch  141  loss:  0.0001490704744355753
Batch  151  loss:  0.00018864266166929156
Batch  161  loss:  0.0002542920410633087
Batch  171  loss:  0.00017675332492217422
Batch  181  loss:  0.00018943168106488883
Batch  191  loss:  0.00021125758939888328
Validation on real data: 
LOSS supervised-train 0.00018409511249046774, valid 0.00021082926832605153
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00019871520635206252
Batch  11  loss:  0.00013213558122515678
Batch  21  loss:  0.00014867766003590077
Batch  31  loss:  0.0001860575284808874
Batch  41  loss:  0.00016677267558407038
Batch  51  loss:  0.00021303734683897346
Batch  61  loss:  0.00018003133300226182
Batch  71  loss:  0.0001583656412549317
Batch  81  loss:  0.0001827776723075658
Batch  91  loss:  0.00014423829270526767
Batch  101  loss:  0.00015781293041072786
Batch  111  loss:  0.00018085922056343406
Batch  121  loss:  0.0002327934926142916
Batch  131  loss:  0.0003477162099443376
Batch  141  loss:  0.00013007658708374947
Batch  151  loss:  0.00019517462351359427
Batch  161  loss:  0.00024257044424302876
Batch  171  loss:  0.00016383681213483214
Batch  181  loss:  0.00016532070003449917
Batch  191  loss:  0.00021744267723988742
Validation on real data: 
LOSS supervised-train 0.00017737990248861024, valid 0.0002314728917554021
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00013501830107998103
Batch  11  loss:  0.00010757627023849636
Batch  21  loss:  0.00020164134912192822
Batch  31  loss:  0.00016302110452670604
Batch  41  loss:  0.00012968429655302316
Batch  51  loss:  0.00016036500164773315
Batch  61  loss:  0.0001247603358933702
Batch  71  loss:  0.0001136379869421944
Batch  81  loss:  0.00017196770932059735
Batch  91  loss:  0.0001392449194099754
Batch  101  loss:  0.000168847109307535
Batch  111  loss:  0.00013429300452116877
Batch  121  loss:  0.00018014140368904918
Batch  131  loss:  0.0003613984736148268
Batch  141  loss:  0.00019183709810022265
Batch  151  loss:  0.00011579455895116553
Batch  161  loss:  0.00025288554024882615
Batch  171  loss:  0.0001663403381826356
Batch  181  loss:  0.00012801373668480664
Batch  191  loss:  0.00026860437355935574
Validation on real data: 
LOSS supervised-train 0.0001710689781612018, valid 0.00021828271565027535
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00023705499188508838
Batch  11  loss:  0.0001558035146445036
Batch  21  loss:  0.00016161958046723157
Batch  31  loss:  0.0001765560737112537
Batch  41  loss:  0.0001716326951282099
Batch  51  loss:  0.0001985483686439693
Batch  61  loss:  0.00013332032540347427
Batch  71  loss:  0.0001520876685390249
Batch  81  loss:  0.00020690183737315238
Batch  91  loss:  0.00011462627298897132
Batch  101  loss:  0.00018885228200815618
Batch  111  loss:  0.00017830191063694656
Batch  121  loss:  0.00022125145187601447
Batch  131  loss:  0.0002912816416937858
Batch  141  loss:  0.00016905284428503364
Batch  151  loss:  0.0002494091459084302
Batch  161  loss:  0.00020016067719552666
Batch  171  loss:  0.00011872314644278958
Batch  181  loss:  0.00021276675397530198
Batch  191  loss:  0.00018499860016163439
Validation on real data: 
LOSS supervised-train 0.00017734294815454632, valid 0.0001860905613284558
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00012669761781580746
Batch  11  loss:  0.00010093411401612684
Batch  21  loss:  0.00016808755754027516
Batch  31  loss:  0.00014953064965084195
Batch  41  loss:  0.00022125241230241954
Batch  51  loss:  0.00018600697512738407
Batch  61  loss:  9.001389844343066e-05
Batch  71  loss:  0.00014407827984541655
Batch  81  loss:  0.0002156421251129359
Batch  91  loss:  0.00014802897931076586
Batch  101  loss:  0.00022072187857702374
Batch  111  loss:  0.00017806222604122013
Batch  121  loss:  0.00021008083422202617
Batch  131  loss:  0.000225063442485407
Batch  141  loss:  0.0001805608335416764
Batch  151  loss:  0.00022425416682381183
Batch  161  loss:  0.000250146520556882
Batch  171  loss:  0.00020896831119898707
Batch  181  loss:  0.0002464504214003682
Batch  191  loss:  0.00012774499191436917
Validation on real data: 
LOSS supervised-train 0.0001818003397784196, valid 0.0002316971804248169
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00014370355347637087
Batch  11  loss:  9.85689548542723e-05
Batch  21  loss:  0.00022365196491591632
Batch  31  loss:  0.0001441515632905066
Batch  41  loss:  0.00018904072931036353
Batch  51  loss:  0.00019193180196452886
Batch  61  loss:  0.00010705819295253605
Batch  71  loss:  0.00015065546904224902
Batch  81  loss:  0.00013290844799485058
Batch  91  loss:  0.00014154550444800407
Batch  101  loss:  0.00022585438273381442
Batch  111  loss:  0.000197552508325316
Batch  121  loss:  0.00018422611174173653
Batch  131  loss:  0.0002403428516117856
Batch  141  loss:  0.00014514432405121624
Batch  151  loss:  0.00017808824486564845
Batch  161  loss:  0.0002636861754581332
Batch  171  loss:  0.00019929002155549824
Batch  181  loss:  0.0002156203263439238
Batch  191  loss:  0.00017997233953792602
Validation on real data: 
LOSS supervised-train 0.0001742194715552614, valid 0.0002824337570928037
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00016866778605617583
Batch  11  loss:  0.00013169042358640581
Batch  21  loss:  0.00015597391757182777
Batch  31  loss:  0.00021075397671665996
Batch  41  loss:  0.00019934661395382136
Batch  51  loss:  0.000235163708566688
Batch  61  loss:  0.0001278617710340768
Batch  71  loss:  0.00014963437570258975
Batch  81  loss:  0.00013846474757883698
Batch  91  loss:  0.00020330079132691026
Batch  101  loss:  0.00015187659300863743
Batch  111  loss:  0.00016372051322832704
Batch  121  loss:  0.0002576526894699782
Batch  131  loss:  0.00019485424854792655
Batch  141  loss:  0.00016470468835905194
Batch  151  loss:  0.00021608240786008537
Batch  161  loss:  0.0002476409135852009
Batch  171  loss:  0.00020084452989976853
Batch  181  loss:  0.00015925253683235496
Batch  191  loss:  0.0001802077458705753
Validation on real data: 
LOSS supervised-train 0.0001757629956046003, valid 0.00018972286488860846
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.00020265647617634386
Batch  11  loss:  0.00016207028238568455
Batch  21  loss:  0.00020326169033069164
Batch  31  loss:  0.0001638391986489296
Batch  41  loss:  0.00020114704966545105
Batch  51  loss:  0.000195503409486264
Batch  61  loss:  0.00012899817375000566
Batch  71  loss:  0.00013539758219849318
Batch  81  loss:  0.00014936462685000151
Batch  91  loss:  0.00015933600661810488
Batch  101  loss:  0.0001588535524206236
Batch  111  loss:  0.00024078688875306398
Batch  121  loss:  0.00020781272905878723
Batch  131  loss:  0.00019953925220761448
Batch  141  loss:  0.0001608591846888885
Batch  151  loss:  0.0002155452239094302
Batch  161  loss:  0.00019963957311119884
Batch  171  loss:  0.0002353062736801803
Batch  181  loss:  0.00014540087431669235
Batch  191  loss:  0.00024072022642940283
Validation on real data: 
LOSS supervised-train 0.00017485334457887803, valid 0.00026553915813565254
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00016430288087576628
Batch  11  loss:  0.00014515560178551823
Batch  21  loss:  0.00014935011859051883
Batch  31  loss:  0.00015572618576698005
Batch  41  loss:  0.00014529263717122376
Batch  51  loss:  0.00018153547716792673
Batch  61  loss:  0.00011666562932077795
Batch  71  loss:  0.00018077220011036843
Batch  81  loss:  0.00015990332758519799
Batch  91  loss:  0.0001411683770129457
Batch  101  loss:  0.00015853374497964978
Batch  111  loss:  0.00016481037891935557
Batch  121  loss:  0.0002436831855447963
Batch  131  loss:  0.0003201129729859531
Batch  141  loss:  0.0001255949609912932
Batch  151  loss:  0.00021798764646518975
Batch  161  loss:  0.00020910322200506926
Batch  171  loss:  0.00015409277693834156
Batch  181  loss:  0.00020087070879526436
Batch  191  loss:  0.0002170146763091907
Validation on real data: 
LOSS supervised-train 0.00017595777713722783, valid 0.0002663371851667762
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0001542059180792421
Batch  11  loss:  0.00010779557487694547
Batch  21  loss:  0.00012478078133426607
Batch  31  loss:  0.00014931210898794234
Batch  41  loss:  0.00014238126459531486
Batch  51  loss:  0.0002000307576963678
Batch  61  loss:  0.0001608431339263916
Batch  71  loss:  0.00011429803271312267
Batch  81  loss:  0.00011316165182506666
Batch  91  loss:  0.00013925803068559617
Batch  101  loss:  0.000210974074434489
Batch  111  loss:  0.0001329517544945702
Batch  121  loss:  0.00020101276459172368
Batch  131  loss:  0.0002916485827881843
Batch  141  loss:  0.00016391559620387852
Batch  151  loss:  0.000190108607057482
Batch  161  loss:  0.0002029242750722915
Batch  171  loss:  0.00015442701987922192
Batch  181  loss:  0.00016734185919631273
Batch  191  loss:  0.00021031599317211658
Validation on real data: 
LOSS supervised-train 0.0001689636869923561, valid 0.00018744004773907363
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.000139710507937707
Batch  11  loss:  0.00014989051851443946
Batch  21  loss:  0.00015113737026695162
Batch  31  loss:  0.00014175783144310117
Batch  41  loss:  0.00015671194705646485
Batch  51  loss:  0.00015120190801098943
Batch  61  loss:  0.0001715880207484588
Batch  71  loss:  0.00012998119927942753
Batch  81  loss:  0.00016537449846509844
Batch  91  loss:  0.00014379635103978217
Batch  101  loss:  0.0001466156099922955
Batch  111  loss:  0.00012784397404175252
Batch  121  loss:  0.00024107408535201102
Batch  131  loss:  0.0002188000362366438
Batch  141  loss:  0.00022440365864895284
Batch  151  loss:  0.00018770672613754869
Batch  161  loss:  0.0002182375465054065
Batch  171  loss:  0.00019309524213895202
Batch  181  loss:  0.00016867514932528138
Batch  191  loss:  0.00019633796182461083
Validation on real data: 
LOSS supervised-train 0.00016310662915202556, valid 0.00022564263781532645
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0001675953681115061
Batch  11  loss:  0.00011632745008682832
Batch  21  loss:  0.0001752169628161937
Batch  31  loss:  0.00017201557056978345
Batch  41  loss:  0.00017079708050005138
Batch  51  loss:  0.00017180295253638178
Batch  61  loss:  0.00014794258459005505
Batch  71  loss:  0.00010458305041538551
Batch  81  loss:  0.0001284838654100895
Batch  91  loss:  0.0002022246626438573
Batch  101  loss:  0.00016656435036566108
Batch  111  loss:  0.00014773676230106503
Batch  121  loss:  0.000148299295688048
Batch  131  loss:  0.00028482789639383554
Batch  141  loss:  0.00015233004523906857
Batch  151  loss:  0.00022262736456468701
Batch  161  loss:  0.00028629088774323463
Batch  171  loss:  0.00019616680219769478
Batch  181  loss:  0.00014872057363390923
Batch  191  loss:  0.00017632725939620286
Validation on real data: 
LOSS supervised-train 0.000161694464404718, valid 0.00016479636542499065
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0001233807415701449
Batch  11  loss:  0.00013799354201182723
Batch  21  loss:  0.0001607855811016634
Batch  31  loss:  0.00015543977497145534
Batch  41  loss:  0.00021957488206680864
Batch  51  loss:  0.00018201391503680497
Batch  61  loss:  0.0001083049428416416
Batch  71  loss:  0.0001437175233149901
Batch  81  loss:  0.00019775090913753957
Batch  91  loss:  0.00013469178520608693
Batch  101  loss:  0.00020539395336527377
Batch  111  loss:  0.0001462974469177425
Batch  121  loss:  0.0001800988393370062
Batch  131  loss:  0.0002600434236228466
Batch  141  loss:  0.00016930412675719708
Batch  151  loss:  0.00015382847050204873
Batch  161  loss:  0.0002196761779487133
Batch  171  loss:  0.00016283495642710477
Batch  181  loss:  0.00016451261762995273
Batch  191  loss:  0.00019936724856961519
Validation on real data: 
LOSS supervised-train 0.00016552997625694842, valid 0.00019514700397849083
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00013662058336194605
Batch  11  loss:  8.658038859721273e-05
Batch  21  loss:  0.0002048976457444951
Batch  31  loss:  0.00019920483464375138
Batch  41  loss:  0.00013342787860892713
Batch  51  loss:  0.00018644209194462746
Batch  61  loss:  0.00019508782133925706
Batch  71  loss:  0.0001301806914852932
Batch  81  loss:  0.00014572523650713265
Batch  91  loss:  0.00014265146455727518
Batch  101  loss:  0.00018677720800042152
Batch  111  loss:  0.00016585405683144927
Batch  121  loss:  0.0001983570255106315
Batch  131  loss:  0.00026200994034297764
Batch  141  loss:  0.00017101253615692258
Batch  151  loss:  0.00018780342361424118
Batch  161  loss:  0.00018046839977614582
Batch  171  loss:  0.0001255381794180721
Batch  181  loss:  0.00021405573352240026
Batch  191  loss:  0.00014065136201679707
Validation on real data: 
LOSS supervised-train 0.00015930096709780627, valid 0.00022553413873538375
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00013262276479508728
Batch  11  loss:  0.00012232658627908677
Batch  21  loss:  0.00017983393627218902
Batch  31  loss:  0.0002069485344691202
Batch  41  loss:  0.00015697444905526936
Batch  51  loss:  0.0002519045665394515
Batch  61  loss:  0.00014469155576080084
Batch  71  loss:  0.00016132097516674548
Batch  81  loss:  0.00016626012802589685
Batch  91  loss:  0.00015855793026275933
Batch  101  loss:  0.00012453540693968534
Batch  111  loss:  0.00024477281840518117
Batch  121  loss:  0.00019558283383958042
Batch  131  loss:  0.00027999619487673044
Batch  141  loss:  0.0001830284745665267
Batch  151  loss:  0.00019612970936577767
Batch  161  loss:  0.00024013587972149253
Batch  171  loss:  0.00011803101369878277
Batch  181  loss:  0.00016000508912838995
Batch  191  loss:  0.0002074744552373886
Validation on real data: 
LOSS supervised-train 0.0001634076133632334, valid 0.00026318078744225204
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0001244294544449076
Batch  11  loss:  9.866233449429274e-05
Batch  21  loss:  0.00015158631140366197
Batch  31  loss:  0.00020087460870854557
Batch  41  loss:  0.00019445027282927185
Batch  51  loss:  0.000222774178837426
Batch  61  loss:  0.00015301743405871093
Batch  71  loss:  0.00013678210962098092
Batch  81  loss:  0.00014956893573980778
Batch  91  loss:  0.0001625685690669343
Batch  101  loss:  0.0001675635139690712
Batch  111  loss:  0.00015050671936478466
Batch  121  loss:  0.00013010791735723615
Batch  131  loss:  0.0002309735573362559
Batch  141  loss:  0.0001739470608299598
Batch  151  loss:  0.00020995482918806374
Batch  161  loss:  0.00017262894834857434
Batch  171  loss:  0.00010755987750599161
Batch  181  loss:  0.000133305205963552
Batch  191  loss:  0.0002457096124999225
Validation on real data: 
LOSS supervised-train 0.00016009814833523706, valid 0.00022722002177033573
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00013127691636327654
Batch  11  loss:  0.00010608213051455095
Batch  21  loss:  0.00022318260744214058
Batch  31  loss:  0.00017383205704391003
Batch  41  loss:  0.00015748092846479267
Batch  51  loss:  0.00016627834702376276
Batch  61  loss:  0.00010242249845759943
Batch  71  loss:  0.00014488289889413863
Batch  81  loss:  0.0001367613294860348
Batch  91  loss:  0.0001579880336066708
Batch  101  loss:  0.00013488387048710138
Batch  111  loss:  0.0001346875069430098
Batch  121  loss:  0.00018913247913587838
Batch  131  loss:  0.00031234780908562243
Batch  141  loss:  0.00017525989096611738
Batch  151  loss:  0.0001956193445948884
Batch  161  loss:  0.00022124462702777237
Batch  171  loss:  0.00015127226652111858
Batch  181  loss:  0.00012659220374189317
Batch  191  loss:  0.0001796805445337668
Validation on real data: 
LOSS supervised-train 0.00015845231209823396, valid 0.000219860696233809
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00011866135173477232
Batch  11  loss:  0.00011223486217204481
Batch  21  loss:  0.0001746802736306563
Batch  31  loss:  0.00011316375457681715
Batch  41  loss:  0.00014458752411883324
Batch  51  loss:  0.00017901802493724972
Batch  61  loss:  0.0001057136760209687
Batch  71  loss:  0.0001193923017126508
Batch  81  loss:  0.00014336050662677735
Batch  91  loss:  0.00016129018331412226
Batch  101  loss:  0.00015592691488564014
Batch  111  loss:  0.00019202829571440816
Batch  121  loss:  0.00017399428179487586
Batch  131  loss:  0.0002449154562782496
Batch  141  loss:  0.0001255767565453425
Batch  151  loss:  0.00014384691894520074
Batch  161  loss:  0.0002114737726515159
Batch  171  loss:  0.00010538302012719214
Batch  181  loss:  0.00015064163017086685
Batch  191  loss:  0.00014076045772526413
Validation on real data: 
LOSS supervised-train 0.00016020536473661197, valid 0.00016758425044827163
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00010741539153968915
Batch  11  loss:  0.00015766455908305943
Batch  21  loss:  0.00014282905613072217
Batch  31  loss:  0.0001876849855761975
Batch  41  loss:  0.00013972226588521153
Batch  51  loss:  0.0001414627186022699
Batch  61  loss:  0.00011295572767267004
Batch  71  loss:  0.0001151962933363393
Batch  81  loss:  0.00017262778419535607
Batch  91  loss:  0.00012870914360973984
Batch  101  loss:  0.00013686271267943084
Batch  111  loss:  0.00015145805082283914
Batch  121  loss:  0.00022778766287956387
Batch  131  loss:  0.00019613115000538528
Batch  141  loss:  0.0001224129373440519
Batch  151  loss:  0.00014416709018405527
Batch  161  loss:  0.00019585777772590518
Batch  171  loss:  0.00015954456466715783
Batch  181  loss:  0.00015678643831051886
Batch  191  loss:  0.00022928039834368974
Validation on real data: 
LOSS supervised-train 0.0001562849208858097, valid 0.00022310265921987593
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00011636689305305481
Batch  11  loss:  0.00014759539044462144
Batch  21  loss:  0.00013877985475119203
Batch  31  loss:  0.00015890327631495893
Batch  41  loss:  0.00016758902347646654
Batch  51  loss:  0.00028636876959353685
Batch  61  loss:  0.00011986996833002195
Batch  71  loss:  0.0001706151815596968
Batch  81  loss:  0.00015450851060450077
Batch  91  loss:  0.00016122135275509208
Batch  101  loss:  0.00015445226745214313
Batch  111  loss:  0.00015492760576307774
Batch  121  loss:  0.0002316035097464919
Batch  131  loss:  0.0002925114531535655
Batch  141  loss:  0.00013372347166296095
Batch  151  loss:  0.00010338883294025436
Batch  161  loss:  0.0002574225072748959
Batch  171  loss:  0.00014958225074224174
Batch  181  loss:  0.0001447451941203326
Batch  191  loss:  0.0001320573646808043
Validation on real data: 
LOSS supervised-train 0.000157867119560251, valid 0.00019312158110551536
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00014221631863620132
Batch  11  loss:  0.00013238932297099382
Batch  21  loss:  0.00017975571972783655
Batch  31  loss:  0.0002205115742981434
Batch  41  loss:  0.0001825977087719366
Batch  51  loss:  0.00016148635768331587
Batch  61  loss:  0.00012958947627339512
Batch  71  loss:  9.452139784116298e-05
Batch  81  loss:  0.00012046571646351367
Batch  91  loss:  0.00014170051144901663
Batch  101  loss:  0.00018154025019612163
Batch  111  loss:  0.0001677310065133497
Batch  121  loss:  0.00022837605501990765
Batch  131  loss:  0.00024636066518723965
Batch  141  loss:  0.00011587204062379897
Batch  151  loss:  0.00016519794007763267
Batch  161  loss:  0.00016269503976218402
Batch  171  loss:  0.00017267331713810563
Batch  181  loss:  0.00017709667736198753
Batch  191  loss:  0.00016026753291953355
Validation on real data: 
LOSS supervised-train 0.00015331325550505426, valid 0.0002117569383699447
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00011729377001756802
Batch  11  loss:  0.00012720255472231656
Batch  21  loss:  0.00017464048869442195
Batch  31  loss:  0.00018353667110204697
Batch  41  loss:  0.00015075744886416942
Batch  51  loss:  0.0001990261225728318
Batch  61  loss:  0.0001259512937394902
Batch  71  loss:  0.00014137431571725756
Batch  81  loss:  0.00013621838297694921
Batch  91  loss:  0.00012537569273263216
Batch  101  loss:  0.00018638517940416932
Batch  111  loss:  0.00013236682571005076
Batch  121  loss:  0.00012211373541504145
Batch  131  loss:  0.00020882821991108358
Batch  141  loss:  0.00011133741645608097
Batch  151  loss:  0.00015146371151786298
Batch  161  loss:  0.0002518631226848811
Batch  171  loss:  0.00015378832176793367
Batch  181  loss:  0.00018332927720621228
Batch  191  loss:  0.0001630606420803815
Validation on real data: 
LOSS supervised-train 0.00014839108920568832, valid 0.00025472292327322066
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00010005391959566623
Batch  11  loss:  9.971817053155974e-05
Batch  21  loss:  0.00014321702474262565
Batch  31  loss:  0.00018590597028378397
Batch  41  loss:  0.00021256160107441247
Batch  51  loss:  0.00020708526426460594
Batch  61  loss:  0.00010891079000430182
Batch  71  loss:  0.00012758375669363886
Batch  81  loss:  0.0001714482350507751
Batch  91  loss:  0.00014734681462869048
Batch  101  loss:  0.00013307789049576968
Batch  111  loss:  0.00017424288671463728
Batch  121  loss:  0.00016192342445719987
Batch  131  loss:  0.00030983632314018905
Batch  141  loss:  0.00013341019803192466
Batch  151  loss:  0.00016425024659838527
Batch  161  loss:  0.0001553060137666762
Batch  171  loss:  0.0001522497332189232
Batch  181  loss:  0.00014212231326382607
Batch  191  loss:  0.00018683231610339135
Validation on real data: 
LOSS supervised-train 0.00015435830062415334, valid 0.00029961415566504
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00012327263539191335
Batch  11  loss:  8.651761891087517e-05
Batch  21  loss:  0.0002056963712675497
Batch  31  loss:  0.00013936198956798762
Batch  41  loss:  0.0001229888730449602
Batch  51  loss:  0.00017146191385108978
Batch  61  loss:  0.00012535866699181497
Batch  71  loss:  0.00011248154623899609
Batch  81  loss:  0.00017149123596027493
Batch  91  loss:  0.00010883679351536557
Batch  101  loss:  0.00014072685735300183
Batch  111  loss:  0.00012138456077082083
Batch  121  loss:  0.00016325234901160002
Batch  131  loss:  0.00028920246404595673
Batch  141  loss:  0.0001103109389077872
Batch  151  loss:  0.0001491231087129563
Batch  161  loss:  0.00019621200044639409
Batch  171  loss:  0.00015107615035958588
Batch  181  loss:  0.00013456317537929863
Batch  191  loss:  0.00017916248179972172
Validation on real data: 
LOSS supervised-train 0.00015099820455361624, valid 0.00025594444014132023
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0001920723298098892
Batch  11  loss:  0.00013274831871967763
Batch  21  loss:  0.00012277047790121287
Batch  31  loss:  0.00013871061673853546
Batch  41  loss:  0.0001748028298607096
Batch  51  loss:  0.00019218648958485574
Batch  61  loss:  0.0001141530810855329
Batch  71  loss:  0.00011278656893409789
Batch  81  loss:  0.0001539848162792623
Batch  91  loss:  0.00011944895231863484
Batch  101  loss:  0.00017413280147593468
Batch  111  loss:  0.00012662328663282096
Batch  121  loss:  0.00012854217493440956
Batch  131  loss:  0.000296116282697767
Batch  141  loss:  0.00015725550474599004
Batch  151  loss:  0.00015790593170095235
Batch  161  loss:  0.0001883218064904213
Batch  171  loss:  0.0001616235385881737
Batch  181  loss:  0.00013802347530145198
Batch  191  loss:  0.00013640585530083627
Validation on real data: 
LOSS supervised-train 0.00015287565787730273, valid 0.00018554521375335753
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00013758143177255988
Batch  11  loss:  0.00013775656407233328
Batch  21  loss:  0.00020724866772070527
Batch  31  loss:  0.00016431850963272154
Batch  41  loss:  0.0001866329403128475
Batch  51  loss:  0.0001892066211439669
Batch  61  loss:  0.00010590088641038164
Batch  71  loss:  0.00014000749797560275
Batch  81  loss:  0.00016782029706519097
Batch  91  loss:  0.00010648755414877087
Batch  101  loss:  0.00013818258594255894
Batch  111  loss:  0.00010864800424315035
Batch  121  loss:  0.0001674213563092053
Batch  131  loss:  0.0001676296815276146
Batch  141  loss:  0.00012174270523246378
Batch  151  loss:  0.00012893261737190187
Batch  161  loss:  0.0001683100126683712
Batch  171  loss:  0.00011729210382327437
Batch  181  loss:  0.0001559689117129892
Batch  191  loss:  0.00015122987679205835
Validation on real data: 
LOSS supervised-train 0.00014689490653836402, valid 0.0001718556450214237
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.000158001683303155
Batch  11  loss:  9.083566692424938e-05
Batch  21  loss:  0.0001446650130674243
Batch  31  loss:  0.00010592219769023359
Batch  41  loss:  0.00014131896023172885
Batch  51  loss:  0.00012712320312857628
Batch  61  loss:  8.647062350064516e-05
Batch  71  loss:  0.00012130512914154679
Batch  81  loss:  0.00014413456665351987
Batch  91  loss:  0.00011884061677847058
Batch  101  loss:  0.000189215803402476
Batch  111  loss:  0.0001224990119226277
Batch  121  loss:  0.0001625764271011576
Batch  131  loss:  0.00024434816441498697
Batch  141  loss:  0.00015213100414257497
Batch  151  loss:  0.0001538753422209993
Batch  161  loss:  0.00022910635743755847
Batch  171  loss:  0.00016585923731327057
Batch  181  loss:  0.00014320151240099221
Batch  191  loss:  0.00017640713485889137
Validation on real data: 
LOSS supervised-train 0.00014928199834685074, valid 0.00013326696353033185
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0001308932842221111
Batch  11  loss:  9.173232683679089e-05
Batch  21  loss:  0.00012890582729596645
Batch  31  loss:  0.00022252921189647168
Batch  41  loss:  0.00011960742995142937
Batch  51  loss:  0.00015717826317995787
Batch  61  loss:  0.0001239087141584605
Batch  71  loss:  0.00011125562741653994
Batch  81  loss:  0.00011562980216694996
Batch  91  loss:  0.00011168047785758972
Batch  101  loss:  0.00017380190547555685
Batch  111  loss:  0.0001906567922560498
Batch  121  loss:  0.00016689962649252266
Batch  131  loss:  0.0002039994578808546
Batch  141  loss:  0.0001231080968864262
Batch  151  loss:  0.00013620532990898937
Batch  161  loss:  0.0001441252970835194
Batch  171  loss:  0.00016340547881554812
Batch  181  loss:  0.00018357140652369708
Batch  191  loss:  0.00014997839753050357
Validation on real data: 
LOSS supervised-train 0.00014291160889115417, valid 0.0002042475825874135
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00012115979188820347
Batch  11  loss:  0.00010837420268217102
Batch  21  loss:  0.00014900621317792684
Batch  31  loss:  0.0001997518993448466
Batch  41  loss:  0.0001170044852187857
Batch  51  loss:  0.00018782660481519997
Batch  61  loss:  0.00011197223648196086
Batch  71  loss:  0.0001416377635905519
Batch  81  loss:  0.00014486745931208134
Batch  91  loss:  0.00011622923921095207
Batch  101  loss:  0.00015213448205031455
Batch  111  loss:  0.00012164323561592028
Batch  121  loss:  0.0002284495421918109
Batch  131  loss:  0.00023657783458475024
Batch  141  loss:  0.00014811614528298378
Batch  151  loss:  0.00013782332825940102
Batch  161  loss:  0.00021412923524621874
Batch  171  loss:  0.00014508306048810482
Batch  181  loss:  0.00011893895862158388
Batch  191  loss:  0.00016173180483747274
Validation on real data: 
LOSS supervised-train 0.0001486710702374694, valid 0.00017847350682131946
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00011548787006177008
Batch  11  loss:  9.57864904194139e-05
Batch  21  loss:  0.0001523071259725839
Batch  31  loss:  0.00017796549946069717
Batch  41  loss:  0.00015289345174096525
Batch  51  loss:  0.0001221351994900033
Batch  61  loss:  0.00012453958333935589
Batch  71  loss:  0.00010843333438970149
Batch  81  loss:  0.00010927477705990896
Batch  91  loss:  0.00013523970847018063
Batch  101  loss:  0.00018865338643081486
Batch  111  loss:  0.0001152876575361006
Batch  121  loss:  0.00017534509242977947
Batch  131  loss:  0.00031067075906321406
Batch  141  loss:  0.0001504973042756319
Batch  151  loss:  0.00015382000128738582
Batch  161  loss:  0.00013025759835727513
Batch  171  loss:  0.0001581375254318118
Batch  181  loss:  7.926028047222644e-05
Batch  191  loss:  0.00016370225057471544
Validation on real data: 
LOSS supervised-train 0.00014169220066833076, valid 0.0001816742296796292
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00012104615598218516
Batch  11  loss:  8.873460319591686e-05
Batch  21  loss:  0.00010894395381910726
Batch  31  loss:  0.00013378770381677896
Batch  41  loss:  0.00015490541409235448
Batch  51  loss:  0.00014470916357822716
Batch  61  loss:  0.00011182428715983406
Batch  71  loss:  0.00010826689685927704
Batch  81  loss:  0.00011123674630653113
Batch  91  loss:  0.00011567826732061803
Batch  101  loss:  0.00017354624287690967
Batch  111  loss:  9.113454871112481e-05
Batch  121  loss:  0.00015071006782818586
Batch  131  loss:  0.0002579823194537312
Batch  141  loss:  0.00010556655615800992
Batch  151  loss:  0.0001294378307648003
Batch  161  loss:  0.00016407441580668092
Batch  171  loss:  0.00012264674296602607
Batch  181  loss:  0.00012128787784604356
Batch  191  loss:  0.0001855449372669682
Validation on real data: 
LOSS supervised-train 0.00014351702742715133, valid 0.00022699052351526916
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00011800727224908769
Batch  11  loss:  9.80728364083916e-05
Batch  21  loss:  0.0001470652932766825
Batch  31  loss:  0.00013951743312645704
Batch  41  loss:  0.00011601233563851565
Batch  51  loss:  0.00016495963791385293
Batch  61  loss:  0.00011719112808350474
Batch  71  loss:  0.00011033847840735689
Batch  81  loss:  0.00011483421258162707
Batch  91  loss:  0.0001699374697636813
Batch  101  loss:  0.00012579988106153905
Batch  111  loss:  0.00019270516349934042
Batch  121  loss:  0.00019058052566833794
Batch  131  loss:  0.0002288029354531318
Batch  141  loss:  0.00013197983207646757
Batch  151  loss:  0.00014398353232536465
Batch  161  loss:  0.0001764692278811708
Batch  171  loss:  0.00013647257583215833
Batch  181  loss:  0.00014355016173794866
Batch  191  loss:  0.00015924833132885396
Validation on real data: 
LOSS supervised-train 0.0001457992964060395, valid 0.00017221891903318465
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0001434648729627952
Batch  11  loss:  0.00011404214455978945
Batch  21  loss:  0.00013200592366047204
Batch  31  loss:  0.0001516515767434612
Batch  41  loss:  0.00010224612196907401
Batch  51  loss:  0.00019069867266807705
Batch  61  loss:  0.00012471027730498463
Batch  71  loss:  0.00011374845780665055
Batch  81  loss:  0.00014975373051129282
Batch  91  loss:  0.00010348427895223722
Batch  101  loss:  0.00012831465573981404
Batch  111  loss:  0.0001427929091732949
Batch  121  loss:  0.00016839004820212722
Batch  131  loss:  0.00021769858722109348
Batch  141  loss:  0.0001267607876798138
Batch  151  loss:  0.0002087573811877519
Batch  161  loss:  0.00019382062600925565
Batch  171  loss:  0.00013330727233551443
Batch  181  loss:  0.00011238799925195053
Batch  191  loss:  0.0001427880342816934
Validation on real data: 
LOSS supervised-train 0.00014171474631439196, valid 0.00022433785488829017
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  9.075827983906493e-05
Batch  11  loss:  9.441636211704463e-05
Batch  21  loss:  0.00015069670917000622
Batch  31  loss:  0.00013846777437720448
Batch  41  loss:  0.0001193629068438895
Batch  51  loss:  0.0001224881416419521
Batch  61  loss:  8.372520824195817e-05
Batch  71  loss:  8.755301678320393e-05
Batch  81  loss:  0.00011683951743179932
Batch  91  loss:  0.00013786074123345315
Batch  101  loss:  0.00014032513718120754
Batch  111  loss:  0.0001886340178316459
Batch  121  loss:  0.00017011308227665722
Batch  131  loss:  0.0002087527245748788
Batch  141  loss:  0.00010505998216103762
Batch  151  loss:  0.00018808520690072328
Batch  161  loss:  0.00017871394811663777
Batch  171  loss:  0.00013889523688703775
Batch  181  loss:  0.0001378864690195769
Batch  191  loss:  0.00016676484665367752
Validation on real data: 
LOSS supervised-train 0.00014187589327775642, valid 0.00016490275447722524
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00012667539704125375
Batch  11  loss:  0.00010704779560910538
Batch  21  loss:  0.00015541022003162652
Batch  31  loss:  0.00014017841021995991
Batch  41  loss:  0.00013094321184325963
Batch  51  loss:  0.0001453558070352301
Batch  61  loss:  0.00010397943697171286
Batch  71  loss:  0.00014424696564674377
Batch  81  loss:  9.386092278873548e-05
Batch  91  loss:  0.00011881537648150697
Batch  101  loss:  0.00013097347982693464
Batch  111  loss:  0.00010999407095368952
Batch  121  loss:  0.00018817299860529602
Batch  131  loss:  0.00020534724171739072
Batch  141  loss:  0.0001524726685602218
Batch  151  loss:  0.0002227170771220699
Batch  161  loss:  0.00019728434563148767
Batch  171  loss:  0.0001231153291882947
Batch  181  loss:  0.00012905331095680594
Batch  191  loss:  0.00015970964159350842
Validation on real data: 
LOSS supervised-train 0.00014122126955044224, valid 0.0001950836885953322
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.000174630869878456
Batch  11  loss:  0.00010118677892023697
Batch  21  loss:  0.00014473400369752198
Batch  31  loss:  0.00011881157843163237
Batch  41  loss:  0.00014757824828848243
Batch  51  loss:  0.00018394063226878643
Batch  61  loss:  7.686356548219919e-05
Batch  71  loss:  0.00010738924174802378
Batch  81  loss:  0.000160412018885836
Batch  91  loss:  0.00012355092621874064
Batch  101  loss:  0.0001482359948568046
Batch  111  loss:  0.00014526888844557106
Batch  121  loss:  0.00019324853201396763
Batch  131  loss:  0.00028836075216531754
Batch  141  loss:  0.00013628401211462915
Batch  151  loss:  0.00011911276669707149
Batch  161  loss:  0.00017310476687271148
Batch  171  loss:  0.00013290313654579222
Batch  181  loss:  0.00013257695536594838
Batch  191  loss:  0.0002518830297049135
Validation on real data: 
LOSS supervised-train 0.00013900069539886319, valid 0.0002876963699236512
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.00012849329505115747
Batch  11  loss:  0.0001273773523280397
Batch  21  loss:  0.0001770221279002726
Batch  31  loss:  0.00011924299906240776
Batch  41  loss:  0.00014092553465161473
Batch  51  loss:  0.00016492382565047592
Batch  61  loss:  9.684648102847859e-05
Batch  71  loss:  0.00013584966654889286
Batch  81  loss:  0.00010943396046059206
Batch  91  loss:  0.00012636765313800424
Batch  101  loss:  0.00015199529298115522
Batch  111  loss:  0.0001372943224851042
Batch  121  loss:  0.00015194667503237724
Batch  131  loss:  0.00021087891946081072
Batch  141  loss:  0.00011426443234086037
Batch  151  loss:  0.0001490166032453999
Batch  161  loss:  0.00020780926570296288
Batch  171  loss:  0.00015603075735270977
Batch  181  loss:  8.60349609865807e-05
Batch  191  loss:  0.00012001133291050792
Validation on real data: 
LOSS supervised-train 0.00013717229438043433, valid 0.00018469613860361278
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00011744710354832932
Batch  11  loss:  7.61872943257913e-05
Batch  21  loss:  0.00013168917212169617
Batch  31  loss:  0.00015937723219394684
Batch  41  loss:  0.00019564415561035275
Batch  51  loss:  0.00017999934789258987
Batch  61  loss:  0.0001013598739518784
Batch  71  loss:  0.00016423557826783508
Batch  81  loss:  0.00012423079169820994
Batch  91  loss:  0.00010688164184102789
Batch  101  loss:  0.00016576620691921562
Batch  111  loss:  0.00014766422100365162
Batch  121  loss:  0.0002239004970761016
Batch  131  loss:  0.00025072868447750807
Batch  141  loss:  0.00012625596718862653
Batch  151  loss:  0.0001333565596723929
Batch  161  loss:  0.00014788401313126087
Batch  171  loss:  0.00015100000018719584
Batch  181  loss:  0.00011235486454097554
Batch  191  loss:  0.00018135926802642643
Validation on real data: 
LOSS supervised-train 0.00013624545466882408, valid 0.00016362749738618731
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00012613619037438184
Batch  11  loss:  9.441313159186393e-05
Batch  21  loss:  0.0001470424176659435
Batch  31  loss:  0.00011591122893150896
Batch  41  loss:  0.00016770765068940818
Batch  51  loss:  0.00013961028889752924
Batch  61  loss:  0.00011241061292821541
Batch  71  loss:  0.00010480266064405441
Batch  81  loss:  0.00015629356494173408
Batch  91  loss:  0.00010542133531998843
Batch  101  loss:  0.00016154396871570498
Batch  111  loss:  0.0001569021405884996
Batch  121  loss:  0.0001602706324774772
Batch  131  loss:  0.00024025078164413571
Batch  141  loss:  9.773552301339805e-05
Batch  151  loss:  0.00016795846750028431
Batch  161  loss:  0.0002203141339123249
Batch  171  loss:  0.00014929303142707795
Batch  181  loss:  0.0001410136028425768
Batch  191  loss:  0.0002049041067948565
Validation on real data: 
LOSS supervised-train 0.00014277508471423063, valid 0.0002281546767335385
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00016086696996353567
Batch  11  loss:  0.00012468940985854715
Batch  21  loss:  0.0001536339259473607
Batch  31  loss:  0.0001515362091595307
Batch  41  loss:  0.0001299935975112021
Batch  51  loss:  0.0001219506812049076
Batch  61  loss:  8.25170282041654e-05
Batch  71  loss:  0.00011541474668774754
Batch  81  loss:  0.00015336832439061254
Batch  91  loss:  0.0001552710309624672
Batch  101  loss:  0.00017431446758564562
Batch  111  loss:  0.00014300431939773262
Batch  121  loss:  0.00018490871298126876
Batch  131  loss:  0.00015529367374256253
Batch  141  loss:  0.0001090408768504858
Batch  151  loss:  0.00019644037820398808
Batch  161  loss:  0.0001625710865482688
Batch  171  loss:  0.00011081348930019885
Batch  181  loss:  0.00011634674592642114
Batch  191  loss:  0.0001661148271523416
Validation on real data: 
LOSS supervised-train 0.00013796314167848322, valid 0.0002452504413668066
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00010330413351766765
Batch  11  loss:  0.00010833241685759276
Batch  21  loss:  0.00011675847781589255
Batch  31  loss:  0.00013652430789079517
Batch  41  loss:  8.647463255329058e-05
Batch  51  loss:  0.00012622939539141953
Batch  61  loss:  9.14431075216271e-05
Batch  71  loss:  0.0001218942052219063
Batch  81  loss:  0.0001336838467977941
Batch  91  loss:  0.00011120516865048558
Batch  101  loss:  0.0001360794121865183
Batch  111  loss:  0.00013577313802670687
Batch  121  loss:  0.00014512515917886049
Batch  131  loss:  0.0002771224535536021
Batch  141  loss:  8.14083541627042e-05
Batch  151  loss:  0.0001420001790393144
Batch  161  loss:  0.00020156563550699502
Batch  171  loss:  9.485880582360551e-05
Batch  181  loss:  8.660036110086367e-05
Batch  191  loss:  0.00017335858137812465
Validation on real data: 
LOSS supervised-train 0.00013418316942988894, valid 0.0002387490530963987
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00012722615792881697
Batch  11  loss:  0.00013009704707656056
Batch  21  loss:  0.00011753791477531195
Batch  31  loss:  0.00014810952416155487
Batch  41  loss:  0.0001852839777711779
Batch  51  loss:  0.00013842077169101685
Batch  61  loss:  0.00010300095163984224
Batch  71  loss:  0.00013710781058762223
Batch  81  loss:  0.00011045470455428585
Batch  91  loss:  9.603096259525046e-05
Batch  101  loss:  0.00012533050903584808
Batch  111  loss:  0.0001393773127347231
Batch  121  loss:  0.0001395848667016253
Batch  131  loss:  0.00024104084877762944
Batch  141  loss:  9.415053500561044e-05
Batch  151  loss:  9.910162043524906e-05
Batch  161  loss:  0.00013072771253064275
Batch  171  loss:  0.00017087238666135818
Batch  181  loss:  0.00013617501826956868
Batch  191  loss:  0.00014381732034962624
Validation on real data: 
LOSS supervised-train 0.0001311616464954568, valid 0.00021693517919629812
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  9.036739356815815e-05
Batch  11  loss:  8.32154619274661e-05
Batch  21  loss:  0.00011872836330439895
Batch  31  loss:  0.00017556520469952375
Batch  41  loss:  0.0001059884816640988
Batch  51  loss:  0.00020468181173782796
Batch  61  loss:  0.00011217427527299151
Batch  71  loss:  0.00011357031326042488
Batch  81  loss:  0.00010235414811177179
Batch  91  loss:  8.526189049007371e-05
Batch  101  loss:  0.00017152517102658749
Batch  111  loss:  0.0001262777077499777
Batch  121  loss:  0.00017737323651090264
Batch  131  loss:  0.0002587659109849483
Batch  141  loss:  0.00010935412137769163
Batch  151  loss:  0.00011736078886315227
Batch  161  loss:  0.00017392788140568882
Batch  171  loss:  0.00011878443910973147
Batch  181  loss:  0.0001316703128395602
Batch  191  loss:  0.00013289063645061105
Validation on real data: 
LOSS supervised-train 0.00013499673612386688, valid 0.0002487754391040653
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00011425560660427436
Batch  11  loss:  9.071797103388235e-05
Batch  21  loss:  9.502374450676143e-05
Batch  31  loss:  0.00014305004151538014
Batch  41  loss:  0.00013257104728836566
Batch  51  loss:  0.00017622046289034188
Batch  61  loss:  9.29969028220512e-05
Batch  71  loss:  9.170331031782553e-05
Batch  81  loss:  0.00013534771278500557
Batch  91  loss:  0.00011512074706843123
Batch  101  loss:  0.00015792620251886547
Batch  111  loss:  0.00011038025695597753
Batch  121  loss:  0.00013939692871645093
Batch  131  loss:  0.0002983779995702207
Batch  141  loss:  0.00010962247324641794
Batch  151  loss:  0.00013578319340012968
Batch  161  loss:  0.00020159607811365277
Batch  171  loss:  0.00013700702402275056
Batch  181  loss:  0.00012592229177244008
Batch  191  loss:  0.00016092867008410394
Validation on real data: 
LOSS supervised-train 0.00013467060540278908, valid 0.0001799088204279542
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00013967053382657468
Batch  11  loss:  0.00010457919415785
Batch  21  loss:  0.00015859445556998253
Batch  31  loss:  0.00012708808935713023
Batch  41  loss:  0.0001618048845557496
Batch  51  loss:  0.0001232335634995252
Batch  61  loss:  8.802147203823552e-05
Batch  71  loss:  0.00011150419595651329
Batch  81  loss:  0.00014815974282100797
Batch  91  loss:  0.00011375975736882538
Batch  101  loss:  0.00015292303578462452
Batch  111  loss:  0.00011614921822911128
Batch  121  loss:  0.00016205859719775617
Batch  131  loss:  0.0002194976550526917
Batch  141  loss:  0.00011420126975281164
Batch  151  loss:  0.00016904721269384027
Batch  161  loss:  0.00015682741650380194
Batch  171  loss:  0.00011379355419194326
Batch  181  loss:  0.00016169332957360893
Batch  191  loss:  0.0001390703982906416
Validation on real data: 
LOSS supervised-train 0.00013415191686362958, valid 0.00022829195950180292
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00010107745765708387
Batch  11  loss:  8.729580440558493e-05
Batch  21  loss:  0.0001453994045732543
Batch  31  loss:  0.0001066264885594137
Batch  41  loss:  0.00016053389117587358
Batch  51  loss:  0.00014970204210840166
Batch  61  loss:  0.00011925380385946482
Batch  71  loss:  0.00010383468179497868
Batch  81  loss:  0.00015833560610190034
Batch  91  loss:  0.0001038558257278055
Batch  101  loss:  0.00017533385835122317
Batch  111  loss:  0.00012902470189146698
Batch  121  loss:  0.00017963054415304214
Batch  131  loss:  0.00021432254288811237
Batch  141  loss:  0.00010920909699052572
Batch  151  loss:  0.00010337632556911558
Batch  161  loss:  0.00016585119010414928
Batch  171  loss:  0.00014790867862757295
Batch  181  loss:  0.00014026957796886563
Batch  191  loss:  0.00014186125190462917
Validation on real data: 
LOSS supervised-train 0.00013626798208861147, valid 0.00020731656695716083
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0001273899688385427
Batch  11  loss:  7.9345925769303e-05
Batch  21  loss:  0.00016177228826563805
Batch  31  loss:  0.00013408738595899194
Batch  41  loss:  0.00014823015953879803
Batch  51  loss:  0.00015122648619581014
Batch  61  loss:  0.00011892483598785475
Batch  71  loss:  0.00010073248267872259
Batch  81  loss:  0.00011710768012562767
Batch  91  loss:  9.644256351748481e-05
Batch  101  loss:  0.00014477764489129186
Batch  111  loss:  0.0001578326482558623
Batch  121  loss:  0.0001289110805373639
Batch  131  loss:  0.0002144753816537559
Batch  141  loss:  0.0001288743515033275
Batch  151  loss:  0.0001535665796836838
Batch  161  loss:  0.00019174788030795753
Batch  171  loss:  0.00010017797467298806
Batch  181  loss:  0.00012545443314593285
Batch  191  loss:  0.0001341660099569708
Validation on real data: 
LOSS supervised-train 0.00013230719287093962, valid 0.00023145854356698692
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  9.820183186093345e-05
Batch  11  loss:  9.542435873299837e-05
Batch  21  loss:  0.00013857414887752384
Batch  31  loss:  0.0001023588702082634
Batch  41  loss:  0.00013359080185182393
Batch  51  loss:  0.0001418000174453482
Batch  61  loss:  0.0001089871657313779
Batch  71  loss:  0.00011735790758393705
Batch  81  loss:  0.00012980867177248
Batch  91  loss:  0.00010629910684656352
Batch  101  loss:  0.0001359787565888837
Batch  111  loss:  0.00010402630141470581
Batch  121  loss:  0.00016836612485349178
Batch  131  loss:  0.00021032962831668556
Batch  141  loss:  0.00010717322584241629
Batch  151  loss:  0.00012703203537967056
Batch  161  loss:  0.00016473194409627467
Batch  171  loss:  0.00011339122283970937
Batch  181  loss:  9.720549132907763e-05
Batch  191  loss:  0.0002157296985387802
Validation on real data: 
LOSS supervised-train 0.0001270698703956441, valid 0.00021665013628080487
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0001382682385155931
Batch  11  loss:  0.00010081646905746311
Batch  21  loss:  0.00012969311501365155
Batch  31  loss:  0.00012324248382356018
Batch  41  loss:  0.0001350677921436727
Batch  51  loss:  0.00017479437519796193
Batch  61  loss:  8.795350004220381e-05
Batch  71  loss:  0.00010149398440262303
Batch  81  loss:  0.00013982337259221822
Batch  91  loss:  8.943374996306375e-05
Batch  101  loss:  0.0001167396767414175
Batch  111  loss:  0.00010805869533214718
Batch  121  loss:  0.00012631076970137656
Batch  131  loss:  0.00020588230108842254
Batch  141  loss:  9.458752174396068e-05
Batch  151  loss:  0.00013981360825709999
Batch  161  loss:  0.00021708052372559905
Batch  171  loss:  0.0001643544965190813
Batch  181  loss:  0.0001351802930003032
Batch  191  loss:  0.00014497047231998295
Validation on real data: 
LOSS supervised-train 0.0001283803048681875, valid 0.00020456063793972135
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0001331839885096997
Batch  11  loss:  9.073736146092415e-05
Batch  21  loss:  0.00011256377911195159
Batch  31  loss:  0.0001254694943781942
Batch  41  loss:  0.00010490029671927914
Batch  51  loss:  0.00016103520465549082
Batch  61  loss:  0.0001628815953154117
Batch  71  loss:  0.00010949509305646643
Batch  81  loss:  0.00019658108067233115
Batch  91  loss:  0.00011681346222758293
Batch  101  loss:  0.00016582100943196565
Batch  111  loss:  0.00016710600175429136
Batch  121  loss:  0.00017952283087652177
Batch  131  loss:  0.0002518398396205157
Batch  141  loss:  0.00012422299187164754
Batch  151  loss:  0.00010944503446808085
Batch  161  loss:  0.00021719768119510263
Batch  171  loss:  0.00010901899804593995
Batch  181  loss:  0.00012671314470935613
Batch  191  loss:  0.00019309848721604794
Validation on real data: 
LOSS supervised-train 0.00013139325552401714, valid 0.00020316921290941536
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00011507900489959866
Batch  11  loss:  9.850300557445735e-05
Batch  21  loss:  0.00012107427755836397
Batch  31  loss:  0.00015899234858807176
Batch  41  loss:  0.00013446580851450562
Batch  51  loss:  0.00015946343773975968
Batch  61  loss:  7.14430570951663e-05
Batch  71  loss:  8.675013668835163e-05
Batch  81  loss:  7.569055742351338e-05
Batch  91  loss:  0.00013660961121786386
Batch  101  loss:  0.00011571491631912068
Batch  111  loss:  0.00010896129970205948
Batch  121  loss:  0.00010777657735161483
Batch  131  loss:  0.00025888398522511125
Batch  141  loss:  0.00011311598063912243
Batch  151  loss:  0.00013832720287609845
Batch  161  loss:  0.00017967188614420593
Batch  171  loss:  8.067987073445693e-05
Batch  181  loss:  0.00011257401638431475
Batch  191  loss:  0.00017124194710049778
Validation on real data: 
LOSS supervised-train 0.0001270970639961888, valid 0.00019764876924455166
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  car ; Model ID: ad45b2d40c7801ef2074a73831d8a3a2
--------------------
Training baseline regression model:  2022-03-30 01:01:23.418587
Detector:  point_transformer
Object:  car
--------------------
device is  cuda
--------------------
Number of trainable parameters:  913090
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.1504276841878891
Batch  11  loss:  0.09840095043182373
Batch  21  loss:  0.08200216293334961
Batch  31  loss:  0.046261582523584366
Batch  41  loss:  0.0298923347145319
Batch  51  loss:  0.0205832626670599
Batch  61  loss:  0.035921283066272736
Batch  71  loss:  0.021527456119656563
Batch  81  loss:  0.026285555213689804
Batch  91  loss:  0.012577082961797714
Batch  101  loss:  0.007679624482989311
Batch  111  loss:  0.007971469312906265
Batch  121  loss:  0.018263675272464752
Batch  131  loss:  0.025494074448943138
Batch  141  loss:  0.00796493049710989
Batch  151  loss:  0.007196614518761635
Batch  161  loss:  0.0067212567664682865
Batch  171  loss:  0.003986581694334745
Batch  181  loss:  0.003921829164028168
Batch  191  loss:  0.004699699115008116
Validation on real data: 
LOSS supervised-train 0.027779462746111677, valid 0.0017639677971601486
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.004800212103873491
Batch  11  loss:  0.004437514580786228
Batch  21  loss:  0.0047484999522566795
Batch  31  loss:  0.009737937711179256
Batch  41  loss:  0.0035186372697353363
Batch  51  loss:  0.0060575176030397415
Batch  61  loss:  0.006661314982920885
Batch  71  loss:  0.004223346244543791
Batch  81  loss:  0.003257676027715206
Batch  91  loss:  0.006635510828346014
Batch  101  loss:  0.0028428498189896345
Batch  111  loss:  0.0019051196286454797
Batch  121  loss:  0.005498526152223349
Batch  131  loss:  0.010393661446869373
Batch  141  loss:  0.0027890377677977085
Batch  151  loss:  0.0024414316285401583
Batch  161  loss:  0.004137060604989529
Batch  171  loss:  0.003317695576697588
Batch  181  loss:  0.001828323700465262
Batch  191  loss:  0.0029289275407791138
Validation on real data: 
LOSS supervised-train 0.003866909719654359, valid 0.0016797021962702274
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.003563427599146962
Batch  11  loss:  0.0031035030260682106
Batch  21  loss:  0.002456345595419407
Batch  31  loss:  0.005734344013035297
Batch  41  loss:  0.0028150081634521484
Batch  51  loss:  0.004233009181916714
Batch  61  loss:  0.004325280897319317
Batch  71  loss:  0.003008382162079215
Batch  81  loss:  0.0022882926277816296
Batch  91  loss:  0.003032546490430832
Batch  101  loss:  0.0014021795941516757
Batch  111  loss:  0.0012548437807708979
Batch  121  loss:  0.005216469522565603
Batch  131  loss:  0.006904549431055784
Batch  141  loss:  0.0024811765179038048
Batch  151  loss:  0.0023590943310409784
Batch  161  loss:  0.0025493085850030184
Batch  171  loss:  0.0029701876919716597
Batch  181  loss:  0.001365939388051629
Batch  191  loss:  0.001804210594855249
Validation on real data: 
LOSS supervised-train 0.002654256854439154, valid 0.0013487535761669278
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.002733070868998766
Batch  11  loss:  0.0024350029416382313
Batch  21  loss:  0.0020529620815068483
Batch  31  loss:  0.0034877595026046038
Batch  41  loss:  0.002699099713936448
Batch  51  loss:  0.0035113180056214333
Batch  61  loss:  0.00302802468650043
Batch  71  loss:  0.0023921967949718237
Batch  81  loss:  0.001488705282099545
Batch  91  loss:  0.0018896108958870173
Batch  101  loss:  0.0010582737158983946
Batch  111  loss:  0.0010793657274916768
Batch  121  loss:  0.00344676966778934
Batch  131  loss:  0.004364849999547005
Batch  141  loss:  0.0016916495515033603
Batch  151  loss:  0.0019536041654646397
Batch  161  loss:  0.001567250699736178
Batch  171  loss:  0.002556075807660818
Batch  181  loss:  0.0014063228154554963
Batch  191  loss:  0.001500821323134005
Validation on real data: 
LOSS supervised-train 0.002024046927399468, valid 0.0009426928590983152
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.002106398344039917
Batch  11  loss:  0.0023354508448392153
Batch  21  loss:  0.0016491982387378812
Batch  31  loss:  0.00281126843765378
Batch  41  loss:  0.0021481846924871206
Batch  51  loss:  0.0032108162995427847
Batch  61  loss:  0.0020582310389727354
Batch  71  loss:  0.0018561482429504395
Batch  81  loss:  0.0015323057305067778
Batch  91  loss:  0.0016116199549287558
Batch  101  loss:  0.0007014874136075377
Batch  111  loss:  0.0010915055172517896
Batch  121  loss:  0.0031621349044144154
Batch  131  loss:  0.0028709147591143847
Batch  141  loss:  0.0012963420012965798
Batch  151  loss:  0.0014720283215865493
Batch  161  loss:  0.0013077232288196683
Batch  171  loss:  0.0021811132319271564
Batch  181  loss:  0.0011140997521579266
Batch  191  loss:  0.0011993712978437543
Validation on real data: 
LOSS supervised-train 0.0016615595846087672, valid 0.0007682618452236056
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.00220349314622581
Batch  11  loss:  0.001571931759826839
Batch  21  loss:  0.0015006682369858027
Batch  31  loss:  0.0023002559319138527
Batch  41  loss:  0.0019440216710790992
Batch  51  loss:  0.0025031440891325474
Batch  61  loss:  0.0017143806908279657
Batch  71  loss:  0.0015301033854484558
Batch  81  loss:  0.0012097806902602315
Batch  91  loss:  0.0011833708267658949
Batch  101  loss:  0.0006489716470241547
Batch  111  loss:  0.000998357660137117
Batch  121  loss:  0.002256914973258972
Batch  131  loss:  0.001877705450169742
Batch  141  loss:  0.0010149625595659018
Batch  151  loss:  0.0013041141210123897
Batch  161  loss:  0.0008183481404557824
Batch  171  loss:  0.0018462513107806444
Batch  181  loss:  0.0010414918651804328
Batch  191  loss:  0.0010511353611946106
Validation on real data: 
LOSS supervised-train 0.001400900665903464, valid 0.0010881246998906136
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.00197365484200418
Batch  11  loss:  0.0016710323980078101
Batch  21  loss:  0.0011106920428574085
Batch  31  loss:  0.0020171906799077988
Batch  41  loss:  0.001689518103376031
Batch  51  loss:  0.002216833410784602
Batch  61  loss:  0.0015625652158632874
Batch  71  loss:  0.0014357351465150714
Batch  81  loss:  0.0011529659386724234
Batch  91  loss:  0.0010197049705311656
Batch  101  loss:  0.0006264691473916173
Batch  111  loss:  0.0008629113435745239
Batch  121  loss:  0.0019450425170361996
Batch  131  loss:  0.0025957238394767046
Batch  141  loss:  0.0009093075059354305
Batch  151  loss:  0.001170036499388516
Batch  161  loss:  0.0007600912358611822
Batch  171  loss:  0.0019127614796161652
Batch  181  loss:  0.0009326158324256539
Batch  191  loss:  0.0008835239568725228
Validation on real data: 
LOSS supervised-train 0.0012050205050036311, valid 0.000658922828733921
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.001310403342358768
Batch  11  loss:  0.0013876606244593859
Batch  21  loss:  0.0010316789848729968
Batch  31  loss:  0.0015716564375907183
Batch  41  loss:  0.00166184245608747
Batch  51  loss:  0.0025677781086415052
Batch  61  loss:  0.0014496820513159037
Batch  71  loss:  0.0014495420036837459
Batch  81  loss:  0.0010817585280165076
Batch  91  loss:  0.0009174050646834075
Batch  101  loss:  0.0005373349413275719
Batch  111  loss:  0.0008037267252802849
Batch  121  loss:  0.001676493207924068
Batch  131  loss:  0.002114918315783143
Batch  141  loss:  0.0007600790704600513
Batch  151  loss:  0.0009325285209342837
Batch  161  loss:  0.0006037774146534503
Batch  171  loss:  0.001638392568565905
Batch  181  loss:  0.0009145521908067167
Batch  191  loss:  0.0006970870890654624
Validation on real data: 
LOSS supervised-train 0.0010989657815662212, valid 0.0006479892763309181
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0013409327948465943
Batch  11  loss:  0.0012624295195564628
Batch  21  loss:  0.0007473206496797502
Batch  31  loss:  0.0013646779116243124
Batch  41  loss:  0.0014110937481746078
Batch  51  loss:  0.0015951985260471702
Batch  61  loss:  0.0014610072830691934
Batch  71  loss:  0.0013707149773836136
Batch  81  loss:  0.000909265480004251
Batch  91  loss:  0.0006832368671894073
Batch  101  loss:  0.0004645867447834462
Batch  111  loss:  0.0008497255621477962
Batch  121  loss:  0.0014759188052266836
Batch  131  loss:  0.001980335684493184
Batch  141  loss:  0.0006652090814895928
Batch  151  loss:  0.000681109435390681
Batch  161  loss:  0.0006634974270127714
Batch  171  loss:  0.0014054349157959223
Batch  181  loss:  0.0008612996316514909
Batch  191  loss:  0.0006102994084358215
Validation on real data: 
LOSS supervised-train 0.0009961327409837395, valid 0.0005261234473437071
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0014600739814341068
Batch  11  loss:  0.001012788969092071
Batch  21  loss:  0.0006712204194627702
Batch  31  loss:  0.0009725573472678661
Batch  41  loss:  0.0013267671456560493
Batch  51  loss:  0.0018840780248865485
Batch  61  loss:  0.001083150738850236
Batch  71  loss:  0.0011627941858023405
Batch  81  loss:  0.0008329126285389066
Batch  91  loss:  0.0006683797109872103
Batch  101  loss:  0.00041173273348249495
Batch  111  loss:  0.0006412766524590552
Batch  121  loss:  0.0012504385085776448
Batch  131  loss:  0.0018568146042525768
Batch  141  loss:  0.000599798746407032
Batch  151  loss:  0.0009158095344901085
Batch  161  loss:  0.00048719733604229987
Batch  171  loss:  0.0016659107059240341
Batch  181  loss:  0.000591407238971442
Batch  191  loss:  0.0006325441645458341
Validation on real data: 
LOSS supervised-train 0.0008737208419188391, valid 0.0005045582656748593
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0013786179479211569
Batch  11  loss:  0.0010373754194006324
Batch  21  loss:  0.0008771504508331418
Batch  31  loss:  0.0011262630578130484
Batch  41  loss:  0.0012036614352837205
Batch  51  loss:  0.001736834878101945
Batch  61  loss:  0.0010953748133033514
Batch  71  loss:  0.0012598329922184348
Batch  81  loss:  0.0007859587203711271
Batch  91  loss:  0.0006938814767636359
Batch  101  loss:  0.0004376990254968405
Batch  111  loss:  0.0005926694720983505
Batch  121  loss:  0.0011164846364408731
Batch  131  loss:  0.0013520745560526848
Batch  141  loss:  0.0005236156866885722
Batch  151  loss:  0.0007637873059138656
Batch  161  loss:  0.0006562971393577754
Batch  171  loss:  0.0009395451634190977
Batch  181  loss:  0.0007417931337840855
Batch  191  loss:  0.0006962543702684343
Validation on real data: 
LOSS supervised-train 0.0008302301068033557, valid 0.0005520032718777657
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0012114917626604438
Batch  11  loss:  0.000997612252831459
Batch  21  loss:  0.0006896999548189342
Batch  31  loss:  0.0008268877863883972
Batch  41  loss:  0.0010780987795442343
Batch  51  loss:  0.0015982991317287087
Batch  61  loss:  0.0009834554512053728
Batch  71  loss:  0.0012410569470375776
Batch  81  loss:  0.0006753198686055839
Batch  91  loss:  0.0006059556035324931
Batch  101  loss:  0.000388581509469077
Batch  111  loss:  0.000468911457573995
Batch  121  loss:  0.0010477385949343443
Batch  131  loss:  0.001148711540736258
Batch  141  loss:  0.0006025431794114411
Batch  151  loss:  0.0007970936130732298
Batch  161  loss:  0.0005536994431167841
Batch  171  loss:  0.0011590549256652594
Batch  181  loss:  0.0006065497291274369
Batch  191  loss:  0.0006151536363177001
Validation on real data: 
LOSS supervised-train 0.0007703459175536409, valid 0.0004429662658367306
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0011166274780407548
Batch  11  loss:  0.0010083529632538557
Batch  21  loss:  0.0005577280535362661
Batch  31  loss:  0.001015918212942779
Batch  41  loss:  0.0009369225590489805
Batch  51  loss:  0.001438163802959025
Batch  61  loss:  0.0008117780089378357
Batch  71  loss:  0.0010556610068306327
Batch  81  loss:  0.0007187261362560093
Batch  91  loss:  0.0005729151307605207
Batch  101  loss:  0.0003914141852874309
Batch  111  loss:  0.0004790526581928134
Batch  121  loss:  0.0009450578363612294
Batch  131  loss:  0.0008443283732049167
Batch  141  loss:  0.0004953329334966838
Batch  151  loss:  0.0005965612945146859
Batch  161  loss:  0.0004943571984767914
Batch  171  loss:  0.0011667258804664016
Batch  181  loss:  0.0006136208539828658
Batch  191  loss:  0.0004759365983773023
Validation on real data: 
LOSS supervised-train 0.0007213368057273329, valid 0.0004013980505988002
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.001010394305922091
Batch  11  loss:  0.0008126202155835927
Batch  21  loss:  0.0005319885676726699
Batch  31  loss:  0.0008277634624391794
Batch  41  loss:  0.000992395682260394
Batch  51  loss:  0.0014683987246826291
Batch  61  loss:  0.0010099179344251752
Batch  71  loss:  0.0007799935410730541
Batch  81  loss:  0.0005851206951774657
Batch  91  loss:  0.0005044591962359846
Batch  101  loss:  0.00044212720240466297
Batch  111  loss:  0.0005847365828230977
Batch  121  loss:  0.0008301137131638825
Batch  131  loss:  0.0011734009021893144
Batch  141  loss:  0.0006120965699665248
Batch  151  loss:  0.0005962428986094892
Batch  161  loss:  0.0003984222421422601
Batch  171  loss:  0.0007724622264504433
Batch  181  loss:  0.0005543582956306636
Batch  191  loss:  0.0004615273210220039
Validation on real data: 
LOSS supervised-train 0.0006735215643129777, valid 0.0004541116941254586
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0009604746592231095
Batch  11  loss:  0.0007590659661218524
Batch  21  loss:  0.0005602348246611655
Batch  31  loss:  0.000736619986128062
Batch  41  loss:  0.0009455530089326203
Batch  51  loss:  0.0012773945927619934
Batch  61  loss:  0.0008326253155246377
Batch  71  loss:  0.0009236237383447587
Batch  81  loss:  0.0004741293960250914
Batch  91  loss:  0.000476099899969995
Batch  101  loss:  0.00040733738569542766
Batch  111  loss:  0.00043604796519503
Batch  121  loss:  0.0008699585450813174
Batch  131  loss:  0.0009261197992600501
Batch  141  loss:  0.0005530411144718528
Batch  151  loss:  0.0005475322250276804
Batch  161  loss:  0.0004840503097511828
Batch  171  loss:  0.0008636742713861167
Batch  181  loss:  0.0005416526109911501
Batch  191  loss:  0.00041262764716520905
Validation on real data: 
LOSS supervised-train 0.0006357971255783923, valid 0.0003989423275925219
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0008352128206752241
Batch  11  loss:  0.0007875178707763553
Batch  21  loss:  0.0005464288988150656
Batch  31  loss:  0.0008512339554727077
Batch  41  loss:  0.0010108354035764933
Batch  51  loss:  0.001182042295113206
Batch  61  loss:  0.0006365684093907475
Batch  71  loss:  0.000722896889783442
Batch  81  loss:  0.0004897523322142661
Batch  91  loss:  0.0005033136112615466
Batch  101  loss:  0.0004281919391360134
Batch  111  loss:  0.0004082383820787072
Batch  121  loss:  0.0007413530838675797
Batch  131  loss:  0.0008386052795685828
Batch  141  loss:  0.0005201590829528868
Batch  151  loss:  0.0005081476410850883
Batch  161  loss:  0.0003765660512726754
Batch  171  loss:  0.0008432116010226309
Batch  181  loss:  0.000570265983697027
Batch  191  loss:  0.0004229540645610541
Validation on real data: 
LOSS supervised-train 0.0005989259434863925, valid 0.0003974399878643453
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0009541955660097301
Batch  11  loss:  0.0007587895961478353
Batch  21  loss:  0.0004629692994058132
Batch  31  loss:  0.0005785781540907919
Batch  41  loss:  0.0008999921265058219
Batch  51  loss:  0.0009485730552114546
Batch  61  loss:  0.0008032640907913446
Batch  71  loss:  0.0007333839312195778
Batch  81  loss:  0.00044745372724719346
Batch  91  loss:  0.0006265685660764575
Batch  101  loss:  0.00038274607504718006
Batch  111  loss:  0.00042110937647521496
Batch  121  loss:  0.0008806366822682321
Batch  131  loss:  0.0008360664360225201
Batch  141  loss:  0.0005137835396453738
Batch  151  loss:  0.00051688909297809
Batch  161  loss:  0.000389194639865309
Batch  171  loss:  0.0009124568896368146
Batch  181  loss:  0.00046895421110093594
Batch  191  loss:  0.00044741787132807076
Validation on real data: 
LOSS supervised-train 0.0005732091104437132, valid 0.0003689012082759291
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.00080435152631253
Batch  11  loss:  0.0006814661319367588
Batch  21  loss:  0.0004765917547047138
Batch  31  loss:  0.000590791751164943
Batch  41  loss:  0.0007182807894423604
Batch  51  loss:  0.0010434899013489485
Batch  61  loss:  0.0005325552774593234
Batch  71  loss:  0.0006901544984430075
Batch  81  loss:  0.0004278126871213317
Batch  91  loss:  0.000477262627100572
Batch  101  loss:  0.00041355087887495756
Batch  111  loss:  0.0004225152952130884
Batch  121  loss:  0.0006480413721874356
Batch  131  loss:  0.0006339415558613837
Batch  141  loss:  0.00035504778497852385
Batch  151  loss:  0.00043382408330217004
Batch  161  loss:  0.00035520538222044706
Batch  171  loss:  0.0006646753754466772
Batch  181  loss:  0.0003773622156586498
Batch  191  loss:  0.0004291662771720439
Validation on real data: 
LOSS supervised-train 0.0005459993335534818, valid 0.0003690661396831274
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0007094357279129326
Batch  11  loss:  0.0004817493900191039
Batch  21  loss:  0.00048453937051817775
Batch  31  loss:  0.00045878879609517753
Batch  41  loss:  0.0006425429019145668
Batch  51  loss:  0.0009161240886896849
Batch  61  loss:  0.0005803260719403625
Batch  71  loss:  0.0005105308955535293
Batch  81  loss:  0.00045884153223596513
Batch  91  loss:  0.0005570320063270628
Batch  101  loss:  0.00033165502827614546
Batch  111  loss:  0.00032028515124693513
Batch  121  loss:  0.0007553438772447407
Batch  131  loss:  0.0006950207171030343
Batch  141  loss:  0.0005411582533270121
Batch  151  loss:  0.0004062969528604299
Batch  161  loss:  0.0003939192683901638
Batch  171  loss:  0.0007534000324085355
Batch  181  loss:  0.000407304527470842
Batch  191  loss:  0.0003365057928021997
Validation on real data: 
LOSS supervised-train 0.0005010586362914182, valid 0.0004231287748552859
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0007972651510499418
Batch  11  loss:  0.0005113246734254062
Batch  21  loss:  0.0003780506376642734
Batch  31  loss:  0.0004838575259782374
Batch  41  loss:  0.0006652487791143358
Batch  51  loss:  0.000921066734008491
Batch  61  loss:  0.0007103826501406729
Batch  71  loss:  0.0008077520178630948
Batch  81  loss:  0.00040437214192934334
Batch  91  loss:  0.0003440551517996937
Batch  101  loss:  0.0002866717695724219
Batch  111  loss:  0.00034841790329664946
Batch  121  loss:  0.0006026420742273331
Batch  131  loss:  0.000545553513802588
Batch  141  loss:  0.0004444190417416394
Batch  151  loss:  0.000557034567464143
Batch  161  loss:  0.0003053602413274348
Batch  171  loss:  0.0006679807556793094
Batch  181  loss:  0.00039043588913045824
Batch  191  loss:  0.00027687253896147013
Validation on real data: 
LOSS supervised-train 0.0004898600951128173, valid 0.0003127550007775426
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0008517273818142712
Batch  11  loss:  0.0005060890107415617
Batch  21  loss:  0.00037997408071532845
Batch  31  loss:  0.0004263789742253721
Batch  41  loss:  0.0007181002292782068
Batch  51  loss:  0.000718546740245074
Batch  61  loss:  0.0006107556400820613
Batch  71  loss:  0.00056967738782987
Batch  81  loss:  0.00039632670814171433
Batch  91  loss:  0.0003838520497083664
Batch  101  loss:  0.00031706495792604983
Batch  111  loss:  0.0003264804254285991
Batch  121  loss:  0.0004458703042473644
Batch  131  loss:  0.0006485903286375105
Batch  141  loss:  0.00042729443521238863
Batch  151  loss:  0.0004109243454877287
Batch  161  loss:  0.0004218127578496933
Batch  171  loss:  0.0006592178251594305
Batch  181  loss:  0.00038861072971485555
Batch  191  loss:  0.0003329505561850965
Validation on real data: 
LOSS supervised-train 0.00046623537593404763, valid 0.0003796181990765035
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.000677948584780097
Batch  11  loss:  0.0005414754268713295
Batch  21  loss:  0.000443706609075889
Batch  31  loss:  0.000536259962245822
Batch  41  loss:  0.0006024505710229278
Batch  51  loss:  0.0008821439696475863
Batch  61  loss:  0.000465298886410892
Batch  71  loss:  0.0005237186560407281
Batch  81  loss:  0.000511353719048202
Batch  91  loss:  0.00041077041532844305
Batch  101  loss:  0.0003488999209366739
Batch  111  loss:  0.0002838713116943836
Batch  121  loss:  0.0004701909201685339
Batch  131  loss:  0.0006725998246110976
Batch  141  loss:  0.0003230932343285531
Batch  151  loss:  0.000339204358169809
Batch  161  loss:  0.0003178814076818526
Batch  171  loss:  0.0007618839154019952
Batch  181  loss:  0.0004443153156898916
Batch  191  loss:  0.00036864253343082964
Validation on real data: 
LOSS supervised-train 0.00045718922759988343, valid 0.0004305741749703884
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0005909011233597994
Batch  11  loss:  0.0004935784381814301
Batch  21  loss:  0.0004740797739941627
Batch  31  loss:  0.0005003048572689295
Batch  41  loss:  0.0005712447455152869
Batch  51  loss:  0.0007098929490894079
Batch  61  loss:  0.0005272927228361368
Batch  71  loss:  0.00042832797043956816
Batch  81  loss:  0.00039963130257092416
Batch  91  loss:  0.00037969491677358747
Batch  101  loss:  0.0003433896927163005
Batch  111  loss:  0.0003141282359138131
Batch  121  loss:  0.00047390861436724663
Batch  131  loss:  0.0006923816981725395
Batch  141  loss:  0.0004899906925857067
Batch  151  loss:  0.0004382529004942626
Batch  161  loss:  0.0003736171929631382
Batch  171  loss:  0.0004544647817965597
Batch  181  loss:  0.00038649089401587844
Batch  191  loss:  0.00033899903064593673
Validation on real data: 
LOSS supervised-train 0.00044434476643800735, valid 0.0002991753281094134
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00065060198539868
Batch  11  loss:  0.00045680164475925267
Batch  21  loss:  0.0003946466604247689
Batch  31  loss:  0.0005795503384433687
Batch  41  loss:  0.0007222159183584154
Batch  51  loss:  0.0007912458968348801
Batch  61  loss:  0.0004467794205993414
Batch  71  loss:  0.0005625530029647052
Batch  81  loss:  0.0003881374723277986
Batch  91  loss:  0.0004373106930870563
Batch  101  loss:  0.0003742585249710828
Batch  111  loss:  0.0002843517286237329
Batch  121  loss:  0.0004892903380095959
Batch  131  loss:  0.000547470641322434
Batch  141  loss:  0.0004267597396392375
Batch  151  loss:  0.00042580917943269014
Batch  161  loss:  0.0002747987164184451
Batch  171  loss:  0.0005474425270222127
Batch  181  loss:  0.00039239978650584817
Batch  191  loss:  0.0003227865672670305
Validation on real data: 
LOSS supervised-train 0.00043939571296505167, valid 0.0003102984628640115
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0006148913525976241
Batch  11  loss:  0.0005019987002015114
Batch  21  loss:  0.0003457087150309235
Batch  31  loss:  0.0003544798819348216
Batch  41  loss:  0.0006314550992101431
Batch  51  loss:  0.000647650274913758
Batch  61  loss:  0.000586789334192872
Batch  71  loss:  0.0004700709832832217
Batch  81  loss:  0.0003624196397140622
Batch  91  loss:  0.00030284308013506234
Batch  101  loss:  0.0003092846891377121
Batch  111  loss:  0.00032014361931942403
Batch  121  loss:  0.00042321046930737793
Batch  131  loss:  0.0005151220830157399
Batch  141  loss:  0.000283791683614254
Batch  151  loss:  0.00033910965430550277
Batch  161  loss:  0.00027115773991681635
Batch  171  loss:  0.0004462481301743537
Batch  181  loss:  0.00034047692315652966
Batch  191  loss:  0.00030062778387218714
Validation on real data: 
LOSS supervised-train 0.0004189079936622875, valid 0.0003264539991505444
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0004914742894470692
Batch  11  loss:  0.0004331768723204732
Batch  21  loss:  0.000425610487582162
Batch  31  loss:  0.0003577988245524466
Batch  41  loss:  0.0006119502359069884
Batch  51  loss:  0.0007200177060440183
Batch  61  loss:  0.0004959115758538246
Batch  71  loss:  0.0004338996659498662
Batch  81  loss:  0.00040720822289586067
Batch  91  loss:  0.00032608225592412055
Batch  101  loss:  0.00027885587769560516
Batch  111  loss:  0.00027949095238000154
Batch  121  loss:  0.00043452135287225246
Batch  131  loss:  0.0004896733444184065
Batch  141  loss:  0.0003714459598995745
Batch  151  loss:  0.00040869577787816525
Batch  161  loss:  0.00030131899984553456
Batch  171  loss:  0.0005972402286715806
Batch  181  loss:  0.0003719732048921287
Batch  191  loss:  0.0003197982150595635
Validation on real data: 
LOSS supervised-train 0.00039887840961455367, valid 0.00027223906363360584
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0006275521009229124
Batch  11  loss:  0.00043141268542967737
Batch  21  loss:  0.00028980348724871874
Batch  31  loss:  0.0004737132985610515
Batch  41  loss:  0.0006254262989386916
Batch  51  loss:  0.000621556187979877
Batch  61  loss:  0.00045430316822603345
Batch  71  loss:  0.00045150480582378805
Batch  81  loss:  0.0003834560338873416
Batch  91  loss:  0.00037789138150401413
Batch  101  loss:  0.00026413172599859536
Batch  111  loss:  0.00028807445778511465
Batch  121  loss:  0.0004369378730189055
Batch  131  loss:  0.0005943012074567378
Batch  141  loss:  0.00033873229403980076
Batch  151  loss:  0.00038463008240796626
Batch  161  loss:  0.0002734955050982535
Batch  171  loss:  0.0005961792776361108
Batch  181  loss:  0.0002787909470498562
Batch  191  loss:  0.00025874015409499407
Validation on real data: 
LOSS supervised-train 0.00039137676838436165, valid 0.0002436602662783116
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0005615707486867905
Batch  11  loss:  0.0003945303615182638
Batch  21  loss:  0.0003079164889641106
Batch  31  loss:  0.0003335671790409833
Batch  41  loss:  0.000537495652679354
Batch  51  loss:  0.00056716293329373
Batch  61  loss:  0.0004425127699505538
Batch  71  loss:  0.00038306298665702343
Batch  81  loss:  0.0003537276934366673
Batch  91  loss:  0.0003127357631456107
Batch  101  loss:  0.0003305385180283338
Batch  111  loss:  0.0003254752664361149
Batch  121  loss:  0.00043075598659925163
Batch  131  loss:  0.0005142639856785536
Batch  141  loss:  0.00035987168666906655
Batch  151  loss:  0.0003944779746234417
Batch  161  loss:  0.0002854586055036634
Batch  171  loss:  0.000506297976244241
Batch  181  loss:  0.00032330697285942733
Batch  191  loss:  0.00030998061993159354
Validation on real data: 
LOSS supervised-train 0.00038410548557294533, valid 0.0003360749105922878
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0005889596068300307
Batch  11  loss:  0.0003759982355404645
Batch  21  loss:  0.00028119608759880066
Batch  31  loss:  0.000424169295001775
Batch  41  loss:  0.00045303802471607924
Batch  51  loss:  0.0007179518579505384
Batch  61  loss:  0.00039471089257858694
Batch  71  loss:  0.00033760457881726325
Batch  81  loss:  0.0003400988061912358
Batch  91  loss:  0.0003449410723987967
Batch  101  loss:  0.00024902771110646427
Batch  111  loss:  0.0002941052953246981
Batch  121  loss:  0.00039622976328246295
Batch  131  loss:  0.0004186245787423104
Batch  141  loss:  0.0002882922999560833
Batch  151  loss:  0.0002868917945306748
Batch  161  loss:  0.0002830901648849249
Batch  171  loss:  0.0005704617942683399
Batch  181  loss:  0.000301347638014704
Batch  191  loss:  0.00032125425059348345
Validation on real data: 
LOSS supervised-train 0.0003769082186772721, valid 0.0002805098192766309
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.000505587609950453
Batch  11  loss:  0.00034351638169027865
Batch  21  loss:  0.00028310794732533395
Batch  31  loss:  0.0003136758168693632
Batch  41  loss:  0.0005145139875821769
Batch  51  loss:  0.0007652600761502981
Batch  61  loss:  0.00038527464494109154
Batch  71  loss:  0.0005109469639137387
Batch  81  loss:  0.000298630737233907
Batch  91  loss:  0.000333272444549948
Batch  101  loss:  0.0002570090291555971
Batch  111  loss:  0.00026172256912104785
Batch  121  loss:  0.0003613885783124715
Batch  131  loss:  0.00047310860827565193
Batch  141  loss:  0.0003875121474266052
Batch  151  loss:  0.0003154980076942593
Batch  161  loss:  0.00027578152366913855
Batch  171  loss:  0.0005391418235376477
Batch  181  loss:  0.000289335468551144
Batch  191  loss:  0.00025269825709983706
Validation on real data: 
LOSS supervised-train 0.00036101120938837996, valid 0.00021091027883812785
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0003904215118382126
Batch  11  loss:  0.00037264139973558486
Batch  21  loss:  0.0002997374103870243
Batch  31  loss:  0.00030696604517288506
Batch  41  loss:  0.0004332936368882656
Batch  51  loss:  0.0005753817385993898
Batch  61  loss:  0.0003558809985406697
Batch  71  loss:  0.0003998644242528826
Batch  81  loss:  0.00032281168387271464
Batch  91  loss:  0.00031777340336702764
Batch  101  loss:  0.00030657960451208055
Batch  111  loss:  0.0002673586714081466
Batch  121  loss:  0.0004135772760491818
Batch  131  loss:  0.0004338349390309304
Batch  141  loss:  0.0003455737023614347
Batch  151  loss:  0.0003304433776065707
Batch  161  loss:  0.00032453524181619287
Batch  171  loss:  0.0005536146927624941
Batch  181  loss:  0.0002580536238383502
Batch  191  loss:  0.0003328866441734135
Validation on real data: 
LOSS supervised-train 0.00035245273174950854, valid 0.0002808155841194093
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0004854854487348348
Batch  11  loss:  0.0003537203010637313
Batch  21  loss:  0.00021774534252472222
Batch  31  loss:  0.0002599228755570948
Batch  41  loss:  0.0004770300292875618
Batch  51  loss:  0.0005587335326708853
Batch  61  loss:  0.0003964018833357841
Batch  71  loss:  0.00041317639988847077
Batch  81  loss:  0.00031554943416267633
Batch  91  loss:  0.00028407658101059496
Batch  101  loss:  0.00024502750602550805
Batch  111  loss:  0.0002834289043676108
Batch  121  loss:  0.00044735727715305984
Batch  131  loss:  0.00044872387661598623
Batch  141  loss:  0.00023850268917158246
Batch  151  loss:  0.00028374636895023286
Batch  161  loss:  0.0002172407112084329
Batch  171  loss:  0.0005206175846979022
Batch  181  loss:  0.00030841692932881415
Batch  191  loss:  0.00024595268769189715
Validation on real data: 
LOSS supervised-train 0.0003402954352350207, valid 0.0002677515149116516
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0004246819007676095
Batch  11  loss:  0.0003629361744970083
Batch  21  loss:  0.0002714744186960161
Batch  31  loss:  0.00040408610948361456
Batch  41  loss:  0.0004532074381131679
Batch  51  loss:  0.0005040933028794825
Batch  61  loss:  0.00039798131911084056
Batch  71  loss:  0.0004089833237230778
Batch  81  loss:  0.000299693871056661
Batch  91  loss:  0.00025608527357690036
Batch  101  loss:  0.0002670740650501102
Batch  111  loss:  0.00027631749981082976
Batch  121  loss:  0.0003622004296630621
Batch  131  loss:  0.0004694024391938001
Batch  141  loss:  0.0002686378138605505
Batch  151  loss:  0.0002503825817257166
Batch  161  loss:  0.00020247850625310093
Batch  171  loss:  0.0003918193106073886
Batch  181  loss:  0.00027568702353164554
Batch  191  loss:  0.00025902828201651573
Validation on real data: 
LOSS supervised-train 0.0003250087945343694, valid 0.00028888811357319355
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0003960072062909603
Batch  11  loss:  0.0003822210419457406
Batch  21  loss:  0.00027295731706544757
Batch  31  loss:  0.0002799929934553802
Batch  41  loss:  0.0004362968320492655
Batch  51  loss:  0.0006313693593256176
Batch  61  loss:  0.0003461695450823754
Batch  71  loss:  0.0003328182501718402
Batch  81  loss:  0.00036830289172939956
Batch  91  loss:  0.00029158894903957844
Batch  101  loss:  0.0002505354059394449
Batch  111  loss:  0.00029090011958032846
Batch  121  loss:  0.0003246033738832921
Batch  131  loss:  0.0004021434288006276
Batch  141  loss:  0.00025146445841528475
Batch  151  loss:  0.0003127558738924563
Batch  161  loss:  0.00028278454556129873
Batch  171  loss:  0.00041221801075153053
Batch  181  loss:  0.00022947991965338588
Batch  191  loss:  0.00029087794246152043
Validation on real data: 
LOSS supervised-train 0.00033084168971981854, valid 0.0002627981302794069
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0003648865385912359
Batch  11  loss:  0.0002787501725833863
Batch  21  loss:  0.00024462322471663356
Batch  31  loss:  0.00029476170311681926
Batch  41  loss:  0.00041500592487864196
Batch  51  loss:  0.0006900199223309755
Batch  61  loss:  0.0003422850277274847
Batch  71  loss:  0.0004273975791875273
Batch  81  loss:  0.0002590425137896091
Batch  91  loss:  0.0002721483469940722
Batch  101  loss:  0.0002662149490788579
Batch  111  loss:  0.00026954407803714275
Batch  121  loss:  0.00039550496148876846
Batch  131  loss:  0.000574427074752748
Batch  141  loss:  0.0002528734621591866
Batch  151  loss:  0.0002927397144958377
Batch  161  loss:  0.00019751045329030603
Batch  171  loss:  0.00045408715959638357
Batch  181  loss:  0.00020939603564329445
Batch  191  loss:  0.000255443446803838
Validation on real data: 
LOSS supervised-train 0.00032103798272146376, valid 0.00029818795155733824
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0003480431332718581
Batch  11  loss:  0.0003413177328184247
Batch  21  loss:  0.0002648416848387569
Batch  31  loss:  0.000312256277538836
Batch  41  loss:  0.00030306936241686344
Batch  51  loss:  0.0004977496573701501
Batch  61  loss:  0.0002961516729556024
Batch  71  loss:  0.00028818845748901367
Batch  81  loss:  0.0003122153866570443
Batch  91  loss:  0.0002641116443555802
Batch  101  loss:  0.00023390587011817843
Batch  111  loss:  0.00027605349896475673
Batch  121  loss:  0.0002397394273430109
Batch  131  loss:  0.0003603035002015531
Batch  141  loss:  0.00026219882420264184
Batch  151  loss:  0.0002350452123209834
Batch  161  loss:  0.00021652411669492722
Batch  171  loss:  0.0005331821157597005
Batch  181  loss:  0.000251423945883289
Batch  191  loss:  0.0002419293305138126
Validation on real data: 
LOSS supervised-train 0.00030011331939022056, valid 0.0002589961513876915
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.000416528491768986
Batch  11  loss:  0.0003348261525388807
Batch  21  loss:  0.0002460188989061862
Batch  31  loss:  0.00029785564402118325
Batch  41  loss:  0.00037824950413778424
Batch  51  loss:  0.0005830416921526194
Batch  61  loss:  0.00032383602228946984
Batch  71  loss:  0.0003453695389907807
Batch  81  loss:  0.0002996940165758133
Batch  91  loss:  0.0002738494658842683
Batch  101  loss:  0.00023711819085292518
Batch  111  loss:  0.00021210593695286661
Batch  121  loss:  0.0002793329185806215
Batch  131  loss:  0.0004237504326738417
Batch  141  loss:  0.00031824869802221656
Batch  151  loss:  0.00024391837359871715
Batch  161  loss:  0.00017174993990920484
Batch  171  loss:  0.00034056982258334756
Batch  181  loss:  0.00024049376952461898
Batch  191  loss:  0.00022361165611073375
Validation on real data: 
LOSS supervised-train 0.0003056463400571374, valid 0.0002993379603140056
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0004078526690136641
Batch  11  loss:  0.00033828330924734473
Batch  21  loss:  0.0002312523574801162
Batch  31  loss:  0.00028028254746459424
Batch  41  loss:  0.00031000550370663404
Batch  51  loss:  0.0005769001436419785
Batch  61  loss:  0.00029650129727087915
Batch  71  loss:  0.0003284742997493595
Batch  81  loss:  0.0002572233497630805
Batch  91  loss:  0.000274931633612141
Batch  101  loss:  0.00018201522470917553
Batch  111  loss:  0.0002742880897130817
Batch  121  loss:  0.0002899795363191515
Batch  131  loss:  0.0003046141646336764
Batch  141  loss:  0.00025531649589538574
Batch  151  loss:  0.00023383478401228786
Batch  161  loss:  0.00026062483084388077
Batch  171  loss:  0.0004189649480395019
Batch  181  loss:  0.0002988568157888949
Batch  191  loss:  0.0002714931033551693
Validation on real data: 
LOSS supervised-train 0.0002938057824212592, valid 0.00026157646789215505
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0004226214368827641
Batch  11  loss:  0.0002225289645139128
Batch  21  loss:  0.0002669096284080297
Batch  31  loss:  0.0002478982205502689
Batch  41  loss:  0.00040885325870476663
Batch  51  loss:  0.0005675276042893529
Batch  61  loss:  0.00035152232157997787
Batch  71  loss:  0.00027330624288879335
Batch  81  loss:  0.0002992070512846112
Batch  91  loss:  0.0003061179886572063
Batch  101  loss:  0.00022152115707285702
Batch  111  loss:  0.0001831213739933446
Batch  121  loss:  0.00026996564702130854
Batch  131  loss:  0.00031062844209372997
Batch  141  loss:  0.00023443001555278897
Batch  151  loss:  0.00024415526422671974
Batch  161  loss:  0.00022403652837965637
Batch  171  loss:  0.0003265035047661513
Batch  181  loss:  0.00026182582951150835
Batch  191  loss:  0.00021681371435988694
Validation on real data: 
LOSS supervised-train 0.00029211855377070606, valid 0.00028477224987000227
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0003520309110172093
Batch  11  loss:  0.0003086451906710863
Batch  21  loss:  0.0002270164986839518
Batch  31  loss:  0.00037099880864843726
Batch  41  loss:  0.00035105744609609246
Batch  51  loss:  0.00044759793672710657
Batch  61  loss:  0.0002630554372444749
Batch  71  loss:  0.0003255179326515645
Batch  81  loss:  0.00024048893828876317
Batch  91  loss:  0.0002676775911822915
Batch  101  loss:  0.00018959108274430037
Batch  111  loss:  0.00024228556139860302
Batch  121  loss:  0.0002174319961341098
Batch  131  loss:  0.0006685992702841759
Batch  141  loss:  0.000274274789262563
Batch  151  loss:  0.00034328619949519634
Batch  161  loss:  0.00021493695385288447
Batch  171  loss:  0.0004287867050152272
Batch  181  loss:  0.00028731152997352183
Batch  191  loss:  0.0002566910407040268
Validation on real data: 
LOSS supervised-train 0.0002901112115796423, valid 0.0002662504557520151
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0003819557896349579
Batch  11  loss:  0.0002468637831043452
Batch  21  loss:  0.0002306560054421425
Batch  31  loss:  0.0002935929805971682
Batch  41  loss:  0.0003521195030771196
Batch  51  loss:  0.0004717335687018931
Batch  61  loss:  0.0003383163420949131
Batch  71  loss:  0.0002807285636663437
Batch  81  loss:  0.00026154896477237344
Batch  91  loss:  0.0002249582757940516
Batch  101  loss:  0.000231710058869794
Batch  111  loss:  0.0002122162259183824
Batch  121  loss:  0.00028019282035529613
Batch  131  loss:  0.00038485563709400594
Batch  141  loss:  0.00020265310013201088
Batch  151  loss:  0.0002352200826862827
Batch  161  loss:  0.00022519694175571203
Batch  171  loss:  0.000315883633447811
Batch  181  loss:  0.00026175545644946396
Batch  191  loss:  0.00025455441209487617
Validation on real data: 
LOSS supervised-train 0.00028708210411423354, valid 0.00022803316824138165
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0003208409179933369
Batch  11  loss:  0.0003204932436347008
Batch  21  loss:  0.0002668538363650441
Batch  31  loss:  0.0002053136267932132
Batch  41  loss:  0.00025428179651498795
Batch  51  loss:  0.0004944779793731868
Batch  61  loss:  0.0002729577536229044
Batch  71  loss:  0.0002635125711094588
Batch  81  loss:  0.0002608546637929976
Batch  91  loss:  0.0002322573564015329
Batch  101  loss:  0.00022868979431223124
Batch  111  loss:  0.00024426757590845227
Batch  121  loss:  0.00022111478028818965
Batch  131  loss:  0.00039060614653863013
Batch  141  loss:  0.00019952408911194652
Batch  151  loss:  0.00023306826187763363
Batch  161  loss:  0.00023257426801137626
Batch  171  loss:  0.0003726372087839991
Batch  181  loss:  0.00022127349802758545
Batch  191  loss:  0.00019498137407936156
Validation on real data: 
LOSS supervised-train 0.00027590512996539474, valid 0.0002523474977351725
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0002900655963458121
Batch  11  loss:  0.0003127971140202135
Batch  21  loss:  0.00023415661416947842
Batch  31  loss:  0.00026355753652751446
Batch  41  loss:  0.0003942187177017331
Batch  51  loss:  0.0005471602780744433
Batch  61  loss:  0.0003087078221142292
Batch  71  loss:  0.0002831270103342831
Batch  81  loss:  0.00024071865482255816
Batch  91  loss:  0.0002227160584880039
Batch  101  loss:  0.00024306423438247293
Batch  111  loss:  0.0002819499932229519
Batch  121  loss:  0.0002600891748443246
Batch  131  loss:  0.00028362643206492066
Batch  141  loss:  0.0002985206083394587
Batch  151  loss:  0.00028118104091845453
Batch  161  loss:  0.00020552925707306713
Batch  171  loss:  0.0003484083863440901
Batch  181  loss:  0.00018455054669175297
Batch  191  loss:  0.00019732145301532
Validation on real data: 
LOSS supervised-train 0.0002774225569010014, valid 0.00020313600543886423
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00031803781166672707
Batch  11  loss:  0.00029291631653904915
Batch  21  loss:  0.00029127817833796144
Batch  31  loss:  0.00016778887948021293
Batch  41  loss:  0.00028105490491725504
Batch  51  loss:  0.0004405572253745049
Batch  61  loss:  0.00033420376712456346
Batch  71  loss:  0.00032920841476880014
Batch  81  loss:  0.00023847210104577243
Batch  91  loss:  0.0002480271505191922
Batch  101  loss:  0.00024207535898312926
Batch  111  loss:  0.00023818931367713958
Batch  121  loss:  0.0002590256917756051
Batch  131  loss:  0.0004105541738681495
Batch  141  loss:  0.00025801282026804984
Batch  151  loss:  0.0002864330599550158
Batch  161  loss:  0.0001913667656481266
Batch  171  loss:  0.0003011307562701404
Batch  181  loss:  0.00020941521506756544
Batch  191  loss:  0.00022592212189920247
Validation on real data: 
LOSS supervised-train 0.00027006087002519054, valid 0.00019037193851545453
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0003308186132926494
Batch  11  loss:  0.0002677177544683218
Batch  21  loss:  0.00022580711811315268
Batch  31  loss:  0.00024021264107432216
Batch  41  loss:  0.00030519766733050346
Batch  51  loss:  0.0003305120044387877
Batch  61  loss:  0.0002706433879211545
Batch  71  loss:  0.00024197771563194692
Batch  81  loss:  0.00022589368745684624
Batch  91  loss:  0.00022880572942085564
Batch  101  loss:  0.00022026427905075252
Batch  111  loss:  0.00021819237736053765
Batch  121  loss:  0.00023284435155801475
Batch  131  loss:  0.00034991378197446465
Batch  141  loss:  0.00022233487106859684
Batch  151  loss:  0.00025246647419407964
Batch  161  loss:  0.00018790480680763721
Batch  171  loss:  0.00033114361576735973
Batch  181  loss:  0.0002351621660636738
Batch  191  loss:  0.00020317267626523972
Validation on real data: 
LOSS supervised-train 0.000257476353799575, valid 0.0002126522595062852
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00035297064459882677
Batch  11  loss:  0.0002036709920503199
Batch  21  loss:  0.0001900527422549203
Batch  31  loss:  0.00024060928262770176
Batch  41  loss:  0.0002972982474602759
Batch  51  loss:  0.0003967184165958315
Batch  61  loss:  0.0002662790648173541
Batch  71  loss:  0.00020717237202916294
Batch  81  loss:  0.00025544551317580044
Batch  91  loss:  0.00023774032888468355
Batch  101  loss:  0.0001978805084945634
Batch  111  loss:  0.00018621888011693954
Batch  121  loss:  0.00022251871996559203
Batch  131  loss:  0.0003112011472694576
Batch  141  loss:  0.00020284444326534867
Batch  151  loss:  0.000267539347987622
Batch  161  loss:  0.00020222562307026237
Batch  171  loss:  0.0003246306150685996
Batch  181  loss:  0.00018499928410165012
Batch  191  loss:  0.00021816889056935906
Validation on real data: 
LOSS supervised-train 0.00025660996376245746, valid 0.00018683481903281063
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0002440350508550182
Batch  11  loss:  0.0002286679664393887
Batch  21  loss:  0.0002001711691264063
Batch  31  loss:  0.00017662759637460113
Batch  41  loss:  0.0003258468641433865
Batch  51  loss:  0.00038719805888831615
Batch  61  loss:  0.0003387966426089406
Batch  71  loss:  0.0003248091379646212
Batch  81  loss:  0.0002911316114477813
Batch  91  loss:  0.00024320135707966983
Batch  101  loss:  0.00027069481438957155
Batch  111  loss:  0.000192585212062113
Batch  121  loss:  0.0002180875017074868
Batch  131  loss:  0.0002732773427851498
Batch  141  loss:  0.00021902660955674946
Batch  151  loss:  0.00020362192299216986
Batch  161  loss:  0.00018223568622488528
Batch  171  loss:  0.00035213015507906675
Batch  181  loss:  0.00024587003281340003
Batch  191  loss:  0.0002550046774558723
Validation on real data: 
LOSS supervised-train 0.00025590132034267296, valid 0.00021111397654749453
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0002692522539291531
Batch  11  loss:  0.00024077719717752188
Batch  21  loss:  0.00021621859923470765
Batch  31  loss:  0.00026285761850886047
Batch  41  loss:  0.000292346318019554
Batch  51  loss:  0.0003164634108543396
Batch  61  loss:  0.000265659939032048
Batch  71  loss:  0.0002895906218327582
Batch  81  loss:  0.00029186985921114683
Batch  91  loss:  0.0002563032030593604
Batch  101  loss:  0.00023137177049648017
Batch  111  loss:  0.0002277167805004865
Batch  121  loss:  0.000234805847867392
Batch  131  loss:  0.00023142383724916726
Batch  141  loss:  0.00017201507580466568
Batch  151  loss:  0.00017998242401517928
Batch  161  loss:  0.00017394786118529737
Batch  171  loss:  0.0003206944384146482
Batch  181  loss:  0.00019087683176621795
Batch  191  loss:  0.0001919316127896309
Validation on real data: 
LOSS supervised-train 0.0002503635448374553, valid 0.00017810476128943264
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.00026582751888781786
Batch  11  loss:  0.0002695009461604059
Batch  21  loss:  0.00019399098528083414
Batch  31  loss:  0.0001834792346926406
Batch  41  loss:  0.00030318679637275636
Batch  51  loss:  0.00036886954330839217
Batch  61  loss:  0.00026752182748168707
Batch  71  loss:  0.0003529715759214014
Batch  81  loss:  0.0002229985111625865
Batch  91  loss:  0.00022700324188917875
Batch  101  loss:  0.000256287312367931
Batch  111  loss:  0.00024059455608949065
Batch  121  loss:  0.00024957905407063663
Batch  131  loss:  0.0003152589488308877
Batch  141  loss:  0.0002143625169992447
Batch  151  loss:  0.0002070565678877756
Batch  161  loss:  0.0002255815197713673
Batch  171  loss:  0.0003247943823225796
Batch  181  loss:  0.0002085893356706947
Batch  191  loss:  0.00020754884462803602
Validation on real data: 
LOSS supervised-train 0.0002489879463973921, valid 0.0002231275284430012
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00025169862783513963
Batch  11  loss:  0.00022464439098257571
Batch  21  loss:  0.00023789900296833366
Batch  31  loss:  0.00019091222202405334
Batch  41  loss:  0.00026985228760167956
Batch  51  loss:  0.0003814205410890281
Batch  61  loss:  0.0002243836352135986
Batch  71  loss:  0.00019977937336079776
Batch  81  loss:  0.00019704707665368915
Batch  91  loss:  0.0002105872699758038
Batch  101  loss:  0.00021381230908446014
Batch  111  loss:  0.00019655871437862515
Batch  121  loss:  0.00025496043963357806
Batch  131  loss:  0.00036934850504621863
Batch  141  loss:  0.00018492773233447224
Batch  151  loss:  0.00020132859935984015
Batch  161  loss:  0.0002480291004758328
Batch  171  loss:  0.00048792874440550804
Batch  181  loss:  0.00021263372036628425
Batch  191  loss:  0.00017715487047098577
Validation on real data: 
LOSS supervised-train 0.0002491404208558379, valid 0.00018864020239561796
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00023796393361408263
Batch  11  loss:  0.0002449761959724128
Batch  21  loss:  0.00019401412282604724
Batch  31  loss:  0.000150203617522493
Batch  41  loss:  0.00023127603344619274
Batch  51  loss:  0.0003000137803610414
Batch  61  loss:  0.00025969697162508965
Batch  71  loss:  0.00024203483189921826
Batch  81  loss:  0.00018359883688390255
Batch  91  loss:  0.00022283523867372423
Batch  101  loss:  0.000193951724213548
Batch  111  loss:  0.00015157194866333157
Batch  121  loss:  0.00018322031246498227
Batch  131  loss:  0.00031946360832080245
Batch  141  loss:  0.00025484542129561305
Batch  151  loss:  0.00018488828209228814
Batch  161  loss:  0.00019001163309440017
Batch  171  loss:  0.00029733043629676104
Batch  181  loss:  0.00021594323334284127
Batch  191  loss:  0.00018783914856612682
Validation on real data: 
LOSS supervised-train 0.000236740128530073, valid 0.00016362176393158734
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00029115984216332436
Batch  11  loss:  0.00023229554062709212
Batch  21  loss:  0.0001939509529620409
Batch  31  loss:  0.0001961548550752923
Batch  41  loss:  0.0002838758227881044
Batch  51  loss:  0.00036573896068148315
Batch  61  loss:  0.00026201619766652584
Batch  71  loss:  0.0002184339246014133
Batch  81  loss:  0.00016760842117946595
Batch  91  loss:  0.00021863776782993227
Batch  101  loss:  0.00022296594397630543
Batch  111  loss:  0.0001958097709575668
Batch  121  loss:  0.00022429830278269947
Batch  131  loss:  0.0002559535496402532
Batch  141  loss:  0.00020993806538172066
Batch  151  loss:  0.0002013634075410664
Batch  161  loss:  0.0001898850896395743
Batch  171  loss:  0.00031162938103079796
Batch  181  loss:  0.00020133548241574317
Batch  191  loss:  0.0002019029634539038
Validation on real data: 
LOSS supervised-train 0.00023139126838941592, valid 0.00019142424571327865
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0002634593693073839
Batch  11  loss:  0.0002077476674458012
Batch  21  loss:  0.000208057725103572
Batch  31  loss:  0.00025416011339984834
Batch  41  loss:  0.00019849817908834666
Batch  51  loss:  0.00032249095966108143
Batch  61  loss:  0.000271587137831375
Batch  71  loss:  0.0003113405837211758
Batch  81  loss:  0.00019716595124918967
Batch  91  loss:  0.0002630750532262027
Batch  101  loss:  0.00016733586380723864
Batch  111  loss:  0.00019793730461969972
Batch  121  loss:  0.00020759622566401958
Batch  131  loss:  0.00024412893981207162
Batch  141  loss:  0.00023847540433052927
Batch  151  loss:  0.00021125911734998226
Batch  161  loss:  0.00016403847257606685
Batch  171  loss:  0.0003254651965107769
Batch  181  loss:  0.00019264307047706097
Batch  191  loss:  0.0001832592679420486
Validation on real data: 
LOSS supervised-train 0.0002278981014387682, valid 0.00017984764417633414
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00024019740521907806
Batch  11  loss:  0.0002460115065332502
Batch  21  loss:  0.000183659911272116
Batch  31  loss:  0.00018374949286226183
Batch  41  loss:  0.00034520842018537223
Batch  51  loss:  0.0003145246591884643
Batch  61  loss:  0.00024438719265162945
Batch  71  loss:  0.0002786948170978576
Batch  81  loss:  0.00015194702427834272
Batch  91  loss:  0.00025554950116202235
Batch  101  loss:  0.00017457103240303695
Batch  111  loss:  0.00019634750788100064
Batch  121  loss:  0.00022925114899408072
Batch  131  loss:  0.00024128456425387412
Batch  141  loss:  0.0002469317114446312
Batch  151  loss:  0.0002407718711765483
Batch  161  loss:  0.00019833879196085036
Batch  171  loss:  0.00024888390908017755
Batch  181  loss:  0.00019233606872148812
Batch  191  loss:  0.00019387317297514528
Validation on real data: 
LOSS supervised-train 0.0002263868688896764, valid 0.0002120262070093304
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00025836756685748696
Batch  11  loss:  0.0001997631770791486
Batch  21  loss:  0.0002256054722238332
Batch  31  loss:  0.00019904827058780938
Batch  41  loss:  0.00026570141199044883
Batch  51  loss:  0.00039112873491831124
Batch  61  loss:  0.0002544715825933963
Batch  71  loss:  0.00021407987514976412
Batch  81  loss:  0.00019461967167444527
Batch  91  loss:  0.0002167358179576695
Batch  101  loss:  0.00017655108240433037
Batch  111  loss:  0.00017750609549693763
Batch  121  loss:  0.00016157314530573785
Batch  131  loss:  0.0002703384670894593
Batch  141  loss:  0.00018026090401690453
Batch  151  loss:  0.00021976382413413376
Batch  161  loss:  0.0001894469460239634
Batch  171  loss:  0.00035014867899008095
Batch  181  loss:  0.00018780771642923355
Batch  191  loss:  0.00016911390412133187
Validation on real data: 
LOSS supervised-train 0.00023041822594677797, valid 0.00017420537187717855
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00026560030528344214
Batch  11  loss:  0.00022266797896008939
Batch  21  loss:  0.0001587172446306795
Batch  31  loss:  0.00019110071298200637
Batch  41  loss:  0.00028347282204777
Batch  51  loss:  0.0003336842346470803
Batch  61  loss:  0.00024366474826820195
Batch  71  loss:  0.00023212525411508977
Batch  81  loss:  0.00018223814549855888
Batch  91  loss:  0.00020300920004956424
Batch  101  loss:  0.00021309634030330926
Batch  111  loss:  0.00016658839012961835
Batch  121  loss:  0.00022719125263392925
Batch  131  loss:  0.0002571404038462788
Batch  141  loss:  0.00019973244343418628
Batch  151  loss:  0.0002088883484248072
Batch  161  loss:  0.00016768537170719355
Batch  171  loss:  0.00036839369568042457
Batch  181  loss:  0.00018696393817663193
Batch  191  loss:  0.00020770383707713336
Validation on real data: 
LOSS supervised-train 0.00022086740522354376, valid 0.00018811570771504194
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0002136084804078564
Batch  11  loss:  0.0002075573429465294
Batch  21  loss:  0.00020837686315644532
Batch  31  loss:  0.000226580144953914
Batch  41  loss:  0.0002690186956897378
Batch  51  loss:  0.0003337017260491848
Batch  61  loss:  0.00026853580493479967
Batch  71  loss:  0.00017341601778753102
Batch  81  loss:  0.00022009987151250243
Batch  91  loss:  0.00021111327805556357
Batch  101  loss:  0.0002076982636936009
Batch  111  loss:  0.00015705122496001422
Batch  121  loss:  0.00019043586507905275
Batch  131  loss:  0.0002793382736854255
Batch  141  loss:  0.00019198439258616418
Batch  151  loss:  0.00019775763212237507
Batch  161  loss:  0.00014278446906246245
Batch  171  loss:  0.0002542396541684866
Batch  181  loss:  0.0001718590356176719
Batch  191  loss:  0.00017610575014259666
Validation on real data: 
LOSS supervised-train 0.0002182641085528303, valid 0.00018081365851685405
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0002435923379380256
Batch  11  loss:  0.00018872402142733335
Batch  21  loss:  0.00022435611754190177
Batch  31  loss:  0.0001927721023093909
Batch  41  loss:  0.00023569811310153455
Batch  51  loss:  0.00030771346064284444
Batch  61  loss:  0.00021488485799636692
Batch  71  loss:  0.00020899863739032298
Batch  81  loss:  0.00015412436914630234
Batch  91  loss:  0.0002269977703690529
Batch  101  loss:  0.00018011475913226604
Batch  111  loss:  0.00016794596740510315
Batch  121  loss:  0.00014969659969210625
Batch  131  loss:  0.00025017952430061996
Batch  141  loss:  0.0002512547071091831
Batch  151  loss:  0.0002097152901114896
Batch  161  loss:  0.00014432350872084498
Batch  171  loss:  0.0003196265606675297
Batch  181  loss:  0.00017637466953601688
Batch  191  loss:  0.0002054865617537871
Validation on real data: 
LOSS supervised-train 0.00021682444210455287, valid 0.00017534302605781704
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0002385513362241909
Batch  11  loss:  0.00022126168187242
Batch  21  loss:  0.00019829667871817946
Batch  31  loss:  0.00017376436153426766
Batch  41  loss:  0.00021458497212734073
Batch  51  loss:  0.00031267321901395917
Batch  61  loss:  0.0002451615291647613
Batch  71  loss:  0.00021548512449953705
Batch  81  loss:  0.0001594474451849237
Batch  91  loss:  0.0002419041411485523
Batch  101  loss:  0.0001766524655977264
Batch  111  loss:  0.00019246498413849622
Batch  121  loss:  0.00021048920461907983
Batch  131  loss:  0.00022563159291166812
Batch  141  loss:  0.00018894753884524107
Batch  151  loss:  0.0002142166340490803
Batch  161  loss:  0.0001892561122076586
Batch  171  loss:  0.00033569170045666397
Batch  181  loss:  0.00018688566342461854
Batch  191  loss:  0.00017868171562440693
Validation on real data: 
LOSS supervised-train 0.0002143854273163015, valid 0.00018510225345380604
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0002726434904616326
Batch  11  loss:  0.00022452171833720058
Batch  21  loss:  0.00017830209981184453
Batch  31  loss:  0.00023097322264220566
Batch  41  loss:  0.00029224209720268846
Batch  51  loss:  0.00035109816235490143
Batch  61  loss:  0.00026937105576507747
Batch  71  loss:  0.0002657487930264324
Batch  81  loss:  0.0001956203195732087
Batch  91  loss:  0.00023103735293261707
Batch  101  loss:  0.0002098329714499414
Batch  111  loss:  0.00017883848340716213
Batch  121  loss:  0.0001791750983102247
Batch  131  loss:  0.00033114044344983995
Batch  141  loss:  0.00021756120258942246
Batch  151  loss:  0.00015251344302669168
Batch  161  loss:  0.00019150755542796105
Batch  171  loss:  0.00023507315199822187
Batch  181  loss:  0.000199642323423177
Batch  191  loss:  0.00021500475122593343
Validation on real data: 
LOSS supervised-train 0.0002180124057485955, valid 0.00017418895731680095
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.00023704189516138285
Batch  11  loss:  0.00024376153305638582
Batch  21  loss:  0.00017931046022567898
Batch  31  loss:  0.00017609029600862414
Batch  41  loss:  0.00023179322306532413
Batch  51  loss:  0.0002675199939403683
Batch  61  loss:  0.00022664295102003962
Batch  71  loss:  0.0002312878059456125
Batch  81  loss:  0.0001498846831964329
Batch  91  loss:  0.00026488883304409683
Batch  101  loss:  0.00019219274690840393
Batch  111  loss:  0.00017321962513960898
Batch  121  loss:  0.0001609370083315298
Batch  131  loss:  0.0002428293228149414
Batch  141  loss:  0.00022882054327055812
Batch  151  loss:  0.00018355071370024234
Batch  161  loss:  0.00015796955267433077
Batch  171  loss:  0.0002882545522879809
Batch  181  loss:  0.000186602323083207
Batch  191  loss:  0.00015227662515826523
Validation on real data: 
LOSS supervised-train 0.00020903536904370412, valid 0.00019422096374910325
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00027782341931015253
Batch  11  loss:  0.0002761250070761889
Batch  21  loss:  0.00019385125779081136
Batch  31  loss:  0.00022984961105976254
Batch  41  loss:  0.00024172401754185557
Batch  51  loss:  0.0003187961701769382
Batch  61  loss:  0.0001991181925404817
Batch  71  loss:  0.00018713227473199368
Batch  81  loss:  0.00018083470058627427
Batch  91  loss:  0.00026755890576168895
Batch  101  loss:  0.00014411033771466464
Batch  111  loss:  0.00016354661784134805
Batch  121  loss:  0.00018203246872872114
Batch  131  loss:  0.0002191029634559527
Batch  141  loss:  0.0001898491900647059
Batch  151  loss:  0.00020227556524332613
Batch  161  loss:  0.00019454352150205523
Batch  171  loss:  0.00027654547011479735
Batch  181  loss:  0.00020592827058862895
Batch  191  loss:  0.00018211091810371727
Validation on real data: 
LOSS supervised-train 0.00020593787059624446, valid 0.00018854628433473408
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00022329304192680866
Batch  11  loss:  0.0002038582315435633
Batch  21  loss:  0.0001925059041241184
Batch  31  loss:  0.00019089407578576356
Batch  41  loss:  0.00024862049031071365
Batch  51  loss:  0.0003129070973955095
Batch  61  loss:  0.00022762703883927315
Batch  71  loss:  0.00023389927810057998
Batch  81  loss:  0.00021499804279301316
Batch  91  loss:  0.00019389546650927514
Batch  101  loss:  0.00015724266995675862
Batch  111  loss:  0.0001312073873123154
Batch  121  loss:  0.00015907558554317802
Batch  131  loss:  0.0002770832215901464
Batch  141  loss:  0.00019348922069184482
Batch  151  loss:  0.00020135636441409588
Batch  161  loss:  0.00015595261356793344
Batch  171  loss:  0.0003203196683898568
Batch  181  loss:  0.00019218948727939278
Batch  191  loss:  0.00015949326916597784
Validation on real data: 
LOSS supervised-train 0.00021265172355924733, valid 0.00019152053573634475
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00022531546710524708
Batch  11  loss:  0.00020727224182337523
Batch  21  loss:  0.0002048557944362983
Batch  31  loss:  0.00017148295592051
Batch  41  loss:  0.0002757919137366116
Batch  51  loss:  0.00032507418654859066
Batch  61  loss:  0.00022839891607873142
Batch  71  loss:  0.00021472775551956147
Batch  81  loss:  0.00021902061416767538
Batch  91  loss:  0.0001457768230466172
Batch  101  loss:  0.0002314406883670017
Batch  111  loss:  0.00019093691662419587
Batch  121  loss:  0.00017430895240977407
Batch  131  loss:  0.00030184004572220147
Batch  141  loss:  0.0001506691478425637
Batch  151  loss:  0.0001638700341572985
Batch  161  loss:  0.00017658484284766018
Batch  171  loss:  0.00026907253777608275
Batch  181  loss:  0.0002291947021149099
Batch  191  loss:  0.0001784523919923231
Validation on real data: 
LOSS supervised-train 0.0002100393723230809, valid 0.00014969393669161946
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00019250053446739912
Batch  11  loss:  0.00021500918956007808
Batch  21  loss:  0.00018982734763994813
Batch  31  loss:  0.00018880500283557922
Batch  41  loss:  0.00025064992951229215
Batch  51  loss:  0.00031508522806689143
Batch  61  loss:  0.00022283328871708363
Batch  71  loss:  0.00024376544752158225
Batch  81  loss:  0.00019844554481096566
Batch  91  loss:  0.0002223950723418966
Batch  101  loss:  0.00017462427786085755
Batch  111  loss:  0.00013417044829111546
Batch  121  loss:  0.00021009493502788246
Batch  131  loss:  0.00018725187692325562
Batch  141  loss:  0.00017725377983879298
Batch  151  loss:  0.00017910554015543312
Batch  161  loss:  0.00018148432718589902
Batch  171  loss:  0.00023698642326053232
Batch  181  loss:  0.00018719144281931221
Batch  191  loss:  0.00017135489906650037
Validation on real data: 
LOSS supervised-train 0.0002009338651987491, valid 0.00018460824503563344
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0002325615205336362
Batch  11  loss:  0.00018248420383315533
Batch  21  loss:  0.00018369179451838136
Batch  31  loss:  0.0001434095756849274
Batch  41  loss:  0.00018867217295337468
Batch  51  loss:  0.00038100432720966637
Batch  61  loss:  0.00019341216830071062
Batch  71  loss:  0.00018748514412436634
Batch  81  loss:  0.00021894671954214573
Batch  91  loss:  0.0001730613730615005
Batch  101  loss:  0.00013788526121061295
Batch  111  loss:  0.00018856617680285126
Batch  121  loss:  0.00017283225315622985
Batch  131  loss:  0.00020914580090902746
Batch  141  loss:  0.00016153504839167
Batch  151  loss:  0.00017191126244142652
Batch  161  loss:  0.000180378818186
Batch  171  loss:  0.00028050749097019434
Batch  181  loss:  0.00022753184020984918
Batch  191  loss:  0.00015444151358678937
Validation on real data: 
LOSS supervised-train 0.0001993132568168221, valid 0.00018381403060629964
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0001830771507229656
Batch  11  loss:  0.00021048745838925242
Batch  21  loss:  0.00018875001114793122
Batch  31  loss:  0.00017755806038621813
Batch  41  loss:  0.0002338510676054284
Batch  51  loss:  0.0002609485818538815
Batch  61  loss:  0.00023621499713044614
Batch  71  loss:  0.00019499880727380514
Batch  81  loss:  0.00013974661123938859
Batch  91  loss:  0.0002114728995366022
Batch  101  loss:  0.00015437157708220184
Batch  111  loss:  0.00018505506159272045
Batch  121  loss:  0.00018911226652562618
Batch  131  loss:  0.00023581943241879344
Batch  141  loss:  0.0001733819517539814
Batch  151  loss:  0.00016009410319384187
Batch  161  loss:  0.00017116271192207932
Batch  171  loss:  0.0002424262056592852
Batch  181  loss:  0.00017961999401450157
Batch  191  loss:  0.00012869690544903278
Validation on real data: 
LOSS supervised-train 0.00019526934611349133, valid 0.00014573645603377372
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00021570529497694224
Batch  11  loss:  0.00021424572332762182
Batch  21  loss:  0.00019489310216158628
Batch  31  loss:  0.00021136931900400668
Batch  41  loss:  0.00026422503287903965
Batch  51  loss:  0.00027023788425140083
Batch  61  loss:  0.00018204677326139063
Batch  71  loss:  0.00021503839525394142
Batch  81  loss:  0.0001903270895127207
Batch  91  loss:  0.0001936738408403471
Batch  101  loss:  0.0001731575175654143
Batch  111  loss:  0.00016079678607638925
Batch  121  loss:  0.00014825000835116953
Batch  131  loss:  0.00028942213975824416
Batch  141  loss:  0.00020421502995304763
Batch  151  loss:  0.0001721977023407817
Batch  161  loss:  0.00016723322914913297
Batch  171  loss:  0.00024356914218515158
Batch  181  loss:  0.0001751207310007885
Batch  191  loss:  0.0001657139218878001
Validation on real data: 
LOSS supervised-train 0.0001944112838464207, valid 0.00017306282825302333
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00018610119877848774
Batch  11  loss:  0.00016383681213483214
Batch  21  loss:  0.00018850281776394695
Batch  31  loss:  0.00014007711433805525
Batch  41  loss:  0.00022317057300824672
Batch  51  loss:  0.0003221239021513611
Batch  61  loss:  0.00021422511781565845
Batch  71  loss:  0.000212411570828408
Batch  81  loss:  0.000157714617671445
Batch  91  loss:  0.00018917232227977365
Batch  101  loss:  0.00016550786676816642
Batch  111  loss:  0.0001715803809929639
Batch  121  loss:  0.00017408101120963693
Batch  131  loss:  0.0002555214159656316
Batch  141  loss:  0.00014319275214802474
Batch  151  loss:  0.00018209739937447011
Batch  161  loss:  0.00017464302072767168
Batch  171  loss:  0.00027661374770104885
Batch  181  loss:  0.0001581874385010451
Batch  191  loss:  0.00013590700109489262
Validation on real data: 
LOSS supervised-train 0.00019226028201956068, valid 0.00014315691078081727
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00017301764455623925
Batch  11  loss:  0.0001651852362556383
Batch  21  loss:  0.0001406258816132322
Batch  31  loss:  0.0001679213746683672
Batch  41  loss:  0.00025881713372655213
Batch  51  loss:  0.00023352724383585155
Batch  61  loss:  0.0002210462262155488
Batch  71  loss:  0.0001992898905882612
Batch  81  loss:  0.00016610919556114823
Batch  91  loss:  0.0001394903229083866
Batch  101  loss:  0.00013591701281256974
Batch  111  loss:  0.00016701107961125672
Batch  121  loss:  0.00015329555026255548
Batch  131  loss:  0.00020406133262440562
Batch  141  loss:  0.00018110878590960056
Batch  151  loss:  0.00018437780090607703
Batch  161  loss:  0.00013957341434434056
Batch  171  loss:  0.00026160944253206253
Batch  181  loss:  0.00017492828192189336
Batch  191  loss:  0.000141167503898032
Validation on real data: 
LOSS supervised-train 0.00018988582283782307, valid 0.00014814469614066184
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00020151033822912723
Batch  11  loss:  0.00016782977036200464
Batch  21  loss:  0.0001913807645905763
Batch  31  loss:  0.00012314891500864178
Batch  41  loss:  0.00022109832207206637
Batch  51  loss:  0.00036862693377770483
Batch  61  loss:  0.00021640428167302161
Batch  71  loss:  0.00020173990924376994
Batch  81  loss:  0.00012535438872873783
Batch  91  loss:  0.00017034410848282278
Batch  101  loss:  0.00017573873628862202
Batch  111  loss:  0.00017201325681526214
Batch  121  loss:  0.00017997239774558693
Batch  131  loss:  0.0003547219967003912
Batch  141  loss:  0.00018427458417136222
Batch  151  loss:  0.00019655056530609727
Batch  161  loss:  0.00015690625878050923
Batch  171  loss:  0.00027176717412658036
Batch  181  loss:  0.0001861596101662144
Batch  191  loss:  0.00015757344954181463
Validation on real data: 
LOSS supervised-train 0.00019511354181304342, valid 0.00015440484276041389
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00016139555373229086
Batch  11  loss:  0.00018605294462759048
Batch  21  loss:  0.0001553476176923141
Batch  31  loss:  0.00015164230717346072
Batch  41  loss:  0.0002012970799114555
Batch  51  loss:  0.00028531241696327925
Batch  61  loss:  0.00019362501916475594
Batch  71  loss:  0.00016476288146805018
Batch  81  loss:  0.00017303922504652292
Batch  91  loss:  0.00017483910778537393
Batch  101  loss:  0.00016002546180970967
Batch  111  loss:  0.00019258550310041755
Batch  121  loss:  0.00019258489191997796
Batch  131  loss:  0.0002870241296477616
Batch  141  loss:  0.0002253777056466788
Batch  151  loss:  0.00019758075359277427
Batch  161  loss:  0.00014550266496371478
Batch  171  loss:  0.00024091113300528377
Batch  181  loss:  0.00015269621508195996
Batch  191  loss:  0.00016901816707104445
Validation on real data: 
LOSS supervised-train 0.00019055095966905356, valid 0.0001358030131086707
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00020298591698519886
Batch  11  loss:  0.00019081179925706238
Batch  21  loss:  0.00014614127576351166
Batch  31  loss:  0.00016635473002679646
Batch  41  loss:  0.00024824103456921875
Batch  51  loss:  0.0002850433811545372
Batch  61  loss:  0.00017840477812569588
Batch  71  loss:  0.0001820969337131828
Batch  81  loss:  0.00018237291078548878
Batch  91  loss:  0.00018824284779839218
Batch  101  loss:  0.0001528312568552792
Batch  111  loss:  0.00018476742843631655
Batch  121  loss:  0.00013971595035400242
Batch  131  loss:  0.00022395994164980948
Batch  141  loss:  0.00017805490642786026
Batch  151  loss:  0.00014965511218179017
Batch  161  loss:  0.00012677119229920208
Batch  171  loss:  0.00021689641289412975
Batch  181  loss:  0.00012839258124586195
Batch  191  loss:  0.00015903856547083706
Validation on real data: 
LOSS supervised-train 0.0001849030230732751, valid 0.00017092129564844072
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00021346568246372044
Batch  11  loss:  0.00018251534493174404
Batch  21  loss:  0.00011268087109783664
Batch  31  loss:  0.00011600788275245577
Batch  41  loss:  0.00023065254208631814
Batch  51  loss:  0.0002539646811783314
Batch  61  loss:  0.00019637648074422032
Batch  71  loss:  0.00020000677614007145
Batch  81  loss:  0.00013910536654293537
Batch  91  loss:  0.0001774582196958363
Batch  101  loss:  0.00015140055620577186
Batch  111  loss:  0.00014091697812546045
Batch  121  loss:  0.00014193268725648522
Batch  131  loss:  0.00028850589296780527
Batch  141  loss:  0.0001627177553018555
Batch  151  loss:  0.00015597359742969275
Batch  161  loss:  0.00015132196131162345
Batch  171  loss:  0.0002375877957092598
Batch  181  loss:  0.00016788595530670136
Batch  191  loss:  0.0001450795098207891
Validation on real data: 
LOSS supervised-train 0.00018576730682980268, valid 0.00015817098028492182
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00018925748008769006
Batch  11  loss:  0.00015871260256972164
Batch  21  loss:  0.0001732061937218532
Batch  31  loss:  0.00019960022473242134
Batch  41  loss:  0.00018721917876973748
Batch  51  loss:  0.0003338047827128321
Batch  61  loss:  0.00018519900913815945
Batch  71  loss:  0.00017284389468841255
Batch  81  loss:  0.00017089038738049567
Batch  91  loss:  0.0001913194719236344
Batch  101  loss:  0.00015291824820451438
Batch  111  loss:  0.0001468226982979104
Batch  121  loss:  0.0001759100705385208
Batch  131  loss:  0.0002946409513242543
Batch  141  loss:  0.00019238467211835086
Batch  151  loss:  0.00016186114226002246
Batch  161  loss:  0.00011994774104095995
Batch  171  loss:  0.00024852139176800847
Batch  181  loss:  0.0001362362818326801
Batch  191  loss:  0.00013576100172940642
Validation on real data: 
LOSS supervised-train 0.00018707341499975882, valid 0.00017999339615926147
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00015814468497410417
Batch  11  loss:  0.00014456630742643028
Batch  21  loss:  0.00014434807235375047
Batch  31  loss:  0.00014281133189797401
Batch  41  loss:  0.00020319299073889852
Batch  51  loss:  0.00030276778852567077
Batch  61  loss:  0.00023195284302346408
Batch  71  loss:  0.0001707721530692652
Batch  81  loss:  0.0001410489931004122
Batch  91  loss:  0.00020425650291144848
Batch  101  loss:  0.00019128061830997467
Batch  111  loss:  0.00014597584959119558
Batch  121  loss:  0.00013515283353626728
Batch  131  loss:  0.00018484133761376143
Batch  141  loss:  0.00011738415196305141
Batch  151  loss:  0.00018747407011687756
Batch  161  loss:  0.00014002836542204022
Batch  171  loss:  0.00029669032664969563
Batch  181  loss:  0.00017044931882992387
Batch  191  loss:  0.00013795227278023958
Validation on real data: 
LOSS supervised-train 0.00018002656925091286, valid 0.00017517042579129338
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00018908824131358415
Batch  11  loss:  0.00018155942962039262
Batch  21  loss:  0.00012229042476974428
Batch  31  loss:  0.00014550596824847162
Batch  41  loss:  0.00022570595319848508
Batch  51  loss:  0.000265825423412025
Batch  61  loss:  0.00018875274690799415
Batch  71  loss:  0.00017763255164027214
Batch  81  loss:  0.0001459272752981633
Batch  91  loss:  0.00013552483869716525
Batch  101  loss:  0.00013466080417856574
Batch  111  loss:  0.00016996685008052737
Batch  121  loss:  0.00017388738342560828
Batch  131  loss:  0.00024097540881484747
Batch  141  loss:  0.00020420744840521365
Batch  151  loss:  0.0001834842114476487
Batch  161  loss:  0.00019007804803550243
Batch  171  loss:  0.00024272587324958295
Batch  181  loss:  0.00016658073582220823
Batch  191  loss:  0.00016272327047772706
Validation on real data: 
LOSS supervised-train 0.00018180309303716057, valid 0.0001650866324780509
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00015829103358555585
Batch  11  loss:  0.00017704750644043088
Batch  21  loss:  0.0001483122177887708
Batch  31  loss:  0.00018161197658628225
Batch  41  loss:  0.00016406463691964746
Batch  51  loss:  0.0002385785337537527
Batch  61  loss:  0.00017627292254474014
Batch  71  loss:  0.00017714420391712338
Batch  81  loss:  0.00015773973427712917
Batch  91  loss:  0.00018277578055858612
Batch  101  loss:  0.00013645521539729089
Batch  111  loss:  0.00012810295447707176
Batch  121  loss:  0.0001673176884651184
Batch  131  loss:  0.00025123581872321665
Batch  141  loss:  0.00018875378009397537
Batch  151  loss:  0.00014846670092083514
Batch  161  loss:  0.00016492014401592314
Batch  171  loss:  0.0002379733050474897
Batch  181  loss:  0.0001739827130222693
Batch  191  loss:  0.00017099929391406476
Validation on real data: 
LOSS supervised-train 0.0001777924284033361, valid 0.0001459121413063258
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00016940022760536522
Batch  11  loss:  0.00016151211457327008
Batch  21  loss:  0.00015237581101246178
Batch  31  loss:  0.00014808664855081588
Batch  41  loss:  0.00019123706442769617
Batch  51  loss:  0.0002921581035479903
Batch  61  loss:  0.00022509228438138962
Batch  71  loss:  0.00017260490858461708
Batch  81  loss:  0.0001554943446535617
Batch  91  loss:  0.0001436818129150197
Batch  101  loss:  0.00014059471141081303
Batch  111  loss:  0.00018580972391646355
Batch  121  loss:  0.0001552698522573337
Batch  131  loss:  0.0001976423227461055
Batch  141  loss:  0.00013089478306937963
Batch  151  loss:  0.00019196431094314903
Batch  161  loss:  0.000145236132084392
Batch  171  loss:  0.00028286565793678164
Batch  181  loss:  0.00015724483819212765
Batch  191  loss:  0.00016285631863866001
Validation on real data: 
LOSS supervised-train 0.00017478516594565007, valid 0.00014548705075867474
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00013639377721119672
Batch  11  loss:  0.00018696706683840603
Batch  21  loss:  0.00016704475274309516
Batch  31  loss:  0.00016122414672281593
Batch  41  loss:  0.00018911562801804394
Batch  51  loss:  0.0002593088138382882
Batch  61  loss:  0.00022202794207260013
Batch  71  loss:  0.00017705827485769987
Batch  81  loss:  0.00016119271458592266
Batch  91  loss:  0.00019014319695997983
Batch  101  loss:  0.00017196136468555778
Batch  111  loss:  0.00011402706149965525
Batch  121  loss:  0.0001378064916934818
Batch  131  loss:  0.00020935223437845707
Batch  141  loss:  0.0001872179564088583
Batch  151  loss:  0.00012546460493467748
Batch  161  loss:  0.00012592140410561115
Batch  171  loss:  0.0002179477014578879
Batch  181  loss:  0.00015395473747048527
Batch  191  loss:  0.00013342108286451548
Validation on real data: 
LOSS supervised-train 0.0001730996107289684, valid 0.00017551802739035338
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00016533299640286714
Batch  11  loss:  0.00014192372327670455
Batch  21  loss:  0.00014241284225136042
Batch  31  loss:  0.00019375440024305135
Batch  41  loss:  0.00022424734197556973
Batch  51  loss:  0.0002077303797705099
Batch  61  loss:  0.00018747340072877705
Batch  71  loss:  0.00017935271898750216
Batch  81  loss:  0.00012993928976356983
Batch  91  loss:  0.0001599249808350578
Batch  101  loss:  0.00012223787780385464
Batch  111  loss:  0.0001391862751916051
Batch  121  loss:  0.0001631444611120969
Batch  131  loss:  0.00024761000531725585
Batch  141  loss:  0.0001230788475368172
Batch  151  loss:  0.00016566803969908506
Batch  161  loss:  9.09431983018294e-05
Batch  171  loss:  0.0002198757865699008
Batch  181  loss:  0.0001654608204262331
Batch  191  loss:  0.00013372374814935029
Validation on real data: 
LOSS supervised-train 0.00017068940123863286, valid 0.0001202940838993527
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0001684843300608918
Batch  11  loss:  0.00017228437354788184
Batch  21  loss:  0.00014368290430866182
Batch  31  loss:  0.0001316877023782581
Batch  41  loss:  0.00018394853395875543
Batch  51  loss:  0.0002754002052824944
Batch  61  loss:  0.00021443852165248245
Batch  71  loss:  0.00019019722822122276
Batch  81  loss:  0.00013946897524874657
Batch  91  loss:  0.0001891206920845434
Batch  101  loss:  0.00014862338139209896
Batch  111  loss:  0.00011032129259547219
Batch  121  loss:  0.00016634099301882088
Batch  131  loss:  0.000245968607487157
Batch  141  loss:  0.00015056367556098849
Batch  151  loss:  0.00018350337632000446
Batch  161  loss:  0.00012574580614455044
Batch  171  loss:  0.00021405972074717283
Batch  181  loss:  0.00015904811152722687
Batch  191  loss:  0.0001465450768591836
Validation on real data: 
LOSS supervised-train 0.0001679252913527307, valid 0.0001508530112914741
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00016209874593187124
Batch  11  loss:  0.0001416724844602868
Batch  21  loss:  0.0001426532689947635
Batch  31  loss:  0.00014577321417164057
Batch  41  loss:  0.00016394957492593676
Batch  51  loss:  0.0003013092209585011
Batch  61  loss:  0.0001993765472434461
Batch  71  loss:  0.00019384807092137635
Batch  81  loss:  0.00013221148401498795
Batch  91  loss:  0.00013550784206017852
Batch  101  loss:  0.0001383548224112019
Batch  111  loss:  0.00014035303320270032
Batch  121  loss:  0.0001418118627043441
Batch  131  loss:  0.00026448359130881727
Batch  141  loss:  0.0001405093789799139
Batch  151  loss:  0.0001788127701729536
Batch  161  loss:  0.00011709570389939472
Batch  171  loss:  0.00020675853011198342
Batch  181  loss:  0.0001660115667618811
Batch  191  loss:  0.00013576577475760132
Validation on real data: 
LOSS supervised-train 0.00017211885719007114, valid 0.0001376768632326275
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00016121375665534288
Batch  11  loss:  0.00014580870629288256
Batch  21  loss:  0.00015280963270924985
Batch  31  loss:  0.00012654611782636493
Batch  41  loss:  0.00016172479081433266
Batch  51  loss:  0.00023940872051753104
Batch  61  loss:  0.00021347569418139756
Batch  71  loss:  0.00015804884606041014
Batch  81  loss:  0.00012995950237382203
Batch  91  loss:  0.0001576193026266992
Batch  101  loss:  0.00018659935449250042
Batch  111  loss:  0.00016659923130646348
Batch  121  loss:  0.00016017627785913646
Batch  131  loss:  0.00017503317212685943
Batch  141  loss:  0.0001840845070546493
Batch  151  loss:  0.0001781143801053986
Batch  161  loss:  0.000185558179509826
Batch  171  loss:  0.0002180007577408105
Batch  181  loss:  0.00016012988635338843
Batch  191  loss:  0.00015643920050933957
Validation on real data: 
LOSS supervised-train 0.00017128925836004783, valid 0.00016186351422220469
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00019688872271217406
Batch  11  loss:  0.0001268311752937734
Batch  21  loss:  0.00012306052667554468
Batch  31  loss:  0.00016003729251679033
Batch  41  loss:  0.0002189098740927875
Batch  51  loss:  0.00028982682852074504
Batch  61  loss:  0.00020319719624239951
Batch  71  loss:  0.00015122428885661066
Batch  81  loss:  0.0001809349050745368
Batch  91  loss:  0.00017382831720169634
Batch  101  loss:  0.00016797443095128983
Batch  111  loss:  0.00014865184493828565
Batch  121  loss:  0.0001313471730099991
Batch  131  loss:  0.00021674679010175169
Batch  141  loss:  0.00019998775678686798
Batch  151  loss:  0.00016225563012994826
Batch  161  loss:  0.00015294803597498685
Batch  171  loss:  0.00021017069229856133
Batch  181  loss:  0.00012240941578056663
Batch  191  loss:  0.00013772028614766896
Validation on real data: 
LOSS supervised-train 0.00017104730344726703, valid 0.00015818155952729285
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0001306303165620193
Batch  11  loss:  0.0001699886634014547
Batch  21  loss:  0.00014389200077857822
Batch  31  loss:  0.00014775512681808323
Batch  41  loss:  0.00014232697139959782
Batch  51  loss:  0.00022849933884572238
Batch  61  loss:  0.00018368195742368698
Batch  71  loss:  0.00014533278590533882
Batch  81  loss:  0.00010690452472772449
Batch  91  loss:  0.00011991500650765374
Batch  101  loss:  0.00013096454495098442
Batch  111  loss:  0.00015866462490521371
Batch  121  loss:  0.00012753829651046544
Batch  131  loss:  0.00020958493405487388
Batch  141  loss:  0.00014543153520207852
Batch  151  loss:  0.0001373428967781365
Batch  161  loss:  0.00013437624147627503
Batch  171  loss:  0.0002712952555157244
Batch  181  loss:  0.00019999261712655425
Batch  191  loss:  0.00012315248022787273
Validation on real data: 
LOSS supervised-train 0.00016324581654771463, valid 0.00014224326878320426
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0001669015473453328
Batch  11  loss:  0.0001572940091136843
Batch  21  loss:  0.00013251876225695014
Batch  31  loss:  0.00017696207214612514
Batch  41  loss:  0.00016071296704467386
Batch  51  loss:  0.0002023122360697016
Batch  61  loss:  0.00019775534747168422
Batch  71  loss:  0.00012844576849602163
Batch  81  loss:  0.00016443320782855153
Batch  91  loss:  0.00014543202996719629
Batch  101  loss:  0.00014808772539254278
Batch  111  loss:  0.00016789291112218052
Batch  121  loss:  0.00011985489982180297
Batch  131  loss:  0.00023941598192323
Batch  141  loss:  0.0001263413141714409
Batch  151  loss:  0.00018159547471441329
Batch  161  loss:  0.00015661063662264496
Batch  171  loss:  0.00018215985619463027
Batch  181  loss:  0.00012334891653154045
Batch  191  loss:  0.00014911968901287764
Validation on real data: 
LOSS supervised-train 0.00016165503231604816, valid 0.0001610047766007483
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00013778533320873976
Batch  11  loss:  0.00014958456449676305
Batch  21  loss:  0.00012465802137739956
Batch  31  loss:  0.00016171859169844538
Batch  41  loss:  0.00015552258992101997
Batch  51  loss:  0.00029854319291189313
Batch  61  loss:  0.00015148097008932382
Batch  71  loss:  0.00017041023238562047
Batch  81  loss:  0.00013462468632496893
Batch  91  loss:  0.00018820655532181263
Batch  101  loss:  0.00013431267871055752
Batch  111  loss:  0.00012316065840423107
Batch  121  loss:  0.0001635514199733734
Batch  131  loss:  0.00023404117382597178
Batch  141  loss:  0.00015727613936178386
Batch  151  loss:  0.0001635751104913652
Batch  161  loss:  0.0001138242005254142
Batch  171  loss:  0.00015646374959032983
Batch  181  loss:  0.00015447504119947553
Batch  191  loss:  0.00011571733193704858
Validation on real data: 
LOSS supervised-train 0.0001623074008966796, valid 0.00011527495371410623
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00015795996296219528
Batch  11  loss:  0.00015059807628858835
Batch  21  loss:  0.00013509778364095837
Batch  31  loss:  0.00013826799113303423
Batch  41  loss:  0.0001547944702906534
Batch  51  loss:  0.00015304279804695398
Batch  61  loss:  0.00017975765513256192
Batch  71  loss:  0.00015553110279142857
Batch  81  loss:  0.00011966250895056874
Batch  91  loss:  0.000156209702254273
Batch  101  loss:  0.00013647232844959944
Batch  111  loss:  0.00012408470502123237
Batch  121  loss:  0.00012049772340105847
Batch  131  loss:  0.00018899702990893275
Batch  141  loss:  0.00014587290934287012
Batch  151  loss:  0.00016254062938969582
Batch  161  loss:  0.00011886855645570904
Batch  171  loss:  0.00019403382611926645
Batch  181  loss:  0.0001563864789204672
Batch  191  loss:  0.00012688897550106049
Validation on real data: 
LOSS supervised-train 0.00015801870940777008, valid 0.000154657696839422
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00017528256285004318
Batch  11  loss:  0.00017055417993105948
Batch  21  loss:  0.00011475934297777712
Batch  31  loss:  0.00017008892609737813
Batch  41  loss:  0.00017271919932682067
Batch  51  loss:  0.0002001058601308614
Batch  61  loss:  0.00021276577899698168
Batch  71  loss:  0.0001550524029880762
Batch  81  loss:  0.00013526171096600592
Batch  91  loss:  0.000178010348463431
Batch  101  loss:  0.00014054581697564572
Batch  111  loss:  9.92872373899445e-05
Batch  121  loss:  0.00016200391110032797
Batch  131  loss:  0.00014165998436510563
Batch  141  loss:  0.00014280893083196133
Batch  151  loss:  0.00013324880274012685
Batch  161  loss:  0.00013151421444490552
Batch  171  loss:  0.00021526566706597805
Batch  181  loss:  0.00015473294479306787
Batch  191  loss:  0.0001721728331176564
Validation on real data: 
LOSS supervised-train 0.00015840792108065216, valid 0.00014312489656731486
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00016264591249637306
Batch  11  loss:  0.00019123870879411697
Batch  21  loss:  0.00015889668429736048
Batch  31  loss:  0.0001579032395966351
Batch  41  loss:  0.00013412124826572835
Batch  51  loss:  0.00020937751105520874
Batch  61  loss:  0.00014974856458138674
Batch  71  loss:  0.0001620705588720739
Batch  81  loss:  0.00016581844829488546
Batch  91  loss:  0.00016416276048403233
Batch  101  loss:  0.000132342946017161
Batch  111  loss:  0.00014753501454833895
Batch  121  loss:  0.00013974633475299925
Batch  131  loss:  0.00024863332509994507
Batch  141  loss:  0.00012291005987208337
Batch  151  loss:  0.000178383503225632
Batch  161  loss:  0.00013792853860650212
Batch  171  loss:  0.00016706172027625144
Batch  181  loss:  0.00011656965943984687
Batch  191  loss:  0.0001241047866642475
Validation on real data: 
LOSS supervised-train 0.0001625768311714637, valid 0.00013502057117875665
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00019663108105305582
Batch  11  loss:  0.00017631202354095876
Batch  21  loss:  0.00015591476403642446
Batch  31  loss:  0.00014537102833855897
Batch  41  loss:  0.00016284985758829862
Batch  51  loss:  0.00023988743487279862
Batch  61  loss:  0.000166583908139728
Batch  71  loss:  0.00016298852278850973
Batch  81  loss:  0.00014257051225285977
Batch  91  loss:  0.0001449942501494661
Batch  101  loss:  0.00014938623644411564
Batch  111  loss:  0.00014855206245556474
Batch  121  loss:  0.00015723073738627136
Batch  131  loss:  0.0001759011938702315
Batch  141  loss:  0.0001335132692474872
Batch  151  loss:  0.00014742364874109626
Batch  161  loss:  0.00010958845814457163
Batch  171  loss:  0.00019688528846018016
Batch  181  loss:  0.00014116778038442135
Batch  191  loss:  9.723476250655949e-05
Validation on real data: 
LOSS supervised-train 0.00015885050706856418, valid 0.00011232183896936476
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0001169169700006023
Batch  11  loss:  0.00011769314005505294
Batch  21  loss:  0.00012632242578547448
Batch  31  loss:  0.0001479444035794586
Batch  41  loss:  0.0002000566164497286
Batch  51  loss:  0.0002751410356722772
Batch  61  loss:  0.00014785850362386554
Batch  71  loss:  0.00017754371219780296
Batch  81  loss:  0.00015683654055465013
Batch  91  loss:  0.00015306669229175895
Batch  101  loss:  0.00013504005619324744
Batch  111  loss:  0.0001377897133352235
Batch  121  loss:  0.00013133678294252604
Batch  131  loss:  0.00013117831258568913
Batch  141  loss:  0.00014219827426131815
Batch  151  loss:  0.00010200530959991738
Batch  161  loss:  0.00011125234595965594
Batch  171  loss:  0.00023551311460323632
Batch  181  loss:  0.00014503400598187
Batch  191  loss:  0.00013481003406923264
Validation on real data: 
LOSS supervised-train 0.00015425023604620947, valid 0.00014035268395673484
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.000157265123561956
Batch  11  loss:  0.00013597289216704667
Batch  21  loss:  0.0001369208621326834
Batch  31  loss:  0.00016695421072654426
Batch  41  loss:  0.00015963285113684833
Batch  51  loss:  0.00021773191110696644
Batch  61  loss:  0.0002020461397478357
Batch  71  loss:  0.0001547215651953593
Batch  81  loss:  0.00013367414067033678
Batch  91  loss:  0.00017273685079999268
Batch  101  loss:  0.0001398596359649673
Batch  111  loss:  0.0001371843827655539
Batch  121  loss:  0.00016003187920432538
Batch  131  loss:  0.0003548029053490609
Batch  141  loss:  0.00014776004536543041
Batch  151  loss:  0.0001463819353375584
Batch  161  loss:  0.00012736217468045652
Batch  171  loss:  0.00019942616927437484
Batch  181  loss:  0.00014132712385617197
Batch  191  loss:  0.00013293146912474185
Validation on real data: 
LOSS supervised-train 0.00015877857127634344, valid 0.0001546840649098158
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00014883026597090065
Batch  11  loss:  0.00014064750575926155
Batch  21  loss:  0.00013151587336324155
Batch  31  loss:  0.00015766426804475486
Batch  41  loss:  0.00015015584358479828
Batch  51  loss:  0.0002625276683829725
Batch  61  loss:  0.00023330292606260628
Batch  71  loss:  0.00017215932894032449
Batch  81  loss:  0.00013649788161274046
Batch  91  loss:  0.00016014983702916652
Batch  101  loss:  0.00014656393614131957
Batch  111  loss:  0.00018180976621806622
Batch  121  loss:  0.00012212629371788353
Batch  131  loss:  0.00019528028497006744
Batch  141  loss:  0.00017072400078177452
Batch  151  loss:  0.00017974080401472747
Batch  161  loss:  0.00012396062084008008
Batch  171  loss:  0.00020656724518630654
Batch  181  loss:  0.00014037414803169668
Batch  191  loss:  0.00011068669846281409
Validation on real data: 
LOSS supervised-train 0.00015621897553501186, valid 0.00011624004400800914
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00017547669995110482
Batch  11  loss:  0.00015263765817508101
Batch  21  loss:  0.0001083441820810549
Batch  31  loss:  0.00011260030441917479
Batch  41  loss:  0.00017222536553163081
Batch  51  loss:  0.00018969617667607963
Batch  61  loss:  0.0001568999869050458
Batch  71  loss:  0.0001252440270036459
Batch  81  loss:  0.00015825594891794026
Batch  91  loss:  0.00016806996427476406
Batch  101  loss:  0.0001251827197847888
Batch  111  loss:  0.00011428853031247854
Batch  121  loss:  9.707482240628451e-05
Batch  131  loss:  0.00016345559561159462
Batch  141  loss:  0.00014256179565563798
Batch  151  loss:  0.00012987713853362948
Batch  161  loss:  0.0001649175537750125
Batch  171  loss:  0.0002014528727158904
Batch  181  loss:  0.00014602653391193599
Batch  191  loss:  0.000142203745781444
Validation on real data: 
LOSS supervised-train 0.00015106341048522155, valid 0.00013156096974853426
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00011641610035439953
Batch  11  loss:  0.00012811670603696257
Batch  21  loss:  0.00016384606715291739
Batch  31  loss:  0.00014449574518948793
Batch  41  loss:  0.00016801517631392926
Batch  51  loss:  0.00022609735606238246
Batch  61  loss:  0.00016972004959825426
Batch  71  loss:  0.00015577518206555396
Batch  81  loss:  0.00015837735554669052
Batch  91  loss:  0.00015605383669026196
Batch  101  loss:  0.0001266569597646594
Batch  111  loss:  0.000152482622070238
Batch  121  loss:  0.00013166283315513283
Batch  131  loss:  0.00015948769578244537
Batch  141  loss:  0.0001272374502150342
Batch  151  loss:  0.0001618391543161124
Batch  161  loss:  0.00011235668353037909
Batch  171  loss:  0.0001786826178431511
Batch  181  loss:  0.00015284729306586087
Batch  191  loss:  0.00012616389722097665
Validation on real data: 
LOSS supervised-train 0.00014963076606363756, valid 0.00015255640028044581
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00013172983017284423
Batch  11  loss:  0.00014373689191415906
Batch  21  loss:  0.00014592075604014099
Batch  31  loss:  0.00014953204663470387
Batch  41  loss:  0.0001628946338314563
Batch  51  loss:  0.00024716462939977646
Batch  61  loss:  0.00019764051830861717
Batch  71  loss:  0.00016650217003189027
Batch  81  loss:  0.00012543673801701516
Batch  91  loss:  0.00013913167640566826
Batch  101  loss:  0.00012428793706931174
Batch  111  loss:  0.00013144938566256315
Batch  121  loss:  0.00014213928079698235
Batch  131  loss:  0.00016805795894470066
Batch  141  loss:  0.0001463594671804458
Batch  151  loss:  0.0001753765536705032
Batch  161  loss:  0.00013800572196487337
Batch  171  loss:  0.00016900799528229982
Batch  181  loss:  0.00013224835856817663
Batch  191  loss:  0.0001163262568297796
Validation on real data: 
LOSS supervised-train 0.00015354626470070798, valid 0.00014064271817915142
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0001322296157013625
Batch  11  loss:  0.00014385436952579767
Batch  21  loss:  0.000128507410408929
Batch  31  loss:  0.0001545181148685515
Batch  41  loss:  0.0001394687278661877
Batch  51  loss:  0.00020547387248370796
Batch  61  loss:  0.0001774116390151903
Batch  71  loss:  0.00015689442807342857
Batch  81  loss:  0.00012493290705606341
Batch  91  loss:  0.00016502519429195672
Batch  101  loss:  0.0001292601227760315
Batch  111  loss:  0.00014802005898673087
Batch  121  loss:  0.00011804595123976469
Batch  131  loss:  0.0001627171441214159
Batch  141  loss:  0.00013114555622451007
Batch  151  loss:  0.00015731983876321465
Batch  161  loss:  0.00012526035425253212
Batch  171  loss:  0.00017280394968111068
Batch  181  loss:  0.00012432153744157404
Batch  191  loss:  0.00014212261885404587
Validation on real data: 
LOSS supervised-train 0.0001552466564317001, valid 0.00016148945724125952
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00017442720127291977
Batch  11  loss:  0.0001236979733221233
Batch  21  loss:  0.00012683556997217238
Batch  31  loss:  0.00014000774535816163
Batch  41  loss:  0.00019284670997876674
Batch  51  loss:  0.0001844014332164079
Batch  61  loss:  0.0001530282897874713
Batch  71  loss:  0.00014801250654272735
Batch  81  loss:  0.0001521394879091531
Batch  91  loss:  0.00014216154522728175
Batch  101  loss:  0.00012527602666523308
Batch  111  loss:  0.00015099163283593953
Batch  121  loss:  0.00014156659017316997
Batch  131  loss:  0.00021832590573467314
Batch  141  loss:  0.0001526682754047215
Batch  151  loss:  0.00020112458150833845
Batch  161  loss:  0.0001388288219459355
Batch  171  loss:  0.00018599511531647295
Batch  181  loss:  0.00015390721091534942
Batch  191  loss:  0.00010512707376619801
Validation on real data: 
LOSS supervised-train 0.00015465922795556252, valid 0.00011904435814358294
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  chair ; Model ID: 1cc6f2ed3d684fa245f213b8994b4a04
--------------------
Training baseline regression model:  2022-03-30 02:05:44.478201
Detector:  point_transformer
Object:  chair
--------------------
device is  cuda
--------------------
Number of trainable parameters:  894622
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.1280113011598587
Batch  11  loss:  0.02940017357468605
Batch  21  loss:  0.011127917096018791
Batch  31  loss:  0.01705757901072502
Batch  41  loss:  0.010222268290817738
Batch  51  loss:  0.004143850412219763
Batch  61  loss:  0.004977797158062458
Batch  71  loss:  0.003566442057490349
Batch  81  loss:  0.003815392730757594
Batch  91  loss:  0.0015666803810745478
Batch  101  loss:  0.003275563707575202
Batch  111  loss:  0.00591699592769146
Batch  121  loss:  0.0019674724899232388
Batch  131  loss:  0.004235845059156418
Batch  141  loss:  0.0033501165453344584
Batch  151  loss:  0.002305732574313879
Batch  161  loss:  0.0028176717460155487
Batch  171  loss:  0.0023549299221485853
Batch  181  loss:  0.004963780287653208
Batch  191  loss:  0.003046315861865878
Validation on real data: 
LOSS supervised-train 0.008618491207598708, valid 0.001004509744234383
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.001917748712003231
Batch  11  loss:  0.0015726116253063083
Batch  21  loss:  0.003125991439446807
Batch  31  loss:  0.0011097807437181473
Batch  41  loss:  0.002370699541643262
Batch  51  loss:  0.0031879672314971685
Batch  61  loss:  0.006924578920006752
Batch  71  loss:  0.0036257104948163033
Batch  81  loss:  0.0036915927194058895
Batch  91  loss:  0.004083516076207161
Batch  101  loss:  0.0008518363465555012
Batch  111  loss:  0.0022949364501982927
Batch  121  loss:  0.0008901217952370644
Batch  131  loss:  0.0019893059507012367
Batch  141  loss:  0.0019692915957421064
Batch  151  loss:  0.0009029654320329428
Batch  161  loss:  0.0011143902083858848
Batch  171  loss:  0.0016302472213283181
Batch  181  loss:  0.00199772953055799
Batch  191  loss:  0.0013355053961277008
Validation on real data: 
LOSS supervised-train 0.002230276282061823, valid 0.0005752693396061659
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0009906325722113252
Batch  11  loss:  0.0013170444872230291
Batch  21  loss:  0.0023615695536136627
Batch  31  loss:  0.0009180111810564995
Batch  41  loss:  0.002149284351617098
Batch  51  loss:  0.0022700936533510685
Batch  61  loss:  0.004508654586970806
Batch  71  loss:  0.0015105282654985785
Batch  81  loss:  0.0033565317280590534
Batch  91  loss:  0.002560526365414262
Batch  101  loss:  0.0004408733802847564
Batch  111  loss:  0.0016238684765994549
Batch  121  loss:  0.0008134759264066815
Batch  131  loss:  0.0015477373963221908
Batch  141  loss:  0.0013057333417236805
Batch  151  loss:  0.0007316230912692845
Batch  161  loss:  0.0007914156885817647
Batch  171  loss:  0.0010741514852270484
Batch  181  loss:  0.001769699971191585
Batch  191  loss:  0.0008588423952460289
Validation on real data: 
LOSS supervised-train 0.0014836273256514686, valid 0.0005172252422198653
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0007491165306419134
Batch  11  loss:  0.0011712772538885474
Batch  21  loss:  0.001720785046927631
Batch  31  loss:  0.0007838580058887601
Batch  41  loss:  0.0017081169644370675
Batch  51  loss:  0.0016003396594896913
Batch  61  loss:  0.002633362077176571
Batch  71  loss:  0.0013329394860193133
Batch  81  loss:  0.0023425761610269547
Batch  91  loss:  0.002183910459280014
Batch  101  loss:  0.00039146130438894033
Batch  111  loss:  0.0013572293100878596
Batch  121  loss:  0.000762075767852366
Batch  131  loss:  0.0011800475185737014
Batch  141  loss:  0.0009320001117885113
Batch  151  loss:  0.0004777537251356989
Batch  161  loss:  0.0007186633883975446
Batch  171  loss:  0.0009441596339456737
Batch  181  loss:  0.0013560348888859153
Batch  191  loss:  0.0006564438226632774
Validation on real data: 
LOSS supervised-train 0.0011475225759204476, valid 0.000566854840144515
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0006005893228575587
Batch  11  loss:  0.0008770465501584113
Batch  21  loss:  0.0013545822585001588
Batch  31  loss:  0.0008292657439596951
Batch  41  loss:  0.0011521541746333241
Batch  51  loss:  0.0010311187943443656
Batch  61  loss:  0.001949969446286559
Batch  71  loss:  0.0006967908120714128
Batch  81  loss:  0.0018624217482283711
Batch  91  loss:  0.0015083008911460638
Batch  101  loss:  0.00031663954723626375
Batch  111  loss:  0.0010744365863502026
Batch  121  loss:  0.000576040125451982
Batch  131  loss:  0.0009894479298964143
Batch  141  loss:  0.0008658863371238112
Batch  151  loss:  0.0004702149308286607
Batch  161  loss:  0.0007141309906728566
Batch  171  loss:  0.0007616993389092386
Batch  181  loss:  0.0010494807502254844
Batch  191  loss:  0.0006347394082695246
Validation on real data: 
LOSS supervised-train 0.0009131672439980321, valid 0.0004476714530028403
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0006174543523229659
Batch  11  loss:  0.0009713586186990142
Batch  21  loss:  0.0010116808116436005
Batch  31  loss:  0.0006211560685187578
Batch  41  loss:  0.000999109703116119
Batch  51  loss:  0.0009652120643295348
Batch  61  loss:  0.0013082042569294572
Batch  71  loss:  0.0007049643318168819
Batch  81  loss:  0.0012745221611112356
Batch  91  loss:  0.0013392781838774681
Batch  101  loss:  0.00037104717921465635
Batch  111  loss:  0.0008066506707109511
Batch  121  loss:  0.0005351415020413697
Batch  131  loss:  0.0008268761448562145
Batch  141  loss:  0.0008703277562744915
Batch  151  loss:  0.00037542657810263336
Batch  161  loss:  0.0006678174831904471
Batch  171  loss:  0.000698101706802845
Batch  181  loss:  0.000838412088342011
Batch  191  loss:  0.0004356843128334731
Validation on real data: 
LOSS supervised-train 0.0007777604050352238, valid 0.0005254596471786499
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0005482110427692533
Batch  11  loss:  0.0008338005864061415
Batch  21  loss:  0.0007870654808357358
Batch  31  loss:  0.0006000050925649703
Batch  41  loss:  0.0008595671970397234
Batch  51  loss:  0.0006359430844895542
Batch  61  loss:  0.000917476718313992
Batch  71  loss:  0.0006033614627085626
Batch  81  loss:  0.0010578883811831474
Batch  91  loss:  0.0011196134146302938
Batch  101  loss:  0.0003255209303461015
Batch  111  loss:  0.0007525659166276455
Batch  121  loss:  0.000416087539633736
Batch  131  loss:  0.0008240456227213144
Batch  141  loss:  0.000724791141692549
Batch  151  loss:  0.00035531280445866287
Batch  161  loss:  0.00047727188211865723
Batch  171  loss:  0.0006160041666589677
Batch  181  loss:  0.0007796939462423325
Batch  191  loss:  0.0004073547606822103
Validation on real data: 
LOSS supervised-train 0.0006623025630688062, valid 0.0003604136873036623
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0005513132782652974
Batch  11  loss:  0.0007170831086114049
Batch  21  loss:  0.0006788677419535816
Batch  31  loss:  0.0006848472403362393
Batch  41  loss:  0.0006378734251484275
Batch  51  loss:  0.00039609751547686756
Batch  61  loss:  0.0006748823798261583
Batch  71  loss:  0.0004360167949926108
Batch  81  loss:  0.000760720344260335
Batch  91  loss:  0.0009604652877897024
Batch  101  loss:  0.0003037376736756414
Batch  111  loss:  0.0007327586063183844
Batch  121  loss:  0.0004992245230823755
Batch  131  loss:  0.0005200039595365524
Batch  141  loss:  0.0006097553414292634
Batch  151  loss:  0.00037155544850975275
Batch  161  loss:  0.0004605100257322192
Batch  171  loss:  0.00040713019552640617
Batch  181  loss:  0.0007405353826470673
Batch  191  loss:  0.0003663125680759549
Validation on real data: 
LOSS supervised-train 0.0005721848147368291, valid 0.00032846227986738086
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.00045127514749765396
Batch  11  loss:  0.0006731927278451622
Batch  21  loss:  0.000551645876839757
Batch  31  loss:  0.0004494539462029934
Batch  41  loss:  0.0005196619895286858
Batch  51  loss:  0.0004486279794946313
Batch  61  loss:  0.0005607984494417906
Batch  71  loss:  0.000358149700332433
Batch  81  loss:  0.0006810446502640843
Batch  91  loss:  0.0006619830965064466
Batch  101  loss:  0.0003417442785575986
Batch  111  loss:  0.0005672440747730434
Batch  121  loss:  0.0003904482291545719
Batch  131  loss:  0.0005941818817518651
Batch  141  loss:  0.0005852319300174713
Batch  151  loss:  0.000269575510174036
Batch  161  loss:  0.00041678271372802556
Batch  171  loss:  0.000609051319770515
Batch  181  loss:  0.0005829225992783904
Batch  191  loss:  0.0003633241285569966
Validation on real data: 
LOSS supervised-train 0.0004972370423638495, valid 0.0004108977736905217
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0004118084325455129
Batch  11  loss:  0.0005467640585266054
Batch  21  loss:  0.000511270307470113
Batch  31  loss:  0.0005094649386592209
Batch  41  loss:  0.0005162408924661577
Batch  51  loss:  0.0003895047993864864
Batch  61  loss:  0.00036827640724368393
Batch  71  loss:  0.00023428365238942206
Batch  81  loss:  0.0007033114088699222
Batch  91  loss:  0.0007989124278537929
Batch  101  loss:  0.00026541281840763986
Batch  111  loss:  0.00045217826846055686
Batch  121  loss:  0.00045823430991731584
Batch  131  loss:  0.0005656969151459634
Batch  141  loss:  0.0006130596157163382
Batch  151  loss:  0.0003043161705136299
Batch  161  loss:  0.00043007705244235694
Batch  171  loss:  0.0005110899801366031
Batch  181  loss:  0.0005075605004094541
Batch  191  loss:  0.00026997661916539073
Validation on real data: 
LOSS supervised-train 0.00046387888309254774, valid 0.00044052035082131624
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0003585889935493469
Batch  11  loss:  0.0006115633295848966
Batch  21  loss:  0.00048094859812408686
Batch  31  loss:  0.0005508203175850213
Batch  41  loss:  0.0005103055154904723
Batch  51  loss:  0.00030783770489506423
Batch  61  loss:  0.0003017176641151309
Batch  71  loss:  0.00023990590125322342
Batch  81  loss:  0.0004524958785623312
Batch  91  loss:  0.0005242652841843665
Batch  101  loss:  0.0002464771969243884
Batch  111  loss:  0.0004567167197819799
Batch  121  loss:  0.00034075722214765847
Batch  131  loss:  0.00040922872722148895
Batch  141  loss:  0.0005182712920941412
Batch  151  loss:  0.00034367345506325364
Batch  161  loss:  0.00036439605173654854
Batch  171  loss:  0.0003937561996281147
Batch  181  loss:  0.0004989585722796619
Batch  191  loss:  0.0002859090454876423
Validation on real data: 
LOSS supervised-train 0.0004126507436740212, valid 0.00033457507379353046
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.00034868516377173364
Batch  11  loss:  0.0004904404631815851
Batch  21  loss:  0.0005074838409200311
Batch  31  loss:  0.000445429963292554
Batch  41  loss:  0.00039537082193419337
Batch  51  loss:  0.0002780744107440114
Batch  61  loss:  0.00027446370222605765
Batch  71  loss:  0.0001732516975607723
Batch  81  loss:  0.00043188451672904193
Batch  91  loss:  0.0004623595450539142
Batch  101  loss:  0.00024426705203950405
Batch  111  loss:  0.00036961911246180534
Batch  121  loss:  0.0003016312839463353
Batch  131  loss:  0.0003956951550208032
Batch  141  loss:  0.0005097599932923913
Batch  151  loss:  0.0002643044281285256
Batch  161  loss:  0.00035265099722892046
Batch  171  loss:  0.00035217611002735794
Batch  181  loss:  0.0005097950925119221
Batch  191  loss:  0.00024297693744301796
Validation on real data: 
LOSS supervised-train 0.00037766217057651376, valid 0.0003659309586510062
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0003857229894492775
Batch  11  loss:  0.0005315088201314211
Batch  21  loss:  0.0003267889260314405
Batch  31  loss:  0.000508488214109093
Batch  41  loss:  0.00032047959393821657
Batch  51  loss:  0.00033334229374304414
Batch  61  loss:  0.00029664806788787246
Batch  71  loss:  0.0002074876392725855
Batch  81  loss:  0.00031554989982396364
Batch  91  loss:  0.00042522780131548643
Batch  101  loss:  0.0002453959605190903
Batch  111  loss:  0.0003172503784298897
Batch  121  loss:  0.0002184768090955913
Batch  131  loss:  0.00027081460575573146
Batch  141  loss:  0.000455255649285391
Batch  151  loss:  0.0002911869087256491
Batch  161  loss:  0.00031370672513730824
Batch  171  loss:  0.0003002060984726995
Batch  181  loss:  0.0005141758592799306
Batch  191  loss:  0.00022539656492881477
Validation on real data: 
LOSS supervised-train 0.0003500724435434677, valid 0.0002919519611168653
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00028524931985884905
Batch  11  loss:  0.0004070567374583334
Batch  21  loss:  0.0003516809665597975
Batch  31  loss:  0.0004522183444350958
Batch  41  loss:  0.0003390451893210411
Batch  51  loss:  0.0002126538020092994
Batch  61  loss:  0.00023800894268788397
Batch  71  loss:  0.00020770280389115214
Batch  81  loss:  0.0002918713726103306
Batch  91  loss:  0.00039630342507734895
Batch  101  loss:  0.0002765461103990674
Batch  111  loss:  0.0003581768833100796
Batch  121  loss:  0.00029940277454443276
Batch  131  loss:  0.0003846866893582046
Batch  141  loss:  0.0004111994057893753
Batch  151  loss:  0.0002749243867583573
Batch  161  loss:  0.0003256399359088391
Batch  171  loss:  0.0003533129929564893
Batch  181  loss:  0.00036371618625707924
Batch  191  loss:  0.00021473760716617107
Validation on real data: 
LOSS supervised-train 0.0003260877558204811, valid 0.00036571259261108935
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0002412218163954094
Batch  11  loss:  0.0004469365521799773
Batch  21  loss:  0.00031715637305751443
Batch  31  loss:  0.00041057958151213825
Batch  41  loss:  0.0002509250771254301
Batch  51  loss:  0.00022619201627094299
Batch  61  loss:  0.000255799648584798
Batch  71  loss:  0.0001754352852003649
Batch  81  loss:  0.00036161739262752235
Batch  91  loss:  0.000374824769096449
Batch  101  loss:  0.0001874391018645838
Batch  111  loss:  0.00033783950493671
Batch  121  loss:  0.0003437472914811224
Batch  131  loss:  0.00031466269865632057
Batch  141  loss:  0.00039655089494772255
Batch  151  loss:  0.00033677960163913667
Batch  161  loss:  0.00028163002571091056
Batch  171  loss:  0.00031415559351444244
Batch  181  loss:  0.00039194992859847844
Batch  191  loss:  0.00021907314658164978
Validation on real data: 
LOSS supervised-train 0.00031658193256589584, valid 0.00028806261252611876
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.00025058528990484774
Batch  11  loss:  0.0003765691362787038
Batch  21  loss:  0.00036189451930113137
Batch  31  loss:  0.00040356485988013446
Batch  41  loss:  0.00032706119236536324
Batch  51  loss:  0.00022866467770654708
Batch  61  loss:  0.0001844955695560202
Batch  71  loss:  0.000136181537527591
Batch  81  loss:  0.00028476648731157184
Batch  91  loss:  0.00033226385130546987
Batch  101  loss:  0.0002303170331288129
Batch  111  loss:  0.00029348640237003565
Batch  121  loss:  0.00029431242728605866
Batch  131  loss:  0.0002716195012908429
Batch  141  loss:  0.0003924285410903394
Batch  151  loss:  0.00025364611065015197
Batch  161  loss:  0.00025087586254812777
Batch  171  loss:  0.00026160103152506053
Batch  181  loss:  0.0003700284578371793
Batch  191  loss:  0.0001891582360258326
Validation on real data: 
LOSS supervised-train 0.0002972168669657549, valid 0.0003830686619039625
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0002952580980490893
Batch  11  loss:  0.00043615244794636965
Batch  21  loss:  0.00028126148390583694
Batch  31  loss:  0.000402760982979089
Batch  41  loss:  0.00023744469217490405
Batch  51  loss:  0.0001676501997280866
Batch  61  loss:  0.00021222486975602806
Batch  71  loss:  0.00016958871856331825
Batch  81  loss:  0.00024113892868626863
Batch  91  loss:  0.0003183289954904467
Batch  101  loss:  0.00020050833700224757
Batch  111  loss:  0.00028385891346260905
Batch  121  loss:  0.0002852544712368399
Batch  131  loss:  0.00021953071700409055
Batch  141  loss:  0.0004273566883057356
Batch  151  loss:  0.00025941504281945527
Batch  161  loss:  0.0002837818756233901
Batch  171  loss:  0.0003254061739426106
Batch  181  loss:  0.0003020605072379112
Batch  191  loss:  0.00017564576410222799
Validation on real data: 
LOSS supervised-train 0.00028700295275484677, valid 0.0003798321122303605
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0002961437276098877
Batch  11  loss:  0.0004203087300993502
Batch  21  loss:  0.0003369213663972914
Batch  31  loss:  0.00037952925777062774
Batch  41  loss:  0.00025578230270184577
Batch  51  loss:  0.00022805681510362774
Batch  61  loss:  0.00016642329865135252
Batch  71  loss:  0.00014030278543941677
Batch  81  loss:  0.00029779094620607793
Batch  91  loss:  0.0003197664045728743
Batch  101  loss:  0.00022238935343921185
Batch  111  loss:  0.00022934026492293924
Batch  121  loss:  0.00025223338161595166
Batch  131  loss:  0.0002968495537061244
Batch  141  loss:  0.00045852974290028214
Batch  151  loss:  0.00026094610802829266
Batch  161  loss:  0.0002645980566740036
Batch  171  loss:  0.00021970334637444466
Batch  181  loss:  0.00024983062758110464
Batch  191  loss:  0.0001750035589793697
Validation on real data: 
LOSS supervised-train 0.0002753808911074884, valid 0.00035233044764027
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.00020771610434167087
Batch  11  loss:  0.00035328345256857574
Batch  21  loss:  0.00023398354824166745
Batch  31  loss:  0.0003721193934325129
Batch  41  loss:  0.00027500183205120265
Batch  51  loss:  0.00018649242701940238
Batch  61  loss:  0.0002062793355435133
Batch  71  loss:  0.00015174092550296336
Batch  81  loss:  0.0002279151522088796
Batch  91  loss:  0.0002642939507495612
Batch  101  loss:  0.0002255259023513645
Batch  111  loss:  0.00028071057749912143
Batch  121  loss:  0.0003422871814109385
Batch  131  loss:  0.00028312692302279174
Batch  141  loss:  0.0003552106791175902
Batch  151  loss:  0.00019451665866654366
Batch  161  loss:  0.0002834522456396371
Batch  171  loss:  0.000282657885691151
Batch  181  loss:  0.00036862530396319926
Batch  191  loss:  0.00017688097432255745
Validation on real data: 
LOSS supervised-train 0.00026718818524386736, valid 0.0002274415601277724
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.00026919724768958986
Batch  11  loss:  0.00036999883013777435
Batch  21  loss:  0.00028557595214806497
Batch  31  loss:  0.0003882082528434694
Batch  41  loss:  0.0002332763688173145
Batch  51  loss:  0.00018130071111954749
Batch  61  loss:  0.00020409096032381058
Batch  71  loss:  0.00015363689453806728
Batch  81  loss:  0.00020346524252090603
Batch  91  loss:  0.0002566611219663173
Batch  101  loss:  0.00022263263235799968
Batch  111  loss:  0.00024083367316052318
Batch  121  loss:  0.00028943843790329993
Batch  131  loss:  0.00018860782438423485
Batch  141  loss:  0.00033453654032200575
Batch  151  loss:  0.0002543725131545216
Batch  161  loss:  0.0002041314437519759
Batch  171  loss:  0.00029555908986367285
Batch  181  loss:  0.00030138177680782974
Batch  191  loss:  0.00016971418517641723
Validation on real data: 
LOSS supervised-train 0.00025267004981287754, valid 0.0002652131952345371
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.00018879494746215641
Batch  11  loss:  0.0003202586667612195
Batch  21  loss:  0.00029572087805718184
Batch  31  loss:  0.00032891659066081047
Batch  41  loss:  0.0002816982159856707
Batch  51  loss:  0.00020954967476427555
Batch  61  loss:  0.00018933489627670497
Batch  71  loss:  0.0001248301414307207
Batch  81  loss:  0.00021094355906825513
Batch  91  loss:  0.0002767202677205205
Batch  101  loss:  0.00022427126532420516
Batch  111  loss:  0.0002525718882679939
Batch  121  loss:  0.00028508080868050456
Batch  131  loss:  0.00017588073387742043
Batch  141  loss:  0.00032304375781677663
Batch  151  loss:  0.000222137663513422
Batch  161  loss:  0.00023086063447408378
Batch  171  loss:  0.00024570460664108396
Batch  181  loss:  0.0003716978244483471
Batch  191  loss:  0.0001423369685653597
Validation on real data: 
LOSS supervised-train 0.00024291142297443004, valid 0.000296645681373775
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.00019608937145676464
Batch  11  loss:  0.0003176842292305082
Batch  21  loss:  0.00023741264885757118
Batch  31  loss:  0.00035182025749236345
Batch  41  loss:  0.00022915691079106182
Batch  51  loss:  0.00021911268413532525
Batch  61  loss:  0.0001760501036187634
Batch  71  loss:  0.0001246266474481672
Batch  81  loss:  0.00020996197417844087
Batch  91  loss:  0.0002510111662559211
Batch  101  loss:  0.0002275709412060678
Batch  111  loss:  0.00018053039093501866
Batch  121  loss:  0.00023687788052484393
Batch  131  loss:  0.0002192491083405912
Batch  141  loss:  0.00023430061992257833
Batch  151  loss:  0.00017753589781932533
Batch  161  loss:  0.00027348200092092156
Batch  171  loss:  0.0002362294471822679
Batch  181  loss:  0.00027885279268957675
Batch  191  loss:  0.00016541939112357795
Validation on real data: 
LOSS supervised-train 0.0002377455150417518, valid 0.00029250510851852596
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00017196094268001616
Batch  11  loss:  0.0002871286415029317
Batch  21  loss:  0.00019375263946130872
Batch  31  loss:  0.00032296450808644295
Batch  41  loss:  0.0001897357578855008
Batch  51  loss:  0.00022155040642246604
Batch  61  loss:  0.00015998967865016311
Batch  71  loss:  0.00012037352280458435
Batch  81  loss:  0.0001846217201091349
Batch  91  loss:  0.00020640103321056813
Batch  101  loss:  0.00017522950656712055
Batch  111  loss:  0.00026316879666410387
Batch  121  loss:  0.0002793185703922063
Batch  131  loss:  0.00019941320351790637
Batch  141  loss:  0.00026623625308275223
Batch  151  loss:  0.00017927118460647762
Batch  161  loss:  0.00017062225379049778
Batch  171  loss:  0.0001881463103927672
Batch  181  loss:  0.00028478671447373927
Batch  191  loss:  0.00018242486112285405
Validation on real data: 
LOSS supervised-train 0.00022843708600703394, valid 0.0002532445651013404
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0001764529151841998
Batch  11  loss:  0.00026386542594991624
Batch  21  loss:  0.00022149797587189823
Batch  31  loss:  0.0003585706581361592
Batch  41  loss:  0.00020774384029209614
Batch  51  loss:  0.0001999960222747177
Batch  61  loss:  0.00019395833078306168
Batch  71  loss:  0.0001614265056559816
Batch  81  loss:  0.00017085127183236182
Batch  91  loss:  0.00025359485880471766
Batch  101  loss:  0.00018686801195144653
Batch  111  loss:  0.00019555134349502623
Batch  121  loss:  0.00026869605062529445
Batch  131  loss:  0.00020643003517761827
Batch  141  loss:  0.00031726076849736273
Batch  151  loss:  0.00017207424389198422
Batch  161  loss:  0.00018044948228634894
Batch  171  loss:  0.00015434433589689434
Batch  181  loss:  0.000307087495457381
Batch  191  loss:  0.000198349793208763
Validation on real data: 
LOSS supervised-train 0.00022231182621908373, valid 0.00033875746885314584
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.00017635055701248348
Batch  11  loss:  0.00026986506418325007
Batch  21  loss:  0.0001974908955162391
Batch  31  loss:  0.0002416551869828254
Batch  41  loss:  0.00020042482356075197
Batch  51  loss:  0.00016362259339075536
Batch  61  loss:  0.00018621550407260656
Batch  71  loss:  0.00019308376067783684
Batch  81  loss:  0.00017261049652006477
Batch  91  loss:  0.00020692142425104976
Batch  101  loss:  0.00017558367107994854
Batch  111  loss:  0.0002171276428271085
Batch  121  loss:  0.0002161976444767788
Batch  131  loss:  0.000190613282029517
Batch  141  loss:  0.00023803996737115085
Batch  151  loss:  0.00013856870646122843
Batch  161  loss:  0.0001547320862300694
Batch  171  loss:  0.0002209717349614948
Batch  181  loss:  0.0002079751284327358
Batch  191  loss:  0.00016052709543146193
Validation on real data: 
LOSS supervised-train 0.00021420445955300238, valid 0.00024182352353818715
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.00014498618838842958
Batch  11  loss:  0.000293937511742115
Batch  21  loss:  0.00019061293278355151
Batch  31  loss:  0.0002965967869386077
Batch  41  loss:  0.0001686864998191595
Batch  51  loss:  0.00016505803796462715
Batch  61  loss:  0.00015890340728219599
Batch  71  loss:  0.00013963539095129818
Batch  81  loss:  0.00020343226788099855
Batch  91  loss:  0.00023347353271674365
Batch  101  loss:  0.00016433057317044586
Batch  111  loss:  0.00019952292495872825
Batch  121  loss:  0.00017017038771882653
Batch  131  loss:  0.0002014440397033468
Batch  141  loss:  0.00022845996136311442
Batch  151  loss:  0.0001313717948505655
Batch  161  loss:  0.00015684346726629883
Batch  171  loss:  0.0002083367871819064
Batch  181  loss:  0.0002127238258253783
Batch  191  loss:  0.00015412904031109065
Validation on real data: 
LOSS supervised-train 0.00020886981445073616, valid 0.00019770435756072402
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00019098880875390023
Batch  11  loss:  0.00033749936847016215
Batch  21  loss:  0.00019365764455869794
Batch  31  loss:  0.0002494405198376626
Batch  41  loss:  0.00021012280194554478
Batch  51  loss:  0.0001374273851979524
Batch  61  loss:  0.00014072640624362975
Batch  71  loss:  0.00013158786168787628
Batch  81  loss:  0.00015080298180691898
Batch  91  loss:  0.00019807761418633163
Batch  101  loss:  0.00021538611326832324
Batch  111  loss:  0.0002667285152710974
Batch  121  loss:  0.00023422043886967003
Batch  131  loss:  0.00020140944980084896
Batch  141  loss:  0.00022660229296889156
Batch  151  loss:  0.00018350823665969074
Batch  161  loss:  0.00016972403682302684
Batch  171  loss:  0.00025309526245109737
Batch  181  loss:  0.00023354931909125298
Batch  191  loss:  0.00017099846445489675
Validation on real data: 
LOSS supervised-train 0.00020663553772465093, valid 0.00022557945339940488
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.00016076549945864826
Batch  11  loss:  0.00021185354853514582
Batch  21  loss:  0.0002707543026190251
Batch  31  loss:  0.00025513808941468596
Batch  41  loss:  0.0001833169808378443
Batch  51  loss:  0.0001724801550153643
Batch  61  loss:  0.00015923156752251089
Batch  71  loss:  0.00011455488856881857
Batch  81  loss:  0.00018909634673036635
Batch  91  loss:  0.0002650608366820961
Batch  101  loss:  0.00016403979680035263
Batch  111  loss:  0.00019397011783439666
Batch  121  loss:  0.00021334082703106105
Batch  131  loss:  0.000164812445291318
Batch  141  loss:  0.0002287143433932215
Batch  151  loss:  0.00018564621859695762
Batch  161  loss:  0.00016885570948943496
Batch  171  loss:  0.00017699347517918795
Batch  181  loss:  0.00021450247731991112
Batch  191  loss:  0.0001406142400810495
Validation on real data: 
LOSS supervised-train 0.0002022055949055357, valid 0.0002324492234038189
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00014410527364816517
Batch  11  loss:  0.00026377287576906383
Batch  21  loss:  0.00018752545292954892
Batch  31  loss:  0.0002864851849153638
Batch  41  loss:  0.00022862505284138024
Batch  51  loss:  0.00012184652587166056
Batch  61  loss:  0.00018353255291003734
Batch  71  loss:  0.00010244044096907601
Batch  81  loss:  0.00020140776177868247
Batch  91  loss:  0.0001719956926535815
Batch  101  loss:  0.00018138342420570552
Batch  111  loss:  0.00023441552184522152
Batch  121  loss:  0.0002201024763053283
Batch  131  loss:  0.00019629040616564453
Batch  141  loss:  0.00024182273773476481
Batch  151  loss:  0.00017912875046022236
Batch  161  loss:  0.00015247352712322026
Batch  171  loss:  0.00018907304911408573
Batch  181  loss:  0.00022183518740348518
Batch  191  loss:  0.00013277701509650797
Validation on real data: 
LOSS supervised-train 0.0001993586152457283, valid 0.0002900265681091696
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0001442720094928518
Batch  11  loss:  0.0002456509682815522
Batch  21  loss:  0.00026191072538495064
Batch  31  loss:  0.00022098880435805768
Batch  41  loss:  0.00017117623065132648
Batch  51  loss:  0.00013920250057708472
Batch  61  loss:  0.00015732317115180194
Batch  71  loss:  9.867612970992923e-05
Batch  81  loss:  0.00017802689399104565
Batch  91  loss:  0.00020672321261372417
Batch  101  loss:  0.00017924621351994574
Batch  111  loss:  0.000190203296369873
Batch  121  loss:  0.0001762379106367007
Batch  131  loss:  0.00017843734531197697
Batch  141  loss:  0.00019339172285981476
Batch  151  loss:  0.00018176526646129787
Batch  161  loss:  0.00019169229199178517
Batch  171  loss:  0.00019089924171566963
Batch  181  loss:  0.00018556708528194577
Batch  191  loss:  0.00014864446711726487
Validation on real data: 
LOSS supervised-train 0.00019645010095700855, valid 0.0002111905487254262
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00017892192408908159
Batch  11  loss:  0.0002638139994814992
Batch  21  loss:  0.00021049859060440212
Batch  31  loss:  0.00030165971838869154
Batch  41  loss:  0.00018564306083135307
Batch  51  loss:  0.00011754229490179569
Batch  61  loss:  0.00012037833948852494
Batch  71  loss:  8.667816291563213e-05
Batch  81  loss:  0.00014315845328383148
Batch  91  loss:  0.00019790412625297904
Batch  101  loss:  0.00014270920655690134
Batch  111  loss:  0.00017208332428708673
Batch  121  loss:  0.00018081664165947586
Batch  131  loss:  0.00016078868065960705
Batch  141  loss:  0.00019331579096615314
Batch  151  loss:  0.000141619035275653
Batch  161  loss:  0.0002484972064848989
Batch  171  loss:  0.0002031814947258681
Batch  181  loss:  0.00022708135657012463
Batch  191  loss:  0.0001114501865231432
Validation on real data: 
LOSS supervised-train 0.00018372580772847867, valid 0.00021454466332215816
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.00019794418767560273
Batch  11  loss:  0.00024241070786956698
Batch  21  loss:  0.0002033649361692369
Batch  31  loss:  0.00026938438531942666
Batch  41  loss:  0.0001864115911303088
Batch  51  loss:  0.00015932306996546686
Batch  61  loss:  0.0001299582509091124
Batch  71  loss:  0.0001271103828912601
Batch  81  loss:  0.00017986862803809345
Batch  91  loss:  0.0002000265958486125
Batch  101  loss:  0.0001598299277247861
Batch  111  loss:  0.00016253541980404407
Batch  121  loss:  0.00019027918460778892
Batch  131  loss:  0.00019723136210814118
Batch  141  loss:  0.00022676055959891528
Batch  151  loss:  0.00014783749065827578
Batch  161  loss:  0.00020892039174214005
Batch  171  loss:  0.00017526121519040316
Batch  181  loss:  0.0001886593090603128
Batch  191  loss:  0.0001349079393548891
Validation on real data: 
LOSS supervised-train 0.0001816364549449645, valid 0.00019521580543369055
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00014469675079453737
Batch  11  loss:  0.00030688653350807726
Batch  21  loss:  0.00019364611944183707
Batch  31  loss:  0.00027672736905515194
Batch  41  loss:  0.0001768231886671856
Batch  51  loss:  0.00011005486885551363
Batch  61  loss:  0.00015172307030297816
Batch  71  loss:  0.0001078294008038938
Batch  81  loss:  0.00015993246051948518
Batch  91  loss:  0.00014179803838487715
Batch  101  loss:  0.0001645578013267368
Batch  111  loss:  0.00018625549273565412
Batch  121  loss:  0.0002046357112703845
Batch  131  loss:  0.0001722501328913495
Batch  141  loss:  0.00013232749188318849
Batch  151  loss:  0.00013252215285319835
Batch  161  loss:  0.0001579505915287882
Batch  171  loss:  0.00019967708794865757
Batch  181  loss:  0.0003061995084863156
Batch  191  loss:  0.00014593912055715919
Validation on real data: 
LOSS supervised-train 0.0001798236827380606, valid 0.0002293093566549942
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00014850127627141774
Batch  11  loss:  0.00021385186119005084
Batch  21  loss:  0.0001803344493964687
Batch  31  loss:  0.0002405340928817168
Batch  41  loss:  0.00017906878201756626
Batch  51  loss:  0.00015141235780902207
Batch  61  loss:  0.00015956051356624812
Batch  71  loss:  0.0001067033372237347
Batch  81  loss:  0.00017552183999214321
Batch  91  loss:  0.00018502195598557591
Batch  101  loss:  0.00012982278713025153
Batch  111  loss:  0.0002100064157275483
Batch  121  loss:  0.00014853259199298918
Batch  131  loss:  0.00013543809473048896
Batch  141  loss:  0.00015538670413661748
Batch  151  loss:  0.00014536564412992448
Batch  161  loss:  0.00012615512241609395
Batch  171  loss:  0.00018067735072690994
Batch  181  loss:  0.00020349099941086024
Batch  191  loss:  0.00010715713142417371
Validation on real data: 
LOSS supervised-train 0.000168211837662966, valid 0.00019846651412080973
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0001403706701239571
Batch  11  loss:  0.00024603470228612423
Batch  21  loss:  0.00014351753634400666
Batch  31  loss:  0.00022342319425661117
Batch  41  loss:  0.00018488043860998005
Batch  51  loss:  0.00015180232003331184
Batch  61  loss:  0.00013391219545155764
Batch  71  loss:  0.00010222093987977132
Batch  81  loss:  0.00014397753693629056
Batch  91  loss:  0.00016705119924154133
Batch  101  loss:  0.00014171555812936276
Batch  111  loss:  0.00017865847621578723
Batch  121  loss:  0.00018528020882513374
Batch  131  loss:  0.00012038303248118609
Batch  141  loss:  0.00018116447608917952
Batch  151  loss:  0.00012631301069632173
Batch  161  loss:  0.00018502194143366069
Batch  171  loss:  0.00017159302660729736
Batch  181  loss:  0.00016270976630039513
Batch  191  loss:  0.00014461117098107934
Validation on real data: 
LOSS supervised-train 0.0001702757756356732, valid 0.00017583338194526732
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00013042417413089424
Batch  11  loss:  0.0002282828645547852
Batch  21  loss:  0.0001944717951118946
Batch  31  loss:  0.0002449865860398859
Batch  41  loss:  0.00013164991105441004
Batch  51  loss:  0.00013194888015277684
Batch  61  loss:  0.00013800208398606628
Batch  71  loss:  0.00011352008004905656
Batch  81  loss:  0.00012872180377598852
Batch  91  loss:  0.00013989067520014942
Batch  101  loss:  0.00014620882575400174
Batch  111  loss:  0.00017657918215263635
Batch  121  loss:  0.0002214604028267786
Batch  131  loss:  0.00014887323777657002
Batch  141  loss:  0.0002521190035622567
Batch  151  loss:  0.00016992117161862552
Batch  161  loss:  0.00017075520008802414
Batch  171  loss:  0.0001633785286685452
Batch  181  loss:  0.00017549483163747936
Batch  191  loss:  0.00014977858518250287
Validation on real data: 
LOSS supervised-train 0.00017131224609329365, valid 0.0001823931816034019
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.00011297832679701969
Batch  11  loss:  0.00018099602311849594
Batch  21  loss:  0.00014599505811929703
Batch  31  loss:  0.00022330017236527056
Batch  41  loss:  0.00016785958723630756
Batch  51  loss:  0.00011794914462370798
Batch  61  loss:  0.00015321079990826547
Batch  71  loss:  0.00012291963503230363
Batch  81  loss:  0.00016001671610865742
Batch  91  loss:  0.00012449445785023272
Batch  101  loss:  0.000131017790408805
Batch  111  loss:  0.0001568664883961901
Batch  121  loss:  0.00018423936853650957
Batch  131  loss:  0.00019974388123955578
Batch  141  loss:  0.00019035901641473174
Batch  151  loss:  0.0001249891793122515
Batch  161  loss:  0.00014404708053916693
Batch  171  loss:  0.0001730401418171823
Batch  181  loss:  0.0002080232952721417
Batch  191  loss:  0.00011694346176227555
Validation on real data: 
LOSS supervised-train 0.0001635592726233881, valid 0.00018795134383253753
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00012884338502772152
Batch  11  loss:  0.00020177203987259418
Batch  21  loss:  0.000162591299158521
Batch  31  loss:  0.0002853843616321683
Batch  41  loss:  0.00018694337632041425
Batch  51  loss:  0.00011618233838817105
Batch  61  loss:  0.00016557663911953568
Batch  71  loss:  0.0001186002918984741
Batch  81  loss:  0.00014606959302909672
Batch  91  loss:  0.00017805889365263283
Batch  101  loss:  0.00014919073146302253
Batch  111  loss:  0.00011972587526543066
Batch  121  loss:  0.00016329903155565262
Batch  131  loss:  0.00016054279694799334
Batch  141  loss:  0.00015426376194227487
Batch  151  loss:  0.00014416893827728927
Batch  161  loss:  0.0001470288698328659
Batch  171  loss:  0.00014384677342604846
Batch  181  loss:  0.00015820916451048106
Batch  191  loss:  0.00010288957128068432
Validation on real data: 
LOSS supervised-train 0.0001610520929170889, valid 0.00014394655590876937
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0001312804379267618
Batch  11  loss:  0.00022518668265547603
Batch  21  loss:  0.00016829160449560732
Batch  31  loss:  0.0002115073730237782
Batch  41  loss:  0.00015449573402293026
Batch  51  loss:  0.00011764734517782927
Batch  61  loss:  0.00014444018597714603
Batch  71  loss:  0.00011894554336322471
Batch  81  loss:  0.00013478998153004795
Batch  91  loss:  0.00015704114048276097
Batch  101  loss:  0.00015381867706310004
Batch  111  loss:  0.000154332781676203
Batch  121  loss:  0.00015985153731890023
Batch  131  loss:  0.00013560651859734207
Batch  141  loss:  0.00017265725182369351
Batch  151  loss:  0.00015143444761633873
Batch  161  loss:  0.00014892450417391956
Batch  171  loss:  0.00015714527398813516
Batch  181  loss:  0.00014277262380346656
Batch  191  loss:  0.0001336074637947604
Validation on real data: 
LOSS supervised-train 0.00015964948808687042, valid 0.00022636682842858136
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00011066701699746773
Batch  11  loss:  0.0002048298338195309
Batch  21  loss:  0.00013892467541154474
Batch  31  loss:  0.00023167318431660533
Batch  41  loss:  0.00016523314116057009
Batch  51  loss:  9.762633999343961e-05
Batch  61  loss:  0.0001261600700672716
Batch  71  loss:  9.964955097530037e-05
Batch  81  loss:  0.0001309176441282034
Batch  91  loss:  0.0001597769878571853
Batch  101  loss:  0.00012329894525464624
Batch  111  loss:  0.00016827680519782007
Batch  121  loss:  0.00016414003039244562
Batch  131  loss:  0.0001861334894783795
Batch  141  loss:  0.00021615733567159623
Batch  151  loss:  0.00014433046453632414
Batch  161  loss:  0.0001406644150847569
Batch  171  loss:  0.00014366459799930453
Batch  181  loss:  0.00017382524674758315
Batch  191  loss:  9.72001435002312e-05
Validation on real data: 
LOSS supervised-train 0.00015705562123912386, valid 0.00018729318981058896
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.00012961564061697572
Batch  11  loss:  0.00018249289132654667
Batch  21  loss:  0.00014110298070590943
Batch  31  loss:  0.0002073596988338977
Batch  41  loss:  0.00013888461398892105
Batch  51  loss:  0.00010596654465189204
Batch  61  loss:  0.00010642880079103634
Batch  71  loss:  0.00011977295798715204
Batch  81  loss:  0.0001518378994660452
Batch  91  loss:  0.0001796776195988059
Batch  101  loss:  0.00010547843703534454
Batch  111  loss:  0.0001690742647042498
Batch  121  loss:  0.00015091244131326675
Batch  131  loss:  0.00013375810522120446
Batch  141  loss:  0.00015472450468223542
Batch  151  loss:  0.00012022870942018926
Batch  161  loss:  0.00013868199312128127
Batch  171  loss:  0.00014148559421300888
Batch  181  loss:  0.00017752181156538427
Batch  191  loss:  9.905917249852791e-05
Validation on real data: 
LOSS supervised-train 0.0001542731804511277, valid 0.00015187055396381766
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00012541032629087567
Batch  11  loss:  0.00021843600552529097
Batch  21  loss:  0.00010055754682980478
Batch  31  loss:  0.00019627790607046336
Batch  41  loss:  0.0001715499529382214
Batch  51  loss:  0.00014317182649392635
Batch  61  loss:  0.00014541714335791767
Batch  71  loss:  0.00011425025149947032
Batch  81  loss:  0.00011403350799810141
Batch  91  loss:  0.0001485873945057392
Batch  101  loss:  0.0001372774422634393
Batch  111  loss:  0.00015533117402810603
Batch  121  loss:  0.00022259072284214199
Batch  131  loss:  0.00013273222430143505
Batch  141  loss:  0.00016277009854093194
Batch  151  loss:  0.00012992817210033536
Batch  161  loss:  0.00011097933747805655
Batch  171  loss:  0.0001552369212731719
Batch  181  loss:  0.0001509587309556082
Batch  191  loss:  0.0001519222860224545
Validation on real data: 
LOSS supervised-train 0.00015290792660380248, valid 0.00015181303024291992
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00014448368165176362
Batch  11  loss:  0.00014739841572009027
Batch  21  loss:  0.00016605343262199312
Batch  31  loss:  0.00019873141718562692
Batch  41  loss:  0.00011891795293195173
Batch  51  loss:  0.0001063738382072188
Batch  61  loss:  0.00012671551667153835
Batch  71  loss:  9.4655915745534e-05
Batch  81  loss:  0.00016092770965769887
Batch  91  loss:  0.0001277686096727848
Batch  101  loss:  0.00011553096555871889
Batch  111  loss:  0.000180328032001853
Batch  121  loss:  0.00021065033797640353
Batch  131  loss:  0.00015011598588898778
Batch  141  loss:  0.0001367737422697246
Batch  151  loss:  0.0001311046362388879
Batch  161  loss:  0.0001348588557448238
Batch  171  loss:  0.00014501201803795993
Batch  181  loss:  0.0001810978283174336
Batch  191  loss:  0.00011005460692103952
Validation on real data: 
LOSS supervised-train 0.00014714864362758818, valid 0.0001606561418157071
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00010978781210724264
Batch  11  loss:  0.00019535010505933315
Batch  21  loss:  0.00017439483781345189
Batch  31  loss:  0.0001738144928822294
Batch  41  loss:  0.0001384298811899498
Batch  51  loss:  9.98473260551691e-05
Batch  61  loss:  0.00013203901471570134
Batch  71  loss:  8.631737000541762e-05
Batch  81  loss:  0.00012263242388144135
Batch  91  loss:  0.00015909313515294343
Batch  101  loss:  0.00012459854769986123
Batch  111  loss:  0.00018370719044469297
Batch  121  loss:  0.0001708755735307932
Batch  131  loss:  0.00012821088603232056
Batch  141  loss:  0.00017277053848374635
Batch  151  loss:  0.00014662122703157365
Batch  161  loss:  0.00014714899589307606
Batch  171  loss:  0.00014965920126996934
Batch  181  loss:  0.0001959318615263328
Batch  191  loss:  0.00010714537347666919
Validation on real data: 
LOSS supervised-train 0.0001472662806190783, valid 0.0001699258864391595
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00012432486983016133
Batch  11  loss:  0.00012393950601108372
Batch  21  loss:  0.00013399224553722888
Batch  31  loss:  0.00017774513980839401
Batch  41  loss:  0.00012906255142297596
Batch  51  loss:  0.00010630748874973506
Batch  61  loss:  0.00012944256013724953
Batch  71  loss:  0.00010303738963557407
Batch  81  loss:  0.00012131278344895691
Batch  91  loss:  0.0001368396624457091
Batch  101  loss:  0.00012407616304699332
Batch  111  loss:  0.00013979963841848075
Batch  121  loss:  0.0001837453164625913
Batch  131  loss:  0.00015315919881686568
Batch  141  loss:  0.00016609459999017417
Batch  151  loss:  0.00013369835505727679
Batch  161  loss:  0.000100469529570546
Batch  171  loss:  0.00015181480557657778
Batch  181  loss:  0.00012387734022922814
Batch  191  loss:  0.00010630748874973506
Validation on real data: 
LOSS supervised-train 0.00014654862687166315, valid 0.00016935078019741923
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00011074233771068975
Batch  11  loss:  0.00018367692246101797
Batch  21  loss:  0.00016472516290377825
Batch  31  loss:  0.00017550251504871994
Batch  41  loss:  0.00012838018301408738
Batch  51  loss:  9.112017141887918e-05
Batch  61  loss:  0.00012939589214511216
Batch  71  loss:  9.373675857204944e-05
Batch  81  loss:  0.00013083757949061692
Batch  91  loss:  0.00015969653031788766
Batch  101  loss:  0.0001031524661812
Batch  111  loss:  0.0002128344785887748
Batch  121  loss:  0.00016535056056454778
Batch  131  loss:  0.0001542753743706271
Batch  141  loss:  0.000170966683072038
Batch  151  loss:  0.00011073552013840526
Batch  161  loss:  0.0001407398231094703
Batch  171  loss:  0.00017833484162110835
Batch  181  loss:  0.00017925072461366653
Batch  191  loss:  0.00013224077702034265
Validation on real data: 
LOSS supervised-train 0.0001461975271377014, valid 0.00017720155301503837
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0001280297146877274
Batch  11  loss:  0.0001880186318885535
Batch  21  loss:  0.00020270422101020813
Batch  31  loss:  0.00014719083264935762
Batch  41  loss:  0.000100300858321134
Batch  51  loss:  0.00011306619853712618
Batch  61  loss:  0.00010744320024969056
Batch  71  loss:  7.724130409769714e-05
Batch  81  loss:  0.00014961273700464517
Batch  91  loss:  0.00016620205133222044
Batch  101  loss:  0.00012760484241880476
Batch  111  loss:  0.00014897518849465996
Batch  121  loss:  0.00016523912199772894
Batch  131  loss:  0.00014378255582414567
Batch  141  loss:  0.00015719841758254915
Batch  151  loss:  0.0001248182525159791
Batch  161  loss:  0.00014686919166706502
Batch  171  loss:  0.00016956320905592293
Batch  181  loss:  0.00015772762708365917
Batch  191  loss:  9.599529585102573e-05
Validation on real data: 
LOSS supervised-train 0.00014184520845446968, valid 0.00021886188187636435
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00010634319914970547
Batch  11  loss:  0.00015371305926237255
Batch  21  loss:  0.00012670013529714197
Batch  31  loss:  0.00020082181436009705
Batch  41  loss:  0.00013820665481034666
Batch  51  loss:  0.0001127440482378006
Batch  61  loss:  0.0001408415992045775
Batch  71  loss:  0.00010788794315885752
Batch  81  loss:  0.00011601472942857072
Batch  91  loss:  0.00010772742825793102
Batch  101  loss:  0.00015719010843895376
Batch  111  loss:  0.00014533103967551142
Batch  121  loss:  0.00012435945973265916
Batch  131  loss:  0.00013930602290201932
Batch  141  loss:  0.0001569779124110937
Batch  151  loss:  0.0001419132313458249
Batch  161  loss:  0.00012899498688057065
Batch  171  loss:  0.00013265882444102317
Batch  181  loss:  0.0001500807993579656
Batch  191  loss:  0.00010918460611719638
Validation on real data: 
LOSS supervised-train 0.00013713072316022589, valid 0.00014270645624492317
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.00010482560901436955
Batch  11  loss:  0.00015930100926198065
Batch  21  loss:  0.00011997589899692684
Batch  31  loss:  0.00018199450278189033
Batch  41  loss:  0.00016591612074989825
Batch  51  loss:  0.00012348810560069978
Batch  61  loss:  0.00015064107719808817
Batch  71  loss:  0.00010110772564075887
Batch  81  loss:  0.00014035790809430182
Batch  91  loss:  0.00012464984320104122
Batch  101  loss:  0.00010346934868721291
Batch  111  loss:  0.00015160234761424363
Batch  121  loss:  0.00013717338151764125
Batch  131  loss:  0.00014619206194765866
Batch  141  loss:  0.0001597847876837477
Batch  151  loss:  0.00012889839126728475
Batch  161  loss:  0.00013031777052674443
Batch  171  loss:  0.00015400111442431808
Batch  181  loss:  0.00017181519069708884
Batch  191  loss:  0.00011731883569154888
Validation on real data: 
LOSS supervised-train 0.00013841226154909236, valid 0.00012760664685629308
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00012227665865793824
Batch  11  loss:  0.00013076819595880806
Batch  21  loss:  0.00011470602476038039
Batch  31  loss:  0.00016309492639265954
Batch  41  loss:  0.0001542607496958226
Batch  51  loss:  9.507101640338078e-05
Batch  61  loss:  0.0001259217387996614
Batch  71  loss:  9.634989692131057e-05
Batch  81  loss:  0.0001585840800544247
Batch  91  loss:  0.000136635746457614
Batch  101  loss:  0.00011631510278675705
Batch  111  loss:  0.00018530941451899707
Batch  121  loss:  0.00018008668848779052
Batch  131  loss:  0.00015407723549287766
Batch  141  loss:  0.0001682010479271412
Batch  151  loss:  0.0001275798276765272
Batch  161  loss:  0.00011632689711404964
Batch  171  loss:  0.00015931283996906132
Batch  181  loss:  0.00016515418246854097
Batch  191  loss:  0.00011061650729971007
Validation on real data: 
LOSS supervised-train 0.0001391825881728437, valid 0.00014660271699540317
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00011029351298930123
Batch  11  loss:  0.00015712725871708244
Batch  21  loss:  0.00010772140376502648
Batch  31  loss:  0.0001642070710659027
Batch  41  loss:  0.0001441249914932996
Batch  51  loss:  0.00010901952191488817
Batch  61  loss:  0.0001669290941208601
Batch  71  loss:  9.464674076298252e-05
Batch  81  loss:  0.00013566238339990377
Batch  91  loss:  0.00011387492850190029
Batch  101  loss:  0.00010392144758952782
Batch  111  loss:  0.00013872420822735876
Batch  121  loss:  0.0001244383311131969
Batch  131  loss:  0.0001393832644680515
Batch  141  loss:  0.00013366287748795003
Batch  151  loss:  0.00014907942386344075
Batch  161  loss:  0.00016614265041425824
Batch  171  loss:  0.00015463704767171293
Batch  181  loss:  0.00016523692465852946
Batch  191  loss:  9.429413330508396e-05
Validation on real data: 
LOSS supervised-train 0.00013291376282722922, valid 0.00016600635717622936
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  9.387591853737831e-05
Batch  11  loss:  0.0001717256527626887
Batch  21  loss:  0.00013165388372726738
Batch  31  loss:  0.00017672311514616013
Batch  41  loss:  0.00011871242895722389
Batch  51  loss:  0.00011077961971750483
Batch  61  loss:  0.000102875659649726
Batch  71  loss:  8.573591185268015e-05
Batch  81  loss:  9.799046529224142e-05
Batch  91  loss:  0.00014260696480050683
Batch  101  loss:  0.00010722478327807039
Batch  111  loss:  0.00014627091877628118
Batch  121  loss:  0.00013021606719121337
Batch  131  loss:  0.00012898733257316053
Batch  141  loss:  0.00015745851851534098
Batch  151  loss:  0.00011137245019199327
Batch  161  loss:  9.465737093705684e-05
Batch  171  loss:  9.02604078873992e-05
Batch  181  loss:  0.00014363991795107722
Batch  191  loss:  8.782749500824139e-05
Validation on real data: 
LOSS supervised-train 0.00013154080352251185, valid 0.0001648810284677893
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00010006699449149892
Batch  11  loss:  0.00012261790107004344
Batch  21  loss:  9.278793731937185e-05
Batch  31  loss:  0.00016373797552660108
Batch  41  loss:  0.00013631708861794323
Batch  51  loss:  0.00010881879279622808
Batch  61  loss:  0.00012252552551217377
Batch  71  loss:  9.656931069912389e-05
Batch  81  loss:  0.00011638200521701947
Batch  91  loss:  0.00010896276216953993
Batch  101  loss:  9.153645078185946e-05
Batch  111  loss:  0.00013240118278190494
Batch  121  loss:  0.00018232257571071386
Batch  131  loss:  0.00012933228572364897
Batch  141  loss:  0.0001529953588033095
Batch  151  loss:  0.00011359324707882479
Batch  161  loss:  0.00011072326014982536
Batch  171  loss:  0.00011922001431230456
Batch  181  loss:  0.00013557655620388687
Batch  191  loss:  0.00012152280396549031
Validation on real data: 
LOSS supervised-train 0.0001290730814798735, valid 0.0001386319927405566
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00011883304250659421
Batch  11  loss:  0.00018380153051111847
Batch  21  loss:  0.00017292365373577923
Batch  31  loss:  0.00017806324467528611
Batch  41  loss:  0.00012417077959980816
Batch  51  loss:  0.00011525997979333624
Batch  61  loss:  0.00012201740901218727
Batch  71  loss:  0.0001017403046716936
Batch  81  loss:  0.00013755423424299806
Batch  91  loss:  0.00011073898349422961
Batch  101  loss:  0.00010347227362217382
Batch  111  loss:  0.00011606093175942078
Batch  121  loss:  0.00012399117986205965
Batch  131  loss:  0.0001355444546788931
Batch  141  loss:  0.0001570454624015838
Batch  151  loss:  0.0001288872299483046
Batch  161  loss:  0.00011180058208992705
Batch  171  loss:  0.0001432581921108067
Batch  181  loss:  0.00014211279631126672
Batch  191  loss:  9.334098285762593e-05
Validation on real data: 
LOSS supervised-train 0.0001286357424396556, valid 0.0001442808861611411
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  7.591193570988253e-05
Batch  11  loss:  0.00013627749285660684
Batch  21  loss:  0.00012368719035293907
Batch  31  loss:  0.00014478879165835679
Batch  41  loss:  0.00012519700976554304
Batch  51  loss:  9.363684512209147e-05
Batch  61  loss:  0.0001311619853368029
Batch  71  loss:  8.32037694635801e-05
Batch  81  loss:  0.00011529983021318913
Batch  91  loss:  0.00012394925579428673
Batch  101  loss:  0.00011951044871238992
Batch  111  loss:  0.00016029502148739994
Batch  121  loss:  0.00013767890050075948
Batch  131  loss:  0.00011525239824550226
Batch  141  loss:  0.00013802193279843777
Batch  151  loss:  0.00011933929636143148
Batch  161  loss:  0.00011474723578430712
Batch  171  loss:  0.00011497038212837651
Batch  181  loss:  0.00014948411262594163
Batch  191  loss:  0.00012006019096588716
Validation on real data: 
LOSS supervised-train 0.0001316829379356932, valid 0.00014938993263058364
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00011038761294912547
Batch  11  loss:  0.0001726553018670529
Batch  21  loss:  0.00016915645392145962
Batch  31  loss:  0.0001613176427781582
Batch  41  loss:  0.00012098049774067476
Batch  51  loss:  9.766807488631457e-05
Batch  61  loss:  0.00012395912199281156
Batch  71  loss:  9.557861631037667e-05
Batch  81  loss:  0.00011640186858130619
Batch  91  loss:  0.00012945089838467538
Batch  101  loss:  0.00011199883738299832
Batch  111  loss:  0.00011177703709108755
Batch  121  loss:  0.00015664460079278797
Batch  131  loss:  0.00011527079914230853
Batch  141  loss:  0.00010974339238600805
Batch  151  loss:  0.00010413952986709774
Batch  161  loss:  0.00011688748054439202
Batch  171  loss:  0.00013179249071981758
Batch  181  loss:  0.00012791375047527254
Batch  191  loss:  8.310428529512137e-05
Validation on real data: 
LOSS supervised-train 0.00012732517217955319, valid 0.0001304160978179425
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  9.936395508702844e-05
Batch  11  loss:  0.00010117208148585632
Batch  21  loss:  0.0001622711424715817
Batch  31  loss:  0.0001273937086807564
Batch  41  loss:  0.00012156774027971551
Batch  51  loss:  0.00010088272392749786
Batch  61  loss:  0.0001366554934065789
Batch  71  loss:  9.099632006837055e-05
Batch  81  loss:  0.0001213692085002549
Batch  91  loss:  0.00011246194480918348
Batch  101  loss:  9.639569179853424e-05
Batch  111  loss:  0.00011807612463599071
Batch  121  loss:  0.00012890259677078575
Batch  131  loss:  0.00010815502173500136
Batch  141  loss:  0.0001416608429281041
Batch  151  loss:  9.769923781277612e-05
Batch  161  loss:  0.00012194809823995456
Batch  171  loss:  0.0001127236319007352
Batch  181  loss:  0.00012709731527138501
Batch  191  loss:  0.00012482982128858566
Validation on real data: 
LOSS supervised-train 0.00012250533804035511, valid 0.00013206747826188803
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00010127975110663101
Batch  11  loss:  0.00015557532606180757
Batch  21  loss:  0.00012936053099110723
Batch  31  loss:  0.0001471420400775969
Batch  41  loss:  0.00012567410885822028
Batch  51  loss:  8.679697202751413e-05
Batch  61  loss:  0.00016508557018823922
Batch  71  loss:  9.34350464376621e-05
Batch  81  loss:  0.00010582556569715962
Batch  91  loss:  7.70247497712262e-05
Batch  101  loss:  9.274746844312176e-05
Batch  111  loss:  0.00012711406452581286
Batch  121  loss:  0.00011696430010488257
Batch  131  loss:  0.00011263712076470256
Batch  141  loss:  0.00011783898662542924
Batch  151  loss:  0.00012863623851444572
Batch  161  loss:  0.00011316502059344202
Batch  171  loss:  0.00012352477642707527
Batch  181  loss:  0.00014381107757799327
Batch  191  loss:  0.00013061166100669652
Validation on real data: 
LOSS supervised-train 0.0001227829313938855, valid 0.00012848011101596057
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  8.708518726052716e-05
Batch  11  loss:  0.00012114794662920758
Batch  21  loss:  0.00010732000373536721
Batch  31  loss:  0.0001321297895628959
Batch  41  loss:  9.544905333314091e-05
Batch  51  loss:  9.362114360556006e-05
Batch  61  loss:  0.00010038261825684458
Batch  71  loss:  9.932254033628851e-05
Batch  81  loss:  0.00012415878882165998
Batch  91  loss:  9.835086530074477e-05
Batch  101  loss:  8.684223575983196e-05
Batch  111  loss:  0.0001250086206709966
Batch  121  loss:  0.00013671378837898374
Batch  131  loss:  0.00013851044059265405
Batch  141  loss:  0.00015989298117347062
Batch  151  loss:  0.00010726349137257785
Batch  161  loss:  0.00010671822383301333
Batch  171  loss:  0.00014763370563741773
Batch  181  loss:  0.00012525779311545193
Batch  191  loss:  0.00010955161269521341
Validation on real data: 
LOSS supervised-train 0.00012363278947304934, valid 0.0001282917510252446
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  8.301906200358644e-05
Batch  11  loss:  0.00010779634612845257
Batch  21  loss:  0.00014009403821546584
Batch  31  loss:  0.00012978013546671718
Batch  41  loss:  0.00011776101746363565
Batch  51  loss:  8.963279833551496e-05
Batch  61  loss:  0.00010102074156748131
Batch  71  loss:  9.735191270010546e-05
Batch  81  loss:  0.00010927868424914777
Batch  91  loss:  9.54273491515778e-05
Batch  101  loss:  8.810659346636385e-05
Batch  111  loss:  0.00010944015957647935
Batch  121  loss:  0.00012339212116785347
Batch  131  loss:  9.268103895010427e-05
Batch  141  loss:  0.00012419346603564918
Batch  151  loss:  0.00012590602273121476
Batch  161  loss:  0.00011052325135096908
Batch  171  loss:  0.00012862308358307928
Batch  181  loss:  0.00013554411998484284
Batch  191  loss:  0.00011634921975200996
Validation on real data: 
LOSS supervised-train 0.00012070462908013724, valid 0.00013836234575137496
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  8.752547728363425e-05
Batch  11  loss:  0.00012148974929004908
Batch  21  loss:  0.00011758142500184476
Batch  31  loss:  0.00013409528764896095
Batch  41  loss:  0.00010798951552715153
Batch  51  loss:  9.754113852977753e-05
Batch  61  loss:  0.0001277975388802588
Batch  71  loss:  8.437879296252504e-05
Batch  81  loss:  0.00010780815500766039
Batch  91  loss:  0.00014787389955017716
Batch  101  loss:  9.455817780690268e-05
Batch  111  loss:  0.00010066939285025
Batch  121  loss:  0.0001391824334859848
Batch  131  loss:  8.93921751412563e-05
Batch  141  loss:  0.0001379233435727656
Batch  151  loss:  0.00011207017814740539
Batch  161  loss:  0.00010493282752577215
Batch  171  loss:  0.00011998548143310472
Batch  181  loss:  0.00013284188753459603
Batch  191  loss:  8.379199425689876e-05
Validation on real data: 
LOSS supervised-train 0.00011912469948583748, valid 0.00012733062612824142
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  8.4956809587311e-05
Batch  11  loss:  0.00012546853395178914
Batch  21  loss:  0.00014140021812636405
Batch  31  loss:  0.00017820241919253021
Batch  41  loss:  0.00015964072372298688
Batch  51  loss:  7.711740181548521e-05
Batch  61  loss:  0.00011460061796242371
Batch  71  loss:  8.109930786304176e-05
Batch  81  loss:  9.832750947680324e-05
Batch  91  loss:  0.00011935966176679358
Batch  101  loss:  9.849655180005357e-05
Batch  111  loss:  0.00013603125989902765
Batch  121  loss:  0.00011736023589037359
Batch  131  loss:  0.00011374902533134446
Batch  141  loss:  0.00014650892990175635
Batch  151  loss:  0.00017124161240644753
Batch  161  loss:  0.00011630542576313019
Batch  171  loss:  0.0001120226806961
Batch  181  loss:  0.00012611054989974946
Batch  191  loss:  0.00010762344754766673
Validation on real data: 
LOSS supervised-train 0.00011850108905491651, valid 0.00012876799155492336
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  9.933097317116335e-05
Batch  11  loss:  9.709379082778469e-05
Batch  21  loss:  0.00015264478861354291
Batch  31  loss:  0.00017315424338448793
Batch  41  loss:  0.0001451210118830204
Batch  51  loss:  8.015029015950859e-05
Batch  61  loss:  0.00010654637299012393
Batch  71  loss:  8.525294833816588e-05
Batch  81  loss:  0.0001204011932713911
Batch  91  loss:  0.00010242160351481289
Batch  101  loss:  8.999766578199342e-05
Batch  111  loss:  0.00011841401283163577
Batch  121  loss:  0.0001440136693418026
Batch  131  loss:  0.00013355162809602916
Batch  141  loss:  0.0001285529142478481
Batch  151  loss:  0.00011716772860381752
Batch  161  loss:  0.00011099717085016891
Batch  171  loss:  0.00010987249697791412
Batch  181  loss:  0.0001388444215990603
Batch  191  loss:  0.00011750883277272806
Validation on real data: 
LOSS supervised-train 0.0001185229308975977, valid 0.00010949178249575198
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  9.069812949746847e-05
Batch  11  loss:  0.00011207807256141677
Batch  21  loss:  0.00010917473264271393
Batch  31  loss:  0.0001623593270778656
Batch  41  loss:  0.00011511532648000866
Batch  51  loss:  0.00010975703480653465
Batch  61  loss:  0.0001070881262421608
Batch  71  loss:  9.25166968954727e-05
Batch  81  loss:  9.874670649878681e-05
Batch  91  loss:  9.561257320456207e-05
Batch  101  loss:  9.462322486797348e-05
Batch  111  loss:  0.00015343363338615745
Batch  121  loss:  0.0001528899883851409
Batch  131  loss:  0.00012083330511813983
Batch  141  loss:  0.00017455678607802838
Batch  151  loss:  0.0001114373080781661
Batch  161  loss:  9.852142829913646e-05
Batch  171  loss:  0.00014878192450851202
Batch  181  loss:  0.00013826727808918804
Batch  191  loss:  0.00010195507638854906
Validation on real data: 
LOSS supervised-train 0.00011488966421893565, valid 0.0001266895851586014
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  9.560031321598217e-05
Batch  11  loss:  0.00012433010851964355
Batch  21  loss:  9.85914230113849e-05
Batch  31  loss:  0.00014343028306029737
Batch  41  loss:  0.00013491966819856316
Batch  51  loss:  0.00011881192767759785
Batch  61  loss:  9.810014307731763e-05
Batch  71  loss:  8.487485320074484e-05
Batch  81  loss:  0.00012706531560979784
Batch  91  loss:  9.937807772075757e-05
Batch  101  loss:  9.83213831204921e-05
Batch  111  loss:  0.00013245301670394838
Batch  121  loss:  0.00012236031761858612
Batch  131  loss:  0.00011977623944403604
Batch  141  loss:  0.0001493825257057324
Batch  151  loss:  0.00010010580444941297
Batch  161  loss:  0.00012156689626863226
Batch  171  loss:  0.00010258908878313377
Batch  181  loss:  0.00012810940097551793
Batch  191  loss:  9.335465438198298e-05
Validation on real data: 
LOSS supervised-train 0.00011696584049786907, valid 0.00013891534763388336
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  8.300160698127002e-05
Batch  11  loss:  0.00012652907753363252
Batch  21  loss:  0.00010720198770286515
Batch  31  loss:  0.0001774121046764776
Batch  41  loss:  9.450311335967854e-05
Batch  51  loss:  8.125347812892869e-05
Batch  61  loss:  0.00012038403656333685
Batch  71  loss:  0.0001002650024020113
Batch  81  loss:  9.636388858780265e-05
Batch  91  loss:  0.00010669688344933093
Batch  101  loss:  0.00010122539242729545
Batch  111  loss:  0.0001074513274943456
Batch  121  loss:  9.913510439218953e-05
Batch  131  loss:  0.00011960238043684512
Batch  141  loss:  0.00012959199375472963
Batch  151  loss:  8.949212497100234e-05
Batch  161  loss:  0.00010509501589694992
Batch  171  loss:  0.00012568706006277353
Batch  181  loss:  0.00012880390568170696
Batch  191  loss:  7.377663132501766e-05
Validation on real data: 
LOSS supervised-train 0.00011308721630484797, valid 0.00011868349247379228
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  8.463982521789148e-05
Batch  11  loss:  0.00011690869723679498
Batch  21  loss:  0.00012564848293550313
Batch  31  loss:  0.00013949455751571804
Batch  41  loss:  0.00010795552225317806
Batch  51  loss:  8.713867282494903e-05
Batch  61  loss:  0.00011420150985941291
Batch  71  loss:  8.23045993456617e-05
Batch  81  loss:  9.706687706056982e-05
Batch  91  loss:  0.00013252528151497245
Batch  101  loss:  9.011261136038229e-05
Batch  111  loss:  0.0001248942717211321
Batch  121  loss:  0.00010186666622757912
Batch  131  loss:  0.0001511064765509218
Batch  141  loss:  0.00012771491310559213
Batch  151  loss:  0.00012597822933457792
Batch  161  loss:  0.00013548116839956492
Batch  171  loss:  0.0001196953235194087
Batch  181  loss:  0.00011263307533226907
Batch  191  loss:  0.00010050644050352275
Validation on real data: 
LOSS supervised-train 0.00011622087782598101, valid 0.00011075010115746409
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  8.180845179595053e-05
Batch  11  loss:  0.0001130680539063178
Batch  21  loss:  9.665593097452074e-05
Batch  31  loss:  0.00015992614498827606
Batch  41  loss:  9.65719809755683e-05
Batch  51  loss:  0.00010210949403699487
Batch  61  loss:  0.00012551774852909148
Batch  71  loss:  8.295290899695829e-05
Batch  81  loss:  0.00010629972530296072
Batch  91  loss:  0.0001413475110894069
Batch  101  loss:  0.00011334093869663775
Batch  111  loss:  0.00011332600115565583
Batch  121  loss:  0.00013089926505926996
Batch  131  loss:  0.00010725660104071721
Batch  141  loss:  0.00011298229946987703
Batch  151  loss:  9.868869528872892e-05
Batch  161  loss:  0.00010583224502624944
Batch  171  loss:  0.00010913029836956412
Batch  181  loss:  0.00014763756189495325
Batch  191  loss:  0.00010810754611156881
Validation on real data: 
LOSS supervised-train 0.00011207656916667474, valid 0.00013800682791043073
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  8.052199700614437e-05
Batch  11  loss:  0.00011248506052652374
Batch  21  loss:  0.00013006686640437692
Batch  31  loss:  0.0001429037656635046
Batch  41  loss:  0.00012022552982671186
Batch  51  loss:  7.85169904702343e-05
Batch  61  loss:  0.0001235689123859629
Batch  71  loss:  0.00010992955503752455
Batch  81  loss:  0.00012770664761774242
Batch  91  loss:  8.608947973698378e-05
Batch  101  loss:  9.593570575816557e-05
Batch  111  loss:  0.00012114304990973324
Batch  121  loss:  0.00012483632599469274
Batch  131  loss:  8.238902955781668e-05
Batch  141  loss:  0.00011245484347455204
Batch  151  loss:  0.0001079902212950401
Batch  161  loss:  8.747757965466008e-05
Batch  171  loss:  0.000114686437882483
Batch  181  loss:  0.00010676126112230122
Batch  191  loss:  8.169591455953196e-05
Validation on real data: 
LOSS supervised-train 0.00011315683877910487, valid 0.00010624401329550892
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  9.486092312727123e-05
Batch  11  loss:  0.0001291039225179702
Batch  21  loss:  0.0001551331952214241
Batch  31  loss:  0.00012033174425596371
Batch  41  loss:  0.00011395863839425147
Batch  51  loss:  8.560036076232791e-05
Batch  61  loss:  0.00013126141857355833
Batch  71  loss:  8.685861394042149e-05
Batch  81  loss:  0.00010480805940460414
Batch  91  loss:  0.00010504588135518134
Batch  101  loss:  8.498990791849792e-05
Batch  111  loss:  0.00013641508121509105
Batch  121  loss:  0.00012516973947640508
Batch  131  loss:  0.00011378026101738214
Batch  141  loss:  0.00013031474372837692
Batch  151  loss:  0.00011032949987566099
Batch  161  loss:  0.00012223391968291253
Batch  171  loss:  0.00016504188533872366
Batch  181  loss:  0.00014049586025066674
Batch  191  loss:  9.424082963960245e-05
Validation on real data: 
LOSS supervised-train 0.00011180760455317795, valid 0.00011056481162086129
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00010774091788334772
Batch  11  loss:  0.00011374886526027694
Batch  21  loss:  0.00010107159323524684
Batch  31  loss:  0.00013396344729699194
Batch  41  loss:  0.00011636494309641421
Batch  51  loss:  0.0001022630458464846
Batch  61  loss:  0.0001262861187569797
Batch  71  loss:  9.834818047238514e-05
Batch  81  loss:  0.0001252525980817154
Batch  91  loss:  8.672862168168649e-05
Batch  101  loss:  0.0001051040890160948
Batch  111  loss:  0.0001156501384684816
Batch  121  loss:  0.00013386132195591927
Batch  131  loss:  0.00012435633107088506
Batch  141  loss:  0.00011339853517711163
Batch  151  loss:  8.503244316671044e-05
Batch  161  loss:  0.00010326829215046018
Batch  171  loss:  0.00014510702749248594
Batch  181  loss:  0.00014063961862120777
Batch  191  loss:  0.0001282062876271084
Validation on real data: 
LOSS supervised-train 0.00011346522111125523, valid 0.0001428053219569847
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  7.471685239579529e-05
Batch  11  loss:  0.00011245048517594114
Batch  21  loss:  0.00014931743498891592
Batch  31  loss:  0.00011308322427794337
Batch  41  loss:  0.00013241919805295765
Batch  51  loss:  9.85160659183748e-05
Batch  61  loss:  0.00012870381760876626
Batch  71  loss:  7.698122499277815e-05
Batch  81  loss:  0.0001164332206826657
Batch  91  loss:  0.00011826872650999576
Batch  101  loss:  0.0001041430004988797
Batch  111  loss:  0.00012151717965025455
Batch  121  loss:  0.00013160139496903867
Batch  131  loss:  0.00013135449262335896
Batch  141  loss:  8.859875379130244e-05
Batch  151  loss:  0.0001535916526336223
Batch  161  loss:  7.421140617225319e-05
Batch  171  loss:  0.00012727676948998123
Batch  181  loss:  0.00014328713587019593
Batch  191  loss:  9.577747550792992e-05
Validation on real data: 
LOSS supervised-train 0.00011214641453989316, valid 0.00011071754852309823
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  9.245259570889175e-05
Batch  11  loss:  0.00012718640209641308
Batch  21  loss:  9.666541154729202e-05
Batch  31  loss:  0.00013921286154072732
Batch  41  loss:  0.00010308727360097691
Batch  51  loss:  8.424668340012431e-05
Batch  61  loss:  0.0001229039771715179
Batch  71  loss:  0.00010383739572716877
Batch  81  loss:  9.320199023932219e-05
Batch  91  loss:  8.468618761980906e-05
Batch  101  loss:  9.215019235853106e-05
Batch  111  loss:  0.00012186195090180263
Batch  121  loss:  0.0001161843174486421
Batch  131  loss:  8.23252703412436e-05
Batch  141  loss:  0.00011767703836085275
Batch  151  loss:  9.350761683890596e-05
Batch  161  loss:  9.165303345071152e-05
Batch  171  loss:  0.00012980337487533689
Batch  181  loss:  0.00012211342982482165
Batch  191  loss:  0.00010535866022109985
Validation on real data: 
LOSS supervised-train 0.000109414800790546, valid 0.0001282479497604072
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  9.400973067386076e-05
Batch  11  loss:  8.419598452746868e-05
Batch  21  loss:  0.00012353878992144018
Batch  31  loss:  0.0001486066757934168
Batch  41  loss:  9.598177712177858e-05
Batch  51  loss:  6.395640957634896e-05
Batch  61  loss:  0.00011797533079516143
Batch  71  loss:  0.00012200845230836421
Batch  81  loss:  0.0001065259421011433
Batch  91  loss:  0.00010542367090238258
Batch  101  loss:  9.907187632052228e-05
Batch  111  loss:  9.458010754315183e-05
Batch  121  loss:  9.781886910786852e-05
Batch  131  loss:  0.00012028162745991722
Batch  141  loss:  0.000143664627103135
Batch  151  loss:  0.00011528845789143816
Batch  161  loss:  9.896039409795776e-05
Batch  171  loss:  8.289646211778745e-05
Batch  181  loss:  0.00013437347661238164
Batch  191  loss:  8.613803947810084e-05
Validation on real data: 
LOSS supervised-train 0.00011073608387960121, valid 0.0001128164294641465
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  8.857640932546929e-05
Batch  11  loss:  8.239864837378263e-05
Batch  21  loss:  0.00012309795420151204
Batch  31  loss:  0.00015045420150272548
Batch  41  loss:  0.00013389799278229475
Batch  51  loss:  0.00010500023927306756
Batch  61  loss:  0.00010047954856418073
Batch  71  loss:  7.717972039245069e-05
Batch  81  loss:  9.015313116833568e-05
Batch  91  loss:  8.493691711919382e-05
Batch  101  loss:  7.434760482283309e-05
Batch  111  loss:  0.00010066905088024214
Batch  121  loss:  0.00012593698920682073
Batch  131  loss:  8.898546366253868e-05
Batch  141  loss:  0.00012258425704203546
Batch  151  loss:  9.598566248314455e-05
Batch  161  loss:  8.188838546629995e-05
Batch  171  loss:  0.00013903436774853617
Batch  181  loss:  0.00014504455612041056
Batch  191  loss:  9.797458187676966e-05
Validation on real data: 
LOSS supervised-train 0.0001065562265648623, valid 8.6211170128081e-05
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  6.811198545619845e-05
Batch  11  loss:  0.00010528739221626893
Batch  21  loss:  8.330160926561803e-05
Batch  31  loss:  0.00015567743685096502
Batch  41  loss:  0.00010874523286474869
Batch  51  loss:  7.110471779014915e-05
Batch  61  loss:  0.0001343238545814529
Batch  71  loss:  7.885394734330475e-05
Batch  81  loss:  9.747519652592018e-05
Batch  91  loss:  0.00011756034655263647
Batch  101  loss:  0.00012055225670337677
Batch  111  loss:  0.00012482340389396995
Batch  121  loss:  0.00012206310202600434
Batch  131  loss:  9.397271787747741e-05
Batch  141  loss:  9.827004396356642e-05
Batch  151  loss:  9.531313116895035e-05
Batch  161  loss:  9.832996875047684e-05
Batch  171  loss:  9.686892008176073e-05
Batch  181  loss:  0.00011373449524398893
Batch  191  loss:  9.430110367247835e-05
Validation on real data: 
LOSS supervised-train 0.00010432578701511375, valid 0.00013254010991659015
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  8.467120642308146e-05
Batch  11  loss:  0.00010697577090468258
Batch  21  loss:  6.984173523960635e-05
Batch  31  loss:  0.00011922595876967534
Batch  41  loss:  0.00010223727440461516
Batch  51  loss:  7.683652074774727e-05
Batch  61  loss:  0.00010233288048766553
Batch  71  loss:  9.629422856960446e-05
Batch  81  loss:  0.00010087409464176744
Batch  91  loss:  8.340009662788361e-05
Batch  101  loss:  0.00011507747694849968
Batch  111  loss:  0.00012586286175064743
Batch  121  loss:  0.00013389444211497903
Batch  131  loss:  0.00010304406168870628
Batch  141  loss:  0.00010355602717027068
Batch  151  loss:  9.587159001966938e-05
Batch  161  loss:  7.724035094724968e-05
Batch  171  loss:  0.00011270766117377207
Batch  181  loss:  0.0001000793999992311
Batch  191  loss:  8.62466185935773e-05
Validation on real data: 
LOSS supervised-train 0.00010566579334408743, valid 9.975588181987405e-05
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  7.863919745432213e-05
Batch  11  loss:  0.00010345593182137236
Batch  21  loss:  0.00010155537893297151
Batch  31  loss:  0.00014245555212255567
Batch  41  loss:  0.00012496914132498205
Batch  51  loss:  8.13747028587386e-05
Batch  61  loss:  0.00010290250065736473
Batch  71  loss:  7.704212475800887e-05
Batch  81  loss:  0.0001094044346245937
Batch  91  loss:  0.00010334477701690048
Batch  101  loss:  9.55395371420309e-05
Batch  111  loss:  0.00010766135528683662
Batch  121  loss:  0.0001147811344708316
Batch  131  loss:  9.795825462788343e-05
Batch  141  loss:  0.00011774480663007125
Batch  151  loss:  0.00011819844075944275
Batch  161  loss:  9.35087155085057e-05
Batch  171  loss:  8.916294609662145e-05
Batch  181  loss:  0.00011599441495491192
Batch  191  loss:  9.639866766519845e-05
Validation on real data: 
LOSS supervised-train 0.00010542259580688551, valid 0.00011236220598220825
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  7.513550372095779e-05
Batch  11  loss:  8.694411371834576e-05
Batch  21  loss:  0.00011461362009868026
Batch  31  loss:  0.00012757471995428205
Batch  41  loss:  8.671110117575154e-05
Batch  51  loss:  8.960202103480697e-05
Batch  61  loss:  8.674398850416765e-05
Batch  71  loss:  7.922408258309588e-05
Batch  81  loss:  8.892628829926252e-05
Batch  91  loss:  0.00010481082426849753
Batch  101  loss:  9.021929872687906e-05
Batch  111  loss:  0.00014271716645453125
Batch  121  loss:  9.748773300088942e-05
Batch  131  loss:  8.421755046583712e-05
Batch  141  loss:  0.00010657896928023547
Batch  151  loss:  0.0001077295164577663
Batch  161  loss:  0.0001042537551256828
Batch  171  loss:  0.0001274757378268987
Batch  181  loss:  0.00013386014325078577
Batch  191  loss:  0.0001048308186000213
Validation on real data: 
LOSS supervised-train 0.00010475198963831645, valid 0.0001095146217267029
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  9.02045940165408e-05
Batch  11  loss:  8.977086690720171e-05
Batch  21  loss:  0.00014135836681816727
Batch  31  loss:  0.00013116300397086889
Batch  41  loss:  0.00010392072726972401
Batch  51  loss:  9.822349966270849e-05
Batch  61  loss:  0.0001304298493778333
Batch  71  loss:  7.435136649291962e-05
Batch  81  loss:  0.00010029503755504265
Batch  91  loss:  9.143842908088118e-05
Batch  101  loss:  7.425095827784389e-05
Batch  111  loss:  0.000132699467940256
Batch  121  loss:  9.490802767686546e-05
Batch  131  loss:  0.00011709080717992038
Batch  141  loss:  0.00012831010099034756
Batch  151  loss:  8.53828969411552e-05
Batch  161  loss:  0.00010857688175747171
Batch  171  loss:  0.00013758415298070759
Batch  181  loss:  8.831286686472595e-05
Batch  191  loss:  9.995333675760776e-05
Validation on real data: 
LOSS supervised-train 0.00010555605971603655, valid 9.096355643123388e-05
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  8.742486534174532e-05
Batch  11  loss:  8.239638555096462e-05
Batch  21  loss:  0.00010850470425793901
Batch  31  loss:  0.00012639432679861784
Batch  41  loss:  7.700517016928643e-05
Batch  51  loss:  7.903096411610022e-05
Batch  61  loss:  9.473267709836364e-05
Batch  71  loss:  0.00010036411549663171
Batch  81  loss:  7.857313175918534e-05
Batch  91  loss:  8.577020344091579e-05
Batch  101  loss:  7.442807691404596e-05
Batch  111  loss:  0.0001226526073878631
Batch  121  loss:  0.00014543505676556379
Batch  131  loss:  0.00012448140478227288
Batch  141  loss:  9.546426736051217e-05
Batch  151  loss:  0.0001245334278792143
Batch  161  loss:  8.757317118579522e-05
Batch  171  loss:  8.95365301403217e-05
Batch  181  loss:  0.0001191042538266629
Batch  191  loss:  7.807138172211125e-05
Validation on real data: 
LOSS supervised-train 0.00010442949187563499, valid 9.072705142898485e-05
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  9.137376764556393e-05
Batch  11  loss:  0.00011202075984328985
Batch  21  loss:  0.0001016706955851987
Batch  31  loss:  0.00014328505494631827
Batch  41  loss:  8.769676060182974e-05
Batch  51  loss:  9.713220060802996e-05
Batch  61  loss:  0.00010888414544751868
Batch  71  loss:  8.023067493923008e-05
Batch  81  loss:  8.920553955249488e-05
Batch  91  loss:  8.161916048265994e-05
Batch  101  loss:  8.374037133762613e-05
Batch  111  loss:  7.701685535721481e-05
Batch  121  loss:  0.00011311948765069246
Batch  131  loss:  8.727136446395889e-05
Batch  141  loss:  0.000106068306195084
Batch  151  loss:  9.799597319215536e-05
Batch  161  loss:  9.034584945766255e-05
Batch  171  loss:  0.0001189277318189852
Batch  181  loss:  0.00011744070798158646
Batch  191  loss:  9.03490508790128e-05
Validation on real data: 
LOSS supervised-train 0.00010224683854175964, valid 0.00011287487723166123
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  8.031792094698176e-05
Batch  11  loss:  0.0001119828739319928
Batch  21  loss:  9.340508404420689e-05
Batch  31  loss:  0.0001414372236467898
Batch  41  loss:  0.00013267950271256268
Batch  51  loss:  7.062459917506203e-05
Batch  61  loss:  0.00010068775009131059
Batch  71  loss:  7.635889778612182e-05
Batch  81  loss:  0.0001063379313563928
Batch  91  loss:  9.563270577928051e-05
Batch  101  loss:  6.712724280077964e-05
Batch  111  loss:  0.00014547894534189254
Batch  121  loss:  0.00012345155118964612
Batch  131  loss:  0.0001199324760818854
Batch  141  loss:  9.900614531943575e-05
Batch  151  loss:  7.146599818952382e-05
Batch  161  loss:  0.00010125416156370193
Batch  171  loss:  0.00014246256614569575
Batch  181  loss:  0.00011092494969489053
Batch  191  loss:  0.00011219814768992364
Validation on real data: 
LOSS supervised-train 0.00010180768096688552, valid 0.00010463008948136121
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00010959848441416398
Batch  11  loss:  0.00010100107465405017
Batch  21  loss:  8.527662430424243e-05
Batch  31  loss:  0.00011192236706847325
Batch  41  loss:  8.491789776599035e-05
Batch  51  loss:  8.323169458890334e-05
Batch  61  loss:  0.00010183278209296986
Batch  71  loss:  8.840807277010754e-05
Batch  81  loss:  0.0001038318732753396
Batch  91  loss:  0.00011527679453138262
Batch  101  loss:  0.00010232482600258663
Batch  111  loss:  0.00010879029287025332
Batch  121  loss:  8.574696403229609e-05
Batch  131  loss:  9.484792099101469e-05
Batch  141  loss:  0.00010033527360064909
Batch  151  loss:  9.486667840974405e-05
Batch  161  loss:  7.061589712975547e-05
Batch  171  loss:  8.8668501120992e-05
Batch  181  loss:  0.00014380707580130547
Batch  191  loss:  8.803814125712961e-05
Validation on real data: 
LOSS supervised-train 0.00010104616558237467, valid 0.00010482557991053909
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  8.387502748519182e-05
Batch  11  loss:  9.398808469995856e-05
Batch  21  loss:  0.00012209388660266995
Batch  31  loss:  0.00011053642083425075
Batch  41  loss:  8.620273001724854e-05
Batch  51  loss:  7.82628558226861e-05
Batch  61  loss:  0.000114401314931456
Batch  71  loss:  7.333179382840171e-05
Batch  81  loss:  9.022177255246788e-05
Batch  91  loss:  9.047778439708054e-05
Batch  101  loss:  0.00010587581346044317
Batch  111  loss:  0.00011463138071121648
Batch  121  loss:  9.684267570264637e-05
Batch  131  loss:  9.407477045897394e-05
Batch  141  loss:  0.00010925487731583416
Batch  151  loss:  9.244900138583034e-05
Batch  161  loss:  6.963191117392853e-05
Batch  171  loss:  0.00011031176836695522
Batch  181  loss:  0.00011646851635305211
Batch  191  loss:  7.823107443982735e-05
Validation on real data: 
LOSS supervised-train 9.900612418277888e-05, valid 9.931159729603678e-05
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  6.407888577086851e-05
Batch  11  loss:  8.756426541367546e-05
Batch  21  loss:  9.989800309995189e-05
Batch  31  loss:  0.00015481341688428074
Batch  41  loss:  9.261386003345251e-05
Batch  51  loss:  7.626154547324404e-05
Batch  61  loss:  0.00011573269875952974
Batch  71  loss:  7.765032933093607e-05
Batch  81  loss:  0.00011859498772537336
Batch  91  loss:  6.524248601635918e-05
Batch  101  loss:  7.433340942952782e-05
Batch  111  loss:  8.399070793529972e-05
Batch  121  loss:  0.00011262154293945059
Batch  131  loss:  0.00010988573922077194
Batch  141  loss:  8.874712511897087e-05
Batch  151  loss:  9.538308222545311e-05
Batch  161  loss:  9.659208444645628e-05
Batch  171  loss:  0.00011680160241667181
Batch  181  loss:  0.00011627020285231993
Batch  191  loss:  8.046617585932836e-05
Validation on real data: 
LOSS supervised-train 9.826932297073654e-05, valid 8.953230280894786e-05
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  7.2424256359227e-05
Batch  11  loss:  0.00010506947000976652
Batch  21  loss:  9.291520836995915e-05
Batch  31  loss:  0.00013261027925182134
Batch  41  loss:  8.799901115708053e-05
Batch  51  loss:  8.490000618621707e-05
Batch  61  loss:  7.456535240635276e-05
Batch  71  loss:  9.967843652702868e-05
Batch  81  loss:  0.00011177537817275152
Batch  91  loss:  9.031531953951344e-05
Batch  101  loss:  8.712269482202828e-05
Batch  111  loss:  0.00011704470671247691
Batch  121  loss:  0.00011562670988496393
Batch  131  loss:  8.113685180433095e-05
Batch  141  loss:  0.0001013842920656316
Batch  151  loss:  8.7767381046433e-05
Batch  161  loss:  7.990758604137227e-05
Batch  171  loss:  0.00013367790961638093
Batch  181  loss:  0.00011894896306330338
Batch  191  loss:  8.159225399140269e-05
Validation on real data: 
LOSS supervised-train 9.864265755822999e-05, valid 8.046196307986975e-05
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  6.822462455602363e-05
Batch  11  loss:  8.539147529518232e-05
Batch  21  loss:  8.914373756852001e-05
Batch  31  loss:  0.0001498152851127088
Batch  41  loss:  0.00010593616752885282
Batch  51  loss:  7.879391341703013e-05
Batch  61  loss:  0.00010747721535153687
Batch  71  loss:  6.965878856135532e-05
Batch  81  loss:  0.0001096176274586469
Batch  91  loss:  8.530307241016999e-05
Batch  101  loss:  9.143771603703499e-05
Batch  111  loss:  0.00010265663149766624
Batch  121  loss:  0.00011507319140946493
Batch  131  loss:  0.0001044924501911737
Batch  141  loss:  0.00011574318341445178
Batch  151  loss:  8.861519017955288e-05
Batch  161  loss:  7.602819096064195e-05
Batch  171  loss:  9.43954146350734e-05
Batch  181  loss:  0.00013457304157782346
Batch  191  loss:  8.833250467432663e-05
Validation on real data: 
LOSS supervised-train 9.944846582584433e-05, valid 0.00011033988266717643
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  6.440263678086922e-05
Batch  11  loss:  9.318522643297911e-05
Batch  21  loss:  0.00012765670544467866
Batch  31  loss:  0.00010830962855834514
Batch  41  loss:  0.00011559254926396534
Batch  51  loss:  6.629597919527441e-05
Batch  61  loss:  0.00010240474512102082
Batch  71  loss:  7.426664524246007e-05
Batch  81  loss:  8.361984509974718e-05
Batch  91  loss:  8.315304148709401e-05
Batch  101  loss:  8.143894956447184e-05
Batch  111  loss:  0.00011088227620348334
Batch  121  loss:  9.27709334064275e-05
Batch  131  loss:  8.873120532371104e-05
Batch  141  loss:  9.201658394886181e-05
Batch  151  loss:  9.775927173905075e-05
Batch  161  loss:  9.78939060587436e-05
Batch  171  loss:  0.00011095611262135208
Batch  181  loss:  0.00011660283053060994
Batch  191  loss:  8.116487151710317e-05
Validation on real data: 
LOSS supervised-train 9.598200780601474e-05, valid 0.00010454765288159251
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  7.644220750080422e-05
Batch  11  loss:  8.764444646658376e-05
Batch  21  loss:  9.753526683198288e-05
Batch  31  loss:  0.00011610127694439143
Batch  41  loss:  0.00011081937554990873
Batch  51  loss:  7.603752601426095e-05
Batch  61  loss:  0.00010293174273101613
Batch  71  loss:  6.0343965742504224e-05
Batch  81  loss:  0.00010702587314881384
Batch  91  loss:  9.774928912520409e-05
Batch  101  loss:  7.783451292198151e-05
Batch  111  loss:  0.0001365792122669518
Batch  121  loss:  0.00011355507012922317
Batch  131  loss:  0.00011611165245994925
Batch  141  loss:  0.00010929722338914871
Batch  151  loss:  9.334783680969849e-05
Batch  161  loss:  8.993564551929012e-05
Batch  171  loss:  0.00010028765973402187
Batch  181  loss:  0.00011848479334730655
Batch  191  loss:  8.653264376334846e-05
Validation on real data: 
LOSS supervised-train 9.892741609291988e-05, valid 0.0001031709834933281
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  7.697424734942615e-05
Batch  11  loss:  0.00011147808254463598
Batch  21  loss:  0.00010618883243296295
Batch  31  loss:  0.00013956397015135735
Batch  41  loss:  9.205668902723119e-05
Batch  51  loss:  8.381666702916846e-05
Batch  61  loss:  8.624696783954278e-05
Batch  71  loss:  7.355176057899371e-05
Batch  81  loss:  0.00010606670548440889
Batch  91  loss:  7.272356015164405e-05
Batch  101  loss:  8.302496280521154e-05
Batch  111  loss:  0.00010514249879634008
Batch  121  loss:  0.00010941091750282794
Batch  131  loss:  0.00010862710769288242
Batch  141  loss:  0.00011795230238931254
Batch  151  loss:  9.872576629277319e-05
Batch  161  loss:  0.00010620140528772026
Batch  171  loss:  0.00013543579552788287
Batch  181  loss:  0.00010541402298258618
Batch  191  loss:  9.037589188665152e-05
Validation on real data: 
LOSS supervised-train 0.00010020485566201386, valid 9.92331188172102e-05
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  7.831416587578133e-05
Batch  11  loss:  8.727057866053656e-05
Batch  21  loss:  9.933776163961738e-05
Batch  31  loss:  9.811084601096809e-05
Batch  41  loss:  9.092137042898685e-05
Batch  51  loss:  7.189324969658628e-05
Batch  61  loss:  9.881769074127078e-05
Batch  71  loss:  7.508716225856915e-05
Batch  81  loss:  9.554227290209383e-05
Batch  91  loss:  9.330677858088166e-05
Batch  101  loss:  7.817197183612734e-05
Batch  111  loss:  0.00010358026338508353
Batch  121  loss:  0.00010478065814822912
Batch  131  loss:  7.754343096166849e-05
Batch  141  loss:  0.00012544469791464508
Batch  151  loss:  0.00011278669262537733
Batch  161  loss:  8.735404844628647e-05
Batch  171  loss:  0.00011036481737392023
Batch  181  loss:  0.00012191970745334402
Batch  191  loss:  9.893526294035837e-05
Validation on real data: 
LOSS supervised-train 9.742802554683294e-05, valid 0.00010007213131757453
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  7.060549251036718e-05
Batch  11  loss:  9.391107596457005e-05
Batch  21  loss:  0.00011701650510076433
Batch  31  loss:  0.00010468345135450363
Batch  41  loss:  9.675550245447084e-05
Batch  51  loss:  7.155699859140441e-05
Batch  61  loss:  0.0001232373178936541
Batch  71  loss:  9.08336733118631e-05
Batch  81  loss:  0.00012071306264260784
Batch  91  loss:  7.399424794130027e-05
Batch  101  loss:  0.00010153656330658123
Batch  111  loss:  0.00010850114631466568
Batch  121  loss:  0.0001410032418789342
Batch  131  loss:  9.166224481305107e-05
Batch  141  loss:  0.0001084315445041284
Batch  151  loss:  9.025081089930609e-05
Batch  161  loss:  7.987306162249297e-05
Batch  171  loss:  0.00011002406245097518
Batch  181  loss:  0.00011199833534192294
Batch  191  loss:  6.6000699007418e-05
Validation on real data: 
LOSS supervised-train 9.696787241409765e-05, valid 8.106384484563023e-05
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  8.250879182014614e-05
Batch  11  loss:  7.819155871402472e-05
Batch  21  loss:  8.812337910057977e-05
Batch  31  loss:  0.00014465740241575986
Batch  41  loss:  0.00011491424811538309
Batch  51  loss:  7.231577910715714e-05
Batch  61  loss:  9.934790432453156e-05
Batch  71  loss:  8.178901043720543e-05
Batch  81  loss:  7.339802687056363e-05
Batch  91  loss:  0.0001117061692639254
Batch  101  loss:  8.60944201122038e-05
Batch  111  loss:  0.0001095321131288074
Batch  121  loss:  0.00010806493082782254
Batch  131  loss:  9.163223148789257e-05
Batch  141  loss:  7.862837082939222e-05
Batch  151  loss:  8.683032501721755e-05
Batch  161  loss:  8.89594157342799e-05
Batch  171  loss:  0.00010534973262110725
Batch  181  loss:  0.000127248335047625
Batch  191  loss:  6.855279207229614e-05
Validation on real data: 
LOSS supervised-train 9.496450193182681e-05, valid 0.00010737046250142157
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  7.189439202193171e-05
Batch  11  loss:  9.955160203389823e-05
Batch  21  loss:  7.959614595165476e-05
Batch  31  loss:  0.00012642562796827406
Batch  41  loss:  0.00010249038314213976
Batch  51  loss:  6.440091965487227e-05
Batch  61  loss:  9.84477810561657e-05
Batch  71  loss:  5.859879820491187e-05
Batch  81  loss:  8.541615534340963e-05
Batch  91  loss:  9.786262671696022e-05
Batch  101  loss:  6.830160418758169e-05
Batch  111  loss:  0.00011870681919390336
Batch  121  loss:  9.666521509643644e-05
Batch  131  loss:  8.867561700753868e-05
Batch  141  loss:  0.00010151948663406074
Batch  151  loss:  0.00011358128540450707
Batch  161  loss:  0.00010911659774137661
Batch  171  loss:  0.00010996693163178861
Batch  181  loss:  9.789835166884586e-05
Batch  191  loss:  7.68796235206537e-05
Validation on real data: 
LOSS supervised-train 9.463673681239016e-05, valid 8.518967661075294e-05
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  7.522964006057009e-05
Batch  11  loss:  0.0001029693812597543
Batch  21  loss:  9.315906936535612e-05
Batch  31  loss:  0.0001234194205608219
Batch  41  loss:  9.075792331714183e-05
Batch  51  loss:  8.100064587779343e-05
Batch  61  loss:  0.00011389011342544109
Batch  71  loss:  8.596861880505458e-05
Batch  81  loss:  8.917078957892954e-05
Batch  91  loss:  7.995112537173554e-05
Batch  101  loss:  8.612558303866535e-05
Batch  111  loss:  9.904283797368407e-05
Batch  121  loss:  8.454819180769846e-05
Batch  131  loss:  8.623817848274484e-05
Batch  141  loss:  0.00010183019185205922
Batch  151  loss:  0.00010809124796651304
Batch  161  loss:  0.00010699249833123758
Batch  171  loss:  8.992892253445461e-05
Batch  181  loss:  0.00010054140875581652
Batch  191  loss:  0.00010125179687747732
Validation on real data: 
LOSS supervised-train 9.56155428866623e-05, valid 9.399258124176413e-05
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  8.676374272909015e-05
Batch  11  loss:  8.001941750990227e-05
Batch  21  loss:  8.111082570394501e-05
Batch  31  loss:  0.00011758547771023586
Batch  41  loss:  0.00010479019692866132
Batch  51  loss:  6.34421594440937e-05
Batch  61  loss:  0.0001180658073280938
Batch  71  loss:  8.730545232538134e-05
Batch  81  loss:  8.561366121284664e-05
Batch  91  loss:  6.987852975726128e-05
Batch  101  loss:  9.667035192251205e-05
Batch  111  loss:  8.555834938306361e-05
Batch  121  loss:  7.455935701727867e-05
Batch  131  loss:  9.352331835543737e-05
Batch  141  loss:  9.86791928880848e-05
Batch  151  loss:  0.0001108580399886705
Batch  161  loss:  7.641062984475866e-05
Batch  171  loss:  0.00010000645852414891
Batch  181  loss:  0.00011741417984012514
Batch  191  loss:  8.036998769966885e-05
Validation on real data: 
LOSS supervised-train 9.260259432267048e-05, valid 0.00011126864410471171
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  6.970120739424601e-05
Batch  11  loss:  9.065184713108465e-05
Batch  21  loss:  8.876936044543982e-05
Batch  31  loss:  0.00010568703874014318
Batch  41  loss:  9.249707363778725e-05
Batch  51  loss:  7.41894546081312e-05
Batch  61  loss:  9.611045970814303e-05
Batch  71  loss:  7.596313662361354e-05
Batch  81  loss:  0.00010062351793749258
Batch  91  loss:  0.0001035372115438804
Batch  101  loss:  7.213756180135533e-05
Batch  111  loss:  0.00011964505392825231
Batch  121  loss:  9.853379742708057e-05
Batch  131  loss:  9.962781041394919e-05
Batch  141  loss:  0.00012153474381193519
Batch  151  loss:  0.00010522126831347123
Batch  161  loss:  9.008720371639356e-05
Batch  171  loss:  9.192698780680075e-05
Batch  181  loss:  0.00011512775381561369
Batch  191  loss:  8.193497342290357e-05
Validation on real data: 
LOSS supervised-train 9.569224534061504e-05, valid 8.96983256097883e-05
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  9.09907539607957e-05
Batch  11  loss:  9.075302659766749e-05
Batch  21  loss:  9.991198749048635e-05
Batch  31  loss:  0.00010260671115247533
Batch  41  loss:  8.463866834063083e-05
Batch  51  loss:  7.867422391427681e-05
Batch  61  loss:  0.00010305967589374632
Batch  71  loss:  8.823782263789326e-05
Batch  81  loss:  9.732707985676825e-05
Batch  91  loss:  9.114072599913925e-05
Batch  101  loss:  8.141951548168436e-05
Batch  111  loss:  9.891725494526327e-05
Batch  121  loss:  0.00011163226736243814
Batch  131  loss:  8.249573147622868e-05
Batch  141  loss:  9.444424358662218e-05
Batch  151  loss:  8.557664114050567e-05
Batch  161  loss:  6.83651160215959e-05
Batch  171  loss:  7.906735118012875e-05
Batch  181  loss:  0.00010636370279826224
Batch  191  loss:  7.697237015236169e-05
Validation on real data: 
LOSS supervised-train 9.43347203610756e-05, valid 9.337984374724329e-05
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  8.085464651230723e-05
Batch  11  loss:  7.676670793443918e-05
Batch  21  loss:  8.385984256165102e-05
Batch  31  loss:  0.00010169517190661281
Batch  41  loss:  0.00010185955761699006
Batch  51  loss:  7.262248254846781e-05
Batch  61  loss:  0.00011406983685446903
Batch  71  loss:  7.427255332004279e-05
Batch  81  loss:  9.036928531713784e-05
Batch  91  loss:  9.479172149440274e-05
Batch  101  loss:  0.00010243581346003339
Batch  111  loss:  9.208820847561583e-05
Batch  121  loss:  0.00010129503789357841
Batch  131  loss:  7.944383833091706e-05
Batch  141  loss:  8.4418679762166e-05
Batch  151  loss:  0.00011095708032371476
Batch  161  loss:  6.845592724857852e-05
Batch  171  loss:  9.664956451160833e-05
Batch  181  loss:  9.273033356294036e-05
Batch  191  loss:  7.780238229315728e-05
Validation on real data: 
LOSS supervised-train 9.192288260237547e-05, valid 8.565295138396323e-05
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  guitar ; Model ID: 5df08ba7af60e7bfe72db292d4e13056
--------------------
Training baseline regression model:  2022-03-30 03:09:41.535949
Detector:  point_transformer
Object:  guitar
--------------------
device is  cuda
--------------------
Number of trainable parameters:  893083
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.10133569687604904
Batch  11  loss:  0.014189617708325386
Batch  21  loss:  0.00957654882222414
Batch  31  loss:  0.0036265074741095304
Batch  41  loss:  0.0029153162613511086
Batch  51  loss:  0.0028530799318104982
Batch  61  loss:  0.0015018576523289084
Batch  71  loss:  0.0016893957508727908
Batch  81  loss:  0.001544748549349606
Batch  91  loss:  0.0010691042989492416
Batch  101  loss:  0.0038181704003363848
Batch  111  loss:  0.0024589726235717535
Batch  121  loss:  0.0011351269204169512
Batch  131  loss:  0.0007739050779491663
Batch  141  loss:  0.0010198135860264301
Batch  151  loss:  0.0023307730443775654
Batch  161  loss:  0.0008628442883491516
Batch  171  loss:  0.0014079415705054998
Batch  181  loss:  0.001037715352140367
Batch  191  loss:  0.0016625493299216032
Validation on real data: 
LOSS supervised-train 0.004710790288227145, valid 0.0012149577960371971
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.000525451498106122
Batch  11  loss:  0.002471640706062317
Batch  21  loss:  0.0009616449242457747
Batch  31  loss:  0.0004484145320020616
Batch  41  loss:  0.0009412450017407537
Batch  51  loss:  0.002138862619176507
Batch  61  loss:  0.0011302561033517122
Batch  71  loss:  0.0009597133030183613
Batch  81  loss:  0.0006980173639021814
Batch  91  loss:  0.00045053474605083466
Batch  101  loss:  0.0014168702764436603
Batch  111  loss:  0.000980078591965139
Batch  121  loss:  0.00046613466111011803
Batch  131  loss:  0.000374043796909973
Batch  141  loss:  0.0012061871821060777
Batch  151  loss:  0.000989214750006795
Batch  161  loss:  0.000500071793794632
Batch  171  loss:  0.0008063974673859775
Batch  181  loss:  0.0009290976449847221
Batch  191  loss:  0.0008179849246516824
Validation on real data: 
LOSS supervised-train 0.0009299010910035577, valid 0.0006074297125451267
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0004009879194200039
Batch  11  loss:  0.0013431762345135212
Batch  21  loss:  0.0005025363643653691
Batch  31  loss:  0.0004964880645275116
Batch  41  loss:  0.0010443005012348294
Batch  51  loss:  0.0017649632645770907
Batch  61  loss:  0.0008499203249812126
Batch  71  loss:  0.0004941909573972225
Batch  81  loss:  0.0005752207362093031
Batch  91  loss:  0.0003100970934610814
Batch  101  loss:  0.0009805266745388508
Batch  111  loss:  0.0008674865239299834
Batch  121  loss:  0.0006489632069133222
Batch  131  loss:  0.0005313577130436897
Batch  141  loss:  0.001018586684949696
Batch  151  loss:  0.0009813872165977955
Batch  161  loss:  0.0004339893930591643
Batch  171  loss:  0.0006826540920883417
Batch  181  loss:  0.0007776344427838922
Batch  191  loss:  0.000883605913259089
Validation on real data: 
LOSS supervised-train 0.0007531385487527586, valid 0.00048431288450956345
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0005113665829412639
Batch  11  loss:  0.0015061305603012443
Batch  21  loss:  0.00023996412346605211
Batch  31  loss:  0.0006130049005150795
Batch  41  loss:  0.0009209693525917828
Batch  51  loss:  0.001487513305619359
Batch  61  loss:  0.0005107681499794126
Batch  71  loss:  0.0004330164229031652
Batch  81  loss:  0.0004048075061291456
Batch  91  loss:  0.00027701203362084925
Batch  101  loss:  0.0010896649910137057
Batch  111  loss:  0.0007008871180005372
Batch  121  loss:  0.0005379896610975266
Batch  131  loss:  0.0004110342124477029
Batch  141  loss:  0.0007189440657384694
Batch  151  loss:  0.00096042052609846
Batch  161  loss:  0.00035922552342526615
Batch  171  loss:  0.00046182217192836106
Batch  181  loss:  0.0004889536066912115
Batch  191  loss:  0.0008936416707001626
Validation on real data: 
LOSS supervised-train 0.0006195722552365623, valid 0.00043965160148218274
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0005165407783351839
Batch  11  loss:  0.0009290909511037171
Batch  21  loss:  0.00022689746401738375
Batch  31  loss:  0.0005256097065284848
Batch  41  loss:  0.0006086559733375907
Batch  51  loss:  0.0008025686256587505
Batch  61  loss:  0.00025323699810542166
Batch  71  loss:  0.0004612957709468901
Batch  81  loss:  0.00033465272281318903
Batch  91  loss:  0.00018554234702605754
Batch  101  loss:  0.0006669603753834963
Batch  111  loss:  0.0007172541227191687
Batch  121  loss:  0.00043907947838306427
Batch  131  loss:  0.00030754986801184714
Batch  141  loss:  0.0003994493745267391
Batch  151  loss:  0.0010266333119943738
Batch  161  loss:  0.0003180517815053463
Batch  171  loss:  0.0003775578225031495
Batch  181  loss:  0.0002952832728624344
Batch  191  loss:  0.0006967693334445357
Validation on real data: 
LOSS supervised-train 0.0005375554216880119, valid 0.0003373120562173426
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0004442422650754452
Batch  11  loss:  0.0006586439558304846
Batch  21  loss:  0.0002292689314344898
Batch  31  loss:  0.00047359883319586515
Batch  41  loss:  0.0004093694733455777
Batch  51  loss:  0.0006178180919960141
Batch  61  loss:  0.0002205665805377066
Batch  71  loss:  0.00046003676834516227
Batch  81  loss:  0.00027701491490006447
Batch  91  loss:  0.00021893918165005744
Batch  101  loss:  0.0005667590303346515
Batch  111  loss:  0.0006850843201391399
Batch  121  loss:  0.0004343003674875945
Batch  131  loss:  0.0001921877556014806
Batch  141  loss:  0.00026793996221385896
Batch  151  loss:  0.0011157470289617777
Batch  161  loss:  0.00023843410599511117
Batch  171  loss:  0.0004212460480630398
Batch  181  loss:  0.00023471450549550354
Batch  191  loss:  0.0005372982122935355
Validation on real data: 
LOSS supervised-train 0.00046579884612583554, valid 0.0002512634382583201
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0004896730533801019
Batch  11  loss:  0.00045027665328234434
Batch  21  loss:  0.0001996145729208365
Batch  31  loss:  0.0003499824379105121
Batch  41  loss:  0.0002273053687531501
Batch  51  loss:  0.00047642638674005866
Batch  61  loss:  0.00021284050308167934
Batch  71  loss:  0.00042611686512827873
Batch  81  loss:  0.0002828534343279898
Batch  91  loss:  0.00016655211220495403
Batch  101  loss:  0.0003587254323065281
Batch  111  loss:  0.00043895456474274397
Batch  121  loss:  0.00034613366005942225
Batch  131  loss:  0.0002066309389192611
Batch  141  loss:  0.00022304983576759696
Batch  151  loss:  0.0009797225939109921
Batch  161  loss:  0.00018419267144054174
Batch  171  loss:  0.00034511194098740816
Batch  181  loss:  0.00023637281265109777
Batch  191  loss:  0.0005340935895219445
Validation on real data: 
LOSS supervised-train 0.00039443274064979053, valid 0.00017421533993910998
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.00043292363989166915
Batch  11  loss:  0.0004037926555611193
Batch  21  loss:  0.00022335757967084646
Batch  31  loss:  0.0003063409239985049
Batch  41  loss:  0.00015020347200334072
Batch  51  loss:  0.00044302857713773847
Batch  61  loss:  0.00022092567814979702
Batch  71  loss:  0.0003649842692539096
Batch  81  loss:  0.00024346745340153575
Batch  91  loss:  0.00018501441809348762
Batch  101  loss:  0.00040656328201293945
Batch  111  loss:  0.0005053349887020886
Batch  121  loss:  0.00029923729016445577
Batch  131  loss:  0.00019543322559911758
Batch  141  loss:  0.00024271619622595608
Batch  151  loss:  0.0006665693363174796
Batch  161  loss:  0.00016217232041526586
Batch  171  loss:  0.00029532547341659665
Batch  181  loss:  0.00020860153017565608
Batch  191  loss:  0.0004951892769895494
Validation on real data: 
LOSS supervised-train 0.00035205051717639434, valid 0.0001539279764983803
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.00033821843680925667
Batch  11  loss:  0.00045639422023668885
Batch  21  loss:  0.00019428330415394157
Batch  31  loss:  0.00029552614432759583
Batch  41  loss:  0.00018067813653033227
Batch  51  loss:  0.00038340655737556517
Batch  61  loss:  0.0001961540401680395
Batch  71  loss:  0.00035874309833161533
Batch  81  loss:  0.0002793557941913605
Batch  91  loss:  0.00015875477402005345
Batch  101  loss:  0.0003516135911922902
Batch  111  loss:  0.0004753886314574629
Batch  121  loss:  0.0002622990286909044
Batch  131  loss:  0.00014871844905428588
Batch  141  loss:  0.00015277550846803933
Batch  151  loss:  0.0006551531841978431
Batch  161  loss:  0.00016865813813637942
Batch  171  loss:  0.0003251540183555335
Batch  181  loss:  0.0002167352067772299
Batch  191  loss:  0.00035065345582552254
Validation on real data: 
LOSS supervised-train 0.00032115750305820255, valid 0.00013402174226939678
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.00036076316609978676
Batch  11  loss:  0.0003307164297439158
Batch  21  loss:  0.00016618236259091645
Batch  31  loss:  0.0002420236123725772
Batch  41  loss:  0.00016399408923462033
Batch  51  loss:  0.00038341476465575397
Batch  61  loss:  0.00020239401783328503
Batch  71  loss:  0.00030630375840701163
Batch  81  loss:  0.0002540936693549156
Batch  91  loss:  0.0002149120846297592
Batch  101  loss:  0.0003311495529487729
Batch  111  loss:  0.0004251235513947904
Batch  121  loss:  0.00023945934663061053
Batch  131  loss:  0.00013838008453603834
Batch  141  loss:  0.00018525247287470847
Batch  151  loss:  0.0005485492292791605
Batch  161  loss:  0.00016244938888121396
Batch  171  loss:  0.00021665968233719468
Batch  181  loss:  0.00021316882339306176
Batch  191  loss:  0.00030963245080783963
Validation on real data: 
LOSS supervised-train 0.000288379411031201, valid 0.0001543934049550444
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0003249344590585679
Batch  11  loss:  0.00034906642395071685
Batch  21  loss:  0.00015338367666117847
Batch  31  loss:  0.00017215758271049708
Batch  41  loss:  0.00014762484352104366
Batch  51  loss:  0.00040955355507321656
Batch  61  loss:  0.0002235019492218271
Batch  71  loss:  0.0003139198524877429
Batch  81  loss:  0.0002167057537008077
Batch  91  loss:  0.00015853418153710663
Batch  101  loss:  0.00029087814618833363
Batch  111  loss:  0.0004111367743462324
Batch  121  loss:  0.0002497039968147874
Batch  131  loss:  0.00012280182272661477
Batch  141  loss:  0.00016569647414144129
Batch  151  loss:  0.00048449053429067135
Batch  161  loss:  0.000145402125781402
Batch  171  loss:  0.0002095298405038193
Batch  181  loss:  0.00018721965898294002
Batch  191  loss:  0.0003413794038351625
Validation on real data: 
LOSS supervised-train 0.0002637042274363921, valid 0.00012897345004603267
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.00039617985021322966
Batch  11  loss:  0.0002943925210274756
Batch  21  loss:  0.0001301643205806613
Batch  31  loss:  0.00018491204536985606
Batch  41  loss:  0.00011695163266267627
Batch  51  loss:  0.00030096108093857765
Batch  61  loss:  0.00020976773521397263
Batch  71  loss:  0.00027571036480367184
Batch  81  loss:  0.00023288186639547348
Batch  91  loss:  0.0001821293990360573
Batch  101  loss:  0.0003995680599473417
Batch  111  loss:  0.00040344573790207505
Batch  121  loss:  0.0002117104595527053
Batch  131  loss:  0.000122049794299528
Batch  141  loss:  0.00014397122140508145
Batch  151  loss:  0.00040942689520306885
Batch  161  loss:  0.0001347125507891178
Batch  171  loss:  0.00030148151563480496
Batch  181  loss:  0.00017615713295526803
Batch  191  loss:  0.00030791154131293297
Validation on real data: 
LOSS supervised-train 0.0002474805963720428, valid 0.00011296919547021389
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.00040026925853453577
Batch  11  loss:  0.00023665941262152046
Batch  21  loss:  0.0001254572853213176
Batch  31  loss:  0.0001368135417578742
Batch  41  loss:  0.00013192676124162972
Batch  51  loss:  0.0002611234085634351
Batch  61  loss:  0.00020004695397801697
Batch  71  loss:  0.00026567935128696263
Batch  81  loss:  0.0001908372505567968
Batch  91  loss:  0.00014512416964862496
Batch  101  loss:  0.00025899146567098796
Batch  111  loss:  0.0003433576785027981
Batch  121  loss:  0.0002109198394464329
Batch  131  loss:  0.0001234144438058138
Batch  141  loss:  0.00010357988503528759
Batch  151  loss:  0.0004549300647340715
Batch  161  loss:  0.0001300556759815663
Batch  171  loss:  0.0002759028284344822
Batch  181  loss:  0.00014684538473375142
Batch  191  loss:  0.0002700567420106381
Validation on real data: 
LOSS supervised-train 0.00022849753451737343, valid 0.00011465473653515801
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00033565625199116766
Batch  11  loss:  0.00026495297788642347
Batch  21  loss:  0.0001255181705346331
Batch  31  loss:  0.00015739910304546356
Batch  41  loss:  0.00010877676686504856
Batch  51  loss:  0.0002520008129067719
Batch  61  loss:  0.00022832075774203986
Batch  71  loss:  0.0001961312082130462
Batch  81  loss:  0.00023191711807157844
Batch  91  loss:  0.00014965498121455312
Batch  101  loss:  0.000270353484665975
Batch  111  loss:  0.00036811380414292216
Batch  121  loss:  0.00018010108033195138
Batch  131  loss:  0.00013091728033032268
Batch  141  loss:  0.00014082749839872122
Batch  151  loss:  0.0004938472411595285
Batch  161  loss:  0.00014310925325844437
Batch  171  loss:  0.00021208473481237888
Batch  181  loss:  0.000184535383596085
Batch  191  loss:  0.0002670119283720851
Validation on real data: 
LOSS supervised-train 0.00021725122202042257, valid 0.0001056071778293699
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.00027871716883964837
Batch  11  loss:  0.0002649782400112599
Batch  21  loss:  0.00011002683459082618
Batch  31  loss:  0.00011017855285899714
Batch  41  loss:  0.00013671473425347358
Batch  51  loss:  0.00028027125517837703
Batch  61  loss:  0.00018368229211773723
Batch  71  loss:  0.0001965891133295372
Batch  81  loss:  0.00017046324501279742
Batch  91  loss:  0.0001636567321838811
Batch  101  loss:  0.0002922626445069909
Batch  111  loss:  0.0002616690471768379
Batch  121  loss:  0.00019133386376779526
Batch  131  loss:  0.00010249677143292502
Batch  141  loss:  0.00013936108734924346
Batch  151  loss:  0.0003341225965414196
Batch  161  loss:  0.00012665738177020103
Batch  171  loss:  0.00019901327323168516
Batch  181  loss:  0.00015714686014689505
Batch  191  loss:  0.0003014712710864842
Validation on real data: 
LOSS supervised-train 0.00020480346858676058, valid 0.00010968503192998469
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0003108821401838213
Batch  11  loss:  0.00020744494395330548
Batch  21  loss:  0.00013602770923171192
Batch  31  loss:  0.00011899010132765397
Batch  41  loss:  0.00012122933549107984
Batch  51  loss:  0.0002458388917148113
Batch  61  loss:  0.00017539777036290616
Batch  71  loss:  0.0002095042000291869
Batch  81  loss:  0.00017488296725787222
Batch  91  loss:  0.00014800005010329187
Batch  101  loss:  0.00019057377357967198
Batch  111  loss:  0.0002804130199365318
Batch  121  loss:  0.00018425688904244453
Batch  131  loss:  0.0001104562688851729
Batch  141  loss:  0.00013003524509258568
Batch  151  loss:  0.0003949735837522894
Batch  161  loss:  0.00014401042426470667
Batch  171  loss:  0.0002073239447781816
Batch  181  loss:  0.00014239465235732496
Batch  191  loss:  0.0002371413429500535
Validation on real data: 
LOSS supervised-train 0.00019194496773707214, valid 9.474922262597829e-05
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.00028848068905062973
Batch  11  loss:  0.00018045979959424585
Batch  21  loss:  0.00011530543997650966
Batch  31  loss:  0.0001216763339471072
Batch  41  loss:  0.0001050388891599141
Batch  51  loss:  0.00026314702699892223
Batch  61  loss:  0.0002176051784772426
Batch  71  loss:  0.00019631377654150128
Batch  81  loss:  0.0001835107832448557
Batch  91  loss:  0.00014724867651239038
Batch  101  loss:  0.00021648468100465834
Batch  111  loss:  0.0002619252190925181
Batch  121  loss:  0.00016657150990795344
Batch  131  loss:  0.00011074764188379049
Batch  141  loss:  0.00012782940757460892
Batch  151  loss:  0.0003407686890568584
Batch  161  loss:  0.00012098284787498415
Batch  171  loss:  0.00016815736307762563
Batch  181  loss:  0.00015823706053197384
Batch  191  loss:  0.0002510136691853404
Validation on real data: 
LOSS supervised-train 0.00018559698131866752, valid 8.275671279989183e-05
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0002622381434775889
Batch  11  loss:  0.00019335909746587276
Batch  21  loss:  0.0001167472728411667
Batch  31  loss:  0.00012902062735520303
Batch  41  loss:  8.775420428719372e-05
Batch  51  loss:  0.00025555610773153603
Batch  61  loss:  0.00018417574756313115
Batch  71  loss:  0.00019014961435459554
Batch  81  loss:  0.0001598196104168892
Batch  91  loss:  0.00013840000610798597
Batch  101  loss:  0.00020621943986043334
Batch  111  loss:  0.00023954444623086601
Batch  121  loss:  0.0001655437081353739
Batch  131  loss:  0.000112302674097009
Batch  141  loss:  0.00012056640116497874
Batch  151  loss:  0.00032490145531482995
Batch  161  loss:  0.00010573841427685693
Batch  171  loss:  0.00024303104146383703
Batch  181  loss:  0.00016703325673006475
Batch  191  loss:  0.0001819586323108524
Validation on real data: 
LOSS supervised-train 0.00017673717073193984, valid 0.00010410137474536896
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.00021503168682102114
Batch  11  loss:  0.00015475480176974088
Batch  21  loss:  0.00011964911391260102
Batch  31  loss:  0.00011934414214920253
Batch  41  loss:  9.835155651671812e-05
Batch  51  loss:  0.00023372459691017866
Batch  61  loss:  0.00015546086069662124
Batch  71  loss:  0.00015744827396702021
Batch  81  loss:  0.0001592339394846931
Batch  91  loss:  0.00012713509204331785
Batch  101  loss:  0.0002532189537305385
Batch  111  loss:  0.0002679885074030608
Batch  121  loss:  0.00016877487360034138
Batch  131  loss:  9.944803605321795e-05
Batch  141  loss:  0.00012039787543471903
Batch  151  loss:  0.0002960561541840434
Batch  161  loss:  0.00012049419456161559
Batch  171  loss:  0.00018649968842510134
Batch  181  loss:  0.0001828958629630506
Batch  191  loss:  0.00023772216809447855
Validation on real data: 
LOSS supervised-train 0.0001670295444637304, valid 7.405284122796729e-05
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.00019009610696230084
Batch  11  loss:  0.0001556772767798975
Batch  21  loss:  0.00012641618377529085
Batch  31  loss:  0.00011644512414932251
Batch  41  loss:  7.994105544639751e-05
Batch  51  loss:  0.00017183696036227047
Batch  61  loss:  0.00013762789603788406
Batch  71  loss:  0.00018506168271414936
Batch  81  loss:  0.00017090777691919357
Batch  91  loss:  0.00011659767187666148
Batch  101  loss:  0.00019067466200795025
Batch  111  loss:  0.00026967262965627015
Batch  121  loss:  0.00014714566350448877
Batch  131  loss:  0.00010444747022120282
Batch  141  loss:  0.00012081522436346859
Batch  151  loss:  0.0003037315618712455
Batch  161  loss:  0.0001354953710688278
Batch  171  loss:  0.0002147269988199696
Batch  181  loss:  0.00015266938135027885
Batch  191  loss:  0.0001855579757830128
Validation on real data: 
LOSS supervised-train 0.00016114733454742236, valid 8.000694651855156e-05
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.00021916285913903266
Batch  11  loss:  0.0001568077423144132
Batch  21  loss:  0.00012281170347705483
Batch  31  loss:  9.629808482713997e-05
Batch  41  loss:  8.873917977325618e-05
Batch  51  loss:  0.00018693569290917367
Batch  61  loss:  0.0001609206519788131
Batch  71  loss:  0.0001281633594771847
Batch  81  loss:  0.00014846782141830772
Batch  91  loss:  0.00011665900819934905
Batch  101  loss:  0.000202984971110709
Batch  111  loss:  0.0002256550214951858
Batch  121  loss:  0.00015390643966384232
Batch  131  loss:  9.556760778650641e-05
Batch  141  loss:  0.00010761249723145738
Batch  151  loss:  0.00030670425621792674
Batch  161  loss:  0.0001171693584183231
Batch  171  loss:  0.00017963684513233602
Batch  181  loss:  0.00012835430970881134
Batch  191  loss:  0.00019097230688203126
Validation on real data: 
LOSS supervised-train 0.00015520472727075684, valid 7.434797589667141e-05
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.00019349480862729251
Batch  11  loss:  0.00014334132720250636
Batch  21  loss:  0.00011160977010149509
Batch  31  loss:  0.00010388399823568761
Batch  41  loss:  8.242078183684498e-05
Batch  51  loss:  0.00017644574108999223
Batch  61  loss:  0.00011201203597011045
Batch  71  loss:  0.00013913815200794488
Batch  81  loss:  0.00014034811465535313
Batch  91  loss:  0.00011424228432588279
Batch  101  loss:  0.00017583172302693129
Batch  111  loss:  0.00024745051632635295
Batch  121  loss:  0.000152652632095851
Batch  131  loss:  9.171563579002395e-05
Batch  141  loss:  0.0001246022729901597
Batch  151  loss:  0.0002381814265390858
Batch  161  loss:  0.00013318519631866366
Batch  171  loss:  0.00015966195496730506
Batch  181  loss:  0.00012095535203116015
Batch  191  loss:  0.00019698383403010666
Validation on real data: 
LOSS supervised-train 0.0001501864229430794, valid 0.00010223581921309233
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00018198348698206246
Batch  11  loss:  0.000139509720611386
Batch  21  loss:  0.00013436151493806392
Batch  31  loss:  0.0001207664972753264
Batch  41  loss:  8.453289774479344e-05
Batch  51  loss:  0.000163528326083906
Batch  61  loss:  0.00015247866394929588
Batch  71  loss:  0.0001307920174440369
Batch  81  loss:  0.00018479791469871998
Batch  91  loss:  0.00013063209189567715
Batch  101  loss:  0.00019041987252421677
Batch  111  loss:  0.00024879566626623273
Batch  121  loss:  0.00015933852409943938
Batch  131  loss:  8.991304639494047e-05
Batch  141  loss:  9.085970668820664e-05
Batch  151  loss:  0.0002648164227139205
Batch  161  loss:  0.00013583721010945737
Batch  171  loss:  0.00019400283053983003
Batch  181  loss:  0.0001259712444152683
Batch  191  loss:  0.00017947840387932956
Validation on real data: 
LOSS supervised-train 0.00014538639439706457, valid 8.468696614727378e-05
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00019388136570341885
Batch  11  loss:  0.00012499556760303676
Batch  21  loss:  0.00013216322986409068
Batch  31  loss:  0.00010193385242018849
Batch  41  loss:  8.52981538628228e-05
Batch  51  loss:  0.00016196999058593065
Batch  61  loss:  9.380723349750042e-05
Batch  71  loss:  0.00010756585834315047
Batch  81  loss:  0.0001280760479858145
Batch  91  loss:  0.00010946709517156705
Batch  101  loss:  0.00019918695033993572
Batch  111  loss:  0.0001870222040452063
Batch  121  loss:  0.000150292573380284
Batch  131  loss:  8.755687304073945e-05
Batch  141  loss:  0.0001013781875371933
Batch  151  loss:  0.00025117609766311944
Batch  161  loss:  0.00012833072105422616
Batch  171  loss:  0.00012620718916878104
Batch  181  loss:  0.00013003285857848823
Batch  191  loss:  0.00017353463044855744
Validation on real data: 
LOSS supervised-train 0.00013801746470562648, valid 7.879996701376513e-05
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.00017528152966406196
Batch  11  loss:  0.00013713772932533175
Batch  21  loss:  0.00010626661241985857
Batch  31  loss:  9.651409345678985e-05
Batch  41  loss:  7.424243813147768e-05
Batch  51  loss:  0.0001646659802645445
Batch  61  loss:  0.00011653986439341679
Batch  71  loss:  0.00012888479977846146
Batch  81  loss:  0.00011846782581415027
Batch  91  loss:  0.00011074664507759735
Batch  101  loss:  0.00019878841703757644
Batch  111  loss:  0.00021598616149276495
Batch  121  loss:  0.00013463148206938058
Batch  131  loss:  7.926177931949496e-05
Batch  141  loss:  0.00010752491652965546
Batch  151  loss:  0.0002524860028643161
Batch  161  loss:  0.00010445625957800075
Batch  171  loss:  0.00015636306488886476
Batch  181  loss:  0.00013667948951479048
Batch  191  loss:  0.00013434802531264722
Validation on real data: 
LOSS supervised-train 0.00013482530390319881, valid 0.00010348361684009433
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0001742129388730973
Batch  11  loss:  0.00012270532897673547
Batch  21  loss:  0.00010069989366456866
Batch  31  loss:  7.711940270382911e-05
Batch  41  loss:  8.504172728862613e-05
Batch  51  loss:  0.00014142818690743297
Batch  61  loss:  0.00013509659038390964
Batch  71  loss:  0.00010418226884212345
Batch  81  loss:  0.00016161434177774936
Batch  91  loss:  0.00010322922753402963
Batch  101  loss:  0.0001758219877956435
Batch  111  loss:  0.00018672853184398264
Batch  121  loss:  0.0001364159252261743
Batch  131  loss:  7.276311953319237e-05
Batch  141  loss:  9.805976878851652e-05
Batch  151  loss:  0.0002674301795195788
Batch  161  loss:  0.00013300403952598572
Batch  171  loss:  0.0001963067043107003
Batch  181  loss:  0.0001232761424034834
Batch  191  loss:  0.00015646520478185266
Validation on real data: 
LOSS supervised-train 0.00013288481957715703, valid 8.03147631813772e-05
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00016587815480306745
Batch  11  loss:  0.0001536987256258726
Batch  21  loss:  0.00010212600318482146
Batch  31  loss:  9.407073230249807e-05
Batch  41  loss:  6.764020508853719e-05
Batch  51  loss:  0.00016710405179765075
Batch  61  loss:  0.00011735816951841116
Batch  71  loss:  0.00012016235268674791
Batch  81  loss:  0.00012581099872477353
Batch  91  loss:  0.0001257905678357929
Batch  101  loss:  0.0001867417013272643
Batch  111  loss:  0.00017867669521365315
Batch  121  loss:  0.0001300368894590065
Batch  131  loss:  7.477828330593184e-05
Batch  141  loss:  0.00010564427793724462
Batch  151  loss:  0.0002209092053817585
Batch  161  loss:  0.00010155010386370122
Batch  171  loss:  0.00011884531704708934
Batch  181  loss:  0.00014755823940504342
Batch  191  loss:  0.00014286715304479003
Validation on real data: 
LOSS supervised-train 0.0001282662148878444, valid 8.93677570275031e-05
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0001524411782156676
Batch  11  loss:  0.00011851989256683737
Batch  21  loss:  9.90287953754887e-05
Batch  31  loss:  7.409034878946841e-05
Batch  41  loss:  7.01110257068649e-05
Batch  51  loss:  0.0001383896014885977
Batch  61  loss:  0.00013071685680188239
Batch  71  loss:  0.00010550933075137436
Batch  81  loss:  0.00012399186380207539
Batch  91  loss:  0.00011968133912887424
Batch  101  loss:  0.00015376947703771293
Batch  111  loss:  0.00017167303303722292
Batch  121  loss:  0.0001268939522560686
Batch  131  loss:  8.538990368833765e-05
Batch  141  loss:  0.00010627666051732376
Batch  151  loss:  0.00021941251179669052
Batch  161  loss:  0.00011475028441054747
Batch  171  loss:  0.00013118472998030484
Batch  181  loss:  0.00014687300426885486
Batch  191  loss:  0.00014203801401890814
Validation on real data: 
LOSS supervised-train 0.0001223403685435187, valid 6.910957745276392e-05
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00016392947873100638
Batch  11  loss:  0.00011227739742025733
Batch  21  loss:  9.571561531629413e-05
Batch  31  loss:  9.048972424352542e-05
Batch  41  loss:  7.999569788808003e-05
Batch  51  loss:  0.00014131772331893444
Batch  61  loss:  0.00010646195005392656
Batch  71  loss:  9.74402719293721e-05
Batch  81  loss:  0.00012378294195514172
Batch  91  loss:  9.533207776257768e-05
Batch  101  loss:  0.00014885238488204777
Batch  111  loss:  0.00017629281501285732
Batch  121  loss:  0.000142124539706856
Batch  131  loss:  7.59530666982755e-05
Batch  141  loss:  9.245968976756558e-05
Batch  151  loss:  0.00021608636598102748
Batch  161  loss:  9.971245162887499e-05
Batch  171  loss:  0.00013592421601060778
Batch  181  loss:  0.00011362135410308838
Batch  191  loss:  0.00012654371676035225
Validation on real data: 
LOSS supervised-train 0.0001202067757367331, valid 8.233109110733494e-05
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0001240487035829574
Batch  11  loss:  0.00010994928743457422
Batch  21  loss:  9.293535549659282e-05
Batch  31  loss:  8.44508977024816e-05
Batch  41  loss:  8.098952093860134e-05
Batch  51  loss:  0.0001427226816304028
Batch  61  loss:  0.00010894244041992351
Batch  71  loss:  0.0001101579109672457
Batch  81  loss:  0.00010901268251473084
Batch  91  loss:  9.760744433151558e-05
Batch  101  loss:  0.00012428521586116403
Batch  111  loss:  0.00016980651707854122
Batch  121  loss:  0.00011220168380532414
Batch  131  loss:  9.74907525232993e-05
Batch  141  loss:  0.00010091502190334722
Batch  151  loss:  0.00020091491751372814
Batch  161  loss:  0.00010941353684756905
Batch  171  loss:  0.0001250459608854726
Batch  181  loss:  0.00011616373376455158
Batch  191  loss:  0.00012030539801344275
Validation on real data: 
LOSS supervised-train 0.00011557839718079777, valid 9.06328932614997e-05
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00013571720046456903
Batch  11  loss:  0.0001207620371133089
Batch  21  loss:  0.00013299411511979997
Batch  31  loss:  9.232443699147552e-05
Batch  41  loss:  7.548619760200381e-05
Batch  51  loss:  0.00017235343693755567
Batch  61  loss:  0.00010149548324989155
Batch  71  loss:  9.137362940236926e-05
Batch  81  loss:  0.00010348339128540829
Batch  91  loss:  9.790541662368923e-05
Batch  101  loss:  0.0001161449690698646
Batch  111  loss:  0.00013984862016513944
Batch  121  loss:  9.55897630774416e-05
Batch  131  loss:  8.161320874933153e-05
Batch  141  loss:  9.476464038016275e-05
Batch  151  loss:  0.0001964875846169889
Batch  161  loss:  8.610797522123903e-05
Batch  171  loss:  0.00013514967577066272
Batch  181  loss:  0.00011095160152763128
Batch  191  loss:  0.00011995888780802488
Validation on real data: 
LOSS supervised-train 0.00011359616755726165, valid 8.257009903900325e-05
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0001432352582924068
Batch  11  loss:  0.0001089385332306847
Batch  21  loss:  0.00010415725409984589
Batch  31  loss:  6.447031046263874e-05
Batch  41  loss:  7.509282295359299e-05
Batch  51  loss:  0.00015347922453656793
Batch  61  loss:  0.00010063124500447884
Batch  71  loss:  8.050525502767414e-05
Batch  81  loss:  0.00011775634629884735
Batch  91  loss:  9.919542935676873e-05
Batch  101  loss:  0.00012145830987719819
Batch  111  loss:  0.0001424003130523488
Batch  121  loss:  0.0001207753230119124
Batch  131  loss:  6.9308043748606e-05
Batch  141  loss:  0.00010236965317744762
Batch  151  loss:  0.00017513449711259454
Batch  161  loss:  9.114063868764788e-05
Batch  171  loss:  0.00010900569031946361
Batch  181  loss:  0.0001047084660967812
Batch  191  loss:  0.0001448678522137925
Validation on real data: 
LOSS supervised-train 0.00010782123539684108, valid 7.724224997218698e-05
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00011514447396621108
Batch  11  loss:  9.584400686435401e-05
Batch  21  loss:  8.484991121804342e-05
Batch  31  loss:  8.398654608754441e-05
Batch  41  loss:  6.371165363816544e-05
Batch  51  loss:  0.00010704556189011782
Batch  61  loss:  8.845962292980403e-05
Batch  71  loss:  8.162310405168682e-05
Batch  81  loss:  0.00010096542973769829
Batch  91  loss:  8.398245699936524e-05
Batch  101  loss:  0.0001603440468898043
Batch  111  loss:  0.00016574528126511723
Batch  121  loss:  0.0001315312401857227
Batch  131  loss:  6.705392297590151e-05
Batch  141  loss:  9.729700104799122e-05
Batch  151  loss:  0.00022865641221869737
Batch  161  loss:  8.079405961325392e-05
Batch  171  loss:  0.00012697410420514643
Batch  181  loss:  0.00011365138925611973
Batch  191  loss:  0.00013134662003722042
Validation on real data: 
LOSS supervised-train 0.00010885469888307852, valid 9.803706780076027e-05
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00014364854723680764
Batch  11  loss:  9.983236668631434e-05
Batch  21  loss:  9.660649811848998e-05
Batch  31  loss:  7.516817277064547e-05
Batch  41  loss:  6.661534280283377e-05
Batch  51  loss:  0.00013141021190676838
Batch  61  loss:  0.00010632325574988499
Batch  71  loss:  8.306088420795277e-05
Batch  81  loss:  0.00012045260518789291
Batch  91  loss:  9.988269448513165e-05
Batch  101  loss:  0.00014777436445001513
Batch  111  loss:  0.00014020760136190802
Batch  121  loss:  0.0001021596763166599
Batch  131  loss:  5.9953432355541736e-05
Batch  141  loss:  8.443733531748876e-05
Batch  151  loss:  0.00020054192282259464
Batch  161  loss:  9.569005487719551e-05
Batch  171  loss:  0.00013065132952760905
Batch  181  loss:  9.815033263294026e-05
Batch  191  loss:  0.00014106668822932988
Validation on real data: 
LOSS supervised-train 0.0001052872648142511, valid 8.17434411146678e-05
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.00014897747314535081
Batch  11  loss:  0.00011472640471765772
Batch  21  loss:  9.595734445611015e-05
Batch  31  loss:  6.083627886255272e-05
Batch  41  loss:  6.427375774364918e-05
Batch  51  loss:  0.00012168916873633862
Batch  61  loss:  0.00010375741840107366
Batch  71  loss:  9.186906390823424e-05
Batch  81  loss:  0.00011551044735824689
Batch  91  loss:  9.82324781944044e-05
Batch  101  loss:  0.00013799499720335007
Batch  111  loss:  0.00014845507394056767
Batch  121  loss:  9.316360956290737e-05
Batch  131  loss:  7.987779099494219e-05
Batch  141  loss:  7.286883919732645e-05
Batch  151  loss:  0.00021390867186710238
Batch  161  loss:  8.802149386610836e-05
Batch  171  loss:  0.00016557899652980268
Batch  181  loss:  9.967491496354342e-05
Batch  191  loss:  0.00011365601676516235
Validation on real data: 
LOSS supervised-train 0.0001042696868898929, valid 7.99070403445512e-05
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00013298615522217005
Batch  11  loss:  0.00011315728625049815
Batch  21  loss:  8.285110379802063e-05
Batch  31  loss:  7.407096563838422e-05
Batch  41  loss:  6.32673327345401e-05
Batch  51  loss:  0.00012835719098802656
Batch  61  loss:  0.00010462574573466554
Batch  71  loss:  9.745128045324236e-05
Batch  81  loss:  9.379519906360656e-05
Batch  91  loss:  9.552121628075838e-05
Batch  101  loss:  0.00012477621203288436
Batch  111  loss:  0.00011370891297701746
Batch  121  loss:  0.0001161919062724337
Batch  131  loss:  6.763722922187299e-05
Batch  141  loss:  7.658095273654908e-05
Batch  151  loss:  0.00016773886454757303
Batch  161  loss:  8.796827751211822e-05
Batch  171  loss:  0.0001352236868115142
Batch  181  loss:  0.00011777589679695666
Batch  191  loss:  0.00012286192213650793
Validation on real data: 
LOSS supervised-train 0.00010065741624202928, valid 6.913176184752956e-05
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.00011927635932806879
Batch  11  loss:  8.206910570152104e-05
Batch  21  loss:  9.696891356725246e-05
Batch  31  loss:  6.538455636473373e-05
Batch  41  loss:  6.63885148242116e-05
Batch  51  loss:  0.00011892800830537453
Batch  61  loss:  0.00010382164327893406
Batch  71  loss:  8.868287113728002e-05
Batch  81  loss:  0.00011910402099601924
Batch  91  loss:  9.244428656529635e-05
Batch  101  loss:  0.00012452233931981027
Batch  111  loss:  0.00016687787137925625
Batch  121  loss:  0.00010827297228388488
Batch  131  loss:  7.890271808719262e-05
Batch  141  loss:  9.843688167165965e-05
Batch  151  loss:  0.00014206470223143697
Batch  161  loss:  8.619896834716201e-05
Batch  171  loss:  9.786588634597138e-05
Batch  181  loss:  9.286444401368499e-05
Batch  191  loss:  0.00011704992357408628
Validation on real data: 
LOSS supervised-train 9.866013149803621e-05, valid 8.105602319119498e-05
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00010691657371353358
Batch  11  loss:  7.647195889148861e-05
Batch  21  loss:  8.43260859255679e-05
Batch  31  loss:  6.985428626649082e-05
Batch  41  loss:  7.755788101349026e-05
Batch  51  loss:  0.00011346686369506642
Batch  61  loss:  0.00012017625704174861
Batch  71  loss:  7.496040780097246e-05
Batch  81  loss:  0.00010387338261352852
Batch  91  loss:  9.591063280822709e-05
Batch  101  loss:  0.00012708714348264039
Batch  111  loss:  0.00012367728049866855
Batch  121  loss:  0.00010724726598709822
Batch  131  loss:  7.006577652646229e-05
Batch  141  loss:  8.131111098919064e-05
Batch  151  loss:  0.0001665806194068864
Batch  161  loss:  9.609956759959459e-05
Batch  171  loss:  0.0001235230447491631
Batch  181  loss:  0.00010126191773451865
Batch  191  loss:  8.892434561857954e-05
Validation on real data: 
LOSS supervised-train 9.669640994616202e-05, valid 7.67220335546881e-05
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.00011205224291188642
Batch  11  loss:  7.620640826644376e-05
Batch  21  loss:  8.20761124487035e-05
Batch  31  loss:  7.620176620548591e-05
Batch  41  loss:  6.368672620737925e-05
Batch  51  loss:  0.00011607391206780449
Batch  61  loss:  0.00010122575622517616
Batch  71  loss:  7.668347825529054e-05
Batch  81  loss:  9.056807903107256e-05
Batch  91  loss:  9.0217828983441e-05
Batch  101  loss:  0.00012089641677448526
Batch  111  loss:  0.00011312439892208204
Batch  121  loss:  0.00011250461102463305
Batch  131  loss:  5.8631030697142705e-05
Batch  141  loss:  7.56661465857178e-05
Batch  151  loss:  0.00015487897326238453
Batch  161  loss:  7.81213675509207e-05
Batch  171  loss:  0.00010738646960817277
Batch  181  loss:  8.070634794421494e-05
Batch  191  loss:  0.00011840131628559902
Validation on real data: 
LOSS supervised-train 9.308757267717738e-05, valid 6.051509262761101e-05
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00011195120168849826
Batch  11  loss:  8.36369363241829e-05
Batch  21  loss:  7.704146992182359e-05
Batch  31  loss:  5.866763603989966e-05
Batch  41  loss:  6.709525041515008e-05
Batch  51  loss:  0.00010474578448338434
Batch  61  loss:  9.634719754103571e-05
Batch  71  loss:  6.833861698396504e-05
Batch  81  loss:  8.721004996914417e-05
Batch  91  loss:  8.201131277019158e-05
Batch  101  loss:  0.00012157807213952765
Batch  111  loss:  0.00012978962331544608
Batch  121  loss:  0.00010654090874595568
Batch  131  loss:  6.870700599392876e-05
Batch  141  loss:  0.00010199831740465015
Batch  151  loss:  0.00016411856631748378
Batch  161  loss:  8.209533552872017e-05
Batch  171  loss:  0.00010022336209658533
Batch  181  loss:  9.093554399441928e-05
Batch  191  loss:  9.520370076643303e-05
Validation on real data: 
LOSS supervised-train 9.344221414721688e-05, valid 6.317943189060315e-05
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  9.232905722456053e-05
Batch  11  loss:  7.52312844269909e-05
Batch  21  loss:  8.749409607844427e-05
Batch  31  loss:  5.7699846365721896e-05
Batch  41  loss:  7.191423355834559e-05
Batch  51  loss:  0.0001101589368772693
Batch  61  loss:  0.0001020132694975473
Batch  71  loss:  9.883016173262149e-05
Batch  81  loss:  0.00010507029219297692
Batch  91  loss:  8.381211227970198e-05
Batch  101  loss:  0.00011249136150581762
Batch  111  loss:  9.536447032587603e-05
Batch  121  loss:  8.955035445978865e-05
Batch  131  loss:  6.81438177707605e-05
Batch  141  loss:  7.273393566720188e-05
Batch  151  loss:  0.0001589590683579445
Batch  161  loss:  8.283263014163822e-05
Batch  171  loss:  0.00012013530067633837
Batch  181  loss:  9.291668538935483e-05
Batch  191  loss:  7.938867202028632e-05
Validation on real data: 
LOSS supervised-train 8.944545113990898e-05, valid 6.720928649883717e-05
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00010233333159703761
Batch  11  loss:  7.695967360632494e-05
Batch  21  loss:  8.866338612278923e-05
Batch  31  loss:  5.913881614105776e-05
Batch  41  loss:  8.608627103967592e-05
Batch  51  loss:  0.00011266106594121084
Batch  61  loss:  8.904949208954349e-05
Batch  71  loss:  7.200315303634852e-05
Batch  81  loss:  8.238862210419029e-05
Batch  91  loss:  7.93004292063415e-05
Batch  101  loss:  0.00010753487731562927
Batch  111  loss:  0.0001160280080512166
Batch  121  loss:  0.00010164035484194756
Batch  131  loss:  7.190071482909843e-05
Batch  141  loss:  7.120868394849822e-05
Batch  151  loss:  0.00014652693062089384
Batch  161  loss:  7.395148713840172e-05
Batch  171  loss:  9.319888340542093e-05
Batch  181  loss:  8.7913686002139e-05
Batch  191  loss:  7.558584184153005e-05
Validation on real data: 
LOSS supervised-train 8.748880854909657e-05, valid 6.48119967081584e-05
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  9.612636495148763e-05
Batch  11  loss:  8.722738857613876e-05
Batch  21  loss:  8.779722702456638e-05
Batch  31  loss:  5.6063232477754354e-05
Batch  41  loss:  5.579599383054301e-05
Batch  51  loss:  0.00010409658716525882
Batch  61  loss:  7.842058403184637e-05
Batch  71  loss:  7.206103327916935e-05
Batch  81  loss:  0.00010338939318899065
Batch  91  loss:  6.952090188860893e-05
Batch  101  loss:  0.0001151150354417041
Batch  111  loss:  0.00011842822277685627
Batch  121  loss:  8.856583008309826e-05
Batch  131  loss:  6.2640305259265e-05
Batch  141  loss:  8.609545329818502e-05
Batch  151  loss:  0.00013567422865889966
Batch  161  loss:  9.422449511475861e-05
Batch  171  loss:  0.00010502065561013296
Batch  181  loss:  8.95809571375139e-05
Batch  191  loss:  8.710330439498648e-05
Validation on real data: 
LOSS supervised-train 8.580499939853326e-05, valid 5.929706458118744e-05
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00013727157784160227
Batch  11  loss:  7.010816625552252e-05
Batch  21  loss:  5.796977347927168e-05
Batch  31  loss:  5.392123421188444e-05
Batch  41  loss:  6.252902676351368e-05
Batch  51  loss:  0.00011532109056133777
Batch  61  loss:  0.00010027671669377014
Batch  71  loss:  7.478756015188992e-05
Batch  81  loss:  7.618786912644282e-05
Batch  91  loss:  7.370325329247862e-05
Batch  101  loss:  0.0001228439505212009
Batch  111  loss:  0.00011798689956776798
Batch  121  loss:  0.00010194528294960037
Batch  131  loss:  6.271374149946496e-05
Batch  141  loss:  7.145000563468784e-05
Batch  151  loss:  0.00014149259368423373
Batch  161  loss:  7.432791608152911e-05
Batch  171  loss:  0.00011378687486285344
Batch  181  loss:  9.667500853538513e-05
Batch  191  loss:  7.55528308218345e-05
Validation on real data: 
LOSS supervised-train 8.430139250776847e-05, valid 6.626214599236846e-05
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  8.304168295580894e-05
Batch  11  loss:  7.451885903719813e-05
Batch  21  loss:  7.100724178599194e-05
Batch  31  loss:  5.794468961539678e-05
Batch  41  loss:  6.86306448187679e-05
Batch  51  loss:  0.00011406637349864468
Batch  61  loss:  7.997590000741184e-05
Batch  71  loss:  9.333480556961149e-05
Batch  81  loss:  9.43649429245852e-05
Batch  91  loss:  7.565495616290718e-05
Batch  101  loss:  9.60634570219554e-05
Batch  111  loss:  0.00010832137922989205
Batch  121  loss:  9.637562470743433e-05
Batch  131  loss:  5.140493522048928e-05
Batch  141  loss:  7.009365799603984e-05
Batch  151  loss:  0.00016771111404523253
Batch  161  loss:  6.984420178923756e-05
Batch  171  loss:  9.66506777331233e-05
Batch  181  loss:  9.033717651618645e-05
Batch  191  loss:  6.843649316579103e-05
Validation on real data: 
LOSS supervised-train 8.368008873731014e-05, valid 6.197772017912939e-05
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00010310297511750832
Batch  11  loss:  7.695957174291834e-05
Batch  21  loss:  8.508034807164222e-05
Batch  31  loss:  5.2437670092331246e-05
Batch  41  loss:  6.302149267867208e-05
Batch  51  loss:  9.285577107220888e-05
Batch  61  loss:  8.688295929459855e-05
Batch  71  loss:  6.436607509385794e-05
Batch  81  loss:  8.903681009542197e-05
Batch  91  loss:  7.209338218672201e-05
Batch  101  loss:  9.673549357103184e-05
Batch  111  loss:  9.650527499616146e-05
Batch  121  loss:  9.637435869080946e-05
Batch  131  loss:  4.8357120249420404e-05
Batch  141  loss:  7.448177348123863e-05
Batch  151  loss:  0.00014044040290173143
Batch  161  loss:  7.165000715758651e-05
Batch  171  loss:  0.00012044468894600868
Batch  181  loss:  8.452036854578182e-05
Batch  191  loss:  8.544712181901559e-05
Validation on real data: 
LOSS supervised-train 8.061123107836466e-05, valid 7.286571053555235e-05
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  8.047678420552984e-05
Batch  11  loss:  6.964884232729673e-05
Batch  21  loss:  6.943658809177577e-05
Batch  31  loss:  5.9429366956464946e-05
Batch  41  loss:  6.40607686364092e-05
Batch  51  loss:  9.92958593997173e-05
Batch  61  loss:  8.695288124727085e-05
Batch  71  loss:  6.93986366968602e-05
Batch  81  loss:  9.644852980272844e-05
Batch  91  loss:  8.345983951585367e-05
Batch  101  loss:  0.000104258579085581
Batch  111  loss:  0.00010168662265641615
Batch  121  loss:  9.260496881324798e-05
Batch  131  loss:  6.013304300722666e-05
Batch  141  loss:  6.468476931331679e-05
Batch  151  loss:  0.00012327716103754938
Batch  161  loss:  7.000748155405745e-05
Batch  171  loss:  0.00012248048733454198
Batch  181  loss:  7.655614899704233e-05
Batch  191  loss:  8.394982432946563e-05
Validation on real data: 
LOSS supervised-train 8.07755234563956e-05, valid 7.408444071188569e-05
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  8.131312642944977e-05
Batch  11  loss:  7.216382800834253e-05
Batch  21  loss:  6.400418351404369e-05
Batch  31  loss:  6.3449697336182e-05
Batch  41  loss:  6.467843195423484e-05
Batch  51  loss:  0.00010635235958034173
Batch  61  loss:  8.759638876654208e-05
Batch  71  loss:  7.754187390673906e-05
Batch  81  loss:  8.019585948204622e-05
Batch  91  loss:  8.82898602867499e-05
Batch  101  loss:  9.272776514990255e-05
Batch  111  loss:  9.281413076678291e-05
Batch  121  loss:  0.0001018055627355352
Batch  131  loss:  6.473778921645135e-05
Batch  141  loss:  7.374263077508658e-05
Batch  151  loss:  0.0001394222490489483
Batch  161  loss:  7.694748637732118e-05
Batch  171  loss:  0.0001118098953156732
Batch  181  loss:  8.332025026902556e-05
Batch  191  loss:  7.964333053678274e-05
Validation on real data: 
LOSS supervised-train 7.909367786851362e-05, valid 6.457662675529718e-05
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  8.101209095912054e-05
Batch  11  loss:  7.822723273420706e-05
Batch  21  loss:  7.992152677616104e-05
Batch  31  loss:  7.440830086125061e-05
Batch  41  loss:  6.36288314126432e-05
Batch  51  loss:  0.00010290911450283602
Batch  61  loss:  7.940083742141724e-05
Batch  71  loss:  7.922671647975221e-05
Batch  81  loss:  7.534631004091352e-05
Batch  91  loss:  6.535031570820138e-05
Batch  101  loss:  0.00010922680667135864
Batch  111  loss:  7.674607331864536e-05
Batch  121  loss:  7.144616392906755e-05
Batch  131  loss:  5.339700146578252e-05
Batch  141  loss:  6.023374226060696e-05
Batch  151  loss:  0.00013174937339499593
Batch  161  loss:  6.691949965897948e-05
Batch  171  loss:  8.910283213481307e-05
Batch  181  loss:  8.765060192672536e-05
Batch  191  loss:  0.0001073909443221055
Validation on real data: 
LOSS supervised-train 7.959948603456723e-05, valid 7.398240268230438e-05
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0001016863388940692
Batch  11  loss:  6.98940348229371e-05
Batch  21  loss:  7.13324043317698e-05
Batch  31  loss:  6.660529470536858e-05
Batch  41  loss:  7.366806676145643e-05
Batch  51  loss:  0.00010055566963274032
Batch  61  loss:  8.575735409976915e-05
Batch  71  loss:  7.88659235695377e-05
Batch  81  loss:  8.331805292982608e-05
Batch  91  loss:  7.424792420351878e-05
Batch  101  loss:  9.033606329467148e-05
Batch  111  loss:  9.096814756048843e-05
Batch  121  loss:  7.93593208072707e-05
Batch  131  loss:  5.482263441081159e-05
Batch  141  loss:  6.213437882252038e-05
Batch  151  loss:  0.00014831694716122001
Batch  161  loss:  7.16603608452715e-05
Batch  171  loss:  9.352910274174064e-05
Batch  181  loss:  8.90940718818456e-05
Batch  191  loss:  7.666840247111395e-05
Validation on real data: 
LOSS supervised-train 7.837338796889525e-05, valid 7.053461740724742e-05
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  8.852733299136162e-05
Batch  11  loss:  6.666473200311884e-05
Batch  21  loss:  5.980581045150757e-05
Batch  31  loss:  6.090905299060978e-05
Batch  41  loss:  5.9917994803981856e-05
Batch  51  loss:  7.546004053438082e-05
Batch  61  loss:  8.133359369821846e-05
Batch  71  loss:  6.472451786976308e-05
Batch  81  loss:  8.566525502828881e-05
Batch  91  loss:  7.447697134921327e-05
Batch  101  loss:  9.055774717126042e-05
Batch  111  loss:  7.493720477214083e-05
Batch  121  loss:  8.858440560288727e-05
Batch  131  loss:  6.472539826063439e-05
Batch  141  loss:  5.676847285940312e-05
Batch  151  loss:  0.00012151288683526218
Batch  161  loss:  7.101955998223275e-05
Batch  171  loss:  0.00010334102262277156
Batch  181  loss:  6.730403401888907e-05
Batch  191  loss:  7.143160473788157e-05
Validation on real data: 
LOSS supervised-train 7.369720386122935e-05, valid 6.251663580769673e-05
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  8.491664630128071e-05
Batch  11  loss:  6.695036427117884e-05
Batch  21  loss:  7.7695949585177e-05
Batch  31  loss:  5.3212643251754344e-05
Batch  41  loss:  5.520622289623134e-05
Batch  51  loss:  0.00011836109479190782
Batch  61  loss:  7.799408922437578e-05
Batch  71  loss:  6.844327435828745e-05
Batch  81  loss:  7.731112418696284e-05
Batch  91  loss:  6.171176210045815e-05
Batch  101  loss:  9.72031120909378e-05
Batch  111  loss:  8.604313916293904e-05
Batch  121  loss:  9.930379746947438e-05
Batch  131  loss:  4.7991667088354006e-05
Batch  141  loss:  5.7993183872895315e-05
Batch  151  loss:  0.00012585391232278198
Batch  161  loss:  6.840433343313634e-05
Batch  171  loss:  9.703727118903771e-05
Batch  181  loss:  7.697910041315481e-05
Batch  191  loss:  7.149827433750033e-05
Validation on real data: 
LOSS supervised-train 7.436610609147465e-05, valid 5.9765799960587174e-05
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  7.45370562071912e-05
Batch  11  loss:  6.61101148580201e-05
Batch  21  loss:  6.78536671330221e-05
Batch  31  loss:  5.686544318450615e-05
Batch  41  loss:  6.844665040262043e-05
Batch  51  loss:  7.987273420440033e-05
Batch  61  loss:  7.027505489531904e-05
Batch  71  loss:  7.334601104957983e-05
Batch  81  loss:  7.898311741882935e-05
Batch  91  loss:  6.381155253620818e-05
Batch  101  loss:  9.556170698488131e-05
Batch  111  loss:  9.48678789427504e-05
Batch  121  loss:  0.00012674798199441284
Batch  131  loss:  5.409196819528006e-05
Batch  141  loss:  6.648753333138302e-05
Batch  151  loss:  0.00014755126903764904
Batch  161  loss:  6.458047573687509e-05
Batch  171  loss:  9.467076597502455e-05
Batch  181  loss:  7.115959306247532e-05
Batch  191  loss:  9.239207429345697e-05
Validation on real data: 
LOSS supervised-train 7.476509528714815e-05, valid 5.918190800002776e-05
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  5.1100421842420474e-05
Batch  11  loss:  6.342005508486181e-05
Batch  21  loss:  8.513011562172323e-05
Batch  31  loss:  4.9313446652377024e-05
Batch  41  loss:  5.6179527746280655e-05
Batch  51  loss:  7.65336662880145e-05
Batch  61  loss:  7.929121784400195e-05
Batch  71  loss:  5.688290548278019e-05
Batch  81  loss:  6.57254786347039e-05
Batch  91  loss:  6.293928890954703e-05
Batch  101  loss:  9.588881221134216e-05
Batch  111  loss:  9.469227370573208e-05
Batch  121  loss:  7.57195011829026e-05
Batch  131  loss:  6.079664308344945e-05
Batch  141  loss:  5.849258741363883e-05
Batch  151  loss:  0.0001285548642044887
Batch  161  loss:  5.4525648010894656e-05
Batch  171  loss:  7.799683226039633e-05
Batch  181  loss:  6.80712764733471e-05
Batch  191  loss:  6.58415156067349e-05
Validation on real data: 
LOSS supervised-train 7.111089207683108e-05, valid 5.462599074235186e-05
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  7.652582280570641e-05
Batch  11  loss:  6.531783583341166e-05
Batch  21  loss:  8.208424696931615e-05
Batch  31  loss:  4.905209789285436e-05
Batch  41  loss:  6.447924533858895e-05
Batch  51  loss:  8.962316496763378e-05
Batch  61  loss:  8.640187297714874e-05
Batch  71  loss:  7.592055771965533e-05
Batch  81  loss:  6.80022785672918e-05
Batch  91  loss:  6.564997602254152e-05
Batch  101  loss:  7.746994378976524e-05
Batch  111  loss:  9.529784438200295e-05
Batch  121  loss:  7.805664063198492e-05
Batch  131  loss:  5.1018625526921824e-05
Batch  141  loss:  6.154239235911518e-05
Batch  151  loss:  0.00013438505993690342
Batch  161  loss:  6.941411265870556e-05
Batch  171  loss:  0.000101842699223198
Batch  181  loss:  6.774568464607e-05
Batch  191  loss:  6.745010614395142e-05
Validation on real data: 
LOSS supervised-train 7.105569195118732e-05, valid 5.60924454475753e-05
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  6.785660661989823e-05
Batch  11  loss:  6.052521348465234e-05
Batch  21  loss:  8.371502190129831e-05
Batch  31  loss:  5.656741268467158e-05
Batch  41  loss:  4.923247979604639e-05
Batch  51  loss:  9.705193224363029e-05
Batch  61  loss:  8.67123671923764e-05
Batch  71  loss:  6.11634022789076e-05
Batch  81  loss:  7.467668910976499e-05
Batch  91  loss:  5.9992460592184216e-05
Batch  101  loss:  6.600658525712788e-05
Batch  111  loss:  9.735969797475263e-05
Batch  121  loss:  8.675696881255135e-05
Batch  131  loss:  4.6091263357084244e-05
Batch  141  loss:  5.819712532684207e-05
Batch  151  loss:  0.00012479462020564824
Batch  161  loss:  5.984458766761236e-05
Batch  171  loss:  7.063093653414398e-05
Batch  181  loss:  7.649238978046924e-05
Batch  191  loss:  5.824573236168362e-05
Validation on real data: 
LOSS supervised-train 6.895718612213386e-05, valid 5.7913865020964295e-05
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  8.222951146308333e-05
Batch  11  loss:  5.566170148085803e-05
Batch  21  loss:  7.061692303977907e-05
Batch  31  loss:  3.837616168311797e-05
Batch  41  loss:  5.239704842097126e-05
Batch  51  loss:  8.603878814028576e-05
Batch  61  loss:  7.400483445962891e-05
Batch  71  loss:  6.561737245647237e-05
Batch  81  loss:  6.27737827016972e-05
Batch  91  loss:  6.181187927722931e-05
Batch  101  loss:  8.643866021884605e-05
Batch  111  loss:  7.845442451070994e-05
Batch  121  loss:  7.472909055650234e-05
Batch  131  loss:  4.910932693746872e-05
Batch  141  loss:  5.612934910459444e-05
Batch  151  loss:  0.00011145503231091425
Batch  161  loss:  5.202064130571671e-05
Batch  171  loss:  9.029803914017975e-05
Batch  181  loss:  6.807545287301764e-05
Batch  191  loss:  7.30038809706457e-05
Validation on real data: 
LOSS supervised-train 6.913397388416342e-05, valid 6.359008693834767e-05
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  8.228013757616282e-05
Batch  11  loss:  6.445740291383117e-05
Batch  21  loss:  6.142048368928954e-05
Batch  31  loss:  5.005575076211244e-05
Batch  41  loss:  5.3062096412759274e-05
Batch  51  loss:  7.59790709707886e-05
Batch  61  loss:  6.794683577027172e-05
Batch  71  loss:  6.033079625922255e-05
Batch  81  loss:  6.835185195086524e-05
Batch  91  loss:  6.987165397731587e-05
Batch  101  loss:  9.098417649511248e-05
Batch  111  loss:  8.552200597478077e-05
Batch  121  loss:  7.473138248315081e-05
Batch  131  loss:  5.52115379832685e-05
Batch  141  loss:  4.728981730295345e-05
Batch  151  loss:  0.00010574616317171603
Batch  161  loss:  5.889082603971474e-05
Batch  171  loss:  0.00010081434447783977
Batch  181  loss:  6.564616342075169e-05
Batch  191  loss:  8.437386713922024e-05
Validation on real data: 
LOSS supervised-train 6.637943388341228e-05, valid 5.8724537666421384e-05
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  5.8094308769796044e-05
Batch  11  loss:  5.2071958634769544e-05
Batch  21  loss:  5.812739982502535e-05
Batch  31  loss:  4.0365939639741555e-05
Batch  41  loss:  6.069476512493566e-05
Batch  51  loss:  7.216892845463008e-05
Batch  61  loss:  8.164224709616974e-05
Batch  71  loss:  5.705172225134447e-05
Batch  81  loss:  6.513647531392053e-05
Batch  91  loss:  6.530743121402338e-05
Batch  101  loss:  0.00010275534441461787
Batch  111  loss:  8.083550346782431e-05
Batch  121  loss:  7.61624687584117e-05
Batch  131  loss:  4.6867458877386525e-05
Batch  141  loss:  6.174368172651157e-05
Batch  151  loss:  8.525122393621132e-05
Batch  161  loss:  5.183588291401975e-05
Batch  171  loss:  7.94012812548317e-05
Batch  181  loss:  8.272666309494525e-05
Batch  191  loss:  6.592475983779877e-05
Validation on real data: 
LOSS supervised-train 6.634325654886197e-05, valid 4.522894596448168e-05
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  6.930913514224812e-05
Batch  11  loss:  6.313117046374828e-05
Batch  21  loss:  6.274379848036915e-05
Batch  31  loss:  4.032864308101125e-05
Batch  41  loss:  4.308827919885516e-05
Batch  51  loss:  9.891708759823814e-05
Batch  61  loss:  6.620989734074101e-05
Batch  71  loss:  6.498458969872445e-05
Batch  81  loss:  6.578037573490292e-05
Batch  91  loss:  5.997916014166549e-05
Batch  101  loss:  8.388351125176996e-05
Batch  111  loss:  7.550707960035652e-05
Batch  121  loss:  8.025241550058126e-05
Batch  131  loss:  4.356192221166566e-05
Batch  141  loss:  5.587107443716377e-05
Batch  151  loss:  0.00010974226461257786
Batch  161  loss:  6.319655221886933e-05
Batch  171  loss:  7.525247929152101e-05
Batch  181  loss:  7.98403489170596e-05
Batch  191  loss:  6.261145608732477e-05
Validation on real data: 
LOSS supervised-train 6.686693386654951e-05, valid 5.710896220989525e-05
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  8.041313412832096e-05
Batch  11  loss:  6.002399459248409e-05
Batch  21  loss:  7.220540283014998e-05
Batch  31  loss:  4.865336086368188e-05
Batch  41  loss:  5.9818725276272744e-05
Batch  51  loss:  9.805926674744114e-05
Batch  61  loss:  6.380199920386076e-05
Batch  71  loss:  4.8922214773483574e-05
Batch  81  loss:  6.690069130854681e-05
Batch  91  loss:  6.768308230675757e-05
Batch  101  loss:  6.52152084512636e-05
Batch  111  loss:  7.870427361922339e-05
Batch  121  loss:  9.297707583755255e-05
Batch  131  loss:  4.635387085727416e-05
Batch  141  loss:  6.581201159860939e-05
Batch  151  loss:  9.885741019388661e-05
Batch  161  loss:  6.461051816586405e-05
Batch  171  loss:  8.967270696302876e-05
Batch  181  loss:  7.878580800024793e-05
Batch  191  loss:  5.694238279829733e-05
Validation on real data: 
LOSS supervised-train 6.460351065470604e-05, valid 5.028611121815629e-05
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  5.0429836846888065e-05
Batch  11  loss:  5.3208288591122255e-05
Batch  21  loss:  6.942281470401213e-05
Batch  31  loss:  5.616237103822641e-05
Batch  41  loss:  5.29813660250511e-05
Batch  51  loss:  7.40847026463598e-05
Batch  61  loss:  6.441209552576765e-05
Batch  71  loss:  4.6131990529829636e-05
Batch  81  loss:  6.498819857370108e-05
Batch  91  loss:  5.816741759190336e-05
Batch  101  loss:  6.525199569296092e-05
Batch  111  loss:  6.213453889358789e-05
Batch  121  loss:  7.919954805402085e-05
Batch  131  loss:  3.8769925595261157e-05
Batch  141  loss:  6.599298649234697e-05
Batch  151  loss:  0.00010873813153011724
Batch  161  loss:  5.1594288379419595e-05
Batch  171  loss:  8.928361057769507e-05
Batch  181  loss:  7.083761011017486e-05
Batch  191  loss:  7.608627493027598e-05
Validation on real data: 
LOSS supervised-train 6.478932831669227e-05, valid 5.8617246395442635e-05
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  6.32165465503931e-05
Batch  11  loss:  6.0850779846077785e-05
Batch  21  loss:  6.009615754010156e-05
Batch  31  loss:  4.1928200516849756e-05
Batch  41  loss:  5.425762356026098e-05
Batch  51  loss:  5.795841934741475e-05
Batch  61  loss:  6.065337947802618e-05
Batch  71  loss:  5.84000808885321e-05
Batch  81  loss:  5.929814142291434e-05
Batch  91  loss:  6.094046693760902e-05
Batch  101  loss:  7.732141966698691e-05
Batch  111  loss:  6.618748011533171e-05
Batch  121  loss:  8.12472208053805e-05
Batch  131  loss:  4.7685211029602215e-05
Batch  141  loss:  5.1145216275472194e-05
Batch  151  loss:  0.00010035416198661551
Batch  161  loss:  4.963843821315095e-05
Batch  171  loss:  8.848033758113161e-05
Batch  181  loss:  7.125123374862596e-05
Batch  191  loss:  6.596717139473185e-05
Validation on real data: 
LOSS supervised-train 6.397198756530998e-05, valid 5.595535913016647e-05
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  6.21582439634949e-05
Batch  11  loss:  6.0753234720323235e-05
Batch  21  loss:  8.296618761960417e-05
Batch  31  loss:  5.0957052735611796e-05
Batch  41  loss:  5.613674147753045e-05
Batch  51  loss:  9.697323548607528e-05
Batch  61  loss:  7.141880632843822e-05
Batch  71  loss:  5.938944377703592e-05
Batch  81  loss:  7.327873754547909e-05
Batch  91  loss:  5.7959136029239744e-05
Batch  101  loss:  8.297929161926731e-05
Batch  111  loss:  7.957989873830229e-05
Batch  121  loss:  8.761617209529504e-05
Batch  131  loss:  4.989842636859976e-05
Batch  141  loss:  5.495571895153262e-05
Batch  151  loss:  0.00011040616664104164
Batch  161  loss:  5.70506599615328e-05
Batch  171  loss:  8.776459435466677e-05
Batch  181  loss:  6.769902392989025e-05
Batch  191  loss:  5.799606879008934e-05
Validation on real data: 
LOSS supervised-train 6.348515154968481e-05, valid 5.964307638350874e-05
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  6.263930845307186e-05
Batch  11  loss:  5.305032755131833e-05
Batch  21  loss:  6.446963379858062e-05
Batch  31  loss:  4.755200643558055e-05
Batch  41  loss:  6.516035500681028e-05
Batch  51  loss:  8.28079937491566e-05
Batch  61  loss:  8.320026972796768e-05
Batch  71  loss:  6.16144243394956e-05
Batch  81  loss:  6.824846059316769e-05
Batch  91  loss:  5.083320502308197e-05
Batch  101  loss:  7.484250818379223e-05
Batch  111  loss:  6.395044329110533e-05
Batch  121  loss:  7.259726407937706e-05
Batch  131  loss:  3.554497016011737e-05
Batch  141  loss:  5.3087838750798255e-05
Batch  151  loss:  0.00010488433326827362
Batch  161  loss:  4.925705070490949e-05
Batch  171  loss:  7.371162064373493e-05
Batch  181  loss:  6.217526970431209e-05
Batch  191  loss:  6.463754834840074e-05
Validation on real data: 
LOSS supervised-train 6.277593367485678e-05, valid 5.67939059692435e-05
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  5.427617361419834e-05
Batch  11  loss:  5.0829748943215236e-05
Batch  21  loss:  6.069489609217271e-05
Batch  31  loss:  3.9673079299973324e-05
Batch  41  loss:  4.847017044085078e-05
Batch  51  loss:  8.058595994953066e-05
Batch  61  loss:  6.98583826306276e-05
Batch  71  loss:  4.928190537611954e-05
Batch  81  loss:  5.870442691957578e-05
Batch  91  loss:  5.3275598475011066e-05
Batch  101  loss:  8.892465120879933e-05
Batch  111  loss:  7.295193063328043e-05
Batch  121  loss:  7.05121346982196e-05
Batch  131  loss:  4.4314747356111184e-05
Batch  141  loss:  5.365889228414744e-05
Batch  151  loss:  0.0001352902181679383
Batch  161  loss:  5.129294368089177e-05
Batch  171  loss:  8.349728886969388e-05
Batch  181  loss:  6.94901536917314e-05
Batch  191  loss:  6.007952833897434e-05
Validation on real data: 
LOSS supervised-train 6.044882307833177e-05, valid 4.27949198638089e-05
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  5.61953493161127e-05
Batch  11  loss:  4.896743121207692e-05
Batch  21  loss:  6.85211707605049e-05
Batch  31  loss:  4.443482248461805e-05
Batch  41  loss:  5.957661414868198e-05
Batch  51  loss:  9.552057599648833e-05
Batch  61  loss:  6.472500535892323e-05
Batch  71  loss:  5.445365241030231e-05
Batch  81  loss:  6.875763938296586e-05
Batch  91  loss:  4.743790123029612e-05
Batch  101  loss:  6.509279774036258e-05
Batch  111  loss:  7.594411727041006e-05
Batch  121  loss:  5.316002352628857e-05
Batch  131  loss:  4.7632096539018676e-05
Batch  141  loss:  5.625785706797615e-05
Batch  151  loss:  8.945606532506645e-05
Batch  161  loss:  5.415536361397244e-05
Batch  171  loss:  6.746354483766481e-05
Batch  181  loss:  6.184694939292967e-05
Batch  191  loss:  6.184226367622614e-05
Validation on real data: 
LOSS supervised-train 5.95886752307706e-05, valid 4.782790347235277e-05
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  6.221435614861548e-05
Batch  11  loss:  4.732536035589874e-05
Batch  21  loss:  5.668214362231083e-05
Batch  31  loss:  5.315480302670039e-05
Batch  41  loss:  4.573032492771745e-05
Batch  51  loss:  8.122596773318946e-05
Batch  61  loss:  6.103383202571422e-05
Batch  71  loss:  4.9449958169134334e-05
Batch  81  loss:  5.302091085468419e-05
Batch  91  loss:  6.059211227693595e-05
Batch  101  loss:  7.131647726055235e-05
Batch  111  loss:  6.411309004761279e-05
Batch  121  loss:  7.17942530172877e-05
Batch  131  loss:  4.858457396039739e-05
Batch  141  loss:  5.4214688134379685e-05
Batch  151  loss:  9.777284867595881e-05
Batch  161  loss:  6.0929374740226194e-05
Batch  171  loss:  7.004717917880043e-05
Batch  181  loss:  6.962128099985421e-05
Batch  191  loss:  4.561348760034889e-05
Validation on real data: 
LOSS supervised-train 5.989106015476864e-05, valid 4.3567219108808786e-05
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  5.348769263946451e-05
Batch  11  loss:  4.593210178427398e-05
Batch  21  loss:  5.566377149079926e-05
Batch  31  loss:  4.0648279536981136e-05
Batch  41  loss:  3.8193287764443085e-05
Batch  51  loss:  6.31752991466783e-05
Batch  61  loss:  7.386928336927667e-05
Batch  71  loss:  5.5335505749098957e-05
Batch  81  loss:  4.869152689934708e-05
Batch  91  loss:  5.257955126580782e-05
Batch  101  loss:  7.223837019409984e-05
Batch  111  loss:  7.492808799725026e-05
Batch  121  loss:  6.474729161709547e-05
Batch  131  loss:  3.8841928471811116e-05
Batch  141  loss:  4.117938442504965e-05
Batch  151  loss:  9.344333375338465e-05
Batch  161  loss:  4.758343493449502e-05
Batch  171  loss:  6.832446524640545e-05
Batch  181  loss:  7.081708463374525e-05
Batch  191  loss:  5.780451465398073e-05
Validation on real data: 
LOSS supervised-train 5.77266377331398e-05, valid 5.489298200700432e-05
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  5.2368370234034956e-05
Batch  11  loss:  5.640751260216348e-05
Batch  21  loss:  5.9359306760597974e-05
Batch  31  loss:  4.470943895285018e-05
Batch  41  loss:  4.874595833825879e-05
Batch  51  loss:  7.475054007954895e-05
Batch  61  loss:  7.096357148839161e-05
Batch  71  loss:  5.368723577703349e-05
Batch  81  loss:  4.9839112762128934e-05
Batch  91  loss:  5.822761886520311e-05
Batch  101  loss:  6.536610453622416e-05
Batch  111  loss:  5.75087069591973e-05
Batch  121  loss:  7.440633635269478e-05
Batch  131  loss:  3.838332122541033e-05
Batch  141  loss:  5.327689359546639e-05
Batch  151  loss:  9.57954689511098e-05
Batch  161  loss:  4.39524301327765e-05
Batch  171  loss:  7.069310231599957e-05
Batch  181  loss:  7.690821803407744e-05
Batch  191  loss:  4.291029472369701e-05
Validation on real data: 
LOSS supervised-train 5.742504161389661e-05, valid 4.3555686715990305e-05
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  4.313424142310396e-05
Batch  11  loss:  4.658966645365581e-05
Batch  21  loss:  5.309348853188567e-05
Batch  31  loss:  3.5844281228492036e-05
Batch  41  loss:  4.499389251577668e-05
Batch  51  loss:  7.210217881947756e-05
Batch  61  loss:  7.010791887296364e-05
Batch  71  loss:  4.8425670684082434e-05
Batch  81  loss:  6.21673752903007e-05
Batch  91  loss:  5.571048677666113e-05
Batch  101  loss:  9.25945132621564e-05
Batch  111  loss:  6.59791476209648e-05
Batch  121  loss:  6.814493099227548e-05
Batch  131  loss:  3.772752097574994e-05
Batch  141  loss:  4.507298945100047e-05
Batch  151  loss:  9.85798760666512e-05
Batch  161  loss:  5.0973405450349674e-05
Batch  171  loss:  6.018433487042785e-05
Batch  181  loss:  5.985757161397487e-05
Batch  191  loss:  4.7629233449697495e-05
Validation on real data: 
LOSS supervised-train 5.651110336657439e-05, valid 4.933913078275509e-05
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  5.740290362155065e-05
Batch  11  loss:  5.223160405876115e-05
Batch  21  loss:  7.198652747320011e-05
Batch  31  loss:  3.583001671358943e-05
Batch  41  loss:  4.941242514178157e-05
Batch  51  loss:  6.18681515334174e-05
Batch  61  loss:  6.442423182306811e-05
Batch  71  loss:  5.0026195822283626e-05
Batch  81  loss:  6.487463542725891e-05
Batch  91  loss:  5.013978807255626e-05
Batch  101  loss:  6.460527220042422e-05
Batch  111  loss:  6.495440902654082e-05
Batch  121  loss:  6.865423347335309e-05
Batch  131  loss:  3.930168895749375e-05
Batch  141  loss:  4.704683306044899e-05
Batch  151  loss:  7.645947334822267e-05
Batch  161  loss:  6.143785867607221e-05
Batch  171  loss:  5.573024827754125e-05
Batch  181  loss:  6.976966687943786e-05
Batch  191  loss:  6.05903442192357e-05
Validation on real data: 
LOSS supervised-train 5.6858029347495175e-05, valid 4.965720290783793e-05
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  4.928235648549162e-05
Batch  11  loss:  4.9802147259470075e-05
Batch  21  loss:  5.807932757306844e-05
Batch  31  loss:  4.022561915917322e-05
Batch  41  loss:  4.615583748091012e-05
Batch  51  loss:  6.37275370536372e-05
Batch  61  loss:  6.314572237897664e-05
Batch  71  loss:  6.174263398861513e-05
Batch  81  loss:  5.8198169426759705e-05
Batch  91  loss:  6.159432814456522e-05
Batch  101  loss:  7.579897646792233e-05
Batch  111  loss:  6.043088069418445e-05
Batch  121  loss:  5.3945368563290685e-05
Batch  131  loss:  4.92734725412447e-05
Batch  141  loss:  5.3659350669477135e-05
Batch  151  loss:  9.555888391332701e-05
Batch  161  loss:  6.40873477095738e-05
Batch  171  loss:  6.0849317378597334e-05
Batch  181  loss:  5.814596079289913e-05
Batch  191  loss:  4.241608621669002e-05
Validation on real data: 
LOSS supervised-train 5.7334391967742704e-05, valid 4.7079025534912944e-05
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  5.5517026339657605e-05
Batch  11  loss:  4.45083060185425e-05
Batch  21  loss:  6.290886813076213e-05
Batch  31  loss:  3.985237344750203e-05
Batch  41  loss:  4.992049071006477e-05
Batch  51  loss:  5.480316758621484e-05
Batch  61  loss:  6.885354378027841e-05
Batch  71  loss:  7.651464693481103e-05
Batch  81  loss:  5.7292952988063917e-05
Batch  91  loss:  6.182827200973406e-05
Batch  101  loss:  5.7726592785911635e-05
Batch  111  loss:  7.033196743577719e-05
Batch  121  loss:  6.581404159078375e-05
Batch  131  loss:  5.021412289352156e-05
Batch  141  loss:  5.365190008888021e-05
Batch  151  loss:  9.176421735901386e-05
Batch  161  loss:  5.399480869527906e-05
Batch  171  loss:  7.535533222835511e-05
Batch  181  loss:  7.509129500249401e-05
Batch  191  loss:  4.8131118091987446e-05
Validation on real data: 
LOSS supervised-train 5.532874733034987e-05, valid 4.540749796433374e-05
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  4.9831156502477825e-05
Batch  11  loss:  4.978887955076061e-05
Batch  21  loss:  6.430569192161784e-05
Batch  31  loss:  3.759965693461709e-05
Batch  41  loss:  6.146844680188224e-05
Batch  51  loss:  7.596010254928842e-05
Batch  61  loss:  5.609733489109203e-05
Batch  71  loss:  5.082221468910575e-05
Batch  81  loss:  5.362241427064873e-05
Batch  91  loss:  5.433701880974695e-05
Batch  101  loss:  9.520781895844266e-05
Batch  111  loss:  5.059811155661009e-05
Batch  121  loss:  6.535952707054093e-05
Batch  131  loss:  4.669886766350828e-05
Batch  141  loss:  4.8736816097516567e-05
Batch  151  loss:  7.139844819903374e-05
Batch  161  loss:  5.5540229368489236e-05
Batch  171  loss:  6.885665061417967e-05
Batch  181  loss:  5.905513899051584e-05
Batch  191  loss:  4.0480284951627254e-05
Validation on real data: 
LOSS supervised-train 5.5018120519889635e-05, valid 3.7144556699786335e-05
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  5.506376692210324e-05
Batch  11  loss:  4.959726720699109e-05
Batch  21  loss:  5.177612183615565e-05
Batch  31  loss:  3.240105070290156e-05
Batch  41  loss:  5.247168883215636e-05
Batch  51  loss:  7.106556586222723e-05
Batch  61  loss:  6.230995495570824e-05
Batch  71  loss:  5.657109431922436e-05
Batch  81  loss:  5.1054525101790205e-05
Batch  91  loss:  4.931615694658831e-05
Batch  101  loss:  7.67215242376551e-05
Batch  111  loss:  5.2944160415790975e-05
Batch  121  loss:  6.383812433341518e-05
Batch  131  loss:  4.7293229727074504e-05
Batch  141  loss:  4.621175321517512e-05
Batch  151  loss:  7.505677785957232e-05
Batch  161  loss:  4.8308920668205246e-05
Batch  171  loss:  6.515437416965142e-05
Batch  181  loss:  6.216123438207433e-05
Batch  191  loss:  5.682978007826023e-05
Validation on real data: 
LOSS supervised-train 5.638672240820597e-05, valid 5.2905306802131236e-05
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  4.891072239843197e-05
Batch  11  loss:  4.8406443966086954e-05
Batch  21  loss:  4.892279685009271e-05
Batch  31  loss:  3.882055534631945e-05
Batch  41  loss:  5.329611667548306e-05
Batch  51  loss:  5.815500844619237e-05
Batch  61  loss:  7.220344559755176e-05
Batch  71  loss:  4.267899566912092e-05
Batch  81  loss:  4.6727156586712226e-05
Batch  91  loss:  4.849943798035383e-05
Batch  101  loss:  6.248442514333874e-05
Batch  111  loss:  6.476997077697888e-05
Batch  121  loss:  6.126518564997241e-05
Batch  131  loss:  4.533079845714383e-05
Batch  141  loss:  4.3771517084678635e-05
Batch  151  loss:  7.811913383193314e-05
Batch  161  loss:  4.329251532908529e-05
Batch  171  loss:  7.257114339154214e-05
Batch  181  loss:  7.435313455061987e-05
Batch  191  loss:  4.979089862899855e-05
Validation on real data: 
LOSS supervised-train 5.579032277637452e-05, valid 4.181396070634946e-05
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  4.617734157363884e-05
Batch  11  loss:  5.0466700486140326e-05
Batch  21  loss:  5.7493551139486954e-05
Batch  31  loss:  3.274622213211842e-05
Batch  41  loss:  4.4165226427139714e-05
Batch  51  loss:  6.875285907881334e-05
Batch  61  loss:  5.71139607927762e-05
Batch  71  loss:  4.965730840922333e-05
Batch  81  loss:  5.1868235459551215e-05
Batch  91  loss:  4.991400783183053e-05
Batch  101  loss:  6.259463407332078e-05
Batch  111  loss:  5.980744754197076e-05
Batch  121  loss:  7.486712274840102e-05
Batch  131  loss:  3.7308436731109396e-05
Batch  141  loss:  4.744766192743555e-05
Batch  151  loss:  7.297239790204912e-05
Batch  161  loss:  4.805180651601404e-05
Batch  171  loss:  7.321567682083696e-05
Batch  181  loss:  7.731568621238694e-05
Batch  191  loss:  5.117393084219657e-05
Validation on real data: 
LOSS supervised-train 5.400710680987686e-05, valid 4.4880303903482854e-05
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  6.633209704887122e-05
Batch  11  loss:  6.120325269876048e-05
Batch  21  loss:  5.531865463126451e-05
Batch  31  loss:  3.101894253632054e-05
Batch  41  loss:  4.707102561951615e-05
Batch  51  loss:  6.564253999385983e-05
Batch  61  loss:  6.419393321266398e-05
Batch  71  loss:  4.8819114454090595e-05
Batch  81  loss:  6.19104685029015e-05
Batch  91  loss:  4.988412547390908e-05
Batch  101  loss:  6.335756916087121e-05
Batch  111  loss:  5.983771552564576e-05
Batch  121  loss:  5.782339576398954e-05
Batch  131  loss:  3.890219159075059e-05
Batch  141  loss:  3.88791631849017e-05
Batch  151  loss:  8.87738133314997e-05
Batch  161  loss:  5.2334959036670625e-05
Batch  171  loss:  6.121607293607667e-05
Batch  181  loss:  4.542031092569232e-05
Batch  191  loss:  4.7262645239243284e-05
Validation on real data: 
LOSS supervised-train 5.449765183584532e-05, valid 5.488719034474343e-05
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  3.791512790485285e-05
Batch  11  loss:  4.763291872222908e-05
Batch  21  loss:  5.942305870121345e-05
Batch  31  loss:  4.00774588342756e-05
Batch  41  loss:  4.122390237171203e-05
Batch  51  loss:  6.002578811603598e-05
Batch  61  loss:  6.7160974140279e-05
Batch  71  loss:  4.4856096792500466e-05
Batch  81  loss:  5.0019891205010936e-05
Batch  91  loss:  4.7123245167313144e-05
Batch  101  loss:  6.105822103563696e-05
Batch  111  loss:  5.551731010200456e-05
Batch  121  loss:  6.699628283968195e-05
Batch  131  loss:  4.3705393181880936e-05
Batch  141  loss:  5.00648093293421e-05
Batch  151  loss:  8.384693501284346e-05
Batch  161  loss:  4.5241024054121226e-05
Batch  171  loss:  6.648297858191654e-05
Batch  181  loss:  6.90668894094415e-05
Batch  191  loss:  4.144810372963548e-05
Validation on real data: 
LOSS supervised-train 5.352878578378295e-05, valid 4.430118860909715e-05
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  4.8424368287669495e-05
Batch  11  loss:  4.1516505007166415e-05
Batch  21  loss:  6.156932795420289e-05
Batch  31  loss:  3.869512147502974e-05
Batch  41  loss:  3.6059111153008416e-05
Batch  51  loss:  6.986156950006261e-05
Batch  61  loss:  5.789829447166994e-05
Batch  71  loss:  5.3345462220022455e-05
Batch  81  loss:  5.3926793043501675e-05
Batch  91  loss:  5.021630204282701e-05
Batch  101  loss:  6.398680852726102e-05
Batch  111  loss:  5.822726234328002e-05
Batch  121  loss:  7.431535050272942e-05
Batch  131  loss:  3.9479062252212316e-05
Batch  141  loss:  3.992781421402469e-05
Batch  151  loss:  7.974117761477828e-05
Batch  161  loss:  4.567191353999078e-05
Batch  171  loss:  7.617076335009187e-05
Batch  181  loss:  6.128676614025608e-05
Batch  191  loss:  6.329517054837197e-05
Validation on real data: 
LOSS supervised-train 5.2862674529023934e-05, valid 4.322609675000422e-05
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  5.51908633497078e-05
Batch  11  loss:  4.5327786210691556e-05
Batch  21  loss:  5.9222493291599676e-05
Batch  31  loss:  2.8886066502309404e-05
Batch  41  loss:  4.323143366491422e-05
Batch  51  loss:  4.4565400457940996e-05
Batch  61  loss:  6.563111674040556e-05
Batch  71  loss:  4.336370693636127e-05
Batch  81  loss:  5.472291013575159e-05
Batch  91  loss:  5.729340045945719e-05
Batch  101  loss:  8.122859435388818e-05
Batch  111  loss:  5.724903894588351e-05
Batch  121  loss:  5.2508308726828545e-05
Batch  131  loss:  4.4621541746892035e-05
Batch  141  loss:  4.481796713662334e-05
Batch  151  loss:  6.956056313356385e-05
Batch  161  loss:  5.032939589000307e-05
Batch  171  loss:  6.599316111532971e-05
Batch  181  loss:  6.524406489916146e-05
Batch  191  loss:  5.001440513296984e-05
Validation on real data: 
LOSS supervised-train 5.118198786476569e-05, valid 4.36165246355813e-05
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  4.305396942072548e-05
Batch  11  loss:  4.5783704990753904e-05
Batch  21  loss:  6.401973951142281e-05
Batch  31  loss:  3.71187343262136e-05
Batch  41  loss:  5.198615326662548e-05
Batch  51  loss:  6.190059502841905e-05
Batch  61  loss:  6.433822272811085e-05
Batch  71  loss:  3.337697853567079e-05
Batch  81  loss:  4.393667768454179e-05
Batch  91  loss:  4.800283568329178e-05
Batch  101  loss:  6.231525912880898e-05
Batch  111  loss:  6.645647226832807e-05
Batch  121  loss:  6.203154771355912e-05
Batch  131  loss:  4.967199856764637e-05
Batch  141  loss:  3.969186218455434e-05
Batch  151  loss:  8.116317621897906e-05
Batch  161  loss:  4.360521415947005e-05
Batch  171  loss:  4.8764839448267594e-05
Batch  181  loss:  5.709360266337171e-05
Batch  191  loss:  4.289803473511711e-05
Validation on real data: 
LOSS supervised-train 5.178929695830448e-05, valid 3.6456411180552095e-05
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  4.876059028902091e-05
Batch  11  loss:  4.327678107074462e-05
Batch  21  loss:  5.2159470214974135e-05
Batch  31  loss:  4.4122294639237225e-05
Batch  41  loss:  3.97231342503801e-05
Batch  51  loss:  6.244775431696326e-05
Batch  61  loss:  5.829241854371503e-05
Batch  71  loss:  4.971237285644747e-05
Batch  81  loss:  5.184116162126884e-05
Batch  91  loss:  4.810705650015734e-05
Batch  101  loss:  7.55923829274252e-05
Batch  111  loss:  6.0156879044370726e-05
Batch  121  loss:  7.754055695841089e-05
Batch  131  loss:  3.8649617636110634e-05
Batch  141  loss:  3.922207542927936e-05
Batch  151  loss:  7.55819128244184e-05
Batch  161  loss:  3.934886262868531e-05
Batch  171  loss:  6.211173604242504e-05
Batch  181  loss:  6.735259376000613e-05
Batch  191  loss:  5.007091021980159e-05
Validation on real data: 
LOSS supervised-train 5.211361318288254e-05, valid 3.392613143660128e-05
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  4.335015546530485e-05
Batch  11  loss:  4.946921035298146e-05
Batch  21  loss:  6.654103344772011e-05
Batch  31  loss:  3.551715053617954e-05
Batch  41  loss:  5.044774661655538e-05
Batch  51  loss:  5.430424789665267e-05
Batch  61  loss:  5.37541855010204e-05
Batch  71  loss:  4.6798697439953685e-05
Batch  81  loss:  5.214002158027142e-05
Batch  91  loss:  5.2276653150329366e-05
Batch  101  loss:  6.016615589032881e-05
Batch  111  loss:  5.8736921346280724e-05
Batch  121  loss:  6.098892117734067e-05
Batch  131  loss:  3.461042797425762e-05
Batch  141  loss:  4.375962816993706e-05
Batch  151  loss:  8.87443165993318e-05
Batch  161  loss:  5.286100349621847e-05
Batch  171  loss:  5.591988519881852e-05
Batch  181  loss:  5.768867049482651e-05
Batch  191  loss:  4.4198324758326635e-05
Validation on real data: 
LOSS supervised-train 5.101510142594634e-05, valid 3.781499981414527e-05
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  3.902769458363764e-05
Batch  11  loss:  4.7229656047420576e-05
Batch  21  loss:  5.404216062743217e-05
Batch  31  loss:  3.3106436603702605e-05
Batch  41  loss:  3.7970727134961635e-05
Batch  51  loss:  5.10228710481897e-05
Batch  61  loss:  6.45578111289069e-05
Batch  71  loss:  4.746822014567442e-05
Batch  81  loss:  4.576843639370054e-05
Batch  91  loss:  4.7559820814058185e-05
Batch  101  loss:  7.524977991124615e-05
Batch  111  loss:  6.183340155985206e-05
Batch  121  loss:  6.034387479303405e-05
Batch  131  loss:  3.301796459709294e-05
Batch  141  loss:  4.461563003133051e-05
Batch  151  loss:  7.654051296412945e-05
Batch  161  loss:  5.078123285784386e-05
Batch  171  loss:  7.219560211524367e-05
Batch  181  loss:  6.595253944396973e-05
Batch  191  loss:  5.258786404738203e-05
Validation on real data: 
LOSS supervised-train 5.142265737958951e-05, valid 4.3717751395888627e-05
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  5.4910353355808184e-05
Batch  11  loss:  4.7633297072025016e-05
Batch  21  loss:  6.45476029603742e-05
Batch  31  loss:  3.659084904938936e-05
Batch  41  loss:  4.958197314408608e-05
Batch  51  loss:  5.9115693147759885e-05
Batch  61  loss:  5.849370427313261e-05
Batch  71  loss:  4.5396063796943054e-05
Batch  81  loss:  5.030810643802397e-05
Batch  91  loss:  5.409414734458551e-05
Batch  101  loss:  6.563694478245452e-05
Batch  111  loss:  6.007838965160772e-05
Batch  121  loss:  6.436823605326936e-05
Batch  131  loss:  3.953244595322758e-05
Batch  141  loss:  4.11030268878676e-05
Batch  151  loss:  8.354988676728681e-05
Batch  161  loss:  4.87755132780876e-05
Batch  171  loss:  5.536829121410847e-05
Batch  181  loss:  6.835062958998606e-05
Batch  191  loss:  5.064217111794278e-05
Validation on real data: 
LOSS supervised-train 5.09167662676191e-05, valid 3.898378781741485e-05
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  4.592967889038846e-05
Batch  11  loss:  4.062537118443288e-05
Batch  21  loss:  6.25805914751254e-05
Batch  31  loss:  4.142586112720892e-05
Batch  41  loss:  3.83940277970396e-05
Batch  51  loss:  6.410042260540649e-05
Batch  61  loss:  4.627863017958589e-05
Batch  71  loss:  5.042380507802591e-05
Batch  81  loss:  4.987682041246444e-05
Batch  91  loss:  4.455554153537378e-05
Batch  101  loss:  6.163107173051685e-05
Batch  111  loss:  5.988795237499289e-05
Batch  121  loss:  6.376772216754034e-05
Batch  131  loss:  3.628860577009618e-05
Batch  141  loss:  3.391333666513674e-05
Batch  151  loss:  7.296405237866566e-05
Batch  161  loss:  4.764348341268487e-05
Batch  171  loss:  5.449866148410365e-05
Batch  181  loss:  6.326998118311167e-05
Batch  191  loss:  4.809707388631068e-05
Validation on real data: 
LOSS supervised-train 4.969272868038388e-05, valid 3.2911731977947056e-05
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  4.0405204345006496e-05
Batch  11  loss:  4.393287963466719e-05
Batch  21  loss:  4.674872616305947e-05
Batch  31  loss:  3.049575025215745e-05
Batch  41  loss:  4.35633774031885e-05
Batch  51  loss:  7.985731645021588e-05
Batch  61  loss:  7.4837873398792e-05
Batch  71  loss:  4.398046075948514e-05
Batch  81  loss:  5.526093809749e-05
Batch  91  loss:  4.5061533455736935e-05
Batch  101  loss:  5.716375744668767e-05
Batch  111  loss:  5.025981226935983e-05
Batch  121  loss:  6.803886935813352e-05
Batch  131  loss:  3.6594647099263966e-05
Batch  141  loss:  3.748203016584739e-05
Batch  151  loss:  8.152645750669762e-05
Batch  161  loss:  4.704648381448351e-05
Batch  171  loss:  5.4185016779229045e-05
Batch  181  loss:  5.483439235831611e-05
Batch  191  loss:  4.57230671599973e-05
Validation on real data: 
LOSS supervised-train 4.880922136180743e-05, valid 4.407735468703322e-05
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  4.6104960347292945e-05
Batch  11  loss:  4.3441290472401306e-05
Batch  21  loss:  6.0231082898098975e-05
Batch  31  loss:  3.229030335205607e-05
Batch  41  loss:  4.374640775495209e-05
Batch  51  loss:  5.6922952353488654e-05
Batch  61  loss:  5.816515476908535e-05
Batch  71  loss:  5.091848652227782e-05
Batch  81  loss:  4.858881948166527e-05
Batch  91  loss:  3.969293538830243e-05
Batch  101  loss:  7.935231406008825e-05
Batch  111  loss:  5.491881529451348e-05
Batch  121  loss:  5.285665247356519e-05
Batch  131  loss:  3.820284109679051e-05
Batch  141  loss:  3.238005956518464e-05
Batch  151  loss:  6.942186155356467e-05
Batch  161  loss:  4.335331323090941e-05
Batch  171  loss:  5.7420282246312127e-05
Batch  181  loss:  5.7166402257280424e-05
Batch  191  loss:  5.623630204354413e-05
Validation on real data: 
LOSS supervised-train 4.911724209705426e-05, valid 4.391675611259416e-05
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  4.346204877947457e-05
Batch  11  loss:  4.357716898084618e-05
Batch  21  loss:  5.415389023255557e-05
Batch  31  loss:  2.673213748494163e-05
Batch  41  loss:  4.2785231926245615e-05
Batch  51  loss:  6.644946552114561e-05
Batch  61  loss:  6.202213990036398e-05
Batch  71  loss:  6.0113834479125217e-05
Batch  81  loss:  5.0388833187753335e-05
Batch  91  loss:  4.9030615627998486e-05
Batch  101  loss:  6.301726534729823e-05
Batch  111  loss:  5.572151349042542e-05
Batch  121  loss:  6.510974344564602e-05
Batch  131  loss:  4.0491315303370357e-05
Batch  141  loss:  3.529498280840926e-05
Batch  151  loss:  0.00010924410889856517
Batch  161  loss:  4.191687185084447e-05
Batch  171  loss:  5.574669557972811e-05
Batch  181  loss:  5.4755488235969096e-05
Batch  191  loss:  3.562738129403442e-05
Validation on real data: 
LOSS supervised-train 4.859335840592394e-05, valid 4.044114029966295e-05
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  4.39645373262465e-05
Batch  11  loss:  4.472552245715633e-05
Batch  21  loss:  3.782279236475006e-05
Batch  31  loss:  3.792273491853848e-05
Batch  41  loss:  4.2000443500000983e-05
Batch  51  loss:  4.4724503823090345e-05
Batch  61  loss:  5.299812983139418e-05
Batch  71  loss:  4.506086042965762e-05
Batch  81  loss:  4.910312418360263e-05
Batch  91  loss:  4.468302722671069e-05
Batch  101  loss:  6.844024028396234e-05
Batch  111  loss:  5.2930576202925295e-05
Batch  121  loss:  5.716175655834377e-05
Batch  131  loss:  3.6461602576309815e-05
Batch  141  loss:  4.165697464486584e-05
Batch  151  loss:  6.63775863358751e-05
Batch  161  loss:  3.8663576560793445e-05
Batch  171  loss:  4.641781561076641e-05
Batch  181  loss:  6.437832053052261e-05
Batch  191  loss:  3.954963540309109e-05
Validation on real data: 
LOSS supervised-train 4.748223961541953e-05, valid 3.6216952139511704e-05
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  4.449324114830233e-05
Batch  11  loss:  3.734809797606431e-05
Batch  21  loss:  4.697852273238823e-05
Batch  31  loss:  3.4149987186538056e-05
Batch  41  loss:  3.9858510717749596e-05
Batch  51  loss:  5.179372601560317e-05
Batch  61  loss:  6.24148451606743e-05
Batch  71  loss:  4.9928501539397985e-05
Batch  81  loss:  4.736518530989997e-05
Batch  91  loss:  3.737929364433512e-05
Batch  101  loss:  5.266978405416012e-05
Batch  111  loss:  4.201725823804736e-05
Batch  121  loss:  5.1379269280005246e-05
Batch  131  loss:  3.569712498574518e-05
Batch  141  loss:  3.422621011850424e-05
Batch  151  loss:  6.74963666824624e-05
Batch  161  loss:  3.708656731760129e-05
Batch  171  loss:  5.8142915804637596e-05
Batch  181  loss:  5.587566556641832e-05
Batch  191  loss:  4.278539199731313e-05
Validation on real data: 
LOSS supervised-train 4.7344079803224304e-05, valid 4.3336927774362266e-05
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  3.8593563658650964e-05
Batch  11  loss:  3.99167365685571e-05
Batch  21  loss:  4.45506811956875e-05
Batch  31  loss:  2.9382195862126537e-05
Batch  41  loss:  4.0502949559595436e-05
Batch  51  loss:  5.945421798969619e-05
Batch  61  loss:  5.777988553745672e-05
Batch  71  loss:  4.051948053529486e-05
Batch  81  loss:  5.2375489758560434e-05
Batch  91  loss:  4.119248842471279e-05
Batch  101  loss:  6.657266203546897e-05
Batch  111  loss:  5.328811676008627e-05
Batch  121  loss:  6.117956945672631e-05
Batch  131  loss:  4.472693399293348e-05
Batch  141  loss:  3.74488445231691e-05
Batch  151  loss:  8.174422691809013e-05
Batch  161  loss:  3.848387859761715e-05
Batch  171  loss:  5.302829958964139e-05
Batch  181  loss:  5.078398680780083e-05
Batch  191  loss:  3.741082764463499e-05
Validation on real data: 
LOSS supervised-train 4.738755426842545e-05, valid 3.3614498534007e-05
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  4.594014899339527e-05
Batch  11  loss:  4.858902684645727e-05
Batch  21  loss:  6.395312084350735e-05
Batch  31  loss:  3.237560304114595e-05
Batch  41  loss:  3.327364174765535e-05
Batch  51  loss:  5.520113336388022e-05
Batch  61  loss:  5.50597433175426e-05
Batch  71  loss:  4.722338053397834e-05
Batch  81  loss:  4.648765025194734e-05
Batch  91  loss:  4.9759881221689284e-05
Batch  101  loss:  5.516272722161375e-05
Batch  111  loss:  3.748515518964268e-05
Batch  121  loss:  4.816102591576055e-05
Batch  131  loss:  3.9560785808134824e-05
Batch  141  loss:  4.2963747546309605e-05
Batch  151  loss:  6.987307278905064e-05
Batch  161  loss:  3.9503916923422366e-05
Batch  171  loss:  6.536390719702467e-05
Batch  181  loss:  5.8685418480308726e-05
Batch  191  loss:  5.679854439222254e-05
Validation on real data: 
LOSS supervised-train 4.868815139161598e-05, valid 4.751913365907967e-05
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  4.872993304161355e-05
Batch  11  loss:  4.5615644921781495e-05
Batch  21  loss:  5.49378055438865e-05
Batch  31  loss:  3.646692130132578e-05
Batch  41  loss:  3.9671551348874345e-05
Batch  51  loss:  4.91200371470768e-05
Batch  61  loss:  5.7277320593129843e-05
Batch  71  loss:  4.058807826368138e-05
Batch  81  loss:  4.022181747131981e-05
Batch  91  loss:  4.745047044707462e-05
Batch  101  loss:  5.69203584745992e-05
Batch  111  loss:  5.478628008859232e-05
Batch  121  loss:  5.220441016717814e-05
Batch  131  loss:  3.459710569586605e-05
Batch  141  loss:  3.735175414476544e-05
Batch  151  loss:  6.751830369466916e-05
Batch  161  loss:  4.5498672989197075e-05
Batch  171  loss:  5.3793246479472145e-05
Batch  181  loss:  7.039148476906121e-05
Batch  191  loss:  3.863506572088227e-05
Validation on real data: 
LOSS supervised-train 4.675773659073457e-05, valid 3.617675247369334e-05
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  4.281906149117276e-05
Batch  11  loss:  4.988512955605984e-05
Batch  21  loss:  7.004525832599029e-05
Batch  31  loss:  3.3326956327073276e-05
Batch  41  loss:  3.949616075260565e-05
Batch  51  loss:  4.306073969928548e-05
Batch  61  loss:  6.593344733119011e-05
Batch  71  loss:  4.557618740363978e-05
Batch  81  loss:  4.861074921791442e-05
Batch  91  loss:  4.1527124267304316e-05
Batch  101  loss:  6.212352309376001e-05
Batch  111  loss:  5.491028423421085e-05
Batch  121  loss:  5.395713742473163e-05
Batch  131  loss:  3.301311153336428e-05
Batch  141  loss:  3.806486711255275e-05
Batch  151  loss:  6.888493953738362e-05
Batch  161  loss:  3.5918037610827014e-05
Batch  171  loss:  4.9053734983317554e-05
Batch  181  loss:  6.138796015875414e-05
Batch  191  loss:  4.42045638919808e-05
Validation on real data: 
LOSS supervised-train 4.7043528747963135e-05, valid 4.1454499296378344e-05
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  4.082047598785721e-05
Batch  11  loss:  4.302765955799259e-05
Batch  21  loss:  4.0183633245760575e-05
Batch  31  loss:  3.4469128877390176e-05
Batch  41  loss:  3.395999374333769e-05
Batch  51  loss:  5.661141767632216e-05
Batch  61  loss:  5.606337799690664e-05
Batch  71  loss:  5.109164703753777e-05
Batch  81  loss:  4.9746045988285914e-05
Batch  91  loss:  4.523812094703317e-05
Batch  101  loss:  5.656284702126868e-05
Batch  111  loss:  5.6204469728982076e-05
Batch  121  loss:  6.354932702379301e-05
Batch  131  loss:  3.562758502084762e-05
Batch  141  loss:  4.4734035327564925e-05
Batch  151  loss:  7.180897955549881e-05
Batch  161  loss:  3.234411633457057e-05
Batch  171  loss:  5.643242548103444e-05
Batch  181  loss:  5.01818758493755e-05
Batch  191  loss:  5.042105840402655e-05
Validation on real data: 
LOSS supervised-train 4.633790088519163e-05, valid 3.926806675735861e-05
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  4.878908657701686e-05
Batch  11  loss:  4.390161484479904e-05
Batch  21  loss:  4.5531698560807854e-05
Batch  31  loss:  3.083145566051826e-05
Batch  41  loss:  2.8468006348703057e-05
Batch  51  loss:  5.960640191915445e-05
Batch  61  loss:  4.501253351918422e-05
Batch  71  loss:  4.658994294004515e-05
Batch  81  loss:  6.028805364621803e-05
Batch  91  loss:  4.422851634444669e-05
Batch  101  loss:  5.6239998230012134e-05
Batch  111  loss:  5.555509778787382e-05
Batch  121  loss:  5.849133594892919e-05
Batch  131  loss:  3.643644959083758e-05
Batch  141  loss:  3.0907453037798405e-05
Batch  151  loss:  6.273772305576131e-05
Batch  161  loss:  4.231567436363548e-05
Batch  171  loss:  4.293854726711288e-05
Batch  181  loss:  5.921585761825554e-05
Batch  191  loss:  4.083477324456908e-05
Validation on real data: 
LOSS supervised-train 4.632076697816956e-05, valid 2.9181852369219996e-05
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  3.0650116968899965e-05
Batch  11  loss:  4.372875264380127e-05
Batch  21  loss:  4.492831794777885e-05
Batch  31  loss:  2.7902686269953847e-05
Batch  41  loss:  3.5742596082855016e-05
Batch  51  loss:  5.4235104471445084e-05
Batch  61  loss:  4.6353601646842435e-05
Batch  71  loss:  3.821139034698717e-05
Batch  81  loss:  4.0307546441908926e-05
Batch  91  loss:  4.894080484518781e-05
Batch  101  loss:  4.944874672219157e-05
Batch  111  loss:  4.411675763549283e-05
Batch  121  loss:  5.4392105084843934e-05
Batch  131  loss:  3.451669908827171e-05
Batch  141  loss:  3.530303365550935e-05
Batch  151  loss:  6.83719918015413e-05
Batch  161  loss:  4.055095632793382e-05
Batch  171  loss:  5.5420980061171576e-05
Batch  181  loss:  5.5428892665077e-05
Batch  191  loss:  4.433930735103786e-05
Validation on real data: 
LOSS supervised-train 4.5382830148810174e-05, valid 4.187881131656468e-05
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  helmet ; Model ID: 3621cf047be0d1ae52fafb0cab311e6a
--------------------
Training baseline regression model:  2022-03-30 04:13:34.554094
Detector:  point_transformer
Object:  helmet
--------------------
device is  cuda
--------------------
Number of trainable parameters:  893083
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.12786865234375
Batch  11  loss:  0.07118799537420273
Batch  21  loss:  0.05215397849678993
Batch  31  loss:  0.04717486724257469
Batch  41  loss:  0.0199823547154665
Batch  51  loss:  0.0134649109095335
Batch  61  loss:  0.008280351758003235
Batch  71  loss:  0.007535366807132959
Batch  81  loss:  0.007382566574960947
Batch  91  loss:  0.006710430141538382
Batch  101  loss:  0.0028514275327324867
Batch  111  loss:  0.013073069974780083
Batch  121  loss:  0.003801400773227215
Batch  131  loss:  0.005740675609558821
Batch  141  loss:  0.008220148272812366
Batch  151  loss:  0.0041415332816541195
Batch  161  loss:  0.00268858065828681
Batch  171  loss:  0.009999355301260948
Batch  181  loss:  0.003110783640295267
Batch  191  loss:  0.00628465274348855
Validation on real data: 
LOSS supervised-train 0.016703552540857345, valid 0.00509606022387743
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.005846264306455851
Batch  11  loss:  0.005277418997138739
Batch  21  loss:  0.0028455310966819525
Batch  31  loss:  0.00791157130151987
Batch  41  loss:  0.0012006084434688091
Batch  51  loss:  0.0019833033438771963
Batch  61  loss:  0.0019140240037813783
Batch  71  loss:  0.002090753521770239
Batch  81  loss:  0.0025467590894550085
Batch  91  loss:  0.0015816749073565006
Batch  101  loss:  0.0018062528688460588
Batch  111  loss:  0.007777962367981672
Batch  121  loss:  0.003255527000874281
Batch  131  loss:  0.0017838585190474987
Batch  141  loss:  0.002139183459803462
Batch  151  loss:  0.002029946306720376
Batch  161  loss:  0.001986294286325574
Batch  171  loss:  0.003375189146026969
Batch  181  loss:  0.00115437270142138
Batch  191  loss:  0.0014677081489935517
Validation on real data: 
LOSS supervised-train 0.003128693857579492, valid 0.0011283470084890723
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0015570885734632611
Batch  11  loss:  0.002370175439864397
Batch  21  loss:  0.0015519586158916354
Batch  31  loss:  0.005898226983845234
Batch  41  loss:  0.0010274044470861554
Batch  51  loss:  0.0012043497990816832
Batch  61  loss:  0.0010756368283182383
Batch  71  loss:  0.0022215782664716244
Batch  81  loss:  0.00199690624140203
Batch  91  loss:  0.0016493488801643252
Batch  101  loss:  0.0012619226472452283
Batch  111  loss:  0.0057844254188239574
Batch  121  loss:  0.002395270625129342
Batch  131  loss:  0.0010174469789490104
Batch  141  loss:  0.001511871349066496
Batch  151  loss:  0.0012294722255319357
Batch  161  loss:  0.0016430140240117908
Batch  171  loss:  0.002629276365041733
Batch  181  loss:  0.001104197814129293
Batch  191  loss:  0.0012216211762279272
Validation on real data: 
LOSS supervised-train 0.002082182235608343, valid 0.0008871146128512919
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0011128737824037671
Batch  11  loss:  0.0019130738219246268
Batch  21  loss:  0.0011566979810595512
Batch  31  loss:  0.0029185598250478506
Batch  41  loss:  0.0009914260590448976
Batch  51  loss:  0.0008832067833282053
Batch  61  loss:  0.0007740439032204449
Batch  71  loss:  0.0018446164904162288
Batch  81  loss:  0.0016396676655858755
Batch  91  loss:  0.0012797905365005136
Batch  101  loss:  0.001063411240465939
Batch  111  loss:  0.004941694438457489
Batch  121  loss:  0.00115564139559865
Batch  131  loss:  0.0008927467279136181
Batch  141  loss:  0.0012689376017078757
Batch  151  loss:  0.000867342168930918
Batch  161  loss:  0.0011753758881241083
Batch  171  loss:  0.0024588063824921846
Batch  181  loss:  0.0008426831336691976
Batch  191  loss:  0.001206545508466661
Validation on real data: 
LOSS supervised-train 0.0015554917143890635, valid 0.0011477289954200387
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0010050764540210366
Batch  11  loss:  0.0016799718141555786
Batch  21  loss:  0.0011052994523197412
Batch  31  loss:  0.0028318380936980247
Batch  41  loss:  0.0008065070142038167
Batch  51  loss:  0.0008620551088824868
Batch  61  loss:  0.0005963356816209853
Batch  71  loss:  0.001353443949483335
Batch  81  loss:  0.0011924683349207044
Batch  91  loss:  0.0009053791873157024
Batch  101  loss:  0.0008483919082209468
Batch  111  loss:  0.003960723057389259
Batch  121  loss:  0.0011055804789066315
Batch  131  loss:  0.0007459409534931183
Batch  141  loss:  0.0008840540540404618
Batch  151  loss:  0.0008518041577190161
Batch  161  loss:  0.0011347257532179356
Batch  171  loss:  0.0016936346655711532
Batch  181  loss:  0.0008249501697719097
Batch  191  loss:  0.0010291230864822865
Validation on real data: 
LOSS supervised-train 0.0012937803276872727, valid 0.0010540700750425458
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0009496075217612088
Batch  11  loss:  0.0013700653798878193
Batch  21  loss:  0.0008483079145662487
Batch  31  loss:  0.0016921361675485969
Batch  41  loss:  0.0007028906838968396
Batch  51  loss:  0.0007996588828973472
Batch  61  loss:  0.0007163278642110527
Batch  71  loss:  0.0009224985260516405
Batch  81  loss:  0.0009784622816368937
Batch  91  loss:  0.000999519950710237
Batch  101  loss:  0.0005831583985127509
Batch  111  loss:  0.003241692902520299
Batch  121  loss:  0.000816433341242373
Batch  131  loss:  0.0009010997018776834
Batch  141  loss:  0.0010088183917105198
Batch  151  loss:  0.0006855583051219583
Batch  161  loss:  0.0008069437462836504
Batch  171  loss:  0.001725956448353827
Batch  181  loss:  0.0007331676897592843
Batch  191  loss:  0.0011440282687544823
Validation on real data: 
LOSS supervised-train 0.0011167130358808207, valid 0.0008790033170953393
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0010066728573292494
Batch  11  loss:  0.0013232935452833772
Batch  21  loss:  0.0007015044684521854
Batch  31  loss:  0.0013410007813945413
Batch  41  loss:  0.0006440344150178134
Batch  51  loss:  0.0009504647459834814
Batch  61  loss:  0.0007053205044940114
Batch  71  loss:  0.001139081665314734
Batch  81  loss:  0.00106899649836123
Batch  91  loss:  0.0007714167586527765
Batch  101  loss:  0.0007426806259900331
Batch  111  loss:  0.002847508992999792
Batch  121  loss:  0.0008263080380856991
Batch  131  loss:  0.0006469981162808836
Batch  141  loss:  0.000880207575391978
Batch  151  loss:  0.000503194925840944
Batch  161  loss:  0.0008180263685062528
Batch  171  loss:  0.0013617143267765641
Batch  181  loss:  0.0006830926286056638
Batch  191  loss:  0.0007612460758537054
Validation on real data: 
LOSS supervised-train 0.0009888009606220294, valid 0.0005995036335662007
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.000775560736656189
Batch  11  loss:  0.0009477916173636913
Batch  21  loss:  0.0005303974612616003
Batch  31  loss:  0.0010651323245838284
Batch  41  loss:  0.0005123319569975138
Batch  51  loss:  0.0006975905271247029
Batch  61  loss:  0.0007604710408486426
Batch  71  loss:  0.0007798639708198607
Batch  81  loss:  0.000643284700345248
Batch  91  loss:  0.0006418871926143765
Batch  101  loss:  0.00039419575477950275
Batch  111  loss:  0.0023956196382641792
Batch  121  loss:  0.0006806246237829328
Batch  131  loss:  0.0008857938228175044
Batch  141  loss:  0.0009442649316042662
Batch  151  loss:  0.00039321347139775753
Batch  161  loss:  0.0006872382364235818
Batch  171  loss:  0.0013134272303432226
Batch  181  loss:  0.0006385725573636591
Batch  191  loss:  0.0008768255356699228
Validation on real data: 
LOSS supervised-train 0.0008538068139750976, valid 0.0006105971406213939
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0006817418034188449
Batch  11  loss:  0.0009953841799870133
Batch  21  loss:  0.0006316032959148288
Batch  31  loss:  0.001217255019582808
Batch  41  loss:  0.0005526777822524309
Batch  51  loss:  0.0005621120799332857
Batch  61  loss:  0.0006523383781313896
Batch  71  loss:  0.0007684630691073835
Batch  81  loss:  0.0006788737373426557
Batch  91  loss:  0.0006974249263294041
Batch  101  loss:  0.0005686658550985157
Batch  111  loss:  0.0024302841629832983
Batch  121  loss:  0.0007389099919237196
Batch  131  loss:  0.00047155245556496084
Batch  141  loss:  0.0006212496664375067
Batch  151  loss:  0.00040957858436740935
Batch  161  loss:  0.0006571805570274591
Batch  171  loss:  0.0009931130334734917
Batch  181  loss:  0.0004970633890479803
Batch  191  loss:  0.0009079399751499295
Validation on real data: 
LOSS supervised-train 0.0007954960443021263, valid 0.0004464779340196401
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0007786115165799856
Batch  11  loss:  0.0011045753490179777
Batch  21  loss:  0.00047835789155215025
Batch  31  loss:  0.000805430521722883
Batch  41  loss:  0.0004742928431369364
Batch  51  loss:  0.0006584818474948406
Batch  61  loss:  0.0005768034607172012
Batch  71  loss:  0.0006268247961997986
Batch  81  loss:  0.0006527688237838447
Batch  91  loss:  0.0006757880328223109
Batch  101  loss:  0.00045338584459386766
Batch  111  loss:  0.002155463909730315
Batch  121  loss:  0.0005131805664859712
Batch  131  loss:  0.000457125308457762
Batch  141  loss:  0.0006060477462597191
Batch  151  loss:  0.00029905824339948595
Batch  161  loss:  0.0006463959580287337
Batch  171  loss:  0.0007139093359000981
Batch  181  loss:  0.0006005064933560789
Batch  191  loss:  0.0009012658847495914
Validation on real data: 
LOSS supervised-train 0.000720252357859863, valid 0.0005152877420186996
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0009960201568901539
Batch  11  loss:  0.0009360316325910389
Batch  21  loss:  0.00048525980673730373
Batch  31  loss:  0.000910834176465869
Batch  41  loss:  0.0006171568529680371
Batch  51  loss:  0.0006666559493169188
Batch  61  loss:  0.0007529613212682307
Batch  71  loss:  0.0006790156476199627
Batch  81  loss:  0.0006194498855620623
Batch  91  loss:  0.0007679559639655054
Batch  101  loss:  0.00040106906089931726
Batch  111  loss:  0.0019512788858264685
Batch  121  loss:  0.0005096759996376932
Batch  131  loss:  0.00035204130108468235
Batch  141  loss:  0.0006937909056432545
Batch  151  loss:  0.00036472082138061523
Batch  161  loss:  0.0006029072101227939
Batch  171  loss:  0.0006511938408948481
Batch  181  loss:  0.0004964903346262872
Batch  191  loss:  0.0006842369330115616
Validation on real data: 
LOSS supervised-train 0.0006509235725388862, valid 0.00046179548371583223
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0006295460043475032
Batch  11  loss:  0.0009239224600605667
Batch  21  loss:  0.0004426664672791958
Batch  31  loss:  0.0006047616479918361
Batch  41  loss:  0.00048517450341023505
Batch  51  loss:  0.0005883152480237186
Batch  61  loss:  0.0007574423216283321
Batch  71  loss:  0.0007432617130689323
Batch  81  loss:  0.0004965169355273247
Batch  91  loss:  0.0006977474549785256
Batch  101  loss:  0.00040048299706541
Batch  111  loss:  0.0015496942214667797
Batch  121  loss:  0.0003986302763223648
Batch  131  loss:  0.00039428225136362016
Batch  141  loss:  0.0007347047212533653
Batch  151  loss:  0.000448174454504624
Batch  161  loss:  0.000643348612356931
Batch  171  loss:  0.0008379162754863501
Batch  181  loss:  0.0004969755536876619
Batch  191  loss:  0.0006224280223250389
Validation on real data: 
LOSS supervised-train 0.0005994663080491591, valid 0.0005276603624224663
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0007447847747243941
Batch  11  loss:  0.0007429160177707672
Batch  21  loss:  0.0005040876567363739
Batch  31  loss:  0.0005932913045398891
Batch  41  loss:  0.0003433966194279492
Batch  51  loss:  0.0005447011790238321
Batch  61  loss:  0.0006664396496489644
Batch  71  loss:  0.0006036019767634571
Batch  81  loss:  0.0006108465604484081
Batch  91  loss:  0.000545404851436615
Batch  101  loss:  0.0004933633026666939
Batch  111  loss:  0.0014312817947939038
Batch  121  loss:  0.00042407034197822213
Batch  131  loss:  0.000443178549176082
Batch  141  loss:  0.0007267395267263055
Batch  151  loss:  0.0003290472086519003
Batch  161  loss:  0.0004906291724182665
Batch  171  loss:  0.0005387585260905325
Batch  181  loss:  0.000411293120123446
Batch  191  loss:  0.0006581554189324379
Validation on real data: 
LOSS supervised-train 0.0005712703948665876, valid 0.0005651585524901748
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0005229369271546602
Batch  11  loss:  0.0007322367746382952
Batch  21  loss:  0.0003925726341549307
Batch  31  loss:  0.000519228691700846
Batch  41  loss:  0.00037088198587298393
Batch  51  loss:  0.0007670170161873102
Batch  61  loss:  0.00039366399869322777
Batch  71  loss:  0.0005232511321082711
Batch  81  loss:  0.0003834294911939651
Batch  91  loss:  0.000591939955484122
Batch  101  loss:  0.0003815770905930549
Batch  111  loss:  0.0015080225421115756
Batch  121  loss:  0.000413999252486974
Batch  131  loss:  0.00037935777800157666
Batch  141  loss:  0.0005270506371743977
Batch  151  loss:  0.00027914976817555726
Batch  161  loss:  0.0006419700221158564
Batch  171  loss:  0.0006363093852996826
Batch  181  loss:  0.0004251512873452157
Batch  191  loss:  0.0006237325724214315
Validation on real data: 
LOSS supervised-train 0.0005443900051614037, valid 0.00033892126521095634
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.00047206252929754555
Batch  11  loss:  0.0006369402981363237
Batch  21  loss:  0.00035568737075664103
Batch  31  loss:  0.0003935358836315572
Batch  41  loss:  0.00041401066118851304
Batch  51  loss:  0.0005078728427179158
Batch  61  loss:  0.0005602884339168668
Batch  71  loss:  0.0005708600510843098
Batch  81  loss:  0.00041159975808113813
Batch  91  loss:  0.0004826433723792434
Batch  101  loss:  0.0003582919598557055
Batch  111  loss:  0.0015992737608030438
Batch  121  loss:  0.000365596788469702
Batch  131  loss:  0.0003502067120280117
Batch  141  loss:  0.0005523901781998575
Batch  151  loss:  0.00028749965713359416
Batch  161  loss:  0.0004794097621925175
Batch  171  loss:  0.0006615107995457947
Batch  181  loss:  0.00043092953274026513
Batch  191  loss:  0.0006539053865708411
Validation on real data: 
LOSS supervised-train 0.0005096933720051311, valid 0.0004542601527646184
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.000448851176770404
Batch  11  loss:  0.0005532526993192732
Batch  21  loss:  0.00039217082667164505
Batch  31  loss:  0.0006564264767803252
Batch  41  loss:  0.0005043985438533127
Batch  51  loss:  0.000679820659570396
Batch  61  loss:  0.0004685100866481662
Batch  71  loss:  0.0005211046081967652
Batch  81  loss:  0.0003996252198703587
Batch  91  loss:  0.00056648621102795
Batch  101  loss:  0.00033735993201844394
Batch  111  loss:  0.0013284477172419429
Batch  121  loss:  0.0005323868244886398
Batch  131  loss:  0.0002689384564291686
Batch  141  loss:  0.0004961040685884655
Batch  151  loss:  0.0003272131725680083
Batch  161  loss:  0.0004137307405471802
Batch  171  loss:  0.0006228837883099914
Batch  181  loss:  0.00045071120257489383
Batch  191  loss:  0.0004948776331730187
Validation on real data: 
LOSS supervised-train 0.0004931864872196457, valid 0.00045526091707870364
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.000488573161419481
Batch  11  loss:  0.0006414487143047154
Batch  21  loss:  0.0003495535929687321
Batch  31  loss:  0.00043937331065535545
Batch  41  loss:  0.0003932758409064263
Batch  51  loss:  0.0006214936729520559
Batch  61  loss:  0.00037653653998859227
Batch  71  loss:  0.00046499064774252474
Batch  81  loss:  0.00037791617796756327
Batch  91  loss:  0.0005417922511696815
Batch  101  loss:  0.0003971860569436103
Batch  111  loss:  0.001081460970453918
Batch  121  loss:  0.00038941038656048477
Batch  131  loss:  0.0003419696004129946
Batch  141  loss:  0.0006412817165255547
Batch  151  loss:  0.0002812171296682209
Batch  161  loss:  0.00040491006802767515
Batch  171  loss:  0.0005133398226462305
Batch  181  loss:  0.00028170784935355186
Batch  191  loss:  0.0005511214840225875
Validation on real data: 
LOSS supervised-train 0.000465863640565658, valid 0.00041447801049798727
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.00046762064448557794
Batch  11  loss:  0.0005601160228252411
Batch  21  loss:  0.0005280172917991877
Batch  31  loss:  0.0003279714728705585
Batch  41  loss:  0.00027737952768802643
Batch  51  loss:  0.0005077302339486778
Batch  61  loss:  0.0004916888428851962
Batch  71  loss:  0.0003683211107272655
Batch  81  loss:  0.00027977803256362677
Batch  91  loss:  0.0004397369921207428
Batch  101  loss:  0.0003843680315185338
Batch  111  loss:  0.0012898978311568499
Batch  121  loss:  0.00032625006861053407
Batch  131  loss:  0.0002801384835038334
Batch  141  loss:  0.00038884152309037745
Batch  151  loss:  0.00026841656654141843
Batch  161  loss:  0.0004724042082671076
Batch  171  loss:  0.0005003547994419932
Batch  181  loss:  0.00032450066646561027
Batch  191  loss:  0.000383322243578732
Validation on real data: 
LOSS supervised-train 0.00043809011818666475, valid 0.0004240545677021146
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0004681452992372215
Batch  11  loss:  0.00041469946154393256
Batch  21  loss:  0.00034505940857343376
Batch  31  loss:  0.00031453583505935967
Batch  41  loss:  0.0002746713871601969
Batch  51  loss:  0.0005086445598863065
Batch  61  loss:  0.00047313389950431883
Batch  71  loss:  0.0004486195684876293
Batch  81  loss:  0.00028301155543886125
Batch  91  loss:  0.000562526285648346
Batch  101  loss:  0.0003845625324174762
Batch  111  loss:  0.0011287854285910726
Batch  121  loss:  0.00048237101873382926
Batch  131  loss:  0.00024121424939949065
Batch  141  loss:  0.0005022545228712261
Batch  151  loss:  0.00034538976615294814
Batch  161  loss:  0.0005274933646433055
Batch  171  loss:  0.0004927729023620486
Batch  181  loss:  0.0004040228377562016
Batch  191  loss:  0.0005402389797382057
Validation on real data: 
LOSS supervised-train 0.00041979133842687586, valid 0.00031832308741286397
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.00036261556670069695
Batch  11  loss:  0.0007598378579132259
Batch  21  loss:  0.00032396140159107745
Batch  31  loss:  0.0004895003512501717
Batch  41  loss:  0.00044539838563650846
Batch  51  loss:  0.0005011075991205871
Batch  61  loss:  0.0004112960014026612
Batch  71  loss:  0.0004155706556048244
Batch  81  loss:  0.000286783033516258
Batch  91  loss:  0.0004163503472227603
Batch  101  loss:  0.0003899874573107809
Batch  111  loss:  0.0009876311523839831
Batch  121  loss:  0.0003874350222758949
Batch  131  loss:  0.00021070241928100586
Batch  141  loss:  0.0004452484135981649
Batch  151  loss:  0.0003630213614087552
Batch  161  loss:  0.000514918880071491
Batch  171  loss:  0.00040982384234666824
Batch  181  loss:  0.0002928392495959997
Batch  191  loss:  0.0005616054986603558
Validation on real data: 
LOSS supervised-train 0.0003992404635937419, valid 0.0003480041050352156
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0004556531785055995
Batch  11  loss:  0.0005323784425854683
Batch  21  loss:  0.0003209807036910206
Batch  31  loss:  0.0003829134220723063
Batch  41  loss:  0.0003111866826657206
Batch  51  loss:  0.0004194283683318645
Batch  61  loss:  0.0003874200629070401
Batch  71  loss:  0.00039651503902859986
Batch  81  loss:  0.0002465872676111758
Batch  91  loss:  0.0005763758672401309
Batch  101  loss:  0.00034037340083159506
Batch  111  loss:  0.0008153195376507938
Batch  121  loss:  0.00042939186096191406
Batch  131  loss:  0.00023068282462190837
Batch  141  loss:  0.000555113423615694
Batch  151  loss:  0.0002449192979838699
Batch  161  loss:  0.0003404378076083958
Batch  171  loss:  0.00035723260953091085
Batch  181  loss:  0.0003230032161809504
Batch  191  loss:  0.0005088432226330042
Validation on real data: 
LOSS supervised-train 0.0003965210662863683, valid 0.0004722668381873518
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.00037987306131981313
Batch  11  loss:  0.0005918776150792837
Batch  21  loss:  0.00025955200544558465
Batch  31  loss:  0.0004372559778857976
Batch  41  loss:  0.0003332159831188619
Batch  51  loss:  0.00047437878674827516
Batch  61  loss:  0.00045339218922890723
Batch  71  loss:  0.00039977103006094694
Batch  81  loss:  0.0003140077751595527
Batch  91  loss:  0.0004307090421207249
Batch  101  loss:  0.0003238702774979174
Batch  111  loss:  0.0009245790424756706
Batch  121  loss:  0.00031357156694866717
Batch  131  loss:  0.00035285335616208613
Batch  141  loss:  0.0004681121790781617
Batch  151  loss:  0.0002738400944508612
Batch  161  loss:  0.00040920672472566366
Batch  171  loss:  0.0005072005442343652
Batch  181  loss:  0.0003011611697729677
Batch  191  loss:  0.0004483757948037237
Validation on real data: 
LOSS supervised-train 0.0003741835436085239, valid 0.0005179144209250808
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0005146284238435328
Batch  11  loss:  0.0004709013446699828
Batch  21  loss:  0.0002767229452729225
Batch  31  loss:  0.00037226133281365037
Batch  41  loss:  0.000380508485250175
Batch  51  loss:  0.0005089608603157103
Batch  61  loss:  0.000385850144084543
Batch  71  loss:  0.00027236356982029974
Batch  81  loss:  0.00026737895677797496
Batch  91  loss:  0.0003838836564682424
Batch  101  loss:  0.00028982976800762117
Batch  111  loss:  0.0009459228022024035
Batch  121  loss:  0.0003966404765378684
Batch  131  loss:  0.000294569501420483
Batch  141  loss:  0.0003212861774954945
Batch  151  loss:  0.00027465849416330457
Batch  161  loss:  0.000507906312122941
Batch  171  loss:  0.0003196929465048015
Batch  181  loss:  0.0002853676851373166
Batch  191  loss:  0.00048614389379508793
Validation on real data: 
LOSS supervised-train 0.00037129580741748216, valid 0.00035154190845787525
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00039176095742732286
Batch  11  loss:  0.0004364603664726019
Batch  21  loss:  0.00038914434844627976
Batch  31  loss:  0.0003902020980603993
Batch  41  loss:  0.0003652779560070485
Batch  51  loss:  0.0005441046669147909
Batch  61  loss:  0.0003837450931314379
Batch  71  loss:  0.00037287460872903466
Batch  81  loss:  0.00025322262081317604
Batch  91  loss:  0.0005162730813026428
Batch  101  loss:  0.00043201984954066575
Batch  111  loss:  0.0007462571957148612
Batch  121  loss:  0.00033547935890965164
Batch  131  loss:  0.0002219611924374476
Batch  141  loss:  0.0003326265432406217
Batch  151  loss:  0.00033828153391368687
Batch  161  loss:  0.0003630252613220364
Batch  171  loss:  0.00037220734520815313
Batch  181  loss:  0.0002000912936637178
Batch  191  loss:  0.0003878397401422262
Validation on real data: 
LOSS supervised-train 0.00035028308222536, valid 0.0003694308106787503
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0003130473487544805
Batch  11  loss:  0.0004985242267139256
Batch  21  loss:  0.00026373390574008226
Batch  31  loss:  0.00032469211146235466
Batch  41  loss:  0.00039293026202358305
Batch  51  loss:  0.0003622927761171013
Batch  61  loss:  0.0004328698560129851
Batch  71  loss:  0.00036161381285637617
Batch  81  loss:  0.0003065505879931152
Batch  91  loss:  0.0004000623885076493
Batch  101  loss:  0.00028467050287872553
Batch  111  loss:  0.0008697761222720146
Batch  121  loss:  0.00034312758361920714
Batch  131  loss:  0.00020402939117047936
Batch  141  loss:  0.0004128312284592539
Batch  151  loss:  0.00021325064881239086
Batch  161  loss:  0.00040390496724285185
Batch  171  loss:  0.00041157566010951996
Batch  181  loss:  0.0003503572952467948
Batch  191  loss:  0.0005209100781939924
Validation on real data: 
LOSS supervised-train 0.00035489730566041545, valid 0.00038520852103829384
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.00032599069527350366
Batch  11  loss:  0.0005377716734074056
Batch  21  loss:  0.0002152241358999163
Batch  31  loss:  0.0002484654833097011
Batch  41  loss:  0.00024215597659349442
Batch  51  loss:  0.0004818605666514486
Batch  61  loss:  0.00038042812957428396
Batch  71  loss:  0.00034001804306171834
Batch  81  loss:  0.0002750357671175152
Batch  91  loss:  0.00041727363714016974
Batch  101  loss:  0.0002513071522116661
Batch  111  loss:  0.0007516784826293588
Batch  121  loss:  0.000318888429319486
Batch  131  loss:  0.0002473407657817006
Batch  141  loss:  0.0004704657185357064
Batch  151  loss:  0.00024921196745708585
Batch  161  loss:  0.0004099294310435653
Batch  171  loss:  0.00029185099992901087
Batch  181  loss:  0.00021903183369431645
Batch  191  loss:  0.00043256921344436705
Validation on real data: 
LOSS supervised-train 0.0003336018700065324, valid 0.00027735039475373924
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00040057802107185125
Batch  11  loss:  0.0006438565906137228
Batch  21  loss:  0.00026644018362276256
Batch  31  loss:  0.0003281962708570063
Batch  41  loss:  0.00032174837542697787
Batch  51  loss:  0.0004104759427718818
Batch  61  loss:  0.00034100128686986864
Batch  71  loss:  0.0003069331869482994
Batch  81  loss:  0.00023339941981248558
Batch  91  loss:  0.0003807552857324481
Batch  101  loss:  0.00021032504446338862
Batch  111  loss:  0.000683200778439641
Batch  121  loss:  0.00025894047576002777
Batch  131  loss:  0.00021778236259706318
Batch  141  loss:  0.0004724546452052891
Batch  151  loss:  0.0002865136484615505
Batch  161  loss:  0.000277485407423228
Batch  171  loss:  0.00041567045263946056
Batch  181  loss:  0.00025427626678720117
Batch  191  loss:  0.00047578796511515975
Validation on real data: 
LOSS supervised-train 0.00032245408394373956, valid 0.0003647617995738983
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0003126663214061409
Batch  11  loss:  0.0005082117859274149
Batch  21  loss:  0.00026197187253274024
Batch  31  loss:  0.00028128991834819317
Batch  41  loss:  0.0003097042499575764
Batch  51  loss:  0.0003960065369028598
Batch  61  loss:  0.00038587121525779366
Batch  71  loss:  0.00033086008625105023
Batch  81  loss:  0.00024632486747577786
Batch  91  loss:  0.000410897599067539
Batch  101  loss:  0.00040814297972247005
Batch  111  loss:  0.0008001216338016093
Batch  121  loss:  0.00027481673168949783
Batch  131  loss:  0.00025156696210615337
Batch  141  loss:  0.0003827829204965383
Batch  151  loss:  0.00020289602980483323
Batch  161  loss:  0.000312813586788252
Batch  171  loss:  0.0002634566626511514
Batch  181  loss:  0.0003008372150361538
Batch  191  loss:  0.0004893403383903205
Validation on real data: 
LOSS supervised-train 0.0003256679540936602, valid 0.0002677137963473797
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00039293256122618914
Batch  11  loss:  0.0004425706865731627
Batch  21  loss:  0.00023892137687653303
Batch  31  loss:  0.00037942721974104643
Batch  41  loss:  0.00035864755045622587
Batch  51  loss:  0.00040392702794633806
Batch  61  loss:  0.0002783089003060013
Batch  71  loss:  0.0002608995127957314
Batch  81  loss:  0.0002613251854199916
Batch  91  loss:  0.00033987200004048645
Batch  101  loss:  0.0002814685576595366
Batch  111  loss:  0.0004972575115971267
Batch  121  loss:  0.00024468422634527087
Batch  131  loss:  0.00027734297327697277
Batch  141  loss:  0.00039979349821805954
Batch  151  loss:  0.0002818201610352844
Batch  161  loss:  0.00035360915353521705
Batch  171  loss:  0.00029634233214892447
Batch  181  loss:  0.00021678541088476777
Batch  191  loss:  0.0004381254257168621
Validation on real data: 
LOSS supervised-train 0.0003068976502254372, valid 0.0003998558968305588
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.00031846901401877403
Batch  11  loss:  0.0004384015337564051
Batch  21  loss:  0.0003136878658551723
Batch  31  loss:  0.0003506980719976127
Batch  41  loss:  0.0003272629401180893
Batch  51  loss:  0.0004672950890380889
Batch  61  loss:  0.00034674457856453955
Batch  71  loss:  0.0002566203474998474
Batch  81  loss:  0.00026529334718361497
Batch  91  loss:  0.0003703690890688449
Batch  101  loss:  0.00031502789352089167
Batch  111  loss:  0.0007075275061652064
Batch  121  loss:  0.0002167033962905407
Batch  131  loss:  0.00024197110906243324
Batch  141  loss:  0.00032450834987685084
Batch  151  loss:  0.00025052428827621043
Batch  161  loss:  0.00022442576300818473
Batch  171  loss:  0.00031493179267272353
Batch  181  loss:  0.00017100945115089417
Batch  191  loss:  0.00044804505887441337
Validation on real data: 
LOSS supervised-train 0.0003059706162457587, valid 0.0004525593831203878
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0003310781321488321
Batch  11  loss:  0.00048232750850729644
Batch  21  loss:  0.00020782776118721813
Batch  31  loss:  0.00042050599586218596
Batch  41  loss:  0.0002293638972332701
Batch  51  loss:  0.0004110480658710003
Batch  61  loss:  0.0003600598720367998
Batch  71  loss:  0.00024972500978037715
Batch  81  loss:  0.00018935443949885666
Batch  91  loss:  0.0003326857404317707
Batch  101  loss:  0.00033841768163256347
Batch  111  loss:  0.0006263955729082227
Batch  121  loss:  0.0002810460573527962
Batch  131  loss:  0.0002897150698117912
Batch  141  loss:  0.0003236987395212054
Batch  151  loss:  0.00025097280740737915
Batch  161  loss:  0.0002720034681260586
Batch  171  loss:  0.00033983096363954246
Batch  181  loss:  0.00025186300626955926
Batch  191  loss:  0.0004122642276342958
Validation on real data: 
LOSS supervised-train 0.00029826026009686755, valid 0.00031407643109560013
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0002765042881947011
Batch  11  loss:  0.00045325286919251084
Batch  21  loss:  0.00021724931139033288
Batch  31  loss:  0.00027299794601276517
Batch  41  loss:  0.00026463065296411514
Batch  51  loss:  0.0003681388043332845
Batch  61  loss:  0.00027577864238992333
Batch  71  loss:  0.00022987085685599595
Batch  81  loss:  0.00021038114209659398
Batch  91  loss:  0.00031304743606597185
Batch  101  loss:  0.00020226197375450283
Batch  111  loss:  0.000566889822948724
Batch  121  loss:  0.0002749601553659886
Batch  131  loss:  0.0002698429743759334
Batch  141  loss:  0.0004788708465639502
Batch  151  loss:  0.00023069481540005654
Batch  161  loss:  0.0002529011690057814
Batch  171  loss:  0.00039793469477444887
Batch  181  loss:  0.00023759942268952727
Batch  191  loss:  0.00038800950278528035
Validation on real data: 
LOSS supervised-train 0.0002932772198255407, valid 0.00030134397093206644
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0002976833493448794
Batch  11  loss:  0.00038319668965414166
Batch  21  loss:  0.0002050265611615032
Batch  31  loss:  0.00029576814267784357
Batch  41  loss:  0.00025757294497452676
Batch  51  loss:  0.00032567678135819733
Batch  61  loss:  0.0003166145761497319
Batch  71  loss:  0.00029254250694066286
Batch  81  loss:  0.0001924085518112406
Batch  91  loss:  0.0002695203584153205
Batch  101  loss:  0.00027638289611786604
Batch  111  loss:  0.0005738845211453736
Batch  121  loss:  0.0002630937669891864
Batch  131  loss:  0.00024103459145408124
Batch  141  loss:  0.00026075023924931884
Batch  151  loss:  0.00021664815722033381
Batch  161  loss:  0.000276337843388319
Batch  171  loss:  0.00027045991737395525
Batch  181  loss:  0.0001645775919314474
Batch  191  loss:  0.000361179030733183
Validation on real data: 
LOSS supervised-train 0.00027704574196832257, valid 0.00022887303202878684
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0004143158148508519
Batch  11  loss:  0.00036698358599096537
Batch  21  loss:  0.0001771704846760258
Batch  31  loss:  0.00028248887974768877
Batch  41  loss:  0.0002924549044109881
Batch  51  loss:  0.0003519025631248951
Batch  61  loss:  0.0003358474641572684
Batch  71  loss:  0.00019179827359039336
Batch  81  loss:  0.00021150056272745132
Batch  91  loss:  0.00027241508360020816
Batch  101  loss:  0.00019871346012223512
Batch  111  loss:  0.0006131567643024027
Batch  121  loss:  0.00026242941385135055
Batch  131  loss:  0.0001875032321549952
Batch  141  loss:  0.0003437040722928941
Batch  151  loss:  0.0002703071222640574
Batch  161  loss:  0.00032294547418132424
Batch  171  loss:  0.00028096247115172446
Batch  181  loss:  0.00022613377950619906
Batch  191  loss:  0.0003619530762080103
Validation on real data: 
LOSS supervised-train 0.0002778513999510324, valid 0.0002566949697211385
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0002960051060654223
Batch  11  loss:  0.0002876025973819196
Batch  21  loss:  0.0002088570618070662
Batch  31  loss:  0.0002871188917197287
Batch  41  loss:  0.00021466694306582212
Batch  51  loss:  0.00035649247001856565
Batch  61  loss:  0.0003643989912234247
Batch  71  loss:  0.00018029051716439426
Batch  81  loss:  0.00019977695774286985
Batch  91  loss:  0.00023887290444690734
Batch  101  loss:  0.00024375658540520817
Batch  111  loss:  0.0005279601900838315
Batch  121  loss:  0.00023426886764355004
Batch  131  loss:  0.0002464673889335245
Batch  141  loss:  0.000380875077098608
Batch  151  loss:  0.00018635898595675826
Batch  161  loss:  0.0002523997100070119
Batch  171  loss:  0.0002818532520905137
Batch  181  loss:  0.000178137983311899
Batch  191  loss:  0.0003891691449098289
Validation on real data: 
LOSS supervised-train 0.00027195879352802876, valid 0.0002768550184555352
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00028393114916980267
Batch  11  loss:  0.00032517078216187656
Batch  21  loss:  0.0002649173547979444
Batch  31  loss:  0.0003049890510737896
Batch  41  loss:  0.0002261048648506403
Batch  51  loss:  0.00033321380033157766
Batch  61  loss:  0.00030718682683072984
Batch  71  loss:  0.0002655076968949288
Batch  81  loss:  0.00020159714040346444
Batch  91  loss:  0.0003079861053265631
Batch  101  loss:  0.0002507517929188907
Batch  111  loss:  0.0005315582384355366
Batch  121  loss:  0.00027528454666025937
Batch  131  loss:  0.00024040219432208687
Batch  141  loss:  0.00041465304093435407
Batch  151  loss:  0.00025743042351678014
Batch  161  loss:  0.0002824966795742512
Batch  171  loss:  0.0003298060910310596
Batch  181  loss:  0.00022682342387270182
Batch  191  loss:  0.0003750518662855029
Validation on real data: 
LOSS supervised-train 0.000271923547188635, valid 0.000298749771900475
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0002781562798190862
Batch  11  loss:  0.00028322794241830707
Batch  21  loss:  0.00023731925466563553
Batch  31  loss:  0.00030249301926232874
Batch  41  loss:  0.00026940935640595853
Batch  51  loss:  0.0003313745546620339
Batch  61  loss:  0.00030777676147408783
Batch  71  loss:  0.00023386458633467555
Batch  81  loss:  0.0002267947420477867
Batch  91  loss:  0.0003657781344372779
Batch  101  loss:  0.00021893865778110921
Batch  111  loss:  0.0004710536450147629
Batch  121  loss:  0.0002476366644259542
Batch  131  loss:  0.00021049559290986508
Batch  141  loss:  0.00027354632038623095
Batch  151  loss:  0.0001983466645469889
Batch  161  loss:  0.0002353832678636536
Batch  171  loss:  0.00029487613937817514
Batch  181  loss:  0.00018703099340200424
Batch  191  loss:  0.00033846378210000694
Validation on real data: 
LOSS supervised-train 0.00025910206510161517, valid 0.00028934021247550845
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0002922852581832558
Batch  11  loss:  0.000398515461711213
Batch  21  loss:  0.00023224446340464056
Batch  31  loss:  0.0002979571290779859
Batch  41  loss:  0.00022470179828815162
Batch  51  loss:  0.0002938576799351722
Batch  61  loss:  0.0002919651451520622
Batch  71  loss:  0.000249651784542948
Batch  81  loss:  0.0002010893076658249
Batch  91  loss:  0.00033998608705587685
Batch  101  loss:  0.00030262014479376376
Batch  111  loss:  0.0006762028206139803
Batch  121  loss:  0.00023812013387214392
Batch  131  loss:  0.000200734575628303
Batch  141  loss:  0.00030404317658394575
Batch  151  loss:  0.0002706956001929939
Batch  161  loss:  0.00024416978703811765
Batch  171  loss:  0.00027644538204185665
Batch  181  loss:  0.0001956626947503537
Batch  191  loss:  0.00032693531829863787
Validation on real data: 
LOSS supervised-train 0.00026530164264841003, valid 0.00026581654674373567
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.00031202269019559026
Batch  11  loss:  0.0004717065312433988
Batch  21  loss:  0.00018239991914015263
Batch  31  loss:  0.0002449129824526608
Batch  41  loss:  0.00021641765488311648
Batch  51  loss:  0.0003996171581093222
Batch  61  loss:  0.0002093306538881734
Batch  71  loss:  0.00021751817257609218
Batch  81  loss:  0.00021738250507041812
Batch  91  loss:  0.00024557020515203476
Batch  101  loss:  0.000231420315685682
Batch  111  loss:  0.00035621397546492517
Batch  121  loss:  0.0002718115574680269
Batch  131  loss:  0.0002346535329706967
Batch  141  loss:  0.0002908895839937031
Batch  151  loss:  0.00021961014135740697
Batch  161  loss:  0.0002459531824570149
Batch  171  loss:  0.00031157955527305603
Batch  181  loss:  0.000249182601692155
Batch  191  loss:  0.00036254266160540283
Validation on real data: 
LOSS supervised-train 0.0002600424038246274, valid 0.00022318941773846745
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0002936187374871224
Batch  11  loss:  0.0002827606804203242
Batch  21  loss:  0.00021573816775344312
Batch  31  loss:  0.0003084670752286911
Batch  41  loss:  0.00021538686996791512
Batch  51  loss:  0.00027132040122523904
Batch  61  loss:  0.0003410025092307478
Batch  71  loss:  0.00028295046649873257
Batch  81  loss:  0.00018077786080539227
Batch  91  loss:  0.0002613381657283753
Batch  101  loss:  0.00022683106362819672
Batch  111  loss:  0.00037022685864940286
Batch  121  loss:  0.00020371384744066745
Batch  131  loss:  0.00024160328030120581
Batch  141  loss:  0.0004297078703530133
Batch  151  loss:  0.00019734316447284073
Batch  161  loss:  0.00024892116198316216
Batch  171  loss:  0.00030912310467101634
Batch  181  loss:  0.00020703936752397567
Batch  191  loss:  0.0003590690903365612
Validation on real data: 
LOSS supervised-train 0.0002531275356159313, valid 0.0002950693015009165
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.00022324470046442002
Batch  11  loss:  0.00037900544703006744
Batch  21  loss:  0.00024440180277451873
Batch  31  loss:  0.00025045539950951934
Batch  41  loss:  0.00020565926388371736
Batch  51  loss:  0.00030403296113945544
Batch  61  loss:  0.0003304629062768072
Batch  71  loss:  0.0002903181593865156
Batch  81  loss:  0.00023321235494222492
Batch  91  loss:  0.00034351853537373245
Batch  101  loss:  0.00021253968589007854
Batch  111  loss:  0.0004297723644413054
Batch  121  loss:  0.00022733159130439162
Batch  131  loss:  0.00021498433488886803
Batch  141  loss:  0.00038508936995640397
Batch  151  loss:  0.00019170803716406226
Batch  161  loss:  0.00019677350064739585
Batch  171  loss:  0.0002803444804158062
Batch  181  loss:  0.00018298176291864365
Batch  191  loss:  0.00041441916255280375
Validation on real data: 
LOSS supervised-train 0.00025221527241228617, valid 0.0002469874743837863
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00035317859146744013
Batch  11  loss:  0.00026062250253744423
Batch  21  loss:  0.00015612800780218095
Batch  31  loss:  0.00021808297606185079
Batch  41  loss:  0.00022033815912436694
Batch  51  loss:  0.000302811007713899
Batch  61  loss:  0.0002868781448341906
Batch  71  loss:  0.00025707363965921104
Batch  81  loss:  0.00023634207900613546
Batch  91  loss:  0.00028073479188606143
Batch  101  loss:  0.00020640213915612549
Batch  111  loss:  0.0006099464953877032
Batch  121  loss:  0.00022049590188544244
Batch  131  loss:  0.00017124910664279014
Batch  141  loss:  0.0003154187579639256
Batch  151  loss:  0.00020661966118495911
Batch  161  loss:  0.0002498910471331328
Batch  171  loss:  0.0002038672537310049
Batch  181  loss:  0.00020752463024109602
Batch  191  loss:  0.00041096098721027374
Validation on real data: 
LOSS supervised-train 0.0002411338501769933, valid 0.00024062240845523775
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00028520182240754366
Batch  11  loss:  0.00031010969541966915
Batch  21  loss:  0.00021436047973111272
Batch  31  loss:  0.0002595890546217561
Batch  41  loss:  0.0002724563528317958
Batch  51  loss:  0.00032886763801798224
Batch  61  loss:  0.00021081916929688305
Batch  71  loss:  0.0002094210940413177
Batch  81  loss:  0.00016204900748562068
Batch  91  loss:  0.0002859837550204247
Batch  101  loss:  0.00018603072385303676
Batch  111  loss:  0.000544393144082278
Batch  121  loss:  0.0002824331750161946
Batch  131  loss:  0.0001960697554750368
Batch  141  loss:  0.0002311762800673023
Batch  151  loss:  0.00016708049224689603
Batch  161  loss:  0.0002490214246790856
Batch  171  loss:  0.00025144583196379244
Batch  181  loss:  0.00021588624804280698
Batch  191  loss:  0.0003795948578044772
Validation on real data: 
LOSS supervised-train 0.00024611921813630036, valid 0.00025881017791107297
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0002772826992440969
Batch  11  loss:  0.0003786830639000982
Batch  21  loss:  0.0001647014287300408
Batch  31  loss:  0.00025315184029750526
Batch  41  loss:  0.00024320490774698555
Batch  51  loss:  0.00022772754891775548
Batch  61  loss:  0.00020816623873542994
Batch  71  loss:  0.00021499543800018728
Batch  81  loss:  0.0001708792260615155
Batch  91  loss:  0.0002559521235525608
Batch  101  loss:  0.0001358531735604629
Batch  111  loss:  0.0004894728190265596
Batch  121  loss:  0.00019479991169646382
Batch  131  loss:  0.00022121811343822628
Batch  141  loss:  0.00026688078651204705
Batch  151  loss:  0.00019777959096245468
Batch  161  loss:  0.00022497314785141498
Batch  171  loss:  0.00021904005552642047
Batch  181  loss:  0.00019862994668073952
Batch  191  loss:  0.0003165443195030093
Validation on real data: 
LOSS supervised-train 0.00023056440273649058, valid 0.0002859937376342714
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00027182247140444815
Batch  11  loss:  0.0003399416455067694
Batch  21  loss:  0.0002043895801762119
Batch  31  loss:  0.0002631654206197709
Batch  41  loss:  0.00023330769909080118
Batch  51  loss:  0.0003272077883593738
Batch  61  loss:  0.00018885958706960082
Batch  71  loss:  0.0002580061263870448
Batch  81  loss:  0.0001525154511909932
Batch  91  loss:  0.00029601663118228316
Batch  101  loss:  0.000198205336346291
Batch  111  loss:  0.00043946472578682005
Batch  121  loss:  0.00018943888426292688
Batch  131  loss:  0.00027477554976940155
Batch  141  loss:  0.0004148482403252274
Batch  151  loss:  0.00018112683028448373
Batch  161  loss:  0.00019553638412617147
Batch  171  loss:  0.0003141360357403755
Batch  181  loss:  0.00022929611441213638
Batch  191  loss:  0.0002600056759547442
Validation on real data: 
LOSS supervised-train 0.00023117821034247755, valid 0.0003113851707894355
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0002825641131494194
Batch  11  loss:  0.0002721540513448417
Batch  21  loss:  0.00015014968812465668
Batch  31  loss:  0.00022927686222828925
Batch  41  loss:  0.00026128857280127704
Batch  51  loss:  0.00028738644323311746
Batch  61  loss:  0.0002596092817839235
Batch  71  loss:  0.00028999202186241746
Batch  81  loss:  0.00018227573309559375
Batch  91  loss:  0.0002611629315651953
Batch  101  loss:  0.00019276930834166706
Batch  111  loss:  0.0004650471673812717
Batch  121  loss:  0.00023011579469311982
Batch  131  loss:  0.00019080670608673245
Batch  141  loss:  0.0003267187566962093
Batch  151  loss:  0.00015444554446730763
Batch  161  loss:  0.00021933039533905685
Batch  171  loss:  0.0002326159446965903
Batch  181  loss:  0.00021715938055422157
Batch  191  loss:  0.00031546555692330003
Validation on real data: 
LOSS supervised-train 0.00023245756725373212, valid 0.00030920657445676625
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0003112926206085831
Batch  11  loss:  0.0003600087948143482
Batch  21  loss:  0.0002532519574742764
Batch  31  loss:  0.0001875333982752636
Batch  41  loss:  0.00020070112077519298
Batch  51  loss:  0.00019542881636880338
Batch  61  loss:  0.0002456389192957431
Batch  71  loss:  0.00019683151913341135
Batch  81  loss:  0.00015919297584332526
Batch  91  loss:  0.0002902131818700582
Batch  101  loss:  0.00018147203081753105
Batch  111  loss:  0.00046650494914501905
Batch  121  loss:  0.0002187773206969723
Batch  131  loss:  0.00022500746126752347
Batch  141  loss:  0.00028554632444866
Batch  151  loss:  0.00020899853552691638
Batch  161  loss:  0.0002572587982285768
Batch  171  loss:  0.00024107126228045672
Batch  181  loss:  0.00019500903727021068
Batch  191  loss:  0.0002778976340778172
Validation on real data: 
LOSS supervised-train 0.0002262869571131887, valid 0.00023314409190788865
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00024007675528991967
Batch  11  loss:  0.0003355977241881192
Batch  21  loss:  0.00020141560526099056
Batch  31  loss:  0.00023135493393056095
Batch  41  loss:  0.000242970694671385
Batch  51  loss:  0.0002972260699607432
Batch  61  loss:  0.00021686500986106694
Batch  71  loss:  0.00016617424262221903
Batch  81  loss:  0.00015088432701304555
Batch  91  loss:  0.00022546324180439115
Batch  101  loss:  0.000255538965575397
Batch  111  loss:  0.00039389528683386743
Batch  121  loss:  0.0001938774366863072
Batch  131  loss:  0.0001766006025718525
Batch  141  loss:  0.00026718591107055545
Batch  151  loss:  0.00019402442558202893
Batch  161  loss:  0.00026812576106749475
Batch  171  loss:  0.00026052628527395427
Batch  181  loss:  0.00022845611965749413
Batch  191  loss:  0.0003428119816817343
Validation on real data: 
LOSS supervised-train 0.00022336912279570243, valid 0.00032649998320266604
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0002399282093392685
Batch  11  loss:  0.00036371219903230667
Batch  21  loss:  0.00020683114416897297
Batch  31  loss:  0.0003164018562529236
Batch  41  loss:  0.0002924606087617576
Batch  51  loss:  0.0002673036069609225
Batch  61  loss:  0.0002967880864161998
Batch  71  loss:  0.0002229159144917503
Batch  81  loss:  0.00016298374976031482
Batch  91  loss:  0.0002667077351361513
Batch  101  loss:  0.00018348917365074158
Batch  111  loss:  0.00035131655749864876
Batch  121  loss:  0.0001645365118747577
Batch  131  loss:  0.00019346398767083883
Batch  141  loss:  0.00030491172219626606
Batch  151  loss:  0.00023508956655859947
Batch  161  loss:  0.00029041344532743096
Batch  171  loss:  0.00026482410612516105
Batch  181  loss:  0.00019983529637102038
Batch  191  loss:  0.0003183895896654576
Validation on real data: 
LOSS supervised-train 0.0002183890008018352, valid 0.0003448288480285555
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00026381536736153066
Batch  11  loss:  0.0002359525387873873
Batch  21  loss:  0.00017209340876433998
Batch  31  loss:  0.0002049160684691742
Batch  41  loss:  0.00023738022719044238
Batch  51  loss:  0.0002431087486911565
Batch  61  loss:  0.00023955055803526193
Batch  71  loss:  0.00021448283223435283
Batch  81  loss:  0.00024291056615766138
Batch  91  loss:  0.00027687818510457873
Batch  101  loss:  0.00022172617900650948
Batch  111  loss:  0.0004603364213835448
Batch  121  loss:  0.00021490437211468816
Batch  131  loss:  0.00023338287428487092
Batch  141  loss:  0.0003140449116472155
Batch  151  loss:  0.00016082111687865108
Batch  161  loss:  0.00024208745162468404
Batch  171  loss:  0.000286276190308854
Batch  181  loss:  0.00021107065549585968
Batch  191  loss:  0.00031715602381154895
Validation on real data: 
LOSS supervised-train 0.00021942568928352558, valid 0.0002488493046257645
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0002697822346817702
Batch  11  loss:  0.00031005818163976073
Batch  21  loss:  0.0001645944721531123
Batch  31  loss:  0.00024768096045590937
Batch  41  loss:  0.0002169427607441321
Batch  51  loss:  0.00035117671359330416
Batch  61  loss:  0.0002811560407280922
Batch  71  loss:  0.00018750141316559166
Batch  81  loss:  0.0002072923962259665
Batch  91  loss:  0.00027112607494927943
Batch  101  loss:  0.00022593350149691105
Batch  111  loss:  0.0005627732025459409
Batch  121  loss:  0.0001909020938910544
Batch  131  loss:  0.0002145750040654093
Batch  141  loss:  0.0002592302917037159
Batch  151  loss:  0.00021333892073016614
Batch  161  loss:  0.00021375909273047
Batch  171  loss:  0.00022876921866554767
Batch  181  loss:  0.0001493884774390608
Batch  191  loss:  0.00023244463955052197
Validation on real data: 
LOSS supervised-train 0.00021920456198131432, valid 0.00027381483232602477
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00023701101599726826
Batch  11  loss:  0.00032192241633310914
Batch  21  loss:  0.00015837456157896668
Batch  31  loss:  0.00027001622947864234
Batch  41  loss:  0.0002345280663575977
Batch  51  loss:  0.00020593094814103097
Batch  61  loss:  0.0003341971314512193
Batch  71  loss:  0.00018871738575398922
Batch  81  loss:  0.0001850342086981982
Batch  91  loss:  0.000260713481111452
Batch  101  loss:  0.00021185557125136256
Batch  111  loss:  0.00048014192725531757
Batch  121  loss:  0.00016765433247201145
Batch  131  loss:  0.0002320845815120265
Batch  141  loss:  0.00030585244530811906
Batch  151  loss:  0.0001872586435638368
Batch  161  loss:  0.00023086894361767918
Batch  171  loss:  0.00017866819689515978
Batch  181  loss:  0.0001621647534193471
Batch  191  loss:  0.00036725925747305155
Validation on real data: 
LOSS supervised-train 0.00021563320526183817, valid 0.00023981407866813242
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0002584069443400949
Batch  11  loss:  0.00024124096671584994
Batch  21  loss:  0.00021618555183522403
Batch  31  loss:  0.00023707481159362942
Batch  41  loss:  0.00021322313114069402
Batch  51  loss:  0.0002445814316160977
Batch  61  loss:  0.00023372498981188983
Batch  71  loss:  0.0001805819192668423
Batch  81  loss:  0.00012763911217916757
Batch  91  loss:  0.000310881354380399
Batch  101  loss:  0.00025395621196366847
Batch  111  loss:  0.0003219279460608959
Batch  121  loss:  0.00021455209935083985
Batch  131  loss:  0.00021661161736119539
Batch  141  loss:  0.0002654990821611136
Batch  151  loss:  0.00021186168305575848
Batch  161  loss:  0.00020596115791704506
Batch  171  loss:  0.00019570416770875454
Batch  181  loss:  0.0002450526808388531
Batch  191  loss:  0.0003140815533697605
Validation on real data: 
LOSS supervised-train 0.00021825543015438598, valid 0.0003838748671114445
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0002454084169585258
Batch  11  loss:  0.00024370383471250534
Batch  21  loss:  0.00020381140348035842
Batch  31  loss:  0.00015559233725070953
Batch  41  loss:  0.0002121453289873898
Batch  51  loss:  0.0002579120628070086
Batch  61  loss:  0.00015531061217188835
Batch  71  loss:  0.00022359534341376275
Batch  81  loss:  0.00014632819511462003
Batch  91  loss:  0.0002916564408224076
Batch  101  loss:  0.0001996627397602424
Batch  111  loss:  0.0003192860458511859
Batch  121  loss:  0.00016260774282272905
Batch  131  loss:  0.00020789957488887012
Batch  141  loss:  0.00032772828126326203
Batch  151  loss:  0.00016720671555958688
Batch  161  loss:  0.00015772828191984445
Batch  171  loss:  0.00013776292325928807
Batch  181  loss:  0.00022075306333135813
Batch  191  loss:  0.00031964434310793877
Validation on real data: 
LOSS supervised-train 0.00021055654357041932, valid 0.00022971566068008542
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00024999535526148975
Batch  11  loss:  0.00024567058426328003
Batch  21  loss:  0.00018141618056688458
Batch  31  loss:  0.00020012210006825626
Batch  41  loss:  0.000176522575202398
Batch  51  loss:  0.00018527585780248046
Batch  61  loss:  0.0002524437732063234
Batch  71  loss:  0.0001563707774039358
Batch  81  loss:  0.00014125970483291894
Batch  91  loss:  0.0002607108326628804
Batch  101  loss:  0.00018494977848604321
Batch  111  loss:  0.00040040037129074335
Batch  121  loss:  0.00018245261162519455
Batch  131  loss:  0.00016069418052211404
Batch  141  loss:  0.0003727140137925744
Batch  151  loss:  0.0001985020498977974
Batch  161  loss:  0.0002669152454473078
Batch  171  loss:  0.00019644932763185352
Batch  181  loss:  0.00021044000459369272
Batch  191  loss:  0.00027654721634462476
Validation on real data: 
LOSS supervised-train 0.00020453846340387827, valid 0.0002837686042767018
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00033112941309809685
Batch  11  loss:  0.00023055901692714542
Batch  21  loss:  0.00017845301772467792
Batch  31  loss:  0.00022888292733114213
Batch  41  loss:  0.0002709650434553623
Batch  51  loss:  0.00022383588657248765
Batch  61  loss:  0.0002678105956874788
Batch  71  loss:  0.0002082900464301929
Batch  81  loss:  0.00014741621271241456
Batch  91  loss:  0.00028085632948204875
Batch  101  loss:  0.00018319602531846613
Batch  111  loss:  0.00032926377025432885
Batch  121  loss:  0.00015885136963333935
Batch  131  loss:  0.00019952018919866532
Batch  141  loss:  0.0002433541667414829
Batch  151  loss:  0.00017695597489364445
Batch  161  loss:  0.0002211037208326161
Batch  171  loss:  0.0002072289789794013
Batch  181  loss:  0.00012913564569316804
Batch  191  loss:  0.00028308885521255434
Validation on real data: 
LOSS supervised-train 0.00020380240177473753, valid 0.00026789287221617997
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.00021937569545116276
Batch  11  loss:  0.0002619482984300703
Batch  21  loss:  0.00019419961608946323
Batch  31  loss:  0.00022676131629850715
Batch  41  loss:  0.00021073401148896664
Batch  51  loss:  0.00019263745343778282
Batch  61  loss:  0.00020796945318579674
Batch  71  loss:  0.0001720349828246981
Batch  81  loss:  0.00013555017358157784
Batch  91  loss:  0.00022021577751729637
Batch  101  loss:  0.00016142451204359531
Batch  111  loss:  0.00029039691435173154
Batch  121  loss:  0.00014444849512074143
Batch  131  loss:  0.00021795446809846908
Batch  141  loss:  0.00033101774170063436
Batch  151  loss:  0.00014175537216942757
Batch  161  loss:  0.00018213070870842785
Batch  171  loss:  0.0002036386722465977
Batch  181  loss:  0.00018908835772890598
Batch  191  loss:  0.0003060369344893843
Validation on real data: 
LOSS supervised-train 0.00020166813796095083, valid 0.00027876454987563193
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00022940784401725978
Batch  11  loss:  0.0002413442125543952
Batch  21  loss:  0.00024138101434800774
Batch  31  loss:  0.00024559078156016767
Batch  41  loss:  0.00021054971148259938
Batch  51  loss:  0.00018616543093230575
Batch  61  loss:  0.00019061652710661292
Batch  71  loss:  0.00020769792899955064
Batch  81  loss:  0.00015712212189100683
Batch  91  loss:  0.00026089686434715986
Batch  101  loss:  0.00014471501344814897
Batch  111  loss:  0.000242912836256437
Batch  121  loss:  0.0001652535720495507
Batch  131  loss:  0.0001504534448031336
Batch  141  loss:  0.0002782704250421375
Batch  151  loss:  0.0002467184385750443
Batch  161  loss:  0.0001931143196998164
Batch  171  loss:  0.00016635764040984213
Batch  181  loss:  0.00019513297593221068
Batch  191  loss:  0.00023851053265389055
Validation on real data: 
LOSS supervised-train 0.00019658912045997568, valid 0.0002780792419798672
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.00022221243125386536
Batch  11  loss:  0.0002522318682167679
Batch  21  loss:  0.00013354407565202564
Batch  31  loss:  0.0001815368013922125
Batch  41  loss:  0.00017801632930058986
Batch  51  loss:  0.00018542067846283317
Batch  61  loss:  0.0002550165809225291
Batch  71  loss:  0.0002141583972843364
Batch  81  loss:  0.0001616162044228986
Batch  91  loss:  0.00028603774262592196
Batch  101  loss:  0.00017251927056349814
Batch  111  loss:  0.00034667132422327995
Batch  121  loss:  0.0001974487240659073
Batch  131  loss:  0.0001873297878773883
Batch  141  loss:  0.0002863428962882608
Batch  151  loss:  0.00018281955271959305
Batch  161  loss:  0.00021500761795323342
Batch  171  loss:  0.00021176954032853246
Batch  181  loss:  0.00015490078658331186
Batch  191  loss:  0.00026772572891786695
Validation on real data: 
LOSS supervised-train 0.0001991008986806264, valid 0.00023333590070251375
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00025498998002149165
Batch  11  loss:  0.00018215096497442573
Batch  21  loss:  0.00018717977218329906
Batch  31  loss:  0.00017657260468695313
Batch  41  loss:  0.0001509231369709596
Batch  51  loss:  0.00018378182721789926
Batch  61  loss:  0.00021330926392693073
Batch  71  loss:  0.000163108081324026
Batch  81  loss:  0.00013323486200533807
Batch  91  loss:  0.00020458207291085273
Batch  101  loss:  0.0001692866499070078
Batch  111  loss:  0.0002439606178086251
Batch  121  loss:  0.00017199691501446068
Batch  131  loss:  0.00021807551092933863
Batch  141  loss:  0.00029638694832101464
Batch  151  loss:  0.00016205335850827396
Batch  161  loss:  0.00020309809769969434
Batch  171  loss:  0.00019795435946434736
Batch  181  loss:  0.0001366209180559963
Batch  191  loss:  0.0002389451110502705
Validation on real data: 
LOSS supervised-train 0.0001920625175262103, valid 0.00025413898401893675
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.00021771108731627464
Batch  11  loss:  0.00017545881564728916
Batch  21  loss:  0.00018278838251717389
Batch  31  loss:  0.00017160324205178767
Batch  41  loss:  0.0001856775052146986
Batch  51  loss:  0.0002628606162033975
Batch  61  loss:  0.0002831839956343174
Batch  71  loss:  0.00019578717183321714
Batch  81  loss:  0.0001542119134683162
Batch  91  loss:  0.00023763500212226063
Batch  101  loss:  0.00020689160737674683
Batch  111  loss:  0.0003560322511475533
Batch  121  loss:  0.00014792938600294292
Batch  131  loss:  0.00012112939293729141
Batch  141  loss:  0.000263766705757007
Batch  151  loss:  0.00020056392531841993
Batch  161  loss:  0.00020649723592214286
Batch  171  loss:  0.00019465613877400756
Batch  181  loss:  0.00019096590403933078
Batch  191  loss:  0.0002669490349944681
Validation on real data: 
LOSS supervised-train 0.0001954030218621483, valid 0.0002151374937966466
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00024831280461512506
Batch  11  loss:  0.0002182849420933053
Batch  21  loss:  0.00018712901510298252
Batch  31  loss:  0.00019021614571101964
Batch  41  loss:  0.0001604983553988859
Batch  51  loss:  0.00019587094720918685
Batch  61  loss:  0.00018875024397857487
Batch  71  loss:  0.00021472855587489903
Batch  81  loss:  0.0001431389682693407
Batch  91  loss:  0.00018421649292577058
Batch  101  loss:  0.0001821832702262327
Batch  111  loss:  0.0003398386179469526
Batch  121  loss:  0.00017736360314302146
Batch  131  loss:  0.00013217848027125
Batch  141  loss:  0.00016996172780636698
Batch  151  loss:  0.00015745434211567044
Batch  161  loss:  0.00020464608678594232
Batch  171  loss:  0.00018575011927168816
Batch  181  loss:  0.00020074487838428468
Batch  191  loss:  0.0002006407594308257
Validation on real data: 
LOSS supervised-train 0.00018683797665289603, valid 0.00024283441598527133
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0002854609047062695
Batch  11  loss:  0.00022050220286473632
Batch  21  loss:  0.00015421808348037302
Batch  31  loss:  0.00015610037371516228
Batch  41  loss:  0.0002010563330259174
Batch  51  loss:  0.0001660010893829167
Batch  61  loss:  0.00015847750182729214
Batch  71  loss:  0.00019773478561546654
Batch  81  loss:  0.0001654951774980873
Batch  91  loss:  0.0003299004165455699
Batch  101  loss:  0.00015037468983791769
Batch  111  loss:  0.00031189146102406085
Batch  121  loss:  0.00015028905181679875
Batch  131  loss:  0.00014646757335867733
Batch  141  loss:  0.00029217387782409787
Batch  151  loss:  0.00013507828407455236
Batch  161  loss:  0.00016878639871720225
Batch  171  loss:  0.00017113744979724288
Batch  181  loss:  0.00023080329992808402
Batch  191  loss:  0.00026453135069459677
Validation on real data: 
LOSS supervised-train 0.00019238435474107973, valid 0.0002620798477437347
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00019100688223261386
Batch  11  loss:  0.0002740449854172766
Batch  21  loss:  0.00018648004333954304
Batch  31  loss:  0.00017367705004289746
Batch  41  loss:  0.00019355572294443846
Batch  51  loss:  0.00018905698379967362
Batch  61  loss:  0.00017597585974726826
Batch  71  loss:  0.00019517030159477144
Batch  81  loss:  0.00015084704500623047
Batch  91  loss:  0.00018271042790729553
Batch  101  loss:  0.00014243223995435983
Batch  111  loss:  0.00032763686613179743
Batch  121  loss:  0.00021606117661576718
Batch  131  loss:  0.0001649844052735716
Batch  141  loss:  0.00021685237879864872
Batch  151  loss:  0.00018272761371918023
Batch  161  loss:  0.0002047558518825099
Batch  171  loss:  0.00014491626643575728
Batch  181  loss:  0.00017333743744529784
Batch  191  loss:  0.0002777767658699304
Validation on real data: 
LOSS supervised-train 0.00018896165671321795, valid 0.0002886233269236982
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00024282692174892873
Batch  11  loss:  0.00024643924552947283
Batch  21  loss:  0.00018412369536235929
Batch  31  loss:  0.00018929260841105133
Batch  41  loss:  0.00018733736942522228
Batch  51  loss:  0.0001680006826063618
Batch  61  loss:  0.00026050160522572696
Batch  71  loss:  0.00015731470193713903
Batch  81  loss:  0.00014977405953686684
Batch  91  loss:  0.0002966728643514216
Batch  101  loss:  0.0001534070906927809
Batch  111  loss:  0.00027218053583055735
Batch  121  loss:  0.00018716360500548035
Batch  131  loss:  0.00017569030751474202
Batch  141  loss:  0.0002520164707675576
Batch  151  loss:  0.00020621206203941256
Batch  161  loss:  0.00015094577975105494
Batch  171  loss:  0.0001693609810899943
Batch  181  loss:  0.00021156156435608864
Batch  191  loss:  0.00019778254500124604
Validation on real data: 
LOSS supervised-train 0.0001887074710248271, valid 0.0002484418801032007
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00020073387713637203
Batch  11  loss:  0.0002669797686394304
Batch  21  loss:  0.0001608460006536916
Batch  31  loss:  0.00018398830434307456
Batch  41  loss:  0.0001726568880258128
Batch  51  loss:  0.00021010484488215297
Batch  61  loss:  0.00023890656302683055
Batch  71  loss:  0.00020596421381924301
Batch  81  loss:  0.00017458848014939576
Batch  91  loss:  0.0002764873206615448
Batch  101  loss:  0.0002180394803872332
Batch  111  loss:  0.00038062952808104455
Batch  121  loss:  0.00018375330546405166
Batch  131  loss:  0.00016734257224015892
Batch  141  loss:  0.00022750957577954978
Batch  151  loss:  0.00014037112123332918
Batch  161  loss:  0.0001527214772067964
Batch  171  loss:  0.00022125523537397385
Batch  181  loss:  0.00019362506282050163
Batch  191  loss:  0.00026912378962151706
Validation on real data: 
LOSS supervised-train 0.00019223748517106288, valid 0.00020904550910927355
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00017148815095424652
Batch  11  loss:  0.00023184131714515388
Batch  21  loss:  0.0001873890869319439
Batch  31  loss:  0.0001599957759026438
Batch  41  loss:  0.00014701898908242583
Batch  51  loss:  0.00019507555407471955
Batch  61  loss:  0.00015453729429282248
Batch  71  loss:  0.0001477014011470601
Batch  81  loss:  0.00017566794122103602
Batch  91  loss:  0.0002463546406943351
Batch  101  loss:  0.00016404231428168714
Batch  111  loss:  0.00026474311016499996
Batch  121  loss:  0.00014456527424044907
Batch  131  loss:  0.0001563122059451416
Batch  141  loss:  0.0003090602403972298
Batch  151  loss:  0.00019245634030085057
Batch  161  loss:  0.00015055021503940225
Batch  171  loss:  0.00017588950868230313
Batch  181  loss:  0.00015775347128510475
Batch  191  loss:  0.00028548826230689883
Validation on real data: 
LOSS supervised-train 0.00018427058985253096, valid 0.00022607302526012063
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00021168320381548256
Batch  11  loss:  0.0001872893190011382
Batch  21  loss:  0.00015922794409561902
Batch  31  loss:  0.00019537266052793711
Batch  41  loss:  0.00016327935736626387
Batch  51  loss:  0.00017761254275683314
Batch  61  loss:  0.00018429849296808243
Batch  71  loss:  0.00018171031842939556
Batch  81  loss:  0.00013506885443348438
Batch  91  loss:  0.000205050062504597
Batch  101  loss:  0.0001154354031314142
Batch  111  loss:  0.0003202476946171373
Batch  121  loss:  0.00011750953854061663
Batch  131  loss:  0.00014976653619669378
Batch  141  loss:  0.00021397139062173665
Batch  151  loss:  0.00023134179355110973
Batch  161  loss:  0.00025893148267641664
Batch  171  loss:  0.00024216037127189338
Batch  181  loss:  0.00015630718553438783
Batch  191  loss:  0.00018222085782326758
Validation on real data: 
LOSS supervised-train 0.00017945372383110225, valid 0.00019316512043587863
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00025696202646940947
Batch  11  loss:  0.00017944580758921802
Batch  21  loss:  0.0001703564339550212
Batch  31  loss:  0.00016464870714116842
Batch  41  loss:  0.00024315656628459692
Batch  51  loss:  0.0002222377952421084
Batch  61  loss:  0.00020932528423145413
Batch  71  loss:  0.00012942159082740545
Batch  81  loss:  0.0001790751121006906
Batch  91  loss:  0.0002302789653185755
Batch  101  loss:  0.00016798157594166696
Batch  111  loss:  0.00027602995396591723
Batch  121  loss:  0.0001656316890148446
Batch  131  loss:  0.0001671972859185189
Batch  141  loss:  0.00019448659440968186
Batch  151  loss:  0.000161934774951078
Batch  161  loss:  0.00018913917301688343
Batch  171  loss:  0.0002025841095019132
Batch  181  loss:  0.00013735838001593947
Batch  191  loss:  0.0002274644502904266
Validation on real data: 
LOSS supervised-train 0.00018285228637978434, valid 0.00019706820603460073
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0001896367612062022
Batch  11  loss:  0.00028899183962494135
Batch  21  loss:  0.00017396590556018054
Batch  31  loss:  0.0001891828142106533
Batch  41  loss:  0.0001598091039340943
Batch  51  loss:  0.00020490307360887527
Batch  61  loss:  0.00014591736544389278
Batch  71  loss:  0.0001574250345584005
Batch  81  loss:  0.0001398706663167104
Batch  91  loss:  0.00022927115787751973
Batch  101  loss:  0.00015988515224307775
Batch  111  loss:  0.0002985515457112342
Batch  121  loss:  0.0001612924679648131
Batch  131  loss:  0.00013657892122864723
Batch  141  loss:  0.00022857099247630686
Batch  151  loss:  0.0001496838522143662
Batch  161  loss:  0.00017334267613478005
Batch  171  loss:  0.0001798270968720317
Batch  181  loss:  0.00019061587227042764
Batch  191  loss:  0.0002039897080976516
Validation on real data: 
LOSS supervised-train 0.0001763464072428178, valid 0.00020110359764657915
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0001959932706085965
Batch  11  loss:  0.00021793667110614479
Batch  21  loss:  0.00013578083598986268
Batch  31  loss:  0.00017917517106980085
Batch  41  loss:  0.00017960139666683972
Batch  51  loss:  0.00016905162192415446
Batch  61  loss:  0.00022820517187938094
Batch  71  loss:  0.0002311278076376766
Batch  81  loss:  0.00017374574963469058
Batch  91  loss:  0.0002258110325783491
Batch  101  loss:  0.00015486583288293332
Batch  111  loss:  0.00035385979572311044
Batch  121  loss:  0.00016448124370072037
Batch  131  loss:  0.00013707185280509293
Batch  141  loss:  0.00020232416864018887
Batch  151  loss:  0.0001697637781035155
Batch  161  loss:  0.00020479278464335948
Batch  171  loss:  0.00018231626017950475
Batch  181  loss:  0.00018205016385763884
Batch  191  loss:  0.0002508356119506061
Validation on real data: 
LOSS supervised-train 0.00018025365767243784, valid 0.00017435660993214697
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0002233740087831393
Batch  11  loss:  0.00024053495144471526
Batch  21  loss:  0.00014783123333472759
Batch  31  loss:  0.00018834808724932373
Batch  41  loss:  0.00012805107689928263
Batch  51  loss:  0.00018912593077402562
Batch  61  loss:  0.00021258663036860526
Batch  71  loss:  0.0001921468210639432
Batch  81  loss:  0.00012225765385665
Batch  91  loss:  0.00023034377954900265
Batch  101  loss:  0.0001558006915729493
Batch  111  loss:  0.00029967105365358293
Batch  121  loss:  0.00016976702318061143
Batch  131  loss:  0.00021794679923914373
Batch  141  loss:  0.00027953542303293943
Batch  151  loss:  0.0001672225189395249
Batch  161  loss:  0.00021946293418295681
Batch  171  loss:  0.00014859213843010366
Batch  181  loss:  0.00014604948228225112
Batch  191  loss:  0.000220066198380664
Validation on real data: 
LOSS supervised-train 0.00018128864790924128, valid 0.00020750223484355956
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0002188457001466304
Batch  11  loss:  0.0002211485552834347
Batch  21  loss:  0.00020442157983779907
Batch  31  loss:  0.00016288664482999593
Batch  41  loss:  0.00019788608187809587
Batch  51  loss:  0.000246720650466159
Batch  61  loss:  0.00017498173110652715
Batch  71  loss:  0.0001604599819984287
Batch  81  loss:  0.00011588114284677431
Batch  91  loss:  0.00019526627147570252
Batch  101  loss:  0.00014230857777874917
Batch  111  loss:  0.0002513242361601442
Batch  121  loss:  0.00017724352073855698
Batch  131  loss:  0.00017963023856282234
Batch  141  loss:  0.00022069306578487158
Batch  151  loss:  0.00021658245532307774
Batch  161  loss:  0.0001617356319911778
Batch  171  loss:  0.00016687526658643037
Batch  181  loss:  0.00013977591879665852
Batch  191  loss:  0.00021527602802962065
Validation on real data: 
LOSS supervised-train 0.00017195746302604676, valid 0.00021338002989068627
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00015279930084943771
Batch  11  loss:  0.00018538984295446426
Batch  21  loss:  0.00022179602819960564
Batch  31  loss:  0.00017146366008091718
Batch  41  loss:  0.00016712835349608213
Batch  51  loss:  0.00017242431931663305
Batch  61  loss:  0.00018453139637131244
Batch  71  loss:  0.00015785571304149926
Batch  81  loss:  0.0001228277978952974
Batch  91  loss:  0.0001774815609678626
Batch  101  loss:  0.00017599487910047174
Batch  111  loss:  0.00027001422131434083
Batch  121  loss:  0.00014583489974029362
Batch  131  loss:  0.00014466699212789536
Batch  141  loss:  0.00024213879078160971
Batch  151  loss:  0.00011644653568509966
Batch  161  loss:  0.00022642328985966742
Batch  171  loss:  0.00010763911996036768
Batch  181  loss:  0.00014806386025156826
Batch  191  loss:  0.00023056953796185553
Validation on real data: 
LOSS supervised-train 0.00017342552866466576, valid 0.0002294358127983287
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00020522726117633283
Batch  11  loss:  0.0002086942840833217
Batch  21  loss:  0.00018176957382820547
Batch  31  loss:  0.00018074428953696042
Batch  41  loss:  0.00016923168732319027
Batch  51  loss:  0.00018870882922783494
Batch  61  loss:  0.0001675946987234056
Batch  71  loss:  0.000196236782358028
Batch  81  loss:  0.00013847675290890038
Batch  91  loss:  0.00020289245003368706
Batch  101  loss:  0.0001613533531781286
Batch  111  loss:  0.00024732944439165294
Batch  121  loss:  0.00013156677596271038
Batch  131  loss:  0.00017454763292334974
Batch  141  loss:  0.0001898113259812817
Batch  151  loss:  0.00020372867584228516
Batch  161  loss:  0.00019282655557617545
Batch  171  loss:  0.00017370899149682373
Batch  181  loss:  0.00017114162619691342
Batch  191  loss:  0.00023295222490560263
Validation on real data: 
LOSS supervised-train 0.00017050325568561674, valid 0.00021086959168314934
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00019109065760858357
Batch  11  loss:  0.00014098201063461602
Batch  21  loss:  0.00014644670591223985
Batch  31  loss:  0.00021450963686220348
Batch  41  loss:  0.00014998181723058224
Batch  51  loss:  0.0002068039175355807
Batch  61  loss:  0.00019333348609507084
Batch  71  loss:  0.00015356505173258483
Batch  81  loss:  0.00011877381621161476
Batch  91  loss:  0.00017268318333663046
Batch  101  loss:  0.00014150630158837885
Batch  111  loss:  0.0003596386522985995
Batch  121  loss:  0.00018874956003855914
Batch  131  loss:  0.0001282644079765305
Batch  141  loss:  0.00018922588787972927
Batch  151  loss:  0.0001547953870613128
Batch  161  loss:  0.00015464331954717636
Batch  171  loss:  0.00016925159434322268
Batch  181  loss:  0.00014664455375168473
Batch  191  loss:  0.00031353533267974854
Validation on real data: 
LOSS supervised-train 0.00017207990083988988, valid 0.0002707134699448943
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00022414076374843717
Batch  11  loss:  0.00020600239804480225
Batch  21  loss:  0.0002268273674417287
Batch  31  loss:  0.00017650172230787575
Batch  41  loss:  0.00011199084838153794
Batch  51  loss:  0.00020063544798176736
Batch  61  loss:  0.0001935835462063551
Batch  71  loss:  0.0001815136056393385
Batch  81  loss:  0.00012409263581503183
Batch  91  loss:  0.0002441082615405321
Batch  101  loss:  0.0001711545482976362
Batch  111  loss:  0.00032517631188966334
Batch  121  loss:  0.00016475569282192737
Batch  131  loss:  0.0001729508803691715
Batch  141  loss:  0.00023293637786991894
Batch  151  loss:  0.00014757328608538955
Batch  161  loss:  0.00016535133181605488
Batch  171  loss:  0.00013889219553675503
Batch  181  loss:  0.00013225231668911874
Batch  191  loss:  0.00022590634762309492
Validation on real data: 
LOSS supervised-train 0.00017099188979045722, valid 0.00016235254588536918
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00022028428793419152
Batch  11  loss:  0.00020704745838884264
Batch  21  loss:  0.00018077237473335117
Batch  31  loss:  0.00015495007392019033
Batch  41  loss:  0.00017233248217962682
Batch  51  loss:  0.0001825723156798631
Batch  61  loss:  0.00016371958190575242
Batch  71  loss:  0.0002066532033495605
Batch  81  loss:  0.00016900979971978813
Batch  91  loss:  0.00024928359198383987
Batch  101  loss:  0.00017213397950399667
Batch  111  loss:  0.00025204222765751183
Batch  121  loss:  0.00015370787878055125
Batch  131  loss:  0.0001646814780542627
Batch  141  loss:  0.0001951910526258871
Batch  151  loss:  0.00010559905786067247
Batch  161  loss:  0.00014003700925968587
Batch  171  loss:  0.00013403198681771755
Batch  181  loss:  0.00014431103772949427
Batch  191  loss:  0.00020934006897732615
Validation on real data: 
LOSS supervised-train 0.00017069663183065132, valid 0.00026735750725492835
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00016911220154725015
Batch  11  loss:  0.00018725039262790233
Batch  21  loss:  0.0001781909668352455
Batch  31  loss:  0.00018058979185298085
Batch  41  loss:  0.0001615399814909324
Batch  51  loss:  0.0002216733555542305
Batch  61  loss:  0.00018150122195947915
Batch  71  loss:  0.00015168213576544076
Batch  81  loss:  0.00013382428733166307
Batch  91  loss:  0.0002149925712728873
Batch  101  loss:  0.00016081008652690798
Batch  111  loss:  0.0002578451531007886
Batch  121  loss:  0.0001798601442715153
Batch  131  loss:  0.00017908662266563624
Batch  141  loss:  0.00025334637030027807
Batch  151  loss:  0.00015228000120259821
Batch  161  loss:  0.0001968693541130051
Batch  171  loss:  0.0001759427977958694
Batch  181  loss:  0.00017867177666630596
Batch  191  loss:  0.00019132175657432526
Validation on real data: 
LOSS supervised-train 0.00017082811336877057, valid 0.0002244031202280894
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00020211060473229736
Batch  11  loss:  0.00015813652134966105
Batch  21  loss:  0.0001353390543954447
Batch  31  loss:  0.00019747819169424474
Batch  41  loss:  0.00015420193085446954
Batch  51  loss:  0.0002234413259429857
Batch  61  loss:  0.00018226771499030292
Batch  71  loss:  0.00014163176820147783
Batch  81  loss:  0.00013081738143227994
Batch  91  loss:  0.00024000655685085803
Batch  101  loss:  0.00016837433213368058
Batch  111  loss:  0.0003461453306954354
Batch  121  loss:  0.00016646864241920412
Batch  131  loss:  0.00014103019202593714
Batch  141  loss:  0.00020584325829986483
Batch  151  loss:  0.00015970601816661656
Batch  161  loss:  0.0001569378364365548
Batch  171  loss:  0.00013808044604957104
Batch  181  loss:  0.0001302060845773667
Batch  191  loss:  0.0002728271938394755
Validation on real data: 
LOSS supervised-train 0.00016930969984969125, valid 0.00017605742323212326
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00018656745669431984
Batch  11  loss:  0.00019808208162430674
Batch  21  loss:  0.0001638595713302493
Batch  31  loss:  0.00014173985982779413
Batch  41  loss:  0.0001766672939993441
Batch  51  loss:  0.0001930592698045075
Batch  61  loss:  0.00013877915625926107
Batch  71  loss:  0.0001615661894902587
Batch  81  loss:  0.00011148507473990321
Batch  91  loss:  0.00024144499911926687
Batch  101  loss:  0.00014529269537888467
Batch  111  loss:  0.0002923699503298849
Batch  121  loss:  0.00015722360694780946
Batch  131  loss:  0.00023005445837043226
Batch  141  loss:  0.00019001295731868595
Batch  151  loss:  0.00012120177416363731
Batch  161  loss:  0.000200070469873026
Batch  171  loss:  0.00014463868865277618
Batch  181  loss:  0.00015709894069004804
Batch  191  loss:  0.00018434578669257462
Validation on real data: 
LOSS supervised-train 0.00016711367716197855, valid 0.00022799290309194475
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00022132658341433853
Batch  11  loss:  0.00023237656569108367
Batch  21  loss:  0.00017851032316684723
Batch  31  loss:  0.0001306023623328656
Batch  41  loss:  0.00018660019850358367
Batch  51  loss:  0.00021666611428372562
Batch  61  loss:  0.0001726219052216038
Batch  71  loss:  0.000192909428733401
Batch  81  loss:  0.00012225752288941294
Batch  91  loss:  0.00019544316455721855
Batch  101  loss:  0.0001306500198552385
Batch  111  loss:  0.00028824835317209363
Batch  121  loss:  0.00016243293066509068
Batch  131  loss:  0.00018276031187269837
Batch  141  loss:  0.00023416838666889817
Batch  151  loss:  0.0001603399869054556
Batch  161  loss:  0.00014600620488636196
Batch  171  loss:  0.0001931871665874496
Batch  181  loss:  0.0001261493598576635
Batch  191  loss:  0.00022558293130714446
Validation on real data: 
LOSS supervised-train 0.0001664040515970555, valid 0.00022285859449766576
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.000190047052456066
Batch  11  loss:  0.00015788660675752908
Batch  21  loss:  0.0001343919284408912
Batch  31  loss:  0.00017734470020513982
Batch  41  loss:  0.0001204443906317465
Batch  51  loss:  0.00014976847160141915
Batch  61  loss:  0.0001348288351437077
Batch  71  loss:  0.00016032386338338256
Batch  81  loss:  0.0001153500925283879
Batch  91  loss:  0.00023425265680998564
Batch  101  loss:  0.0001416142040397972
Batch  111  loss:  0.0002445858553983271
Batch  121  loss:  0.0001362803450319916
Batch  131  loss:  0.00017869465227704495
Batch  141  loss:  0.00016119240899570286
Batch  151  loss:  0.00015529041411355138
Batch  161  loss:  0.00020399200730025768
Batch  171  loss:  0.00019773424719460309
Batch  181  loss:  0.00020097904780413955
Batch  191  loss:  0.0001962885435204953
Validation on real data: 
LOSS supervised-train 0.00016286465146549744, valid 0.0001909486309159547
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00021928867499809712
Batch  11  loss:  0.0001259634445887059
Batch  21  loss:  0.00022821451420895755
Batch  31  loss:  0.00019681463891174644
Batch  41  loss:  0.00017587542242836207
Batch  51  loss:  0.00023131132184062153
Batch  61  loss:  0.0002105841413140297
Batch  71  loss:  0.00015705428086221218
Batch  81  loss:  0.00015565735520794988
Batch  91  loss:  0.00021357803780119866
Batch  101  loss:  0.00016789058281574398
Batch  111  loss:  0.00020012818276882172
Batch  121  loss:  0.00015454928507097065
Batch  131  loss:  0.0001304936595261097
Batch  141  loss:  0.00018602276395540684
Batch  151  loss:  0.00018490840739104897
Batch  161  loss:  0.0002030185132753104
Batch  171  loss:  0.00018734658078756183
Batch  181  loss:  0.0001323829492321238
Batch  191  loss:  0.00023727006919216365
Validation on real data: 
LOSS supervised-train 0.00016474717383971438, valid 0.0002564416208770126
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0002041194966295734
Batch  11  loss:  0.0001900139031931758
Batch  21  loss:  0.00015418157272506505
Batch  31  loss:  0.00018692850426305085
Batch  41  loss:  0.00016142487584147602
Batch  51  loss:  0.0001000220509013161
Batch  61  loss:  0.0001621714181965217
Batch  71  loss:  0.00015503099712077528
Batch  81  loss:  0.00014950017794035375
Batch  91  loss:  0.0002781654184218496
Batch  101  loss:  0.00012955341662745923
Batch  111  loss:  0.00023907245486043394
Batch  121  loss:  0.00017093814676627517
Batch  131  loss:  0.00016144014080055058
Batch  141  loss:  0.00024493897217325866
Batch  151  loss:  0.00015672724111936986
Batch  161  loss:  0.00020860933000221848
Batch  171  loss:  0.0001990637683775276
Batch  181  loss:  0.00018035521497949958
Batch  191  loss:  0.00020601235155481845
Validation on real data: 
LOSS supervised-train 0.00016098783271445427, valid 0.00017663586186245084
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.00019839394371956587
Batch  11  loss:  0.00015636954049114138
Batch  21  loss:  0.00015828780306037515
Batch  31  loss:  0.00017748563550412655
Batch  41  loss:  0.00013322581071406603
Batch  51  loss:  0.00012531736865639687
Batch  61  loss:  0.0001842430792748928
Batch  71  loss:  0.00015356346557382494
Batch  81  loss:  0.0001292615634156391
Batch  91  loss:  0.00019421003526076674
Batch  101  loss:  0.00014874142652843148
Batch  111  loss:  0.00025823796750046313
Batch  121  loss:  0.00013135027256794274
Batch  131  loss:  0.00014117789396550506
Batch  141  loss:  0.00019042668282054365
Batch  151  loss:  0.00019394907576497644
Batch  161  loss:  0.00017797190230339766
Batch  171  loss:  0.00018761846877168864
Batch  181  loss:  0.00015606137458235025
Batch  191  loss:  0.00020787747052963823
Validation on real data: 
LOSS supervised-train 0.00015728859099908732, valid 0.00024442619178444147
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00016871726256795228
Batch  11  loss:  0.00011622776946751401
Batch  21  loss:  0.00015624075604137033
Batch  31  loss:  0.00011982949217781425
Batch  41  loss:  0.00014605143223889172
Batch  51  loss:  0.00016355677507817745
Batch  61  loss:  0.00014254878624342382
Batch  71  loss:  0.00011753884609788656
Batch  81  loss:  0.00016468983085360378
Batch  91  loss:  0.00017642497550696135
Batch  101  loss:  0.00016665477596689016
Batch  111  loss:  0.00023025894188322127
Batch  121  loss:  0.00018607608217280358
Batch  131  loss:  0.00016030251572374254
Batch  141  loss:  0.00021143892081454396
Batch  151  loss:  0.000225932351895608
Batch  161  loss:  0.00018287752754986286
Batch  171  loss:  0.00014082012057770044
Batch  181  loss:  0.00015354828792624176
Batch  191  loss:  0.0002444254932925105
Validation on real data: 
LOSS supervised-train 0.00016284363733575446, valid 0.0001699458807706833
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00022033843561075628
Batch  11  loss:  0.0001649040641495958
Batch  21  loss:  0.00019902830536011606
Batch  31  loss:  0.00017692189430817962
Batch  41  loss:  0.00018161324260290712
Batch  51  loss:  0.00015194570005405694
Batch  61  loss:  0.00017613913223613054
Batch  71  loss:  0.0001501656079199165
Batch  81  loss:  0.00010348700016038492
Batch  91  loss:  0.00019395661365706474
Batch  101  loss:  0.00015628832625225186
Batch  111  loss:  0.0003121091576758772
Batch  121  loss:  0.00015354278730228543
Batch  131  loss:  0.00010559359361650422
Batch  141  loss:  0.00017906769062392414
Batch  151  loss:  0.00015343798440881073
Batch  161  loss:  0.0002286317467223853
Batch  171  loss:  0.0001659514382481575
Batch  181  loss:  0.00014888084842823446
Batch  191  loss:  0.00019124902610201389
Validation on real data: 
LOSS supervised-train 0.00015822226654563565, valid 0.00023552501806989312
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00020972368656657636
Batch  11  loss:  0.00014708492381032556
Batch  21  loss:  0.00011527023889357224
Batch  31  loss:  0.0001483247906435281
Batch  41  loss:  0.0001543407270219177
Batch  51  loss:  0.0001522345410194248
Batch  61  loss:  0.00018973821715917438
Batch  71  loss:  0.0001484523236285895
Batch  81  loss:  0.000134039088152349
Batch  91  loss:  0.00024442202993668616
Batch  101  loss:  0.00021622475469484925
Batch  111  loss:  0.0002528635086491704
Batch  121  loss:  0.00019033219723496586
Batch  131  loss:  0.00016980415966827422
Batch  141  loss:  0.00019279455591458827
Batch  151  loss:  0.0001661103160586208
Batch  161  loss:  0.00023004326794762164
Batch  171  loss:  0.00014329470286611468
Batch  181  loss:  0.00013163605763111264
Batch  191  loss:  0.00020438495266716927
Validation on real data: 
LOSS supervised-train 0.00015868243706790964, valid 0.00019905788940377533
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00018604189972393215
Batch  11  loss:  0.0001734992692945525
Batch  21  loss:  0.00019365576736163348
Batch  31  loss:  0.00014198102871887386
Batch  41  loss:  0.00015872145013418049
Batch  51  loss:  0.00011020852252840996
Batch  61  loss:  0.00014216481940820813
Batch  71  loss:  0.00013851716357748955
Batch  81  loss:  0.00014853314496576786
Batch  91  loss:  0.00019611774769145995
Batch  101  loss:  0.00014577100228052586
Batch  111  loss:  0.0002427909494144842
Batch  121  loss:  0.00011803906818386167
Batch  131  loss:  0.00018371528130955994
Batch  141  loss:  0.00015299870574381202
Batch  151  loss:  0.00013552751624956727
Batch  161  loss:  0.00018360848480369896
Batch  171  loss:  0.00017033687618095428
Batch  181  loss:  0.00015192519640550017
Batch  191  loss:  0.00019617278303485364
Validation on real data: 
LOSS supervised-train 0.00015405620677483967, valid 0.00021445335005410016
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00022405682830139995
Batch  11  loss:  0.00017265476344618946
Batch  21  loss:  0.00014469324378296733
Batch  31  loss:  0.00017445515550207347
Batch  41  loss:  0.00016892097482923418
Batch  51  loss:  0.0001944225950865075
Batch  61  loss:  0.00018119826563633978
Batch  71  loss:  0.00014776134048588574
Batch  81  loss:  0.00014053325867280364
Batch  91  loss:  0.00020209803187754005
Batch  101  loss:  0.00015316640201490372
Batch  111  loss:  0.0002432463224977255
Batch  121  loss:  0.00013142594252713025
Batch  131  loss:  0.00012833498476538807
Batch  141  loss:  0.000192898660316132
Batch  151  loss:  0.00013876341108698398
Batch  161  loss:  0.00018099437875207514
Batch  171  loss:  0.00014292016567196697
Batch  181  loss:  0.00013312141527421772
Batch  191  loss:  0.0002009867603192106
Validation on real data: 
LOSS supervised-train 0.00015318162488256348, valid 0.00020251810201443732
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00017338991165161133
Batch  11  loss:  0.00014712191477883607
Batch  21  loss:  0.00018905576143879443
Batch  31  loss:  0.00014461769023910165
Batch  41  loss:  0.00019266597519163042
Batch  51  loss:  0.00018821026606019586
Batch  61  loss:  0.00018154743884224445
Batch  71  loss:  0.0001429127441952005
Batch  81  loss:  0.00012421129213180393
Batch  91  loss:  0.00020952176419086754
Batch  101  loss:  0.0001381217734888196
Batch  111  loss:  0.00020438662613742054
Batch  121  loss:  0.00016449924441985786
Batch  131  loss:  9.825728920986876e-05
Batch  141  loss:  0.0001793821866158396
Batch  151  loss:  0.00015530352538917214
Batch  161  loss:  0.00014111062046140432
Batch  171  loss:  0.00015628745313733816
Batch  181  loss:  0.00012080299347871915
Batch  191  loss:  0.0002074350049952045
Validation on real data: 
LOSS supervised-train 0.00015453595442522784, valid 0.00023531327315140516
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00020539680554065853
Batch  11  loss:  0.00013034620496910065
Batch  21  loss:  0.0001316618436248973
Batch  31  loss:  0.00011537142563611269
Batch  41  loss:  0.00014804454986006021
Batch  51  loss:  0.0002317013859283179
Batch  61  loss:  0.0002289064577780664
Batch  71  loss:  0.0001327477366430685
Batch  81  loss:  0.00017559748084750026
Batch  91  loss:  0.00018693013407755643
Batch  101  loss:  0.00014481517428066581
Batch  111  loss:  0.00019199329835828394
Batch  121  loss:  0.00015737324429210275
Batch  131  loss:  0.00012205503298901021
Batch  141  loss:  0.00021046157053206116
Batch  151  loss:  0.0001455167803214863
Batch  161  loss:  0.0001739744475344196
Batch  171  loss:  0.00014594988897442818
Batch  181  loss:  0.00016138082719407976
Batch  191  loss:  0.00021682673832401633
Validation on real data: 
LOSS supervised-train 0.00015477515782549744, valid 0.00019209904712624848
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00022247333254199475
Batch  11  loss:  0.00015431352949235588
Batch  21  loss:  0.00016472414426971227
Batch  31  loss:  0.00017587524780537933
Batch  41  loss:  0.00017571663192939013
Batch  51  loss:  0.00019546455587260425
Batch  61  loss:  0.00015344165149144828
Batch  71  loss:  0.00012356600200291723
Batch  81  loss:  0.00012527684157248586
Batch  91  loss:  0.00023338221944868565
Batch  101  loss:  0.00012000691640423611
Batch  111  loss:  0.00023705865896772593
Batch  121  loss:  0.00011854239710373804
Batch  131  loss:  0.00014731683768332005
Batch  141  loss:  0.00016687311290297657
Batch  151  loss:  0.00014807385741733015
Batch  161  loss:  0.00015138625167310238
Batch  171  loss:  0.0001651359343668446
Batch  181  loss:  0.0001488043344579637
Batch  191  loss:  0.000224643197725527
Validation on real data: 
LOSS supervised-train 0.00015708673898188864, valid 0.00021282178931869566
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00019965220417361706
Batch  11  loss:  0.0001275207760045305
Batch  21  loss:  0.00016149305156432092
Batch  31  loss:  0.000148544815601781
Batch  41  loss:  0.0001655499218031764
Batch  51  loss:  0.0001630217011552304
Batch  61  loss:  0.00014021809329278767
Batch  71  loss:  0.0001518443168606609
Batch  81  loss:  0.00011629369691945612
Batch  91  loss:  0.00022063405776862055
Batch  101  loss:  0.00014886220742482692
Batch  111  loss:  0.0002673372800927609
Batch  121  loss:  0.0001590340689290315
Batch  131  loss:  0.00012853318185079843
Batch  141  loss:  0.00021076171833556145
Batch  151  loss:  0.0001397471787640825
Batch  161  loss:  0.00015673853340558708
Batch  171  loss:  0.0001428134855814278
Batch  181  loss:  0.00012891305959783494
Batch  191  loss:  0.0002378433564445004
Validation on real data: 
LOSS supervised-train 0.0001512719801030471, valid 0.0001877793692983687
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00022375599655788392
Batch  11  loss:  0.00014784176892135292
Batch  21  loss:  0.00010703211592044681
Batch  31  loss:  0.00017809371638577431
Batch  41  loss:  9.334846254205331e-05
Batch  51  loss:  0.00016442181367892772
Batch  61  loss:  0.00014386785915121436
Batch  71  loss:  0.00012659469211939722
Batch  81  loss:  0.00010612633195705712
Batch  91  loss:  0.0001842758065322414
Batch  101  loss:  0.00012117744336137548
Batch  111  loss:  0.00022766746405977756
Batch  121  loss:  0.00011848267604364082
Batch  131  loss:  0.00015422713477164507
Batch  141  loss:  0.00013877477613277733
Batch  151  loss:  0.00013507880794350058
Batch  161  loss:  0.00015512753452640027
Batch  171  loss:  0.00014460852253250778
Batch  181  loss:  0.0001386939111398533
Batch  191  loss:  0.00022390592494048178
Validation on real data: 
LOSS supervised-train 0.00015172096696915106, valid 0.0002013072371482849
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00016135077748913318
Batch  11  loss:  0.00010640659456839785
Batch  21  loss:  0.0001654333173064515
Batch  31  loss:  0.00015779657405801117
Batch  41  loss:  0.0001742523891152814
Batch  51  loss:  0.00015518415602855384
Batch  61  loss:  0.00012266714475117624
Batch  71  loss:  0.00015208058175630867
Batch  81  loss:  0.00012637817417271435
Batch  91  loss:  0.000192394174518995
Batch  101  loss:  0.00013421937183011323
Batch  111  loss:  0.00028192641912028193
Batch  121  loss:  0.00013472586579155177
Batch  131  loss:  0.00016868847887963057
Batch  141  loss:  0.00017278727318625897
Batch  151  loss:  0.00015424359298776835
Batch  161  loss:  0.00019307038746774197
Batch  171  loss:  0.00015403443831019104
Batch  181  loss:  0.00020421303634066135
Batch  191  loss:  0.00023451715242117643
Validation on real data: 
LOSS supervised-train 0.00014537251990986988, valid 0.00021815077343489975
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00017176159599330276
Batch  11  loss:  9.898090502247214e-05
Batch  21  loss:  0.00019969063578173518
Batch  31  loss:  0.00013933856098446995
Batch  41  loss:  0.0001207051973324269
Batch  51  loss:  0.0001981571695068851
Batch  61  loss:  0.00018274570174980909
Batch  71  loss:  0.0001351334503851831
Batch  81  loss:  0.00012048098142258823
Batch  91  loss:  0.00024019103148020804
Batch  101  loss:  0.0001436728343833238
Batch  111  loss:  0.00019465413060970604
Batch  121  loss:  0.00011304022336844355
Batch  131  loss:  0.00015375783550553024
Batch  141  loss:  0.0001836790470406413
Batch  151  loss:  0.00013155998021829873
Batch  161  loss:  0.0001488850830355659
Batch  171  loss:  0.00010966953414026648
Batch  181  loss:  0.00012054498074576259
Batch  191  loss:  0.00021697748161386698
Validation on real data: 
LOSS supervised-train 0.00014971081141993637, valid 0.00020081389811821282
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0002092215872835368
Batch  11  loss:  0.000188942882232368
Batch  21  loss:  0.00016908063844311982
Batch  31  loss:  0.0001474124874221161
Batch  41  loss:  0.00011713748244801536
Batch  51  loss:  0.00013623849372379482
Batch  61  loss:  0.00014442224346566945
Batch  71  loss:  0.00017055087664630264
Batch  81  loss:  0.00013130430306773633
Batch  91  loss:  0.00019467988749966025
Batch  101  loss:  0.00016150137525983155
Batch  111  loss:  0.00022368109785020351
Batch  121  loss:  0.00011010469461325556
Batch  131  loss:  0.00013495086750481278
Batch  141  loss:  0.0001864269288489595
Batch  151  loss:  0.00013966950064059347
Batch  161  loss:  0.00016848676023073494
Batch  171  loss:  0.00014641389134339988
Batch  181  loss:  8.683895430294797e-05
Batch  191  loss:  0.0001751064701238647
Validation on real data: 
LOSS supervised-train 0.0001477140294628043, valid 0.00021563784684985876
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00017680763266980648
Batch  11  loss:  0.0001880444906419143
Batch  21  loss:  0.0001374529820168391
Batch  31  loss:  0.00011233091936446726
Batch  41  loss:  0.00011482484114822
Batch  51  loss:  0.00015557394362986088
Batch  61  loss:  0.00019676714146044105
Batch  71  loss:  0.0001587968145031482
Batch  81  loss:  0.00012475972471293062
Batch  91  loss:  0.00021868628391530365
Batch  101  loss:  0.0001551568420836702
Batch  111  loss:  0.00023218388378154486
Batch  121  loss:  0.00015851804346311837
Batch  131  loss:  0.00014976302918512374
Batch  141  loss:  0.00013767155178356916
Batch  151  loss:  0.0001401354093104601
Batch  161  loss:  0.0001181225452455692
Batch  171  loss:  0.00013385966303758323
Batch  181  loss:  0.0001362619805149734
Batch  191  loss:  0.00017896447388920933
Validation on real data: 
LOSS supervised-train 0.00014609238864068174, valid 0.00018552507390268147
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  knife ; Model ID: 819e16fd120732f4609e2d916fa0da27
--------------------
Training baseline regression model:  2022-03-30 05:16:57.875526
Detector:  point_transformer
Object:  knife
--------------------
device is  cuda
--------------------
Number of trainable parameters:  888466
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.10519471019506454
Batch  11  loss:  0.007686137221753597
Batch  21  loss:  0.006703002378344536
Batch  31  loss:  0.005590083543211222
Batch  41  loss:  0.00477272504940629
Batch  51  loss:  0.0042013064958155155
Batch  61  loss:  0.0048683276399970055
Batch  71  loss:  0.003167349612340331
Batch  81  loss:  0.00357405305840075
Batch  91  loss:  0.0027071558870375156
Batch  101  loss:  0.0029774873983114958
Batch  111  loss:  0.002282885368913412
Batch  121  loss:  0.0019175915513187647
Batch  131  loss:  0.0013475647429004312
Batch  141  loss:  0.0015907869674265385
Batch  151  loss:  0.0009413163061253726
Batch  161  loss:  0.0009174284059554338
Batch  171  loss:  0.0012093950062990189
Batch  181  loss:  0.001146967406384647
Batch  191  loss:  0.0007871129200793803
Validation on real data: 
LOSS supervised-train 0.004877768901351374, valid 0.0019998145289719105
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.0006429231143556535
Batch  11  loss:  0.0005757772596552968
Batch  21  loss:  0.0005725472583435476
Batch  31  loss:  0.0005101661663502455
Batch  41  loss:  0.0005921584088355303
Batch  51  loss:  0.0006046904600225389
Batch  61  loss:  0.0005074711516499519
Batch  71  loss:  0.0004668000328820199
Batch  81  loss:  0.0005658142035827041
Batch  91  loss:  0.0005369932041503489
Batch  101  loss:  0.00042128300992771983
Batch  111  loss:  0.00045974276144988835
Batch  121  loss:  0.000498203793540597
Batch  131  loss:  0.0006886067567393184
Batch  141  loss:  0.0002173811662942171
Batch  151  loss:  0.0002733211440499872
Batch  161  loss:  0.00030734724714420736
Batch  171  loss:  0.0003814687952399254
Batch  181  loss:  0.00023949795286171138
Batch  191  loss:  0.00039143895264714956
Validation on real data: 
LOSS supervised-train 0.0005820706775557482, valid 0.0006415514508262277
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0002677144657354802
Batch  11  loss:  0.00035500741796568036
Batch  21  loss:  0.0005219978629611433
Batch  31  loss:  0.0007031281129457057
Batch  41  loss:  0.0008175498223863542
Batch  51  loss:  0.0003057725843973458
Batch  61  loss:  0.00043283472768962383
Batch  71  loss:  0.0005873024929314852
Batch  81  loss:  0.0004787965153809637
Batch  91  loss:  0.0003648327256087214
Batch  101  loss:  0.0003890420775860548
Batch  111  loss:  0.0002262524067191407
Batch  121  loss:  0.00025452926638536155
Batch  131  loss:  0.0002793044550344348
Batch  141  loss:  0.0002507622120901942
Batch  151  loss:  0.00021924484462942928
Batch  161  loss:  0.0002150474610971287
Batch  171  loss:  0.00028905278304591775
Batch  181  loss:  0.0002387843414908275
Batch  191  loss:  0.00019739678828045726
Validation on real data: 
LOSS supervised-train 0.0004198000341420993, valid 0.00043524097418412566
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.00016530865104869008
Batch  11  loss:  0.00036250895936973393
Batch  21  loss:  0.0004955296171829104
Batch  31  loss:  0.0007007796666584909
Batch  41  loss:  0.0008185573969967663
Batch  51  loss:  0.0002286585804540664
Batch  61  loss:  0.0003735915815923363
Batch  71  loss:  0.00047077564522624016
Batch  81  loss:  0.00046163200750015676
Batch  91  loss:  0.00025753796217031777
Batch  101  loss:  0.0002866869035642594
Batch  111  loss:  0.00017925513384398073
Batch  121  loss:  0.00026582161081023514
Batch  131  loss:  0.00026660412549972534
Batch  141  loss:  0.0002034204371739179
Batch  151  loss:  0.00020994825172238052
Batch  161  loss:  0.0001805783249437809
Batch  171  loss:  0.0001975736377062276
Batch  181  loss:  0.0001732178352540359
Batch  191  loss:  0.00017430941807106137
Validation on real data: 
LOSS supervised-train 0.00034537494066171347, valid 0.0005660104798153043
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.00011951304622925818
Batch  11  loss:  0.0002717432798817754
Batch  21  loss:  0.00036047230241820216
Batch  31  loss:  0.00043361986172385514
Batch  41  loss:  0.00041084171971306205
Batch  51  loss:  0.00017291094991378486
Batch  61  loss:  0.0005329003324732184
Batch  71  loss:  0.0004202854470349848
Batch  81  loss:  0.00045181583845987916
Batch  91  loss:  0.00020739178580697626
Batch  101  loss:  0.00020925189892295748
Batch  111  loss:  0.00020659624715335667
Batch  121  loss:  0.0001671568606980145
Batch  131  loss:  0.00026270627859048545
Batch  141  loss:  0.00017883490363601595
Batch  151  loss:  0.00016079093620646745
Batch  161  loss:  0.00015167196397669613
Batch  171  loss:  0.00016792493988759816
Batch  181  loss:  0.00022407672076951712
Batch  191  loss:  0.00015965005150064826
Validation on real data: 
LOSS supervised-train 0.0003089968109270558, valid 0.00040018209256231785
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.00012328011507634073
Batch  11  loss:  0.0001494084863224998
Batch  21  loss:  0.00022212485782802105
Batch  31  loss:  0.00035056538763456047
Batch  41  loss:  0.00041975913336500525
Batch  51  loss:  0.0001325851189903915
Batch  61  loss:  0.00038483686512336135
Batch  71  loss:  0.00038688298081979156
Batch  81  loss:  0.00022685241128783673
Batch  91  loss:  0.00019876948499586433
Batch  101  loss:  0.00016489122936036438
Batch  111  loss:  0.00022616839851252735
Batch  121  loss:  0.00014826771803200245
Batch  131  loss:  0.0001429654221283272
Batch  141  loss:  0.00020991814380977303
Batch  151  loss:  0.00017501979891676456
Batch  161  loss:  0.00013009422400500625
Batch  171  loss:  0.0001159268940682523
Batch  181  loss:  0.0001650224585318938
Batch  191  loss:  0.00012224388774484396
Validation on real data: 
LOSS supervised-train 0.00026247935336868976, valid 0.00012975677964277565
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.00010254832886857912
Batch  11  loss:  0.00018162965716328472
Batch  21  loss:  0.00015132504631765187
Batch  31  loss:  0.00025424285558983684
Batch  41  loss:  0.00029385826201178133
Batch  51  loss:  0.00013909208064433187
Batch  61  loss:  0.0003520477039273828
Batch  71  loss:  0.00040319759864360094
Batch  81  loss:  0.00019855705613736063
Batch  91  loss:  0.00017436261987313628
Batch  101  loss:  0.0001674521336099133
Batch  111  loss:  0.0001605034776730463
Batch  121  loss:  0.0001373749691992998
Batch  131  loss:  0.000175244509591721
Batch  141  loss:  0.0001701312867226079
Batch  151  loss:  0.00024482549633830786
Batch  161  loss:  0.00014597563131246716
Batch  171  loss:  9.916358976624906e-05
Batch  181  loss:  0.00019225462165195495
Batch  191  loss:  0.00011649689986370504
Validation on real data: 
LOSS supervised-train 0.00024013730195292736, valid 0.00010819785529747605
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.00012677616905421019
Batch  11  loss:  0.00017969585314858705
Batch  21  loss:  0.0001448531256755814
Batch  31  loss:  0.0002763260854408145
Batch  41  loss:  0.0003419899148866534
Batch  51  loss:  0.00011932235065614805
Batch  61  loss:  0.0002550463832449168
Batch  71  loss:  0.00035583271528594196
Batch  81  loss:  0.00014611110964324325
Batch  91  loss:  0.00016752429655753076
Batch  101  loss:  0.00021515369007829577
Batch  111  loss:  0.00015433460066560656
Batch  121  loss:  0.00011811283184215426
Batch  131  loss:  0.00010813552944455296
Batch  141  loss:  0.00015156876179389656
Batch  151  loss:  0.00013709273480344564
Batch  161  loss:  9.930712258210406e-05
Batch  171  loss:  0.00012906434130854905
Batch  181  loss:  0.00017612001101952046
Batch  191  loss:  0.0001357951550744474
Validation on real data: 
LOSS supervised-train 0.0002235130384360673, valid 0.0001299760479014367
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  9.246297122444957e-05
Batch  11  loss:  0.00010504302190383896
Batch  21  loss:  0.00014113896759226918
Batch  31  loss:  0.00025116937467828393
Batch  41  loss:  0.0002517880348023027
Batch  51  loss:  0.0001231632923008874
Batch  61  loss:  0.00020711336401291192
Batch  71  loss:  0.00019122760568279773
Batch  81  loss:  0.00011403336975490674
Batch  91  loss:  0.00012341779074631631
Batch  101  loss:  0.00016910482372622937
Batch  111  loss:  0.00013315750402398407
Batch  121  loss:  0.00010920157365035266
Batch  131  loss:  0.00010882877541007474
Batch  141  loss:  0.00017078399832826108
Batch  151  loss:  0.0001441185304429382
Batch  161  loss:  0.0001322747120866552
Batch  171  loss:  7.527921843575314e-05
Batch  181  loss:  0.0001577021466800943
Batch  191  loss:  0.00010693236254155636
Validation on real data: 
LOSS supervised-train 0.000208742343565973, valid 0.00015662521764170378
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  9.341392433270812e-05
Batch  11  loss:  0.00010983886022586375
Batch  21  loss:  0.00013093561574351043
Batch  31  loss:  0.00030685082310810685
Batch  41  loss:  0.000359692785423249
Batch  51  loss:  9.170999692287296e-05
Batch  61  loss:  0.0001377874577883631
Batch  71  loss:  0.00021839409600943327
Batch  81  loss:  0.00012547228834591806
Batch  91  loss:  0.00014860392548143864
Batch  101  loss:  0.0001806468062568456
Batch  111  loss:  0.000140562784508802
Batch  121  loss:  0.00010230489715468138
Batch  131  loss:  9.323954145656899e-05
Batch  141  loss:  0.00021378709061536938
Batch  151  loss:  0.0001545526465633884
Batch  161  loss:  9.513337136013433e-05
Batch  171  loss:  0.00010643551649991423
Batch  181  loss:  0.00018248586275149137
Batch  191  loss:  8.352689474122599e-05
Validation on real data: 
LOSS supervised-train 0.00019580726646381663, valid 0.00015499710571020842
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.00010290626232745126
Batch  11  loss:  0.00011316761811031029
Batch  21  loss:  0.00011735714360838756
Batch  31  loss:  0.0002786836994346231
Batch  41  loss:  0.00026925234124064445
Batch  51  loss:  0.00011606539919739589
Batch  61  loss:  0.00012078623694833368
Batch  71  loss:  0.00017482371185906231
Batch  81  loss:  9.55692958086729e-05
Batch  91  loss:  0.0001075685941032134
Batch  101  loss:  0.00019292350043542683
Batch  111  loss:  0.00011110039486084133
Batch  121  loss:  0.00012550454994197935
Batch  131  loss:  9.461955778533593e-05
Batch  141  loss:  0.00018091435777023435
Batch  151  loss:  0.00013389397645369172
Batch  161  loss:  0.00010125445987796411
Batch  171  loss:  9.022496669786051e-05
Batch  181  loss:  0.0001615884539205581
Batch  191  loss:  0.0001227091415785253
Validation on real data: 
LOSS supervised-train 0.00019174921697413084, valid 0.00021234608720988035
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.00012115962454117835
Batch  11  loss:  0.0001177787926280871
Batch  21  loss:  0.00014328626275528222
Batch  31  loss:  0.00021615730656776577
Batch  41  loss:  0.0002729697444010526
Batch  51  loss:  9.19243466341868e-05
Batch  61  loss:  0.00013896127347834408
Batch  71  loss:  0.0001703092420939356
Batch  81  loss:  0.00010168974404223263
Batch  91  loss:  9.091504034586251e-05
Batch  101  loss:  0.000182211835635826
Batch  111  loss:  0.00012452591909095645
Batch  121  loss:  9.101138857658952e-05
Batch  131  loss:  8.969638292910531e-05
Batch  141  loss:  0.0001248098851647228
Batch  151  loss:  9.390881314175203e-05
Batch  161  loss:  0.00010812413529492915
Batch  171  loss:  0.00010104102693730965
Batch  181  loss:  0.00013291336654219776
Batch  191  loss:  9.133228013524786e-05
Validation on real data: 
LOSS supervised-train 0.00017303764761891215, valid 0.0002159042633138597
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.00012036042608087882
Batch  11  loss:  0.00013110414147377014
Batch  21  loss:  0.0001028599581331946
Batch  31  loss:  0.00020987460447940975
Batch  41  loss:  0.0003033308603335172
Batch  51  loss:  0.00010934448800981045
Batch  61  loss:  0.00012011385842924938
Batch  71  loss:  0.0001429551630280912
Batch  81  loss:  8.717797754798084e-05
Batch  91  loss:  0.00010848791862372309
Batch  101  loss:  0.00019907591922674328
Batch  111  loss:  8.292101847473532e-05
Batch  121  loss:  8.649486699141562e-05
Batch  131  loss:  0.00012354149657767266
Batch  141  loss:  0.00012607422831933945
Batch  151  loss:  0.00011887326400028542
Batch  161  loss:  0.00010374511475674808
Batch  171  loss:  0.00010856630251510069
Batch  181  loss:  0.00014626023767050356
Batch  191  loss:  9.711916936794296e-05
Validation on real data: 
LOSS supervised-train 0.00016702688868463155, valid 0.00040431926026940346
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00011784575326601043
Batch  11  loss:  9.221231448464096e-05
Batch  21  loss:  0.0001049150523613207
Batch  31  loss:  0.0002436027571093291
Batch  41  loss:  0.00028238818049430847
Batch  51  loss:  9.288133151130751e-05
Batch  61  loss:  0.00014292138803284615
Batch  71  loss:  0.00016681286797393113
Batch  81  loss:  7.328596984734759e-05
Batch  91  loss:  0.0001124733462347649
Batch  101  loss:  0.00015856871323194355
Batch  111  loss:  8.203218749258667e-05
Batch  121  loss:  7.321673911064863e-05
Batch  131  loss:  0.00010609179298626259
Batch  141  loss:  0.00015663544763810933
Batch  151  loss:  0.00010282919538440183
Batch  161  loss:  9.939352457877249e-05
Batch  171  loss:  9.123580093728378e-05
Batch  181  loss:  0.00014363105583470315
Batch  191  loss:  0.00011579493730096146
Validation on real data: 
LOSS supervised-train 0.00015729386923339915, valid 0.00014043599367141724
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  9.53764611040242e-05
Batch  11  loss:  0.00013014899741392583
Batch  21  loss:  9.853141818894073e-05
Batch  31  loss:  0.00019259718828834593
Batch  41  loss:  0.0002662348560988903
Batch  51  loss:  6.645985558861867e-05
Batch  61  loss:  0.00011800703941844404
Batch  71  loss:  0.00012702561798505485
Batch  81  loss:  6.926612695679069e-05
Batch  91  loss:  9.331833280157298e-05
Batch  101  loss:  0.00013605535787064582
Batch  111  loss:  9.554356074659154e-05
Batch  121  loss:  8.269982208730653e-05
Batch  131  loss:  8.411769522354007e-05
Batch  141  loss:  0.00013327829947229475
Batch  151  loss:  9.132776904152706e-05
Batch  161  loss:  9.435143874725327e-05
Batch  171  loss:  6.964098429307342e-05
Batch  181  loss:  0.00011772161087719724
Batch  191  loss:  0.00010475092130945995
Validation on real data: 
LOSS supervised-train 0.00015511990346567473, valid 0.00023075166973285377
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  9.611466521164402e-05
Batch  11  loss:  0.00010174223280046135
Batch  21  loss:  9.686294652055949e-05
Batch  31  loss:  0.00022781042207498103
Batch  41  loss:  0.000279662839602679
Batch  51  loss:  8.967672329163179e-05
Batch  61  loss:  0.00011264177010161802
Batch  71  loss:  0.0001302651216974482
Batch  81  loss:  5.1292085117893293e-05
Batch  91  loss:  9.126834629569203e-05
Batch  101  loss:  0.00013576290803030133
Batch  111  loss:  8.494953362969682e-05
Batch  121  loss:  7.456048479070887e-05
Batch  131  loss:  0.00010131126327905804
Batch  141  loss:  0.00010386233043391258
Batch  151  loss:  9.489538933848962e-05
Batch  161  loss:  9.521193715045229e-05
Batch  171  loss:  6.616223254241049e-05
Batch  181  loss:  9.420256537850946e-05
Batch  191  loss:  8.646124479128048e-05
Validation on real data: 
LOSS supervised-train 0.00014299315416792523, valid 8.497547241859138e-05
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  9.047228377312422e-05
Batch  11  loss:  0.00010072540317196399
Batch  21  loss:  8.71948286658153e-05
Batch  31  loss:  0.00013948373089078814
Batch  41  loss:  0.0001931408914970234
Batch  51  loss:  6.0512084019137546e-05
Batch  61  loss:  0.000123024990898557
Batch  71  loss:  0.00013978654169477522
Batch  81  loss:  6.709001900162548e-05
Batch  91  loss:  8.456344221485779e-05
Batch  101  loss:  0.00016627642617095262
Batch  111  loss:  9.112113912124187e-05
Batch  121  loss:  8.930596959544346e-05
Batch  131  loss:  9.533104457659647e-05
Batch  141  loss:  0.00012865994358435273
Batch  151  loss:  8.280400652438402e-05
Batch  161  loss:  0.00011061798431910574
Batch  171  loss:  8.711019472684711e-05
Batch  181  loss:  9.487153874943033e-05
Batch  191  loss:  0.00011035135685233399
Validation on real data: 
LOSS supervised-train 0.0001382765583184664, valid 7.445858500432223e-05
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  8.517603419022635e-05
Batch  11  loss:  0.00011837316560558975
Batch  21  loss:  8.624657493783161e-05
Batch  31  loss:  0.0001445549714844674
Batch  41  loss:  0.00021985993953421712
Batch  51  loss:  9.414024680154398e-05
Batch  61  loss:  0.00010861353803193197
Batch  71  loss:  0.00015867680485825986
Batch  81  loss:  6.232202576939017e-05
Batch  91  loss:  8.583399903727695e-05
Batch  101  loss:  0.00015320551756303757
Batch  111  loss:  8.392622839892283e-05
Batch  121  loss:  8.938650717027485e-05
Batch  131  loss:  0.0001010161213343963
Batch  141  loss:  9.204720117850229e-05
Batch  151  loss:  0.00010463493526913226
Batch  161  loss:  9.496152051724494e-05
Batch  171  loss:  5.8564193750498816e-05
Batch  181  loss:  8.893821359379217e-05
Batch  191  loss:  8.971949864644557e-05
Validation on real data: 
LOSS supervised-train 0.0001335497167383437, valid 0.0001422779751010239
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  7.144863775465637e-05
Batch  11  loss:  9.625332313589752e-05
Batch  21  loss:  7.263993029482663e-05
Batch  31  loss:  0.0002136619295924902
Batch  41  loss:  0.00020421612134668976
Batch  51  loss:  6.94316768203862e-05
Batch  61  loss:  0.00011950250336667523
Batch  71  loss:  0.00013634796778205782
Batch  81  loss:  7.836527947802097e-05
Batch  91  loss:  7.862369966460392e-05
Batch  101  loss:  0.00013013438729103655
Batch  111  loss:  8.012380567379296e-05
Batch  121  loss:  8.83192551555112e-05
Batch  131  loss:  6.99468728271313e-05
Batch  141  loss:  9.770083124749362e-05
Batch  151  loss:  9.864493767963722e-05
Batch  161  loss:  0.0001023964723572135
Batch  171  loss:  6.562672206200659e-05
Batch  181  loss:  0.00011113347864011303
Batch  191  loss:  0.00011005785199813545
Validation on real data: 
LOSS supervised-train 0.0001322069285379257, valid 0.00010868272511288524
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  8.670750685269013e-05
Batch  11  loss:  0.00010751414811238647
Batch  21  loss:  8.825503755360842e-05
Batch  31  loss:  0.00016792400856502354
Batch  41  loss:  0.00023268189397640526
Batch  51  loss:  7.229540642583743e-05
Batch  61  loss:  0.00011519600957399234
Batch  71  loss:  0.00010530630243010819
Batch  81  loss:  5.4978838306851685e-05
Batch  91  loss:  7.904530502855778e-05
Batch  101  loss:  0.00011996270768577233
Batch  111  loss:  6.331122131086886e-05
Batch  121  loss:  9.588954708306119e-05
Batch  131  loss:  8.686406363267452e-05
Batch  141  loss:  9.755956125445664e-05
Batch  151  loss:  8.38076084619388e-05
Batch  161  loss:  0.00010321949957869947
Batch  171  loss:  7.744347385596484e-05
Batch  181  loss:  9.280948870582506e-05
Batch  191  loss:  9.848079207586125e-05
Validation on real data: 
LOSS supervised-train 0.00012699252341917598, valid 0.0001318888389505446
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  9.158261673292145e-05
Batch  11  loss:  8.574837556807324e-05
Batch  21  loss:  9.32600669329986e-05
Batch  31  loss:  0.00017335827578790486
Batch  41  loss:  0.00019376106502022594
Batch  51  loss:  5.759174018749036e-05
Batch  61  loss:  0.00012963343760930002
Batch  71  loss:  0.00012664559471886605
Batch  81  loss:  7.083654054440558e-05
Batch  91  loss:  7.7202967077028e-05
Batch  101  loss:  0.00012077741848770529
Batch  111  loss:  8.628522482467815e-05
Batch  121  loss:  6.085885615902953e-05
Batch  131  loss:  7.763610483380035e-05
Batch  141  loss:  7.706760516157374e-05
Batch  151  loss:  9.091484389500692e-05
Batch  161  loss:  0.0001098153370548971
Batch  171  loss:  6.706169369863346e-05
Batch  181  loss:  8.892847108654678e-05
Batch  191  loss:  0.00010916482278844342
Validation on real data: 
LOSS supervised-train 0.00012354959719232283, valid 7.636405644007027e-05
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  5.781185609521344e-05
Batch  11  loss:  6.909011426614597e-05
Batch  21  loss:  8.445229468634352e-05
Batch  31  loss:  0.00014971861673984677
Batch  41  loss:  0.00022198761871550232
Batch  51  loss:  6.662651139777154e-05
Batch  61  loss:  0.00010661353735486045
Batch  71  loss:  0.00010177890362683684
Batch  81  loss:  5.7445904531050473e-05
Batch  91  loss:  7.180843385867774e-05
Batch  101  loss:  0.00016072846483439207
Batch  111  loss:  7.12566907168366e-05
Batch  121  loss:  9.49773020693101e-05
Batch  131  loss:  8.088127651717514e-05
Batch  141  loss:  9.72103007370606e-05
Batch  151  loss:  7.011007983237505e-05
Batch  161  loss:  8.776140020927414e-05
Batch  171  loss:  5.695923027815297e-05
Batch  181  loss:  7.79296169639565e-05
Batch  191  loss:  9.262895764550194e-05
Validation on real data: 
LOSS supervised-train 0.00011787197970988928, valid 8.77431157277897e-05
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  6.620022031711414e-05
Batch  11  loss:  9.513113764114678e-05
Batch  21  loss:  8.63754830788821e-05
Batch  31  loss:  0.0001245195890078321
Batch  41  loss:  0.00018108326185029
Batch  51  loss:  6.115192081779242e-05
Batch  61  loss:  8.932808123063296e-05
Batch  71  loss:  9.374072396894917e-05
Batch  81  loss:  5.39370157639496e-05
Batch  91  loss:  7.381036994047463e-05
Batch  101  loss:  0.00011061652912758291
Batch  111  loss:  6.883601599838585e-05
Batch  121  loss:  8.566551696276292e-05
Batch  131  loss:  6.241091614356264e-05
Batch  141  loss:  9.668080747360364e-05
Batch  151  loss:  7.709502096986398e-05
Batch  161  loss:  0.00015466887271031737
Batch  171  loss:  6.464077159762383e-05
Batch  181  loss:  9.33273695409298e-05
Batch  191  loss:  8.871463069226593e-05
Validation on real data: 
LOSS supervised-train 0.0001157499267537787, valid 6.353802746161819e-05
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  7.346001075347885e-05
Batch  11  loss:  7.715948595432565e-05
Batch  21  loss:  9.019601566251367e-05
Batch  31  loss:  0.00014631300291512161
Batch  41  loss:  0.00017754216969478875
Batch  51  loss:  5.025593054597266e-05
Batch  61  loss:  0.00013325655891094357
Batch  71  loss:  0.00014184323663357645
Batch  81  loss:  6.199233757797629e-05
Batch  91  loss:  7.235019438667223e-05
Batch  101  loss:  9.994590072892606e-05
Batch  111  loss:  7.2034279583022e-05
Batch  121  loss:  7.914313755463809e-05
Batch  131  loss:  7.250851922435686e-05
Batch  141  loss:  7.254647061927244e-05
Batch  151  loss:  7.62480849516578e-05
Batch  161  loss:  9.948399383574724e-05
Batch  171  loss:  6.495066918432713e-05
Batch  181  loss:  6.385754386428744e-05
Batch  191  loss:  8.74044417287223e-05
Validation on real data: 
LOSS supervised-train 0.00011186187550265458, valid 5.807715933769941e-05
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  6.0698632296407595e-05
Batch  11  loss:  7.17787625035271e-05
Batch  21  loss:  7.26500729797408e-05
Batch  31  loss:  0.0001569946762174368
Batch  41  loss:  0.00015220967179629952
Batch  51  loss:  5.869541200809181e-05
Batch  61  loss:  0.00013500232307706028
Batch  71  loss:  9.619342745281756e-05
Batch  81  loss:  5.2004546887474135e-05
Batch  91  loss:  6.206521356943995e-05
Batch  101  loss:  9.602577483747154e-05
Batch  111  loss:  4.61865020042751e-05
Batch  121  loss:  8.573490777052939e-05
Batch  131  loss:  7.411192200379446e-05
Batch  141  loss:  8.790915308054537e-05
Batch  151  loss:  7.332200038945302e-05
Batch  161  loss:  0.00010175478382734582
Batch  171  loss:  6.418695556931198e-05
Batch  181  loss:  6.500582821900025e-05
Batch  191  loss:  8.607112249592319e-05
Validation on real data: 
LOSS supervised-train 0.00010793173900310649, valid 6.918008875800297e-05
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  6.900088919792324e-05
Batch  11  loss:  7.879333134042099e-05
Batch  21  loss:  6.568642857018858e-05
Batch  31  loss:  0.0001639866823097691
Batch  41  loss:  0.0001505625987192616
Batch  51  loss:  7.03985060681589e-05
Batch  61  loss:  8.028802403714508e-05
Batch  71  loss:  0.00010872329585254192
Batch  81  loss:  5.867481013410725e-05
Batch  91  loss:  7.475243910448626e-05
Batch  101  loss:  0.00011436732165748253
Batch  111  loss:  7.831125549273565e-05
Batch  121  loss:  9.302730177296326e-05
Batch  131  loss:  8.081083069555461e-05
Batch  141  loss:  6.756310176569968e-05
Batch  151  loss:  7.985239062691107e-05
Batch  161  loss:  0.00010140906670130789
Batch  171  loss:  5.807606430607848e-05
Batch  181  loss:  6.864377064630389e-05
Batch  191  loss:  7.729868229944259e-05
Validation on real data: 
LOSS supervised-train 0.00010648210227373056, valid 4.715054092230275e-05
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  7.650216139154509e-05
Batch  11  loss:  7.737311534583569e-05
Batch  21  loss:  6.948540976736695e-05
Batch  31  loss:  0.00013219338143244386
Batch  41  loss:  0.0001590135507285595
Batch  51  loss:  5.0756822020048276e-05
Batch  61  loss:  0.00010123346874024719
Batch  71  loss:  0.00011530080519150943
Batch  81  loss:  8.217267168220133e-05
Batch  91  loss:  6.489905354101211e-05
Batch  101  loss:  9.836476237978786e-05
Batch  111  loss:  7.130535959731787e-05
Batch  121  loss:  6.918702274560928e-05
Batch  131  loss:  6.973306881263852e-05
Batch  141  loss:  7.06335122231394e-05
Batch  151  loss:  7.312034722417593e-05
Batch  161  loss:  0.00010034607839770615
Batch  171  loss:  5.3278163250070065e-05
Batch  181  loss:  6.020713408361189e-05
Batch  191  loss:  7.690866914344952e-05
Validation on real data: 
LOSS supervised-train 0.00010508914087040467, valid 6.858950655441731e-05
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  7.863176142564043e-05
Batch  11  loss:  5.6866210798034444e-05
Batch  21  loss:  7.183848356362432e-05
Batch  31  loss:  0.00013000276521779597
Batch  41  loss:  0.0001555453782202676
Batch  51  loss:  5.373418389353901e-05
Batch  61  loss:  0.00011168287164764479
Batch  71  loss:  9.237668564310297e-05
Batch  81  loss:  6.16084216744639e-05
Batch  91  loss:  7.765688496874645e-05
Batch  101  loss:  0.00010408458911115304
Batch  111  loss:  6.776964437449351e-05
Batch  121  loss:  6.76752461004071e-05
Batch  131  loss:  6.803590076742694e-05
Batch  141  loss:  7.973452738951892e-05
Batch  151  loss:  5.836236960021779e-05
Batch  161  loss:  8.339745545526966e-05
Batch  171  loss:  6.144075450720266e-05
Batch  181  loss:  6.135918374639004e-05
Batch  191  loss:  8.602987509220839e-05
Validation on real data: 
LOSS supervised-train 0.00010021792457337142, valid 5.47002418898046e-05
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  7.5431918958202e-05
Batch  11  loss:  7.797060970915481e-05
Batch  21  loss:  8.073665230767801e-05
Batch  31  loss:  0.00015958340372890234
Batch  41  loss:  0.0001563690893817693
Batch  51  loss:  4.5620938180945814e-05
Batch  61  loss:  0.00010603584087220952
Batch  71  loss:  7.663833821425214e-05
Batch  81  loss:  5.5222713854163885e-05
Batch  91  loss:  7.034652662696317e-05
Batch  101  loss:  8.135668031172827e-05
Batch  111  loss:  6.797944661229849e-05
Batch  121  loss:  6.258713256102055e-05
Batch  131  loss:  7.127019489416853e-05
Batch  141  loss:  6.526694778585806e-05
Batch  151  loss:  6.0191181546542794e-05
Batch  161  loss:  0.00010680833656806499
Batch  171  loss:  6.171000859467313e-05
Batch  181  loss:  6.303989357547835e-05
Batch  191  loss:  7.482662476832047e-05
Validation on real data: 
LOSS supervised-train 9.967912998035899e-05, valid 6.265458068810403e-05
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  6.230135477380827e-05
Batch  11  loss:  5.456406870507635e-05
Batch  21  loss:  8.307520329253748e-05
Batch  31  loss:  0.00011935434304177761
Batch  41  loss:  0.00014665056369267404
Batch  51  loss:  5.767692709923722e-05
Batch  61  loss:  0.00010144995030714199
Batch  71  loss:  9.739289089338854e-05
Batch  81  loss:  5.8267582062399015e-05
Batch  91  loss:  6.315747305052355e-05
Batch  101  loss:  9.045548358699307e-05
Batch  111  loss:  5.865154162165709e-05
Batch  121  loss:  6.599473272217438e-05
Batch  131  loss:  5.9170881286263466e-05
Batch  141  loss:  8.364504901692271e-05
Batch  151  loss:  6.879383727209643e-05
Batch  161  loss:  8.784296369412914e-05
Batch  171  loss:  5.5744927522027865e-05
Batch  181  loss:  5.966158278170042e-05
Batch  191  loss:  9.070489613804966e-05
Validation on real data: 
LOSS supervised-train 9.518083999864756e-05, valid 4.9461414164397866e-05
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  6.179721094667912e-05
Batch  11  loss:  7.142985850805417e-05
Batch  21  loss:  8.657731086714193e-05
Batch  31  loss:  0.00013925431994721293
Batch  41  loss:  0.0001489082060288638
Batch  51  loss:  6.0564540035557e-05
Batch  61  loss:  0.00011224266199860722
Batch  71  loss:  7.527725392719731e-05
Batch  81  loss:  4.557572174235247e-05
Batch  91  loss:  6.16584875388071e-05
Batch  101  loss:  8.52803495945409e-05
Batch  111  loss:  5.8633097069105133e-05
Batch  121  loss:  6.187401595525444e-05
Batch  131  loss:  6.508428486995399e-05
Batch  141  loss:  7.366036152234301e-05
Batch  151  loss:  6.002275040373206e-05
Batch  161  loss:  9.540117753203958e-05
Batch  171  loss:  5.999486893415451e-05
Batch  181  loss:  5.083474388811737e-05
Batch  191  loss:  8.424823317909613e-05
Validation on real data: 
LOSS supervised-train 9.404643617017428e-05, valid 3.939290400012396e-05
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  8.91192612471059e-05
Batch  11  loss:  7.85972224548459e-05
Batch  21  loss:  7.521465158788487e-05
Batch  31  loss:  0.00012267894635442644
Batch  41  loss:  0.00014540144184138626
Batch  51  loss:  6.508728256449103e-05
Batch  61  loss:  0.00010877592285396531
Batch  71  loss:  6.558746099472046e-05
Batch  81  loss:  5.239702659309842e-05
Batch  91  loss:  7.966506382217631e-05
Batch  101  loss:  9.136345033766702e-05
Batch  111  loss:  6.408573244698346e-05
Batch  121  loss:  7.868011016398668e-05
Batch  131  loss:  7.304514292627573e-05
Batch  141  loss:  6.67766435071826e-05
Batch  151  loss:  5.9575810155365616e-05
Batch  161  loss:  0.0001018723996821791
Batch  171  loss:  5.647652506013401e-05
Batch  181  loss:  5.3329811635194346e-05
Batch  191  loss:  7.10562671883963e-05
Validation on real data: 
LOSS supervised-train 9.457129046495538e-05, valid 5.418743967311457e-05
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  5.945808152318932e-05
Batch  11  loss:  5.731447527068667e-05
Batch  21  loss:  7.001613266766071e-05
Batch  31  loss:  9.724695701152086e-05
Batch  41  loss:  0.00011295915464870632
Batch  51  loss:  4.0693350456422195e-05
Batch  61  loss:  0.00011011599417543039
Batch  71  loss:  0.00010023266077041626
Batch  81  loss:  5.277933814795688e-05
Batch  91  loss:  7.06441278452985e-05
Batch  101  loss:  8.03026559879072e-05
Batch  111  loss:  5.793008676846512e-05
Batch  121  loss:  7.869982800912112e-05
Batch  131  loss:  7.304505561478436e-05
Batch  141  loss:  6.983600178500637e-05
Batch  151  loss:  6.580528861377388e-05
Batch  161  loss:  8.88189097167924e-05
Batch  171  loss:  6.077961370465346e-05
Batch  181  loss:  4.698839256889187e-05
Batch  191  loss:  7.302205631276593e-05
Validation on real data: 
LOSS supervised-train 8.58734991379606e-05, valid 4.33193999924697e-05
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  6.942129402887076e-05
Batch  11  loss:  9.312465408584103e-05
Batch  21  loss:  5.36801599082537e-05
Batch  31  loss:  9.404310549143702e-05
Batch  41  loss:  0.00012956246791873127
Batch  51  loss:  5.1899147365475073e-05
Batch  61  loss:  8.07547548902221e-05
Batch  71  loss:  8.916822844184935e-05
Batch  81  loss:  5.193889592192136e-05
Batch  91  loss:  5.600005169981159e-05
Batch  101  loss:  8.71923184604384e-05
Batch  111  loss:  6.792481144657359e-05
Batch  121  loss:  7.055463356664404e-05
Batch  131  loss:  5.765444439020939e-05
Batch  141  loss:  6.695854244753718e-05
Batch  151  loss:  5.822003731736913e-05
Batch  161  loss:  8.466395956929773e-05
Batch  171  loss:  5.05705684190616e-05
Batch  181  loss:  5.377661364036612e-05
Batch  191  loss:  5.821948798256926e-05
Validation on real data: 
LOSS supervised-train 8.61748884562985e-05, valid 4.9916903662960976e-05
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  6.472804670920596e-05
Batch  11  loss:  6.14705277257599e-05
Batch  21  loss:  5.4812382586533204e-05
Batch  31  loss:  0.000114943235530518
Batch  41  loss:  0.00010954962635878474
Batch  51  loss:  4.8258625611197203e-05
Batch  61  loss:  8.60826185089536e-05
Batch  71  loss:  9.161749039776623e-05
Batch  81  loss:  5.979438356007449e-05
Batch  91  loss:  4.916608304483816e-05
Batch  101  loss:  0.00010179126547882333
Batch  111  loss:  4.7197332605719566e-05
Batch  121  loss:  6.331241456791759e-05
Batch  131  loss:  6.10609058639966e-05
Batch  141  loss:  6.185248639667407e-05
Batch  151  loss:  7.404513598885387e-05
Batch  161  loss:  0.0001170284376712516
Batch  171  loss:  5.6820776080712676e-05
Batch  181  loss:  6.753498746547848e-05
Batch  191  loss:  6.83098187437281e-05
Validation on real data: 
LOSS supervised-train 8.453260241367389e-05, valid 3.6254652513889596e-05
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  5.521188359125517e-05
Batch  11  loss:  9.066257189260796e-05
Batch  21  loss:  6.485989433713257e-05
Batch  31  loss:  0.00013968124403618276
Batch  41  loss:  0.00013131287414580584
Batch  51  loss:  5.325634265318513e-05
Batch  61  loss:  8.392555173486471e-05
Batch  71  loss:  7.693275983911008e-05
Batch  81  loss:  6.39680220047012e-05
Batch  91  loss:  4.513528983807191e-05
Batch  101  loss:  8.260451431851834e-05
Batch  111  loss:  7.370704406639561e-05
Batch  121  loss:  6.810778722865507e-05
Batch  131  loss:  6.635812314925715e-05
Batch  141  loss:  7.732811354799196e-05
Batch  151  loss:  6.010852666804567e-05
Batch  161  loss:  8.855553460307419e-05
Batch  171  loss:  4.988369983038865e-05
Batch  181  loss:  4.23758065153379e-05
Batch  191  loss:  8.02208305685781e-05
Validation on real data: 
LOSS supervised-train 8.38737348021823e-05, valid 3.6555618862621486e-05
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  5.451763718156144e-05
Batch  11  loss:  7.633488712599501e-05
Batch  21  loss:  5.7913865020964295e-05
Batch  31  loss:  0.00012980634346604347
Batch  41  loss:  0.0001173947166535072
Batch  51  loss:  3.4342137951171026e-05
Batch  61  loss:  5.760357089457102e-05
Batch  71  loss:  6.905234477017075e-05
Batch  81  loss:  5.4733482102165e-05
Batch  91  loss:  5.300623888615519e-05
Batch  101  loss:  0.00010659122926881537
Batch  111  loss:  4.997429277864285e-05
Batch  121  loss:  5.7224893680540845e-05
Batch  131  loss:  6.21716826572083e-05
Batch  141  loss:  5.670054815709591e-05
Batch  151  loss:  6.985221989452839e-05
Batch  161  loss:  0.0001010495179798454
Batch  171  loss:  5.069889812148176e-05
Batch  181  loss:  5.54560792807024e-05
Batch  191  loss:  7.825443026376888e-05
Validation on real data: 
LOSS supervised-train 8.160827408573823e-05, valid 8.210882515413687e-05
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  6.16172474110499e-05
Batch  11  loss:  6.801400013500825e-05
Batch  21  loss:  5.92239412071649e-05
Batch  31  loss:  0.00010251750063616782
Batch  41  loss:  0.00013171056343708187
Batch  51  loss:  3.309761450509541e-05
Batch  61  loss:  9.747855801833794e-05
Batch  71  loss:  0.0001063748131855391
Batch  81  loss:  4.7834742872510105e-05
Batch  91  loss:  4.416333467816003e-05
Batch  101  loss:  7.930185529403389e-05
Batch  111  loss:  5.843760663992725e-05
Batch  121  loss:  6.1014234233880416e-05
Batch  131  loss:  5.2570903790183365e-05
Batch  141  loss:  6.291375029832125e-05
Batch  151  loss:  5.9048205002909526e-05
Batch  161  loss:  9.601760393707082e-05
Batch  171  loss:  4.951925438945182e-05
Batch  181  loss:  5.312281064107083e-05
Batch  191  loss:  6.322099943645298e-05
Validation on real data: 
LOSS supervised-train 8.121783930619131e-05, valid 4.764836194226518e-05
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  6.1678139900323e-05
Batch  11  loss:  6.665586988674477e-05
Batch  21  loss:  5.722802961827256e-05
Batch  31  loss:  8.859555964590982e-05
Batch  41  loss:  0.0001011767890304327
Batch  51  loss:  5.4506512242369354e-05
Batch  61  loss:  8.160020661307499e-05
Batch  71  loss:  8.95344783202745e-05
Batch  81  loss:  4.270309000276029e-05
Batch  91  loss:  4.917951810057275e-05
Batch  101  loss:  8.135907410178334e-05
Batch  111  loss:  5.7676945289131254e-05
Batch  121  loss:  6.278159708017483e-05
Batch  131  loss:  6.0452330217231065e-05
Batch  141  loss:  5.1489099860191345e-05
Batch  151  loss:  6.477453280240297e-05
Batch  161  loss:  8.259318565251306e-05
Batch  171  loss:  4.541602902463637e-05
Batch  181  loss:  5.481526022776961e-05
Batch  191  loss:  7.406832446577027e-05
Validation on real data: 
LOSS supervised-train 7.947700094518951e-05, valid 3.320300311315805e-05
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  6.488327926490456e-05
Batch  11  loss:  6.19792626821436e-05
Batch  21  loss:  6.160842895042151e-05
Batch  31  loss:  9.121184120886028e-05
Batch  41  loss:  0.00010605144052533433
Batch  51  loss:  4.217398600303568e-05
Batch  61  loss:  7.800613093422726e-05
Batch  71  loss:  7.929005369078368e-05
Batch  81  loss:  5.329335544956848e-05
Batch  91  loss:  4.712052395916544e-05
Batch  101  loss:  5.9983143728459254e-05
Batch  111  loss:  5.509518814506009e-05
Batch  121  loss:  6.315677455859259e-05
Batch  131  loss:  5.4888161685084924e-05
Batch  141  loss:  5.8535639254841954e-05
Batch  151  loss:  5.011129906051792e-05
Batch  161  loss:  8.807374251773581e-05
Batch  171  loss:  3.5665139876073226e-05
Batch  181  loss:  4.594999336404726e-05
Batch  191  loss:  7.884129445301369e-05
Validation on real data: 
LOSS supervised-train 7.709423452979536e-05, valid 5.701571353711188e-05
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  5.401934322435409e-05
Batch  11  loss:  7.004224607953802e-05
Batch  21  loss:  5.72862772969529e-05
Batch  31  loss:  0.00010442757047712803
Batch  41  loss:  0.0001036692556226626
Batch  51  loss:  4.148841617279686e-05
Batch  61  loss:  7.451007695635781e-05
Batch  71  loss:  7.198189996415749e-05
Batch  81  loss:  4.875325976172462e-05
Batch  91  loss:  4.2052230128319934e-05
Batch  101  loss:  6.809279875596985e-05
Batch  111  loss:  5.522730134543963e-05
Batch  121  loss:  7.165444549173117e-05
Batch  131  loss:  4.877200262853876e-05
Batch  141  loss:  5.543560109799728e-05
Batch  151  loss:  6.321255932562053e-05
Batch  161  loss:  8.722572238184512e-05
Batch  171  loss:  4.952223025611602e-05
Batch  181  loss:  5.5194392189150676e-05
Batch  191  loss:  5.578835407504812e-05
Validation on real data: 
LOSS supervised-train 7.643542696314398e-05, valid 3.811892383964732e-05
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  6.817919347668067e-05
Batch  11  loss:  7.46865916880779e-05
Batch  21  loss:  4.236028325976804e-05
Batch  31  loss:  8.965977031039074e-05
Batch  41  loss:  9.746797877596691e-05
Batch  51  loss:  5.297935422277078e-05
Batch  61  loss:  8.225517376558855e-05
Batch  71  loss:  8.270641410490498e-05
Batch  81  loss:  5.475074067362584e-05
Batch  91  loss:  5.37553132744506e-05
Batch  101  loss:  8.657863509142771e-05
Batch  111  loss:  5.55105616513174e-05
Batch  121  loss:  6.756721995770931e-05
Batch  131  loss:  5.2045874326722696e-05
Batch  141  loss:  6.345088331727311e-05
Batch  151  loss:  6.101918188505806e-05
Batch  161  loss:  8.801365038380027e-05
Batch  171  loss:  5.223046900937334e-05
Batch  181  loss:  4.3132131395395845e-05
Batch  191  loss:  6.244272663025185e-05
Validation on real data: 
LOSS supervised-train 7.539454074503737e-05, valid 3.532580012688413e-05
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  4.728494241135195e-05
Batch  11  loss:  7.129851292120293e-05
Batch  21  loss:  5.6172924814745784e-05
Batch  31  loss:  0.00010999641381204128
Batch  41  loss:  8.048560266615823e-05
Batch  51  loss:  5.411271922639571e-05
Batch  61  loss:  6.231215957086533e-05
Batch  71  loss:  7.059462950564921e-05
Batch  81  loss:  4.764620462083258e-05
Batch  91  loss:  4.223783253110014e-05
Batch  101  loss:  9.040493023348972e-05
Batch  111  loss:  5.8481196901993826e-05
Batch  121  loss:  6.187129474710673e-05
Batch  131  loss:  6.662601663265377e-05
Batch  141  loss:  5.747456816607155e-05
Batch  151  loss:  5.9388527006376535e-05
Batch  161  loss:  7.887020183261484e-05
Batch  171  loss:  3.9351871237158775e-05
Batch  181  loss:  4.302773595554754e-05
Batch  191  loss:  6.593588477699086e-05
Validation on real data: 
LOSS supervised-train 7.399845556392392e-05, valid 3.828750777756795e-05
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  5.569546920014545e-05
Batch  11  loss:  6.378455145750195e-05
Batch  21  loss:  5.36937550350558e-05
Batch  31  loss:  9.632795990910381e-05
Batch  41  loss:  9.962450712919235e-05
Batch  51  loss:  5.03399787703529e-05
Batch  61  loss:  8.221052121371031e-05
Batch  71  loss:  4.8286179662682116e-05
Batch  81  loss:  5.0981983804376796e-05
Batch  91  loss:  4.8744510422693565e-05
Batch  101  loss:  6.646756082773209e-05
Batch  111  loss:  6.233951717149466e-05
Batch  121  loss:  5.6496857723686844e-05
Batch  131  loss:  4.9270438466919586e-05
Batch  141  loss:  5.410268931882456e-05
Batch  151  loss:  6.323842535493895e-05
Batch  161  loss:  7.495708268834278e-05
Batch  171  loss:  4.0659950172994286e-05
Batch  181  loss:  3.8994865462882444e-05
Batch  191  loss:  4.8941903514787555e-05
Validation on real data: 
LOSS supervised-train 7.379141731689743e-05, valid 3.971367550548166e-05
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  4.5064887672197074e-05
Batch  11  loss:  5.974699161015451e-05
Batch  21  loss:  5.088915349915624e-05
Batch  31  loss:  8.820812945486978e-05
Batch  41  loss:  0.00010557503992458805
Batch  51  loss:  4.85527707496658e-05
Batch  61  loss:  7.088226266205311e-05
Batch  71  loss:  7.857874152250588e-05
Batch  81  loss:  4.627067391993478e-05
Batch  91  loss:  4.740341319120489e-05
Batch  101  loss:  7.081774674588814e-05
Batch  111  loss:  5.440001405077055e-05
Batch  121  loss:  5.4883956181583926e-05
Batch  131  loss:  4.7959983930923045e-05
Batch  141  loss:  4.951534356223419e-05
Batch  151  loss:  4.648337562684901e-05
Batch  161  loss:  8.852944301906973e-05
Batch  171  loss:  4.008485848316923e-05
Batch  181  loss:  5.285392762743868e-05
Batch  191  loss:  6.265957927098498e-05
Validation on real data: 
LOSS supervised-train 6.950653201784007e-05, valid 4.276274557923898e-05
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  4.769617589772679e-05
Batch  11  loss:  4.702789738075808e-05
Batch  21  loss:  5.3334941185312346e-05
Batch  31  loss:  8.043826528592035e-05
Batch  41  loss:  8.795526809990406e-05
Batch  51  loss:  3.4801818401319906e-05
Batch  61  loss:  5.442272231448442e-05
Batch  71  loss:  5.2066792704863474e-05
Batch  81  loss:  3.386780372238718e-05
Batch  91  loss:  4.085011823917739e-05
Batch  101  loss:  7.893310976214707e-05
Batch  111  loss:  5.122096263221465e-05
Batch  121  loss:  6.729138840455562e-05
Batch  131  loss:  3.918063885066658e-05
Batch  141  loss:  5.459874591906555e-05
Batch  151  loss:  7.101097435224801e-05
Batch  161  loss:  7.767967326799408e-05
Batch  171  loss:  4.75648048450239e-05
Batch  181  loss:  3.345682853250764e-05
Batch  191  loss:  6.100980317569338e-05
Validation on real data: 
LOSS supervised-train 6.869096146147057e-05, valid 4.584944690577686e-05
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  5.6970471632666886e-05
Batch  11  loss:  6.316805956885219e-05
Batch  21  loss:  4.562661342788488e-05
Batch  31  loss:  9.693777246866375e-05
Batch  41  loss:  0.00012147286906838417
Batch  51  loss:  3.984113209298812e-05
Batch  61  loss:  7.11669199517928e-05
Batch  71  loss:  7.299295248230919e-05
Batch  81  loss:  4.334232653491199e-05
Batch  91  loss:  4.140038072364405e-05
Batch  101  loss:  5.840021185576916e-05
Batch  111  loss:  4.7417754103662446e-05
Batch  121  loss:  4.662703213398345e-05
Batch  131  loss:  5.280414188746363e-05
Batch  141  loss:  5.837789649376646e-05
Batch  151  loss:  6.192223372636363e-05
Batch  161  loss:  6.559752364410087e-05
Batch  171  loss:  4.309431460569613e-05
Batch  181  loss:  5.4143285524332896e-05
Batch  191  loss:  5.083058204036206e-05
Validation on real data: 
LOSS supervised-train 6.860531724669273e-05, valid 4.3787717004306614e-05
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  5.997514381306246e-05
Batch  11  loss:  4.989812077837996e-05
Batch  21  loss:  4.73446307296399e-05
Batch  31  loss:  0.00011138256377307698
Batch  41  loss:  9.034255344886333e-05
Batch  51  loss:  5.525146480067633e-05
Batch  61  loss:  5.460555985337123e-05
Batch  71  loss:  6.962765473872423e-05
Batch  81  loss:  4.2543793824734166e-05
Batch  91  loss:  4.626935697160661e-05
Batch  101  loss:  5.958866677246988e-05
Batch  111  loss:  5.3543604735750705e-05
Batch  121  loss:  5.105091986479238e-05
Batch  131  loss:  4.3769789044745266e-05
Batch  141  loss:  5.0274124077986926e-05
Batch  151  loss:  5.668423546012491e-05
Batch  161  loss:  8.07939431979321e-05
Batch  171  loss:  3.683816976263188e-05
Batch  181  loss:  3.570051194401458e-05
Batch  191  loss:  5.276402225717902e-05
Validation on real data: 
LOSS supervised-train 6.726343346599606e-05, valid 3.642569936346263e-05
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  5.102757131680846e-05
Batch  11  loss:  6.205924728419632e-05
Batch  21  loss:  4.189493847661652e-05
Batch  31  loss:  8.951251220423728e-05
Batch  41  loss:  8.190656080842018e-05
Batch  51  loss:  3.197038313373923e-05
Batch  61  loss:  6.386620225384831e-05
Batch  71  loss:  5.9040401538368315e-05
Batch  81  loss:  4.19319694628939e-05
Batch  91  loss:  4.474792876862921e-05
Batch  101  loss:  6.261243106564507e-05
Batch  111  loss:  5.33099337189924e-05
Batch  121  loss:  4.794162305188365e-05
Batch  131  loss:  4.5408633013721555e-05
Batch  141  loss:  5.615794361801818e-05
Batch  151  loss:  5.0199734687339514e-05
Batch  161  loss:  6.570117693627253e-05
Batch  171  loss:  4.676979733631015e-05
Batch  181  loss:  5.996724576107226e-05
Batch  191  loss:  4.786193312611431e-05
Validation on real data: 
LOSS supervised-train 6.814519376348471e-05, valid 4.740986696560867e-05
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  5.576222247327678e-05
Batch  11  loss:  6.121180194895715e-05
Batch  21  loss:  5.577360207098536e-05
Batch  31  loss:  9.084460907615721e-05
Batch  41  loss:  9.34213021537289e-05
Batch  51  loss:  3.916075365850702e-05
Batch  61  loss:  7.687542529311031e-05
Batch  71  loss:  7.746855408186093e-05
Batch  81  loss:  5.201403837418184e-05
Batch  91  loss:  4.45561163360253e-05
Batch  101  loss:  5.695270374417305e-05
Batch  111  loss:  5.5837383115431294e-05
Batch  121  loss:  4.668485780712217e-05
Batch  131  loss:  3.639314309111796e-05
Batch  141  loss:  5.6139724620152265e-05
Batch  151  loss:  5.196183701627888e-05
Batch  161  loss:  7.874272705521435e-05
Batch  171  loss:  5.205866909818724e-05
Batch  181  loss:  5.1103372243233025e-05
Batch  191  loss:  6.148510874481872e-05
Validation on real data: 
LOSS supervised-train 6.659610298811458e-05, valid 3.083743649767712e-05
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  5.9701138525269926e-05
Batch  11  loss:  7.310419459827244e-05
Batch  21  loss:  4.7737099521327764e-05
Batch  31  loss:  8.54394820635207e-05
Batch  41  loss:  8.089026960078627e-05
Batch  51  loss:  3.957604349125177e-05
Batch  61  loss:  5.3981319069862366e-05
Batch  71  loss:  6.378022953867912e-05
Batch  81  loss:  3.7752608477603644e-05
Batch  91  loss:  4.822355185751803e-05
Batch  101  loss:  5.4219992307480425e-05
Batch  111  loss:  5.4205094784265384e-05
Batch  121  loss:  4.098262434126809e-05
Batch  131  loss:  4.2801231757039204e-05
Batch  141  loss:  3.84899067285005e-05
Batch  151  loss:  6.530177779495716e-05
Batch  161  loss:  7.130269659683108e-05
Batch  171  loss:  4.599857857101597e-05
Batch  181  loss:  5.246397995506413e-05
Batch  191  loss:  6.305552233243361e-05
Validation on real data: 
LOSS supervised-train 6.426397944778728e-05, valid 3.489653317956254e-05
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  5.333887384040281e-05
Batch  11  loss:  4.3288593587931246e-05
Batch  21  loss:  4.095849362784065e-05
Batch  31  loss:  7.806530629750341e-05
Batch  41  loss:  7.256405660882592e-05
Batch  51  loss:  4.3887699575861916e-05
Batch  61  loss:  5.87422946409788e-05
Batch  71  loss:  4.7510609874734655e-05
Batch  81  loss:  4.722481389762834e-05
Batch  91  loss:  4.579941014526412e-05
Batch  101  loss:  5.366542609408498e-05
Batch  111  loss:  4.913247175863944e-05
Batch  121  loss:  5.373789826990105e-05
Batch  131  loss:  5.7028726587304845e-05
Batch  141  loss:  3.536319127306342e-05
Batch  151  loss:  4.255637395544909e-05
Batch  161  loss:  8.003627590369433e-05
Batch  171  loss:  3.9828140870667994e-05
Batch  181  loss:  4.737224298878573e-05
Batch  191  loss:  4.8615442210575566e-05
Validation on real data: 
LOSS supervised-train 6.348441620502853e-05, valid 2.9534312488976866e-05
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  4.8827572754817083e-05
Batch  11  loss:  6.561303598573431e-05
Batch  21  loss:  4.659380283555947e-05
Batch  31  loss:  7.518913480453193e-05
Batch  41  loss:  9.742469410412014e-05
Batch  51  loss:  3.7733680073870346e-05
Batch  61  loss:  6.813811341999099e-05
Batch  71  loss:  6.314530037343502e-05
Batch  81  loss:  5.535953096114099e-05
Batch  91  loss:  3.613113585743122e-05
Batch  101  loss:  6.576810119440779e-05
Batch  111  loss:  4.883176370640285e-05
Batch  121  loss:  3.787132664001547e-05
Batch  131  loss:  4.7967201680876315e-05
Batch  141  loss:  4.216837260173634e-05
Batch  151  loss:  6.318215309875086e-05
Batch  161  loss:  7.047121471259743e-05
Batch  171  loss:  3.9032289350871e-05
Batch  181  loss:  5.1981442084070295e-05
Batch  191  loss:  4.983096368960105e-05
Validation on real data: 
LOSS supervised-train 6.390789692886756e-05, valid 4.8859579692361876e-05
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  5.381408118410036e-05
Batch  11  loss:  5.5622764193685725e-05
Batch  21  loss:  4.404726860229857e-05
Batch  31  loss:  9.678408969193697e-05
Batch  41  loss:  8.887335570761934e-05
Batch  51  loss:  3.9769583963789046e-05
Batch  61  loss:  5.652421168633737e-05
Batch  71  loss:  5.3961117373546585e-05
Batch  81  loss:  4.953211828251369e-05
Batch  91  loss:  4.462114520720206e-05
Batch  101  loss:  5.9655369113897905e-05
Batch  111  loss:  4.9874550313688815e-05
Batch  121  loss:  4.8666930524632335e-05
Batch  131  loss:  3.627828846219927e-05
Batch  141  loss:  5.383920870372094e-05
Batch  151  loss:  5.81297863391228e-05
Batch  161  loss:  6.942159961909056e-05
Batch  171  loss:  4.3710999307222664e-05
Batch  181  loss:  4.2389645386720076e-05
Batch  191  loss:  5.828210851177573e-05
Validation on real data: 
LOSS supervised-train 6.138298374935402e-05, valid 4.890243508270942e-05
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  4.355466808192432e-05
Batch  11  loss:  4.9889073125086725e-05
Batch  21  loss:  4.846827141591348e-05
Batch  31  loss:  8.944870933191851e-05
Batch  41  loss:  7.369907689280808e-05
Batch  51  loss:  4.887840987066738e-05
Batch  61  loss:  7.949858991196379e-05
Batch  71  loss:  5.4253552661975846e-05
Batch  81  loss:  4.646501474780962e-05
Batch  91  loss:  4.477171023609117e-05
Batch  101  loss:  6.31315415375866e-05
Batch  111  loss:  5.5127682571765035e-05
Batch  121  loss:  4.112395254196599e-05
Batch  131  loss:  5.249498281045817e-05
Batch  141  loss:  4.086704211658798e-05
Batch  151  loss:  7.123418617993593e-05
Batch  161  loss:  8.629288640804589e-05
Batch  171  loss:  4.5600383600685745e-05
Batch  181  loss:  5.2955831051804125e-05
Batch  191  loss:  5.5792650528019294e-05
Validation on real data: 
LOSS supervised-train 6.25010001112969e-05, valid 4.1131439502350986e-05
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  4.472629007068463e-05
Batch  11  loss:  6.008456330164336e-05
Batch  21  loss:  3.660289803519845e-05
Batch  31  loss:  7.786809146637097e-05
Batch  41  loss:  8.52606535772793e-05
Batch  51  loss:  3.980035398853943e-05
Batch  61  loss:  5.3120100346859545e-05
Batch  71  loss:  6.094272976042703e-05
Batch  81  loss:  4.403749335324392e-05
Batch  91  loss:  4.1173370846081525e-05
Batch  101  loss:  5.734064325224608e-05
Batch  111  loss:  5.0641214329516515e-05
Batch  121  loss:  4.351667666924186e-05
Batch  131  loss:  3.3311083825537935e-05
Batch  141  loss:  4.380883910926059e-05
Batch  151  loss:  5.7660283346194774e-05
Batch  161  loss:  8.104441803880036e-05
Batch  171  loss:  3.6977162380935624e-05
Batch  181  loss:  4.33768036600668e-05
Batch  191  loss:  4.23759083787445e-05
Validation on real data: 
LOSS supervised-train 5.933659229413024e-05, valid 3.0128194339340553e-05
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  5.3025156375952065e-05
Batch  11  loss:  5.09441269969102e-05
Batch  21  loss:  4.084778993274085e-05
Batch  31  loss:  8.125005842885002e-05
Batch  41  loss:  7.356966671068221e-05
Batch  51  loss:  3.83665828849189e-05
Batch  61  loss:  5.558102566283196e-05
Batch  71  loss:  4.609299867297523e-05
Batch  81  loss:  2.7972384486929514e-05
Batch  91  loss:  3.707290306920186e-05
Batch  101  loss:  4.74642074550502e-05
Batch  111  loss:  5.236539072939195e-05
Batch  121  loss:  4.77085959573742e-05
Batch  131  loss:  2.7947495254920796e-05
Batch  141  loss:  5.4483105486724526e-05
Batch  151  loss:  5.535148738999851e-05
Batch  161  loss:  8.673368574818596e-05
Batch  171  loss:  3.913395994459279e-05
Batch  181  loss:  5.137377229402773e-05
Batch  191  loss:  4.6822475269436836e-05
Validation on real data: 
LOSS supervised-train 5.955882638772891e-05, valid 3.858576747006737e-05
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  4.104261097381823e-05
Batch  11  loss:  5.3290284995455295e-05
Batch  21  loss:  4.733146488433704e-05
Batch  31  loss:  9.1101712314412e-05
Batch  41  loss:  6.579622277058661e-05
Batch  51  loss:  4.2631018004613e-05
Batch  61  loss:  5.2732433687197044e-05
Batch  71  loss:  5.385542317526415e-05
Batch  81  loss:  3.830640343949199e-05
Batch  91  loss:  4.470731073524803e-05
Batch  101  loss:  5.6775355915306136e-05
Batch  111  loss:  4.599343810696155e-05
Batch  121  loss:  3.773271600948647e-05
Batch  131  loss:  3.8895654142834246e-05
Batch  141  loss:  4.8649479140294716e-05
Batch  151  loss:  5.419306762632914e-05
Batch  161  loss:  8.465057180728763e-05
Batch  171  loss:  5.412034079199657e-05
Batch  181  loss:  4.260045534465462e-05
Batch  191  loss:  5.3862135246163234e-05
Validation on real data: 
LOSS supervised-train 5.969961914161104e-05, valid 2.7749794753617607e-05
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  4.069855640409514e-05
Batch  11  loss:  4.8731777496868744e-05
Batch  21  loss:  4.9906659114640206e-05
Batch  31  loss:  7.919669587863609e-05
Batch  41  loss:  0.00010096059850184247
Batch  51  loss:  3.960193134844303e-05
Batch  61  loss:  5.486669033416547e-05
Batch  71  loss:  5.198373401071876e-05
Batch  81  loss:  4.2041221604449674e-05
Batch  91  loss:  4.0512757550459355e-05
Batch  101  loss:  5.689188401447609e-05
Batch  111  loss:  5.256538497633301e-05
Batch  121  loss:  4.187689410173334e-05
Batch  131  loss:  3.0683117074659094e-05
Batch  141  loss:  4.6607197873527184e-05
Batch  151  loss:  5.481025073095225e-05
Batch  161  loss:  6.24585518380627e-05
Batch  171  loss:  3.68265864381101e-05
Batch  181  loss:  4.037074540974572e-05
Batch  191  loss:  4.3664265831466764e-05
Validation on real data: 
LOSS supervised-train 5.8567789092194287e-05, valid 2.6462152163730934e-05
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  4.899126724922098e-05
Batch  11  loss:  4.7687375626992434e-05
Batch  21  loss:  4.190973413642496e-05
Batch  31  loss:  7.06163264112547e-05
Batch  41  loss:  5.512591451406479e-05
Batch  51  loss:  3.9687023672740906e-05
Batch  61  loss:  4.985704072169028e-05
Batch  71  loss:  5.772128133685328e-05
Batch  81  loss:  4.4606968003790826e-05
Batch  91  loss:  3.7860598240513355e-05
Batch  101  loss:  4.9643535021459684e-05
Batch  111  loss:  5.630508894682862e-05
Batch  121  loss:  5.556284304475412e-05
Batch  131  loss:  3.2871554140001535e-05
Batch  141  loss:  4.057381738675758e-05
Batch  151  loss:  6.201457290444523e-05
Batch  161  loss:  6.311788456514478e-05
Batch  171  loss:  3.399609340704046e-05
Batch  181  loss:  4.9314359785057604e-05
Batch  191  loss:  6.141953053884208e-05
Validation on real data: 
LOSS supervised-train 5.6698778353165834e-05, valid 2.910955299739726e-05
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  4.3902611650992185e-05
Batch  11  loss:  6.215945904841647e-05
Batch  21  loss:  4.706071558757685e-05
Batch  31  loss:  8.091158815659583e-05
Batch  41  loss:  7.892120629549026e-05
Batch  51  loss:  3.898144495906308e-05
Batch  61  loss:  5.0772196118487045e-05
Batch  71  loss:  5.267658707452938e-05
Batch  81  loss:  3.612970976973884e-05
Batch  91  loss:  2.883866909542121e-05
Batch  101  loss:  5.1709572289837524e-05
Batch  111  loss:  5.213837357587181e-05
Batch  121  loss:  3.843811646220274e-05
Batch  131  loss:  3.992398706031963e-05
Batch  141  loss:  3.68885557691101e-05
Batch  151  loss:  4.9983817007159814e-05
Batch  161  loss:  6.400866550393403e-05
Batch  171  loss:  4.37291273556184e-05
Batch  181  loss:  4.114126568310894e-05
Batch  191  loss:  4.1395705920876935e-05
Validation on real data: 
LOSS supervised-train 5.66454226282076e-05, valid 3.339438626426272e-05
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  4.280182838556357e-05
Batch  11  loss:  5.695713480236009e-05
Batch  21  loss:  4.447442552191205e-05
Batch  31  loss:  7.067655678838491e-05
Batch  41  loss:  6.315120117506012e-05
Batch  51  loss:  3.0608363886130974e-05
Batch  61  loss:  4.304065805627033e-05
Batch  71  loss:  4.766132406075485e-05
Batch  81  loss:  4.893507866654545e-05
Batch  91  loss:  3.636507608462125e-05
Batch  101  loss:  4.2218736780341715e-05
Batch  111  loss:  4.839543180423789e-05
Batch  121  loss:  3.9947201003087685e-05
Batch  131  loss:  4.0803628508001566e-05
Batch  141  loss:  3.800668127951212e-05
Batch  151  loss:  4.89101730636321e-05
Batch  161  loss:  6.07775837124791e-05
Batch  171  loss:  4.0792085201246664e-05
Batch  181  loss:  3.8222457078518346e-05
Batch  191  loss:  4.1265448089689016e-05
Validation on real data: 
LOSS supervised-train 5.4227188038566964e-05, valid 2.895844227168709e-05
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  4.2744090023916215e-05
Batch  11  loss:  4.8589849029667675e-05
Batch  21  loss:  3.485686829662882e-05
Batch  31  loss:  7.486104732379317e-05
Batch  41  loss:  8.062833512667567e-05
Batch  51  loss:  3.2754731364548206e-05
Batch  61  loss:  4.7878060286166146e-05
Batch  71  loss:  4.5821394451195374e-05
Batch  81  loss:  3.739915700862184e-05
Batch  91  loss:  3.388947880011983e-05
Batch  101  loss:  5.264229912427254e-05
Batch  111  loss:  5.544714804273099e-05
Batch  121  loss:  4.207766687613912e-05
Batch  131  loss:  4.3748925236286595e-05
Batch  141  loss:  3.620120696723461e-05
Batch  151  loss:  6.373172800522298e-05
Batch  161  loss:  6.147876410977915e-05
Batch  171  loss:  4.318364881328307e-05
Batch  181  loss:  4.224797521601431e-05
Batch  191  loss:  3.732190089067444e-05
Validation on real data: 
LOSS supervised-train 5.4292367631205705e-05, valid 3.691199526656419e-05
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  3.826770989689976e-05
Batch  11  loss:  6.011533332639374e-05
Batch  21  loss:  3.9433016354450956e-05
Batch  31  loss:  5.995980973239057e-05
Batch  41  loss:  6.592388672288507e-05
Batch  51  loss:  4.2910622141789645e-05
Batch  61  loss:  4.6410717914113775e-05
Batch  71  loss:  5.4569929488934577e-05
Batch  81  loss:  4.6230637963162735e-05
Batch  91  loss:  4.0319257095688954e-05
Batch  101  loss:  4.9632450100034475e-05
Batch  111  loss:  5.001394310966134e-05
Batch  121  loss:  3.606219252105802e-05
Batch  131  loss:  4.038887345814146e-05
Batch  141  loss:  3.889688741764985e-05
Batch  151  loss:  5.775609315605834e-05
Batch  161  loss:  7.532907329732552e-05
Batch  171  loss:  3.85291496058926e-05
Batch  181  loss:  4.506983532337472e-05
Batch  191  loss:  4.666721724788658e-05
Validation on real data: 
LOSS supervised-train 5.286016408717842e-05, valid 3.068288788199425e-05
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  4.408929089549929e-05
Batch  11  loss:  6.005587783874944e-05
Batch  21  loss:  3.9857710362412035e-05
Batch  31  loss:  7.616224320372567e-05
Batch  41  loss:  5.903745113755576e-05
Batch  51  loss:  4.371374598122202e-05
Batch  61  loss:  5.220652383286506e-05
Batch  71  loss:  4.092590825166553e-05
Batch  81  loss:  3.685595220304094e-05
Batch  91  loss:  3.803192521445453e-05
Batch  101  loss:  5.838966171722859e-05
Batch  111  loss:  4.854950384469703e-05
Batch  121  loss:  4.1724506445461884e-05
Batch  131  loss:  3.6993711546529084e-05
Batch  141  loss:  4.49545586889144e-05
Batch  151  loss:  5.674999192706309e-05
Batch  161  loss:  6.131426926003769e-05
Batch  171  loss:  4.006618837593123e-05
Batch  181  loss:  3.943459887523204e-05
Batch  191  loss:  4.8650792450644076e-05
Validation on real data: 
LOSS supervised-train 5.383967982197646e-05, valid 3.791659401031211e-05
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  5.348449485609308e-05
Batch  11  loss:  4.381363032734953e-05
Batch  21  loss:  4.4884181988891214e-05
Batch  31  loss:  6.336117075989023e-05
Batch  41  loss:  8.31550350994803e-05
Batch  51  loss:  3.0305007385322824e-05
Batch  61  loss:  6.461099110310897e-05
Batch  71  loss:  3.6250126868253574e-05
Batch  81  loss:  3.72127651644405e-05
Batch  91  loss:  3.0017568860785104e-05
Batch  101  loss:  4.5259799662744626e-05
Batch  111  loss:  4.476006142795086e-05
Batch  121  loss:  4.532987077254802e-05
Batch  131  loss:  3.790174378082156e-05
Batch  141  loss:  4.633856951841153e-05
Batch  151  loss:  5.110528218210675e-05
Batch  161  loss:  6.085163477109745e-05
Batch  171  loss:  3.4582135413074866e-05
Batch  181  loss:  4.624382927431725e-05
Batch  191  loss:  4.581804387271404e-05
Validation on real data: 
LOSS supervised-train 5.240345776655886e-05, valid 3.368817488080822e-05
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  4.183095006737858e-05
Batch  11  loss:  4.999095108360052e-05
Batch  21  loss:  4.786259887623601e-05
Batch  31  loss:  5.832796887261793e-05
Batch  41  loss:  6.942981417523697e-05
Batch  51  loss:  3.429309435887262e-05
Batch  61  loss:  4.913990414934233e-05
Batch  71  loss:  4.691191861638799e-05
Batch  81  loss:  2.7282325390842743e-05
Batch  91  loss:  3.233536335756071e-05
Batch  101  loss:  5.572357986238785e-05
Batch  111  loss:  3.994657890871167e-05
Batch  121  loss:  3.8094742194516584e-05
Batch  131  loss:  3.972350532421842e-05
Batch  141  loss:  4.193465065327473e-05
Batch  151  loss:  5.491926276590675e-05
Batch  161  loss:  7.765520422253758e-05
Batch  171  loss:  2.7559706722968258e-05
Batch  181  loss:  4.4189266191096976e-05
Batch  191  loss:  4.254411032889038e-05
Validation on real data: 
LOSS supervised-train 5.180428907806345e-05, valid 3.16073746944312e-05
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  4.250275014783256e-05
Batch  11  loss:  4.369523230707273e-05
Batch  21  loss:  4.4153792259749025e-05
Batch  31  loss:  6.080356615711935e-05
Batch  41  loss:  6.69664514134638e-05
Batch  51  loss:  3.7745765439467505e-05
Batch  61  loss:  4.853358041145839e-05
Batch  71  loss:  4.602215994964354e-05
Batch  81  loss:  3.142655987176113e-05
Batch  91  loss:  3.293044937890954e-05
Batch  101  loss:  4.2181909520877525e-05
Batch  111  loss:  4.182826160104014e-05
Batch  121  loss:  3.853854286717251e-05
Batch  131  loss:  3.6314195313025266e-05
Batch  141  loss:  3.361848212080076e-05
Batch  151  loss:  4.645309309125878e-05
Batch  161  loss:  5.474139834404923e-05
Batch  171  loss:  3.410157296457328e-05
Batch  181  loss:  3.74467927031219e-05
Batch  191  loss:  4.3876192648895085e-05
Validation on real data: 
LOSS supervised-train 5.034288528804609e-05, valid 2.8556325560202822e-05
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  3.879730138578452e-05
Batch  11  loss:  5.3185845899861306e-05
Batch  21  loss:  3.1384897738462314e-05
Batch  31  loss:  6.611321441596374e-05
Batch  41  loss:  7.074304448906332e-05
Batch  51  loss:  3.283597106928937e-05
Batch  61  loss:  3.958756133215502e-05
Batch  71  loss:  5.0562732212711126e-05
Batch  81  loss:  5.415339910541661e-05
Batch  91  loss:  3.6788875149795786e-05
Batch  101  loss:  4.331352101871744e-05
Batch  111  loss:  4.455654925550334e-05
Batch  121  loss:  3.663842653622851e-05
Batch  131  loss:  2.7465130187920295e-05
Batch  141  loss:  3.049013866984751e-05
Batch  151  loss:  6.324011337710544e-05
Batch  161  loss:  8.058739331318066e-05
Batch  171  loss:  4.082901796209626e-05
Batch  181  loss:  3.288038351456635e-05
Batch  191  loss:  4.8588961362838745e-05
Validation on real data: 
LOSS supervised-train 5.036689588450827e-05, valid 3.5640161513583735e-05
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  3.88299667974934e-05
Batch  11  loss:  4.690631976700388e-05
Batch  21  loss:  4.714346505352296e-05
Batch  31  loss:  5.900106771150604e-05
Batch  41  loss:  6.161597411846742e-05
Batch  51  loss:  3.359834590810351e-05
Batch  61  loss:  5.099039481137879e-05
Batch  71  loss:  4.093700772500597e-05
Batch  81  loss:  3.756576916202903e-05
Batch  91  loss:  3.551226473064162e-05
Batch  101  loss:  5.313031215337105e-05
Batch  111  loss:  4.5823133405065164e-05
Batch  121  loss:  4.6192937588784844e-05
Batch  131  loss:  3.422316876822151e-05
Batch  141  loss:  4.642884960048832e-05
Batch  151  loss:  5.148618220118806e-05
Batch  161  loss:  6.7044013121631e-05
Batch  171  loss:  4.408044333104044e-05
Batch  181  loss:  3.9587343053426594e-05
Batch  191  loss:  4.193208587821573e-05
Validation on real data: 
LOSS supervised-train 5.1432798027235546e-05, valid 2.449496605549939e-05
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  3.535717405611649e-05
Batch  11  loss:  4.342426473158412e-05
Batch  21  loss:  3.752047996385954e-05
Batch  31  loss:  6.32887240499258e-05
Batch  41  loss:  5.981934737064876e-05
Batch  51  loss:  4.0195853216573596e-05
Batch  61  loss:  4.1847357351798564e-05
Batch  71  loss:  4.751223968924023e-05
Batch  81  loss:  2.703050267882645e-05
Batch  91  loss:  3.522965562297031e-05
Batch  101  loss:  5.51853772776667e-05
Batch  111  loss:  4.6770775952609256e-05
Batch  121  loss:  4.5344575482886285e-05
Batch  131  loss:  3.679258588817902e-05
Batch  141  loss:  3.8809204852441326e-05
Batch  151  loss:  6.0050151660107076e-05
Batch  161  loss:  6.1010468925815076e-05
Batch  171  loss:  3.7718695239163935e-05
Batch  181  loss:  3.665911208372563e-05
Batch  191  loss:  3.8081583625171334e-05
Validation on real data: 
LOSS supervised-train 4.948583125042205e-05, valid 2.2606509446632117e-05
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  4.856714076595381e-05
Batch  11  loss:  4.7895435272948816e-05
Batch  21  loss:  3.4982364013558254e-05
Batch  31  loss:  6.408675108104944e-05
Batch  41  loss:  6.669507274636999e-05
Batch  51  loss:  3.58666802640073e-05
Batch  61  loss:  4.009221083833836e-05
Batch  71  loss:  4.063014785060659e-05
Batch  81  loss:  3.3448195608798414e-05
Batch  91  loss:  3.5326411307323724e-05
Batch  101  loss:  4.335923949838616e-05
Batch  111  loss:  3.873959940392524e-05
Batch  121  loss:  3.7458681617863476e-05
Batch  131  loss:  4.346100831753574e-05
Batch  141  loss:  3.711682802531868e-05
Batch  151  loss:  5.00013557029888e-05
Batch  161  loss:  5.9652065829141065e-05
Batch  171  loss:  3.577545066946186e-05
Batch  181  loss:  3.8883998058736324e-05
Batch  191  loss:  3.0231578421080485e-05
Validation on real data: 
LOSS supervised-train 4.7859154237812616e-05, valid 2.498058893252164e-05
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  3.657693014247343e-05
Batch  11  loss:  4.4473672460298985e-05
Batch  21  loss:  3.830176865449175e-05
Batch  31  loss:  7.08931329427287e-05
Batch  41  loss:  6.398119876394048e-05
Batch  51  loss:  4.23110177507624e-05
Batch  61  loss:  4.3064181227236986e-05
Batch  71  loss:  4.362763138487935e-05
Batch  81  loss:  3.499228478176519e-05
Batch  91  loss:  3.922122050425969e-05
Batch  101  loss:  5.161087756277993e-05
Batch  111  loss:  5.037200389779173e-05
Batch  121  loss:  3.3749613066902384e-05
Batch  131  loss:  3.968603050452657e-05
Batch  141  loss:  4.1377206798642874e-05
Batch  151  loss:  4.816712680622004e-05
Batch  161  loss:  5.317923569236882e-05
Batch  171  loss:  3.399273191462271e-05
Batch  181  loss:  3.1788738851901144e-05
Batch  191  loss:  3.389084304217249e-05
Validation on real data: 
LOSS supervised-train 4.840030002924323e-05, valid 2.8418708097888157e-05
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  3.35891563736368e-05
Batch  11  loss:  3.81966310669668e-05
Batch  21  loss:  3.991075573139824e-05
Batch  31  loss:  5.316177703207359e-05
Batch  41  loss:  6.151904381113127e-05
Batch  51  loss:  3.122491398244165e-05
Batch  61  loss:  6.1068705690559e-05
Batch  71  loss:  5.206374771660194e-05
Batch  81  loss:  3.6191533581586555e-05
Batch  91  loss:  3.514508716762066e-05
Batch  101  loss:  4.536537380772643e-05
Batch  111  loss:  4.5961307478137314e-05
Batch  121  loss:  3.722964902408421e-05
Batch  131  loss:  5.026956932852045e-05
Batch  141  loss:  3.0795534257777035e-05
Batch  151  loss:  5.5709602747811005e-05
Batch  161  loss:  5.600967779173516e-05
Batch  171  loss:  3.353212741785683e-05
Batch  181  loss:  4.300683212932199e-05
Batch  191  loss:  4.424988219398074e-05
Validation on real data: 
LOSS supervised-train 4.8847778080016726e-05, valid 2.781074545055162e-05
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  3.25075197906699e-05
Batch  11  loss:  4.371016984805465e-05
Batch  21  loss:  3.338604074087925e-05
Batch  31  loss:  5.738242907682434e-05
Batch  41  loss:  6.84254482621327e-05
Batch  51  loss:  3.2779051252873614e-05
Batch  61  loss:  5.6974524341057986e-05
Batch  71  loss:  3.937954897992313e-05
Batch  81  loss:  3.284298873040825e-05
Batch  91  loss:  3.271263267379254e-05
Batch  101  loss:  4.052544682053849e-05
Batch  111  loss:  4.3332813220331445e-05
Batch  121  loss:  3.53495343006216e-05
Batch  131  loss:  3.740362808457576e-05
Batch  141  loss:  3.50911432178691e-05
Batch  151  loss:  5.3385039791464806e-05
Batch  161  loss:  6.0958293033763766e-05
Batch  171  loss:  2.8183003450976685e-05
Batch  181  loss:  4.0166360122384503e-05
Batch  191  loss:  3.3830958273028955e-05
Validation on real data: 
LOSS supervised-train 4.864897154220671e-05, valid 2.5480116164544597e-05
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  3.9295930037042126e-05
Batch  11  loss:  4.9633174057817087e-05
Batch  21  loss:  3.560502955224365e-05
Batch  31  loss:  6.695454067084938e-05
Batch  41  loss:  6.365082663251087e-05
Batch  51  loss:  3.5680415749084204e-05
Batch  61  loss:  3.821594873443246e-05
Batch  71  loss:  4.040701242047362e-05
Batch  81  loss:  3.2280528103001416e-05
Batch  91  loss:  3.5579992982093245e-05
Batch  101  loss:  4.7326488129328936e-05
Batch  111  loss:  3.854533861158416e-05
Batch  121  loss:  3.759786341106519e-05
Batch  131  loss:  4.787163561559282e-05
Batch  141  loss:  4.52500389656052e-05
Batch  151  loss:  5.942134521319531e-05
Batch  161  loss:  6.384242442436516e-05
Batch  171  loss:  3.294680573162623e-05
Batch  181  loss:  3.172180367982946e-05
Batch  191  loss:  4.63956494058948e-05
Validation on real data: 
LOSS supervised-train 4.7828189044594185e-05, valid 2.088009932776913e-05
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  3.6452122003538534e-05
Batch  11  loss:  4.519267531577498e-05
Batch  21  loss:  4.0871014789445326e-05
Batch  31  loss:  5.817347118863836e-05
Batch  41  loss:  6.276889325818047e-05
Batch  51  loss:  3.080148235312663e-05
Batch  61  loss:  5.531397619051859e-05
Batch  71  loss:  4.004486618214287e-05
Batch  81  loss:  3.328199090901762e-05
Batch  91  loss:  4.061975414515473e-05
Batch  101  loss:  4.45380610472057e-05
Batch  111  loss:  4.7813089622650295e-05
Batch  121  loss:  3.458127685007639e-05
Batch  131  loss:  3.191428550053388e-05
Batch  141  loss:  4.0062259358819574e-05
Batch  151  loss:  4.0628565329825506e-05
Batch  161  loss:  6.67118511046283e-05
Batch  171  loss:  2.929977017629426e-05
Batch  181  loss:  3.799831392825581e-05
Batch  191  loss:  3.2368603569921106e-05
Validation on real data: 
LOSS supervised-train 4.696793215771322e-05, valid 2.3005777620710433e-05
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  3.461010055616498e-05
Batch  11  loss:  3.9399568777298555e-05
Batch  21  loss:  3.1080191547516733e-05
Batch  31  loss:  6.793378270231187e-05
Batch  41  loss:  6.316735380096361e-05
Batch  51  loss:  3.020499752892647e-05
Batch  61  loss:  5.106930620968342e-05
Batch  71  loss:  4.815512875211425e-05
Batch  81  loss:  3.15627439704258e-05
Batch  91  loss:  3.580612610676326e-05
Batch  101  loss:  3.824825762421824e-05
Batch  111  loss:  4.522916424321011e-05
Batch  121  loss:  3.32854688167572e-05
Batch  131  loss:  3.4271826734766364e-05
Batch  141  loss:  3.476045458228327e-05
Batch  151  loss:  5.782743755844422e-05
Batch  161  loss:  4.985202758689411e-05
Batch  171  loss:  3.4182347008027136e-05
Batch  181  loss:  3.109641693299636e-05
Batch  191  loss:  3.339838804095052e-05
Validation on real data: 
LOSS supervised-train 4.5922074732516194e-05, valid 1.9805209376499988e-05
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  3.517403092700988e-05
Batch  11  loss:  3.868588464683853e-05
Batch  21  loss:  3.0398612580029294e-05
Batch  31  loss:  6.811458297306672e-05
Batch  41  loss:  6.26468172413297e-05
Batch  51  loss:  3.253924660384655e-05
Batch  61  loss:  4.387816443340853e-05
Batch  71  loss:  4.637772144633345e-05
Batch  81  loss:  3.954167914343998e-05
Batch  91  loss:  2.982538535434287e-05
Batch  101  loss:  4.3289346649544314e-05
Batch  111  loss:  4.256420288584195e-05
Batch  121  loss:  3.0724004318472e-05
Batch  131  loss:  2.8496719096438028e-05
Batch  141  loss:  3.909843508154154e-05
Batch  151  loss:  4.889377669314854e-05
Batch  161  loss:  5.163376044947654e-05
Batch  171  loss:  4.106098276679404e-05
Batch  181  loss:  3.793659925577231e-05
Batch  191  loss:  4.059319326188415e-05
Validation on real data: 
LOSS supervised-train 4.60657229541539e-05, valid 3.180780913680792e-05
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  4.38139250036329e-05
Batch  11  loss:  5.251850961940363e-05
Batch  21  loss:  3.894670953741297e-05
Batch  31  loss:  7.143655238905922e-05
Batch  41  loss:  5.909039100515656e-05
Batch  51  loss:  2.523467810533475e-05
Batch  61  loss:  3.500847742543556e-05
Batch  71  loss:  3.5457618650980294e-05
Batch  81  loss:  2.809979378071148e-05
Batch  91  loss:  3.931897663278505e-05
Batch  101  loss:  4.3708747398341075e-05
Batch  111  loss:  4.669060945161618e-05
Batch  121  loss:  3.2506242860108614e-05
Batch  131  loss:  3.321452459204011e-05
Batch  141  loss:  3.1776176911080256e-05
Batch  151  loss:  4.966582127963193e-05
Batch  161  loss:  6.057510108803399e-05
Batch  171  loss:  3.546960942912847e-05
Batch  181  loss:  3.096809086855501e-05
Batch  191  loss:  3.223958992748521e-05
Validation on real data: 
LOSS supervised-train 4.493466898566112e-05, valid 2.2388165234588087e-05
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  3.604070298024453e-05
Batch  11  loss:  4.167368388152681e-05
Batch  21  loss:  3.850749635603279e-05
Batch  31  loss:  4.781498500960879e-05
Batch  41  loss:  6.163372745504603e-05
Batch  51  loss:  3.415299215703271e-05
Batch  61  loss:  4.818262095795944e-05
Batch  71  loss:  3.3963016903726384e-05
Batch  81  loss:  4.01622528443113e-05
Batch  91  loss:  3.394960003788583e-05
Batch  101  loss:  3.667156852316111e-05
Batch  111  loss:  4.767881182488054e-05
Batch  121  loss:  2.7196934752282687e-05
Batch  131  loss:  3.429520802455954e-05
Batch  141  loss:  2.760806819424033e-05
Batch  151  loss:  3.9487120375270024e-05
Batch  161  loss:  4.987126885680482e-05
Batch  171  loss:  2.7860285626957193e-05
Batch  181  loss:  3.6442990676732734e-05
Batch  191  loss:  3.657894558273256e-05
Validation on real data: 
LOSS supervised-train 4.3452922573123945e-05, valid 3.0427509045694023e-05
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  3.168250259477645e-05
Batch  11  loss:  3.157616447424516e-05
Batch  21  loss:  4.595705104293302e-05
Batch  31  loss:  7.072221342241392e-05
Batch  41  loss:  5.283344580675475e-05
Batch  51  loss:  2.5453358830418438e-05
Batch  61  loss:  4.438969335751608e-05
Batch  71  loss:  5.064193464932032e-05
Batch  81  loss:  3.8252142985584214e-05
Batch  91  loss:  3.883072713506408e-05
Batch  101  loss:  3.0629213142674416e-05
Batch  111  loss:  3.814408046309836e-05
Batch  121  loss:  3.733279663720168e-05
Batch  131  loss:  4.0085873479256406e-05
Batch  141  loss:  3.6489149351837113e-05
Batch  151  loss:  4.817873195861466e-05
Batch  161  loss:  5.9387413784861565e-05
Batch  171  loss:  4.0595892642159015e-05
Batch  181  loss:  3.61487218469847e-05
Batch  191  loss:  3.495628334349021e-05
Validation on real data: 
LOSS supervised-train 4.5003318091403345e-05, valid 2.078940815408714e-05
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  3.567755629774183e-05
Batch  11  loss:  4.695995448855683e-05
Batch  21  loss:  3.221705628675409e-05
Batch  31  loss:  4.505548349698074e-05
Batch  41  loss:  4.264362360117957e-05
Batch  51  loss:  3.310582542326301e-05
Batch  61  loss:  4.257457112544216e-05
Batch  71  loss:  3.958279557991773e-05
Batch  81  loss:  3.8584632420679554e-05
Batch  91  loss:  3.603236109483987e-05
Batch  101  loss:  4.1934617911465466e-05
Batch  111  loss:  3.660387301351875e-05
Batch  121  loss:  4.18387062381953e-05
Batch  131  loss:  3.539193858159706e-05
Batch  141  loss:  3.2000301871448755e-05
Batch  151  loss:  5.428638178273104e-05
Batch  161  loss:  6.487589416792616e-05
Batch  171  loss:  3.193411976099014e-05
Batch  181  loss:  3.6087756598135456e-05
Batch  191  loss:  4.11916189477779e-05
Validation on real data: 
LOSS supervised-train 4.3401622160672556e-05, valid 1.987874202313833e-05
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  3.726757859112695e-05
Batch  11  loss:  4.291809455025941e-05
Batch  21  loss:  3.590615961002186e-05
Batch  31  loss:  4.92117069370579e-05
Batch  41  loss:  5.2868432248942554e-05
Batch  51  loss:  3.0399984098039567e-05
Batch  61  loss:  4.35713300248608e-05
Batch  71  loss:  3.836139876511879e-05
Batch  81  loss:  2.8197779101901688e-05
Batch  91  loss:  2.776809924398549e-05
Batch  101  loss:  4.207861638860777e-05
Batch  111  loss:  3.540018587955274e-05
Batch  121  loss:  3.401955109438859e-05
Batch  131  loss:  3.463296525296755e-05
Batch  141  loss:  2.9994742362760007e-05
Batch  151  loss:  5.316184615367092e-05
Batch  161  loss:  6.341638072626665e-05
Batch  171  loss:  3.654006286524236e-05
Batch  181  loss:  3.479828228591941e-05
Batch  191  loss:  3.11930307361763e-05
Validation on real data: 
LOSS supervised-train 4.296705462365935e-05, valid 2.963546103273984e-05
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  3.858945638057776e-05
Batch  11  loss:  4.796296707354486e-05
Batch  21  loss:  3.0222641726140864e-05
Batch  31  loss:  4.5882276026532054e-05
Batch  41  loss:  4.993836773792282e-05
Batch  51  loss:  2.7774924092227593e-05
Batch  61  loss:  3.988829848822206e-05
Batch  71  loss:  4.2828687583096325e-05
Batch  81  loss:  2.9070664822938852e-05
Batch  91  loss:  2.7775269700214267e-05
Batch  101  loss:  4.670171983889304e-05
Batch  111  loss:  4.762060780194588e-05
Batch  121  loss:  3.532940536388196e-05
Batch  131  loss:  3.421897781663574e-05
Batch  141  loss:  4.198068563709967e-05
Batch  151  loss:  5.3048723202664405e-05
Batch  161  loss:  5.8249555877409875e-05
Batch  171  loss:  2.7105314075015485e-05
Batch  181  loss:  4.380075915833004e-05
Batch  191  loss:  3.108238888671622e-05
Validation on real data: 
LOSS supervised-train 4.227013956551673e-05, valid 2.4306677005370148e-05
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  2.9208407795522362e-05
Batch  11  loss:  3.7825971958227456e-05
Batch  21  loss:  3.163766086800024e-05
Batch  31  loss:  5.536037861020304e-05
Batch  41  loss:  4.3817912228405476e-05
Batch  51  loss:  3.926221688743681e-05
Batch  61  loss:  3.518441008054651e-05
Batch  71  loss:  3.8313737604767084e-05
Batch  81  loss:  3.132053461740725e-05
Batch  91  loss:  2.5272955099353567e-05
Batch  101  loss:  4.2221839976264164e-05
Batch  111  loss:  3.855605609714985e-05
Batch  121  loss:  3.6092322261538357e-05
Batch  131  loss:  3.334891880513169e-05
Batch  141  loss:  3.113528146059252e-05
Batch  151  loss:  4.325167537899688e-05
Batch  161  loss:  4.544505645753816e-05
Batch  171  loss:  2.7244503144174814e-05
Batch  181  loss:  3.494002885418013e-05
Batch  191  loss:  3.311219552415423e-05
Validation on real data: 
LOSS supervised-train 4.169617470324738e-05, valid 2.4988690711325034e-05
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  3.126587034785189e-05
Batch  11  loss:  3.630560604506172e-05
Batch  21  loss:  3.290849053882994e-05
Batch  31  loss:  4.165050995652564e-05
Batch  41  loss:  5.509036418516189e-05
Batch  51  loss:  2.8178037609905005e-05
Batch  61  loss:  3.412470323382877e-05
Batch  71  loss:  3.81641075364314e-05
Batch  81  loss:  2.7629648684524e-05
Batch  91  loss:  2.6153020371566527e-05
Batch  101  loss:  3.487143840175122e-05
Batch  111  loss:  4.9476151616545394e-05
Batch  121  loss:  2.8641205062740482e-05
Batch  131  loss:  3.737920633284375e-05
Batch  141  loss:  3.302167897345498e-05
Batch  151  loss:  3.919541632058099e-05
Batch  161  loss:  5.083588257548399e-05
Batch  171  loss:  3.3120082662208006e-05
Batch  181  loss:  3.578532414394431e-05
Batch  191  loss:  3.0314549803733826e-05
Validation on real data: 
LOSS supervised-train 4.2205607242067346e-05, valid 3.1830641091801226e-05
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  3.356088564032689e-05
Batch  11  loss:  4.0251132304547355e-05
Batch  21  loss:  3.409545752219856e-05
Batch  31  loss:  6.238099012989551e-05
Batch  41  loss:  5.544094165088609e-05
Batch  51  loss:  2.9103461201884784e-05
Batch  61  loss:  4.485014142119326e-05
Batch  71  loss:  2.944172047136817e-05
Batch  81  loss:  2.463305281708017e-05
Batch  91  loss:  2.7525707992026582e-05
Batch  101  loss:  3.8723592297174037e-05
Batch  111  loss:  4.164753408986144e-05
Batch  121  loss:  2.194251646869816e-05
Batch  131  loss:  3.1455023417947814e-05
Batch  141  loss:  3.1081101042218506e-05
Batch  151  loss:  4.700151475844905e-05
Batch  161  loss:  5.3395058785099536e-05
Batch  171  loss:  3.9525584725197405e-05
Batch  181  loss:  2.943131039501168e-05
Batch  191  loss:  3.496866702334955e-05
Validation on real data: 
LOSS supervised-train 4.152192715991987e-05, valid 2.144626523659099e-05
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  3.7278678064467385e-05
Batch  11  loss:  4.6932331315474585e-05
Batch  21  loss:  3.4487755328882486e-05
Batch  31  loss:  4.437768438947387e-05
Batch  41  loss:  4.908203845843673e-05
Batch  51  loss:  2.8603371902136132e-05
Batch  61  loss:  3.7766931200167164e-05
Batch  71  loss:  3.1753788789501414e-05
Batch  81  loss:  3.444255344220437e-05
Batch  91  loss:  3.102490518358536e-05
Batch  101  loss:  4.969623114448041e-05
Batch  111  loss:  4.137022187933326e-05
Batch  121  loss:  3.3774529583752155e-05
Batch  131  loss:  4.018061372335069e-05
Batch  141  loss:  2.1106592612341046e-05
Batch  151  loss:  4.9508998927194625e-05
Batch  161  loss:  4.218145113554783e-05
Batch  171  loss:  2.5161625671898946e-05
Batch  181  loss:  4.266366158844903e-05
Batch  191  loss:  3.732626646524295e-05
Validation on real data: 
LOSS supervised-train 4.107021658455778e-05, valid 2.268059688503854e-05
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  3.628955164458603e-05
Batch  11  loss:  3.721924076671712e-05
Batch  21  loss:  4.4264284952078015e-05
Batch  31  loss:  6.015455073793419e-05
Batch  41  loss:  4.964220352121629e-05
Batch  51  loss:  2.537863292673137e-05
Batch  61  loss:  4.9807000323198736e-05
Batch  71  loss:  3.843591912300326e-05
Batch  81  loss:  3.142468267469667e-05
Batch  91  loss:  2.6350629923399538e-05
Batch  101  loss:  3.506294888211414e-05
Batch  111  loss:  3.56139826180879e-05
Batch  121  loss:  3.3396794606233016e-05
Batch  131  loss:  3.543124330462888e-05
Batch  141  loss:  3.614351589931175e-05
Batch  151  loss:  4.496289693634026e-05
Batch  161  loss:  4.7958121285773814e-05
Batch  171  loss:  3.157421815558337e-05
Batch  181  loss:  3.5141889384249225e-05
Batch  191  loss:  3.103741255472414e-05
Validation on real data: 
LOSS supervised-train 4.0865201990527565e-05, valid 2.6904679543804377e-05
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  3.161629501846619e-05
Batch  11  loss:  3.513362389639951e-05
Batch  21  loss:  3.2574476790614426e-05
Batch  31  loss:  4.4635653466684744e-05
Batch  41  loss:  4.317339335102588e-05
Batch  51  loss:  2.777964255074039e-05
Batch  61  loss:  3.339290924486704e-05
Batch  71  loss:  2.586901064205449e-05
Batch  81  loss:  2.9101902327965945e-05
Batch  91  loss:  2.297723403898999e-05
Batch  101  loss:  3.504607593640685e-05
Batch  111  loss:  4.45393925474491e-05
Batch  121  loss:  2.4986331482068636e-05
Batch  131  loss:  5.360614522942342e-05
Batch  141  loss:  3.0139246518956497e-05
Batch  151  loss:  3.7280180549714714e-05
Batch  161  loss:  5.3679439588449895e-05
Batch  171  loss:  2.9314402127056383e-05
Batch  181  loss:  2.7750669687520713e-05
Batch  191  loss:  3.45649168593809e-05
Validation on real data: 
LOSS supervised-train 3.9477369746236946e-05, valid 2.5147164706140757e-05
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  3.063738768105395e-05
Batch  11  loss:  3.307150473119691e-05
Batch  21  loss:  2.9779906981275417e-05
Batch  31  loss:  5.428835720522329e-05
Batch  41  loss:  5.454810525407083e-05
Batch  51  loss:  2.6491799872019328e-05
Batch  61  loss:  4.295926555641927e-05
Batch  71  loss:  4.1340616007801145e-05
Batch  81  loss:  3.2486714189872146e-05
Batch  91  loss:  3.172399738105014e-05
Batch  101  loss:  4.165470818406902e-05
Batch  111  loss:  4.353605254436843e-05
Batch  121  loss:  2.3921187676023692e-05
Batch  131  loss:  4.019468906335533e-05
Batch  141  loss:  2.9754821298411116e-05
Batch  151  loss:  4.8156412958633155e-05
Batch  161  loss:  5.187861461308785e-05
Batch  171  loss:  3.2467614801134914e-05
Batch  181  loss:  2.874408346542623e-05
Batch  191  loss:  3.068226942559704e-05
Validation on real data: 
LOSS supervised-train 3.951183159188077e-05, valid 2.3000869987299666e-05
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  4.0152648580260575e-05
Batch  11  loss:  4.696666292147711e-05
Batch  21  loss:  3.53646682924591e-05
Batch  31  loss:  4.4679021812044084e-05
Batch  41  loss:  5.06664982822258e-05
Batch  51  loss:  2.8359158022794873e-05
Batch  61  loss:  3.709658994921483e-05
Batch  71  loss:  3.307158840470947e-05
Batch  81  loss:  3.4712116757873446e-05
Batch  91  loss:  3.158271647407673e-05
Batch  101  loss:  3.693282269523479e-05
Batch  111  loss:  3.688992728712037e-05
Batch  121  loss:  2.567985029600095e-05
Batch  131  loss:  3.804207517532632e-05
Batch  141  loss:  2.4649680199217983e-05
Batch  151  loss:  4.430080662132241e-05
Batch  161  loss:  4.692429502028972e-05
Batch  171  loss:  3.016488153662067e-05
Batch  181  loss:  3.464236215222627e-05
Batch  191  loss:  3.395996100152843e-05
Validation on real data: 
LOSS supervised-train 3.986886622442398e-05, valid 2.1328094589989632e-05
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  3.497635043459013e-05
Batch  11  loss:  3.222237137379125e-05
Batch  21  loss:  3.705132257891819e-05
Batch  31  loss:  5.990794670651667e-05
Batch  41  loss:  4.8508591135032475e-05
Batch  51  loss:  2.7106867491966113e-05
Batch  61  loss:  4.024610825581476e-05
Batch  71  loss:  3.0412524210987613e-05
Batch  81  loss:  3.6510162317426875e-05
Batch  91  loss:  3.391938662389293e-05
Batch  101  loss:  4.0171900764107704e-05
Batch  111  loss:  3.670268051791936e-05
Batch  121  loss:  3.1796182156540453e-05
Batch  131  loss:  2.6506542781135067e-05
Batch  141  loss:  2.8125672542955726e-05
Batch  151  loss:  4.439907934283838e-05
Batch  161  loss:  4.638452082872391e-05
Batch  171  loss:  3.606047175708227e-05
Batch  181  loss:  3.439055581111461e-05
Batch  191  loss:  2.551844227127731e-05
Validation on real data: 
LOSS supervised-train 3.941331615351373e-05, valid 2.1678806660929695e-05
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  3.248987559345551e-05
Batch  11  loss:  4.54188120784238e-05
Batch  21  loss:  4.014892328996211e-05
Batch  31  loss:  4.8183956096181646e-05
Batch  41  loss:  4.9005335313268006e-05
Batch  51  loss:  2.638731530169025e-05
Batch  61  loss:  2.9183333026594482e-05
Batch  71  loss:  3.639522037701681e-05
Batch  81  loss:  2.6707242795964703e-05
Batch  91  loss:  3.252436363254674e-05
Batch  101  loss:  2.7906024115509354e-05
Batch  111  loss:  4.746700869873166e-05
Batch  121  loss:  3.061093229916878e-05
Batch  131  loss:  3.0121651434455998e-05
Batch  141  loss:  3.227765773772262e-05
Batch  151  loss:  3.305560312583111e-05
Batch  161  loss:  3.758230741368607e-05
Batch  171  loss:  2.8139142159488983e-05
Batch  181  loss:  3.5344433854334056e-05
Batch  191  loss:  3.07773043459747e-05
Validation on real data: 
LOSS supervised-train 3.886946592501772e-05, valid 2.3247355784405954e-05
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  3.597085378714837e-05
Batch  11  loss:  3.473992910585366e-05
Batch  21  loss:  3.0047944164834917e-05
Batch  31  loss:  4.3400748836575076e-05
Batch  41  loss:  3.9485355955548584e-05
Batch  51  loss:  2.9898283173679374e-05
Batch  61  loss:  3.068238220294006e-05
Batch  71  loss:  3.9174479752546176e-05
Batch  81  loss:  3.581312921596691e-05
Batch  91  loss:  2.9461923986673355e-05
Batch  101  loss:  3.3455886295996606e-05
Batch  111  loss:  4.60170631413348e-05
Batch  121  loss:  4.013034049421549e-05
Batch  131  loss:  2.4932764063123614e-05
Batch  141  loss:  2.4131972168106586e-05
Batch  151  loss:  4.9943122576223686e-05
Batch  161  loss:  4.634617653209716e-05
Batch  171  loss:  2.6725911084213294e-05
Batch  181  loss:  2.735451744229067e-05
Batch  191  loss:  3.119638131465763e-05
Validation on real data: 
LOSS supervised-train 3.916830831258267e-05, valid 2.6455258193891495e-05
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  2.7538528229342774e-05
Batch  11  loss:  3.57835742761381e-05
Batch  21  loss:  3.0704257369507104e-05
Batch  31  loss:  4.415467265062034e-05
Batch  41  loss:  6.0438633227022365e-05
Batch  51  loss:  2.58339787251316e-05
Batch  61  loss:  3.511192335281521e-05
Batch  71  loss:  3.9672777347732335e-05
Batch  81  loss:  2.292784847668372e-05
Batch  91  loss:  3.0175011488609016e-05
Batch  101  loss:  3.5440691135590896e-05
Batch  111  loss:  4.112276656087488e-05
Batch  121  loss:  2.6883881218964234e-05
Batch  131  loss:  3.6291694414103404e-05
Batch  141  loss:  2.8355767426546663e-05
Batch  151  loss:  3.720875974977389e-05
Batch  161  loss:  4.2588115320540965e-05
Batch  171  loss:  3.4290314943064004e-05
Batch  181  loss:  2.8710517653962597e-05
Batch  191  loss:  2.9331347832339816e-05
Validation on real data: 
LOSS supervised-train 3.7486245364561907e-05, valid 2.7179052267456427e-05
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  3.0441771741607226e-05
Batch  11  loss:  4.48919199698139e-05
Batch  21  loss:  3.693264443427324e-05
Batch  31  loss:  4.604543573805131e-05
Batch  41  loss:  4.141209501540288e-05
Batch  51  loss:  2.702091478568036e-05
Batch  61  loss:  4.225294105708599e-05
Batch  71  loss:  2.9887207347201183e-05
Batch  81  loss:  3.048429971386213e-05
Batch  91  loss:  3.627856494858861e-05
Batch  101  loss:  4.176167567493394e-05
Batch  111  loss:  3.7487516237888485e-05
Batch  121  loss:  2.383004357398022e-05
Batch  131  loss:  3.202336301910691e-05
Batch  141  loss:  3.062799441977404e-05
Batch  151  loss:  4.819450623472221e-05
Batch  161  loss:  4.499254282563925e-05
Batch  171  loss:  2.578263774921652e-05
Batch  181  loss:  2.8966647732886486e-05
Batch  191  loss:  2.8669810490100645e-05
Validation on real data: 
LOSS supervised-train 3.821257891104324e-05, valid 2.192759347963147e-05
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  2.798990135488566e-05
Batch  11  loss:  2.630164090078324e-05
Batch  21  loss:  2.7964084438281134e-05
Batch  31  loss:  4.908652772428468e-05
Batch  41  loss:  3.393222868908197e-05
Batch  51  loss:  2.6414629246573895e-05
Batch  61  loss:  3.3653443097136915e-05
Batch  71  loss:  2.939635669463314e-05
Batch  81  loss:  2.3407665139529854e-05
Batch  91  loss:  3.3594125852687284e-05
Batch  101  loss:  3.432619269005954e-05
Batch  111  loss:  4.3725947762141004e-05
Batch  121  loss:  2.9017679480602965e-05
Batch  131  loss:  3.364901931490749e-05
Batch  141  loss:  3.046861274924595e-05
Batch  151  loss:  4.655700467992574e-05
Batch  161  loss:  5.629126826534048e-05
Batch  171  loss:  2.8950236810487695e-05
Batch  181  loss:  3.239405850763433e-05
Batch  191  loss:  3.2013413147069514e-05
Validation on real data: 
LOSS supervised-train 3.729004669366986e-05, valid 2.1679028577636927e-05
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  3.116819789283909e-05
Batch  11  loss:  3.363575160619803e-05
Batch  21  loss:  3.381962960702367e-05
Batch  31  loss:  3.689770892378874e-05
Batch  41  loss:  3.9073973312042654e-05
Batch  51  loss:  2.933083123934921e-05
Batch  61  loss:  3.942945841117762e-05
Batch  71  loss:  3.677527274703607e-05
Batch  81  loss:  2.716433118621353e-05
Batch  91  loss:  3.1206705898512155e-05
Batch  101  loss:  3.504733831505291e-05
Batch  111  loss:  4.0272523619933054e-05
Batch  121  loss:  2.8382424716255628e-05
Batch  131  loss:  2.471012885507662e-05
Batch  141  loss:  2.8333501177257858e-05
Batch  151  loss:  4.396000804263167e-05
Batch  161  loss:  4.887406976195052e-05
Batch  171  loss:  3.611289139371365e-05
Batch  181  loss:  4.116487252758816e-05
Batch  191  loss:  2.2870191969559528e-05
Validation on real data: 
LOSS supervised-train 3.792150850131293e-05, valid 1.8660008208826184e-05
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  laptop ; Model ID: 519e98268bee56dddbb1de10c9529bf7
--------------------
Training baseline regression model:  2022-03-30 06:20:30.913698
Detector:  point_transformer
Object:  laptop
--------------------
device is  cuda
--------------------
Number of trainable parameters:  888466
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.24224521219730377
Batch  11  loss:  0.0659404769539833
Batch  21  loss:  0.02696339040994644
Batch  31  loss:  0.031249141320586205
Batch  41  loss:  0.06944204121828079
Batch  51  loss:  0.02375500649213791
Batch  61  loss:  0.022461775690317154
Batch  71  loss:  0.006001263856887817
Batch  81  loss:  0.014020785689353943
Batch  91  loss:  0.004500300623476505
Batch  101  loss:  0.006914434488862753
Batch  111  loss:  0.006119645666331053
Batch  121  loss:  0.0029846723191440105
Batch  131  loss:  0.006308975163847208
Batch  141  loss:  0.0016318232519552112
Batch  151  loss:  0.005316585768014193
Batch  161  loss:  0.0027040312997996807
Batch  171  loss:  0.007486195303499699
Batch  181  loss:  0.00659114308655262
Batch  191  loss:  0.004068342503160238
Validation on real data: 
LOSS supervised-train 0.019715141408960336, valid 0.0019918852485716343
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.006428095977753401
Batch  11  loss:  0.0021460270509123802
Batch  21  loss:  0.002630482194945216
Batch  31  loss:  0.0025892548728734255
Batch  41  loss:  0.005804026965051889
Batch  51  loss:  0.002059437334537506
Batch  61  loss:  0.004363649524748325
Batch  71  loss:  0.001599171431735158
Batch  81  loss:  0.0026332347188144922
Batch  91  loss:  0.0015794740756973624
Batch  101  loss:  0.001880944473668933
Batch  111  loss:  0.0010781594319269061
Batch  121  loss:  0.002159160329028964
Batch  131  loss:  0.0020314445719122887
Batch  141  loss:  0.0011421643430367112
Batch  151  loss:  0.004292013123631477
Batch  161  loss:  0.0017362561775371432
Batch  171  loss:  0.0033097369596362114
Batch  181  loss:  0.0025710444897413254
Batch  191  loss:  0.0024096001870930195
Validation on real data: 
LOSS supervised-train 0.0023583463209797626, valid 0.0014655820559710264
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0032969084568321705
Batch  11  loss:  0.0016118892235681415
Batch  21  loss:  0.0016993271419778466
Batch  31  loss:  0.0014909256715327501
Batch  41  loss:  0.004332349169999361
Batch  51  loss:  0.0013237394159659743
Batch  61  loss:  0.002932809293270111
Batch  71  loss:  0.0014155451208353043
Batch  81  loss:  0.002075126627460122
Batch  91  loss:  0.0009428022312931716
Batch  101  loss:  0.001563193160109222
Batch  111  loss:  0.000786800985224545
Batch  121  loss:  0.0014638513093814254
Batch  131  loss:  0.0010771373054012656
Batch  141  loss:  0.0015066938940435648
Batch  151  loss:  0.0027908331248909235
Batch  161  loss:  0.001391369616612792
Batch  171  loss:  0.0027545380871742964
Batch  181  loss:  0.001832545385695994
Batch  191  loss:  0.001936390995979309
Validation on real data: 
LOSS supervised-train 0.0015758614166406914, valid 0.0009381662239320576
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0023525652941316366
Batch  11  loss:  0.001722624059766531
Batch  21  loss:  0.0013007201487198472
Batch  31  loss:  0.0010996042983606458
Batch  41  loss:  0.003137510269880295
Batch  51  loss:  0.0013123112730681896
Batch  61  loss:  0.0016725137829780579
Batch  71  loss:  0.0014017523499205709
Batch  81  loss:  0.001403755391947925
Batch  91  loss:  0.0010244563454762101
Batch  101  loss:  0.0011190951336175203
Batch  111  loss:  0.0008395020267926157
Batch  121  loss:  0.0012557023437693715
Batch  131  loss:  0.0009615685557946563
Batch  141  loss:  0.0009493720135651529
Batch  151  loss:  0.0025488226674497128
Batch  161  loss:  0.0009424032177776098
Batch  171  loss:  0.001504074432887137
Batch  181  loss:  0.0009568111272528768
Batch  191  loss:  0.0015176663873717189
Validation on real data: 
LOSS supervised-train 0.001265959035954438, valid 0.000740249000955373
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0018398985266685486
Batch  11  loss:  0.0010983789106830955
Batch  21  loss:  0.000906033325009048
Batch  31  loss:  0.0009066474158316851
Batch  41  loss:  0.002635170705616474
Batch  51  loss:  0.0010569898877292871
Batch  61  loss:  0.0014317409368231893
Batch  71  loss:  0.0011575646931305528
Batch  81  loss:  0.0011162001173943281
Batch  91  loss:  0.001007809885777533
Batch  101  loss:  0.0007398799061775208
Batch  111  loss:  0.0005901834811083972
Batch  121  loss:  0.0008907510200515389
Batch  131  loss:  0.0007275404059328139
Batch  141  loss:  0.0008244498167186975
Batch  151  loss:  0.0020617081318050623
Batch  161  loss:  0.0007741366280242801
Batch  171  loss:  0.0013346817577257752
Batch  181  loss:  0.0008517075330018997
Batch  191  loss:  0.0014780861092731357
Validation on real data: 
LOSS supervised-train 0.001028293078124989, valid 0.0006815755041316152
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.001477004843764007
Batch  11  loss:  0.0009393004584126174
Batch  21  loss:  0.0006968703819438815
Batch  31  loss:  0.0006135141593404114
Batch  41  loss:  0.0023079996462911367
Batch  51  loss:  0.0008290421101264656
Batch  61  loss:  0.0009967468213289976
Batch  71  loss:  0.0011517159873619676
Batch  81  loss:  0.0008999697165563703
Batch  91  loss:  0.0005903648561798036
Batch  101  loss:  0.0007857967866584659
Batch  111  loss:  0.0004321075975894928
Batch  121  loss:  0.0009885862236842513
Batch  131  loss:  0.0005375277251005173
Batch  141  loss:  0.0009864643216133118
Batch  151  loss:  0.0015682026278227568
Batch  161  loss:  0.0008538554538972676
Batch  171  loss:  0.0009828958427533507
Batch  181  loss:  0.0006794403307139874
Batch  191  loss:  0.0014583369484171271
Validation on real data: 
LOSS supervised-train 0.0008674585535482038, valid 0.000635618285741657
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0013234582729637623
Batch  11  loss:  0.0009017112897709012
Batch  21  loss:  0.0007682884461246431
Batch  31  loss:  0.0007278704433701932
Batch  41  loss:  0.002536233514547348
Batch  51  loss:  0.0008979610283859074
Batch  61  loss:  0.0009673336753621697
Batch  71  loss:  0.0009254488977603614
Batch  81  loss:  0.0007466556853614748
Batch  91  loss:  0.0006484234472736716
Batch  101  loss:  0.000633755000308156
Batch  111  loss:  0.0005447705625556409
Batch  121  loss:  0.0008312486461363733
Batch  131  loss:  0.000797175569459796
Batch  141  loss:  0.0009804231813177466
Batch  151  loss:  0.0013319537974894047
Batch  161  loss:  0.0008007059805095196
Batch  171  loss:  0.0008798872004263103
Batch  181  loss:  0.0007304732571355999
Batch  191  loss:  0.0009399897535331547
Validation on real data: 
LOSS supervised-train 0.0007818384672282264, valid 0.0006170497508719563
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0013826967915520072
Batch  11  loss:  0.0008193106041289866
Batch  21  loss:  0.000906770525034517
Batch  31  loss:  0.000751221610698849
Batch  41  loss:  0.0020153173245489597
Batch  51  loss:  0.0005985881434753537
Batch  61  loss:  0.0009240734507329762
Batch  71  loss:  0.0008526870515197515
Batch  81  loss:  0.0009555096039548516
Batch  91  loss:  0.0004989464650861919
Batch  101  loss:  0.0006169855478219688
Batch  111  loss:  0.0004364900232758373
Batch  121  loss:  0.0006622429937124252
Batch  131  loss:  0.0005343855591490865
Batch  141  loss:  0.0006952330586500466
Batch  151  loss:  0.001178645878098905
Batch  161  loss:  0.0006574603612534702
Batch  171  loss:  0.0011582854203879833
Batch  181  loss:  0.0005540892598219216
Batch  191  loss:  0.0011552409268915653
Validation on real data: 
LOSS supervised-train 0.0007252108464308549, valid 0.0005807968555018306
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0011075976071879268
Batch  11  loss:  0.0007939611678011715
Batch  21  loss:  0.0006566083175130188
Batch  31  loss:  0.0006336199585348368
Batch  41  loss:  0.0020751096308231354
Batch  51  loss:  0.000608366506639868
Batch  61  loss:  0.0007755759288556874
Batch  71  loss:  0.0008184162434190512
Batch  81  loss:  0.00047258197446353734
Batch  91  loss:  0.0005322298384271562
Batch  101  loss:  0.0006110440590418875
Batch  111  loss:  0.00045944564044475555
Batch  121  loss:  0.0005467720329761505
Batch  131  loss:  0.00046044751070439816
Batch  141  loss:  0.0005968579207547009
Batch  151  loss:  0.0014481943799182773
Batch  161  loss:  0.0005591310909949243
Batch  171  loss:  0.0008243782795034349
Batch  181  loss:  0.00044806094956584275
Batch  191  loss:  0.0009856667602434754
Validation on real data: 
LOSS supervised-train 0.0006607376909232699, valid 0.0004999628872610629
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0011423557298257947
Batch  11  loss:  0.0006747095030732453
Batch  21  loss:  0.0006781754200346768
Batch  31  loss:  0.0004915688186883926
Batch  41  loss:  0.001636392087675631
Batch  51  loss:  0.000536593608558178
Batch  61  loss:  0.0007167974836193025
Batch  71  loss:  0.0007920300704427063
Batch  81  loss:  0.0005978470435366035
Batch  91  loss:  0.00047903298400342464
Batch  101  loss:  0.0004824618808925152
Batch  111  loss:  0.0004119312798138708
Batch  121  loss:  0.0006115902215242386
Batch  131  loss:  0.00044089576113037765
Batch  141  loss:  0.0006472283275797963
Batch  151  loss:  0.0011516875820234418
Batch  161  loss:  0.0004476738686207682
Batch  171  loss:  0.0007493242737837136
Batch  181  loss:  0.0004253751249052584
Batch  191  loss:  0.0008605202892795205
Validation on real data: 
LOSS supervised-train 0.0006058158593077678, valid 0.0004916568868793547
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0009242554660886526
Batch  11  loss:  0.0006289091543294489
Batch  21  loss:  0.0006188586703501642
Batch  31  loss:  0.0004918587510474026
Batch  41  loss:  0.001510864356532693
Batch  51  loss:  0.0006858286214992404
Batch  61  loss:  0.0006957697914913297
Batch  71  loss:  0.0007552186725661159
Batch  81  loss:  0.00038715152186341584
Batch  91  loss:  0.0005569006316363811
Batch  101  loss:  0.0005157283158041537
Batch  111  loss:  0.0003557181335054338
Batch  121  loss:  0.0006011907826177776
Batch  131  loss:  0.000493550265673548
Batch  141  loss:  0.0005757536855526268
Batch  151  loss:  0.001041145296767354
Batch  161  loss:  0.00046993745490908623
Batch  171  loss:  0.0005888568703085184
Batch  181  loss:  0.0002388625143794343
Batch  191  loss:  0.0009339266107417643
Validation on real data: 
LOSS supervised-train 0.0005739502586220624, valid 0.00039652222767472267
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.00093793123960495
Batch  11  loss:  0.0005994043895043433
Batch  21  loss:  0.0005518557736650109
Batch  31  loss:  0.0004827840602956712
Batch  41  loss:  0.0015818612882867455
Batch  51  loss:  0.0005945689626969397
Batch  61  loss:  0.0006468095234595239
Batch  71  loss:  0.0007707517361268401
Batch  81  loss:  0.0004872516728937626
Batch  91  loss:  0.0003656447515822947
Batch  101  loss:  0.000493875180836767
Batch  111  loss:  0.000391777022741735
Batch  121  loss:  0.0006814523367211223
Batch  131  loss:  0.00037391536170616746
Batch  141  loss:  0.0005389485158957541
Batch  151  loss:  0.00102139450609684
Batch  161  loss:  0.0005395059706643224
Batch  171  loss:  0.000606608809903264
Batch  181  loss:  0.0003864192985929549
Batch  191  loss:  0.0006572201964445412
Validation on real data: 
LOSS supervised-train 0.0005436042790825013, valid 0.0004036895406898111
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0006959360907785594
Batch  11  loss:  0.0004719937569461763
Batch  21  loss:  0.0004723876481875777
Batch  31  loss:  0.00040734908543527126
Batch  41  loss:  0.0010910698911175132
Batch  51  loss:  0.0006218379712663591
Batch  61  loss:  0.0006434234092012048
Batch  71  loss:  0.0007593066547997296
Batch  81  loss:  0.00039833562914282084
Batch  91  loss:  0.00038143605343066156
Batch  101  loss:  0.000517115811817348
Batch  111  loss:  0.00037145588430576026
Batch  121  loss:  0.0004320213047321886
Batch  131  loss:  0.00033796916250139475
Batch  141  loss:  0.00047967684804461896
Batch  151  loss:  0.0008989877533167601
Batch  161  loss:  0.0003352648054715246
Batch  171  loss:  0.0006155857117846608
Batch  181  loss:  0.0002895407087635249
Batch  191  loss:  0.0005436335923150182
Validation on real data: 
LOSS supervised-train 0.0005054675936844433, valid 0.0005302950739860535
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0006306633003987372
Batch  11  loss:  0.0006624404340982437
Batch  21  loss:  0.0004328346985857934
Batch  31  loss:  0.0003684697730932385
Batch  41  loss:  0.001222046441398561
Batch  51  loss:  0.0003726677969098091
Batch  61  loss:  0.00048059283290058374
Batch  71  loss:  0.0006806563469581306
Batch  81  loss:  0.0004618530219886452
Batch  91  loss:  0.0004836809530388564
Batch  101  loss:  0.0005486028385348618
Batch  111  loss:  0.00027294663595966995
Batch  121  loss:  0.00038940226659178734
Batch  131  loss:  0.00040312259807251394
Batch  141  loss:  0.0004194819775875658
Batch  151  loss:  0.0009045457118190825
Batch  161  loss:  0.00036160348099656403
Batch  171  loss:  0.0006105006905272603
Batch  181  loss:  0.0003062907780986279
Batch  191  loss:  0.0006675986223854125
Validation on real data: 
LOSS supervised-train 0.00046616633197118064, valid 0.0004239467089064419
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.00073719717329368
Batch  11  loss:  0.0005771569558419287
Batch  21  loss:  0.00035316930734552443
Batch  31  loss:  0.00038832431891933084
Batch  41  loss:  0.0010562322568148375
Batch  51  loss:  0.0005590672953985631
Batch  61  loss:  0.0004538772045634687
Batch  71  loss:  0.0004737093113362789
Batch  81  loss:  0.0005782939260825515
Batch  91  loss:  0.00040698936209082603
Batch  101  loss:  0.00037571994471363723
Batch  111  loss:  0.00024349629529751837
Batch  121  loss:  0.00048680207692086697
Batch  131  loss:  0.0003732880868483335
Batch  141  loss:  0.00038103817496448755
Batch  151  loss:  0.0007073036977089942
Batch  161  loss:  0.0005897509981878102
Batch  171  loss:  0.0005015884526073933
Batch  181  loss:  0.0002480262774042785
Batch  191  loss:  0.0005197165301069617
Validation on real data: 
LOSS supervised-train 0.0004503483685402898, valid 0.0003587894607335329
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0006201357464306056
Batch  11  loss:  0.0004578035732265562
Batch  21  loss:  0.00044526986312121153
Batch  31  loss:  0.0003962483024224639
Batch  41  loss:  0.0008814312168397009
Batch  51  loss:  0.0003564020444173366
Batch  61  loss:  0.0004080993530806154
Batch  71  loss:  0.0006542725604958832
Batch  81  loss:  0.0006064560147933662
Batch  91  loss:  0.0003090731624979526
Batch  101  loss:  0.00032709274091757834
Batch  111  loss:  0.00038134207716211677
Batch  121  loss:  0.00035116076469421387
Batch  131  loss:  0.0003310403262730688
Batch  141  loss:  0.00040103524224832654
Batch  151  loss:  0.0006985283689573407
Batch  161  loss:  0.00032280932646244764
Batch  171  loss:  0.0003926230128854513
Batch  181  loss:  0.0003201863437425345
Batch  191  loss:  0.0005913030472584069
Validation on real data: 
LOSS supervised-train 0.00042359715029306245, valid 0.0003071613027714193
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0007114278851076961
Batch  11  loss:  0.0003385322052054107
Batch  21  loss:  0.0003921987081412226
Batch  31  loss:  0.00042760756332427263
Batch  41  loss:  0.0010173924965783954
Batch  51  loss:  0.00030857266392558813
Batch  61  loss:  0.0004793613334186375
Batch  71  loss:  0.00048016777145676315
Batch  81  loss:  0.0005278177559375763
Batch  91  loss:  0.0002735548769123852
Batch  101  loss:  0.00034662356483750045
Batch  111  loss:  0.00030935945687815547
Batch  121  loss:  0.0005141364526934922
Batch  131  loss:  0.0003795742231886834
Batch  141  loss:  0.0004491246654652059
Batch  151  loss:  0.0007536595803685486
Batch  161  loss:  0.0003549443499650806
Batch  171  loss:  0.000575808749999851
Batch  181  loss:  0.00029244853067211807
Batch  191  loss:  0.0005676192813552916
Validation on real data: 
LOSS supervised-train 0.0004125886502879439, valid 0.00033673731377348304
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0006749218446202576
Batch  11  loss:  0.00043045467464253306
Batch  21  loss:  0.00030047085601836443
Batch  31  loss:  0.0003362712450325489
Batch  41  loss:  0.0009355081128887832
Batch  51  loss:  0.00047473612357862294
Batch  61  loss:  0.000448331527877599
Batch  71  loss:  0.000469015707494691
Batch  81  loss:  0.0003902166208717972
Batch  91  loss:  0.0003332030028104782
Batch  101  loss:  0.00035235274117439985
Batch  111  loss:  0.00023850672005210072
Batch  121  loss:  0.0005769589333795011
Batch  131  loss:  0.0004704653983935714
Batch  141  loss:  0.0004153850895818323
Batch  151  loss:  0.0007858200115151703
Batch  161  loss:  0.0003079318266827613
Batch  171  loss:  0.0005178324645385146
Batch  181  loss:  0.00023579064873047173
Batch  191  loss:  0.0005072859930805862
Validation on real data: 
LOSS supervised-train 0.00040725424871197903, valid 0.00030907592736184597
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0006734643247909844
Batch  11  loss:  0.00042620787280611694
Batch  21  loss:  0.0003567575477063656
Batch  31  loss:  0.0002453903725836426
Batch  41  loss:  0.0007197369704954326
Batch  51  loss:  0.00043424576870165765
Batch  61  loss:  0.0004823035269510001
Batch  71  loss:  0.0004814269195776433
Batch  81  loss:  0.0002791433653328568
Batch  91  loss:  0.00031143054366111755
Batch  101  loss:  0.00033818534575402737
Batch  111  loss:  0.00028170336736366153
Batch  121  loss:  0.00040110500412993133
Batch  131  loss:  0.00026343943318352103
Batch  141  loss:  0.00038181684794835746
Batch  151  loss:  0.0006505567580461502
Batch  161  loss:  0.0004378130252007395
Batch  171  loss:  0.0004571595636662096
Batch  181  loss:  0.0002820405934471637
Batch  191  loss:  0.0004881516797468066
Validation on real data: 
LOSS supervised-train 0.00037842544210434423, valid 0.00031341006979346275
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.000541775836609304
Batch  11  loss:  0.0003955835709348321
Batch  21  loss:  0.0004927852423861623
Batch  31  loss:  0.00033137446735054255
Batch  41  loss:  0.0009363767458125949
Batch  51  loss:  0.00029695910052396357
Batch  61  loss:  0.0003715357161127031
Batch  71  loss:  0.0005049254395999014
Batch  81  loss:  0.00034362825681455433
Batch  91  loss:  0.0003541313053574413
Batch  101  loss:  0.00035930759622715414
Batch  111  loss:  0.00025407096836715937
Batch  121  loss:  0.00036645427462644875
Batch  131  loss:  0.0004295083926990628
Batch  141  loss:  0.00041665229946374893
Batch  151  loss:  0.0007090528379194438
Batch  161  loss:  0.00034812570083886385
Batch  171  loss:  0.0004465016536414623
Batch  181  loss:  0.00031706810113973916
Batch  191  loss:  0.0005850220913998783
Validation on real data: 
LOSS supervised-train 0.0003780475349049084, valid 0.00024653953732922673
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0007147188880480826
Batch  11  loss:  0.00035625710734166205
Batch  21  loss:  0.0003560293116606772
Batch  31  loss:  0.0004125695559196174
Batch  41  loss:  0.0007677958928979933
Batch  51  loss:  0.0003496299614198506
Batch  61  loss:  0.0004558046057354659
Batch  71  loss:  0.0005824215477332473
Batch  81  loss:  0.0003829923225566745
Batch  91  loss:  0.0002765425306279212
Batch  101  loss:  0.0003407576587051153
Batch  111  loss:  0.00023603567387908697
Batch  121  loss:  0.0002774417807813734
Batch  131  loss:  0.0003856028488371521
Batch  141  loss:  0.0002148644271073863
Batch  151  loss:  0.000705343612935394
Batch  161  loss:  0.00030271493596956134
Batch  171  loss:  0.0003771626215893775
Batch  181  loss:  0.00021131492394488305
Batch  191  loss:  0.00040202049422077835
Validation on real data: 
LOSS supervised-train 0.0003777005003212253, valid 0.00035043677780777216
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0006165871163830161
Batch  11  loss:  0.00036362005630508065
Batch  21  loss:  0.000402156263589859
Batch  31  loss:  0.0002453064953442663
Batch  41  loss:  0.0007782818283885717
Batch  51  loss:  0.0003424636088311672
Batch  61  loss:  0.00044551334576681256
Batch  71  loss:  0.000328282592818141
Batch  81  loss:  0.00034970504930242896
Batch  91  loss:  0.0002746635000221431
Batch  101  loss:  0.0002964168379548937
Batch  111  loss:  0.00021038911654613912
Batch  121  loss:  0.0003332003834657371
Batch  131  loss:  0.0002972162328660488
Batch  141  loss:  0.00037484357017092407
Batch  151  loss:  0.0008973985677585006
Batch  161  loss:  0.000328380468999967
Batch  171  loss:  0.0004809203965123743
Batch  181  loss:  0.0002548921329434961
Batch  191  loss:  0.0005355775356292725
Validation on real data: 
LOSS supervised-train 0.00036720707415952345, valid 0.00029315962456166744
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0007322913152165711
Batch  11  loss:  0.00043603568337857723
Batch  21  loss:  0.0003460697771515697
Batch  31  loss:  0.00031947955721989274
Batch  41  loss:  0.0006748349987901747
Batch  51  loss:  0.0003017618437297642
Batch  61  loss:  0.00036265747621655464
Batch  71  loss:  0.00040162980440072715
Batch  81  loss:  0.00032947538420557976
Batch  91  loss:  0.00026354059809818864
Batch  101  loss:  0.000296144891763106
Batch  111  loss:  0.00023496012727264315
Batch  121  loss:  0.0002794937463477254
Batch  131  loss:  0.00032307265792042017
Batch  141  loss:  0.00029719379381276667
Batch  151  loss:  0.0006495874840766191
Batch  161  loss:  0.0003287698491476476
Batch  171  loss:  0.00029682100284844637
Batch  181  loss:  0.00022857765725348145
Batch  191  loss:  0.0005459368694573641
Validation on real data: 
LOSS supervised-train 0.0003552329180092784, valid 0.00029734172858297825
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0006019530119374394
Batch  11  loss:  0.00040916368016041815
Batch  21  loss:  0.00028752454090863466
Batch  31  loss:  0.0003044572949875146
Batch  41  loss:  0.0007800079765729606
Batch  51  loss:  0.0003839113051071763
Batch  61  loss:  0.0005334102315828204
Batch  71  loss:  0.0006022138404659927
Batch  81  loss:  0.00041628137114457786
Batch  91  loss:  0.0002907215093728155
Batch  101  loss:  0.00037950428668409586
Batch  111  loss:  0.00021076750999782234
Batch  121  loss:  0.00030083846650086343
Batch  131  loss:  0.0002748690312728286
Batch  141  loss:  0.00027577130822464824
Batch  151  loss:  0.0006016417755745351
Batch  161  loss:  0.00030994456028565764
Batch  171  loss:  0.0003910112427547574
Batch  181  loss:  0.00018774702039081603
Batch  191  loss:  0.0004360323946457356
Validation on real data: 
LOSS supervised-train 0.0003495120134903118, valid 0.00022367233759723604
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0005868495209142566
Batch  11  loss:  0.0003783667052630335
Batch  21  loss:  0.00030864670407027006
Batch  31  loss:  0.00023328102543018758
Batch  41  loss:  0.0005671663675457239
Batch  51  loss:  0.00036435379297472537
Batch  61  loss:  0.000396203511627391
Batch  71  loss:  0.0003818582044914365
Batch  81  loss:  0.0002534196537453681
Batch  91  loss:  0.00026944695855490863
Batch  101  loss:  0.000359700876288116
Batch  111  loss:  0.00024708412820473313
Batch  121  loss:  0.0003315624489914626
Batch  131  loss:  0.00021870156342629343
Batch  141  loss:  0.00028875144198536873
Batch  151  loss:  0.00046359197585843503
Batch  161  loss:  0.00023922599211800843
Batch  171  loss:  0.0002668994711712003
Batch  181  loss:  0.00026464081020094454
Batch  191  loss:  0.0004850612604059279
Validation on real data: 
LOSS supervised-train 0.00034712614797172137, valid 0.00031665252754464746
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0006157850148156285
Batch  11  loss:  0.00042423108243383467
Batch  21  loss:  0.0003285811108071357
Batch  31  loss:  0.0002495477965567261
Batch  41  loss:  0.0005804882966913283
Batch  51  loss:  0.000288718641968444
Batch  61  loss:  0.00028096482856199145
Batch  71  loss:  0.0005357138579711318
Batch  81  loss:  0.00030626062653027475
Batch  91  loss:  0.00020981035777367651
Batch  101  loss:  0.00027533824322745204
Batch  111  loss:  0.000295450707199052
Batch  121  loss:  0.0003201141080353409
Batch  131  loss:  0.0003229261201340705
Batch  141  loss:  0.00031015140120871365
Batch  151  loss:  0.0005048912134952843
Batch  161  loss:  0.00026775270816870034
Batch  171  loss:  0.0002617920399643481
Batch  181  loss:  0.00023278423759620637
Batch  191  loss:  0.0003391936479602009
Validation on real data: 
LOSS supervised-train 0.0003228735863376642, valid 0.00026052771136164665
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00042364365071989596
Batch  11  loss:  0.0004358639707788825
Batch  21  loss:  0.00035187057801522315
Batch  31  loss:  0.00023390818387269974
Batch  41  loss:  0.0004911146825179458
Batch  51  loss:  0.0003408770135138184
Batch  61  loss:  0.00041805102955549955
Batch  71  loss:  0.0005723251961171627
Batch  81  loss:  0.0002736960013862699
Batch  91  loss:  0.00029729751986451447
Batch  101  loss:  0.0003313638153485954
Batch  111  loss:  0.00018007724429480731
Batch  121  loss:  0.0002828894357662648
Batch  131  loss:  0.00026251288363710046
Batch  141  loss:  0.00020064832642674446
Batch  151  loss:  0.0005390894366428256
Batch  161  loss:  0.0002451244799885899
Batch  171  loss:  0.0003058293950743973
Batch  181  loss:  0.00019271225028205663
Batch  191  loss:  0.0003993517893832177
Validation on real data: 
LOSS supervised-train 0.0003121177864522906, valid 0.00034189753932878375
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.00046556489542126656
Batch  11  loss:  0.00039024389116093516
Batch  21  loss:  0.0002781040675472468
Batch  31  loss:  0.00021078360441606492
Batch  41  loss:  0.0005309981643222272
Batch  51  loss:  0.00019900078768841922
Batch  61  loss:  0.00029827203252352774
Batch  71  loss:  0.00041554871131666005
Batch  81  loss:  0.00029107843874953687
Batch  91  loss:  0.00027424629661254585
Batch  101  loss:  0.00023926689755171537
Batch  111  loss:  0.00025058077881112695
Batch  121  loss:  0.00025975098833441734
Batch  131  loss:  0.00022907335369382054
Batch  141  loss:  0.00035005269455723464
Batch  151  loss:  0.0004484345263335854
Batch  161  loss:  0.00024499595747329295
Batch  171  loss:  0.00027908870833925903
Batch  181  loss:  0.0002254434220958501
Batch  191  loss:  0.0003487510548438877
Validation on real data: 
LOSS supervised-train 0.00030331541296618525, valid 0.00021056443802081048
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00039445969741791487
Batch  11  loss:  0.00032226767507381737
Batch  21  loss:  0.0002849640150088817
Batch  31  loss:  0.00016122533997986466
Batch  41  loss:  0.00045040875556878746
Batch  51  loss:  0.00027947124908678234
Batch  61  loss:  0.0003920240269508213
Batch  71  loss:  0.0004311321536079049
Batch  81  loss:  0.00024825913715176284
Batch  91  loss:  0.00018818325770553201
Batch  101  loss:  0.0002589409414213151
Batch  111  loss:  0.00026812421856448054
Batch  121  loss:  0.0002745063975453377
Batch  131  loss:  0.000171586376382038
Batch  141  loss:  0.00019135950424242765
Batch  151  loss:  0.0005441774846985936
Batch  161  loss:  0.00033327023265883327
Batch  171  loss:  0.0002768206177279353
Batch  181  loss:  0.00023920235980767757
Batch  191  loss:  0.000432156230090186
Validation on real data: 
LOSS supervised-train 0.00031255338166374713, valid 0.00027084184694103897
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0005478800158016384
Batch  11  loss:  0.0003951184335164726
Batch  21  loss:  0.00042915137601085007
Batch  31  loss:  0.0002836350176949054
Batch  41  loss:  0.0005441426183097064
Batch  51  loss:  0.00019456821610219777
Batch  61  loss:  0.00037593222805298865
Batch  71  loss:  0.0003579113108571619
Batch  81  loss:  0.00021280828514136374
Batch  91  loss:  0.00023074341879691929
Batch  101  loss:  0.00034453734406270087
Batch  111  loss:  0.00018351183098275214
Batch  121  loss:  0.00031265788129530847
Batch  131  loss:  0.00016113992023747414
Batch  141  loss:  0.00027276144828647375
Batch  151  loss:  0.0005344953387975693
Batch  161  loss:  0.0002656430588103831
Batch  171  loss:  0.000263367488514632
Batch  181  loss:  0.00018159605679102242
Batch  191  loss:  0.0003320662654004991
Validation on real data: 
LOSS supervised-train 0.0002994819552986883, valid 0.00022456349688582122
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0004820861795451492
Batch  11  loss:  0.00026970362523570657
Batch  21  loss:  0.00023226323537528515
Batch  31  loss:  0.00030802187393419445
Batch  41  loss:  0.0005431706085801125
Batch  51  loss:  0.00027440881240181625
Batch  61  loss:  0.0003262271929997951
Batch  71  loss:  0.0003081627073697746
Batch  81  loss:  0.00019660245743580163
Batch  91  loss:  0.0002386781561654061
Batch  101  loss:  0.0002340800128877163
Batch  111  loss:  0.00015637819888070226
Batch  121  loss:  0.0003725676506292075
Batch  131  loss:  0.0003102930204477161
Batch  141  loss:  0.0002569389180280268
Batch  151  loss:  0.0004245712189003825
Batch  161  loss:  0.00027091774973087013
Batch  171  loss:  0.0002792497689370066
Batch  181  loss:  0.00021587169612757862
Batch  191  loss:  0.00030572983087040484
Validation on real data: 
LOSS supervised-train 0.0002944625963573344, valid 0.00025413642288185656
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0004173259949311614
Batch  11  loss:  0.00030424882424995303
Batch  21  loss:  0.000294340803520754
Batch  31  loss:  0.00018711670418269932
Batch  41  loss:  0.0004865138907916844
Batch  51  loss:  0.0002180888259317726
Batch  61  loss:  0.0002568999188952148
Batch  71  loss:  0.00037900148890912533
Batch  81  loss:  0.0002265971270389855
Batch  91  loss:  0.00026184626040048897
Batch  101  loss:  0.0002625717024784535
Batch  111  loss:  0.00018353710765950382
Batch  121  loss:  0.00031316367676481605
Batch  131  loss:  0.0002655110729392618
Batch  141  loss:  0.00030071285436861217
Batch  151  loss:  0.0003793915384449065
Batch  161  loss:  0.00018469098722562194
Batch  171  loss:  0.0003199881175532937
Batch  181  loss:  0.0001427878742106259
Batch  191  loss:  0.0003463434986770153
Validation on real data: 
LOSS supervised-train 0.0002786030573770404, valid 0.00026285884086973965
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00045694768778048456
Batch  11  loss:  0.00028794523677788675
Batch  21  loss:  0.0002620378218125552
Batch  31  loss:  0.00022066591191105545
Batch  41  loss:  0.00041896276525221765
Batch  51  loss:  0.0002481446717865765
Batch  61  loss:  0.0003195020544808358
Batch  71  loss:  0.000298596074571833
Batch  81  loss:  0.00027371037867851555
Batch  91  loss:  0.00021699393982999027
Batch  101  loss:  0.00028840466984547675
Batch  111  loss:  0.00021140139142517
Batch  121  loss:  0.00031309257610701025
Batch  131  loss:  0.00025718932738527656
Batch  141  loss:  0.000169896797160618
Batch  151  loss:  0.0005410952726379037
Batch  161  loss:  0.00022417808941099793
Batch  171  loss:  0.0003655028122011572
Batch  181  loss:  0.000192340521607548
Batch  191  loss:  0.0002665271167643368
Validation on real data: 
LOSS supervised-train 0.0002792227927420754, valid 0.00017204071627929807
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.000449424609541893
Batch  11  loss:  0.00033963529858738184
Batch  21  loss:  0.00022465606161858886
Batch  31  loss:  0.0002203849289799109
Batch  41  loss:  0.0004868427640758455
Batch  51  loss:  0.00024228039546869695
Batch  61  loss:  0.0003345571458339691
Batch  71  loss:  0.000388018146622926
Batch  81  loss:  0.00032099243253469467
Batch  91  loss:  0.00018534327682573348
Batch  101  loss:  0.0002623839827720076
Batch  111  loss:  0.00018288515275344253
Batch  121  loss:  0.0002663418126758188
Batch  131  loss:  0.0002780073555186391
Batch  141  loss:  0.00019885516667272896
Batch  151  loss:  0.000519735214766115
Batch  161  loss:  0.00019336611148901284
Batch  171  loss:  0.00031283771386370063
Batch  181  loss:  0.00027916583348996937
Batch  191  loss:  0.00034168653655797243
Validation on real data: 
LOSS supervised-train 0.0002771962460974464, valid 0.0001901813520817086
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.00036222170456312597
Batch  11  loss:  0.00025395580451004207
Batch  21  loss:  0.00033783266553655267
Batch  31  loss:  0.00020757112361025065
Batch  41  loss:  0.0005235756398178637
Batch  51  loss:  0.00024398141249548644
Batch  61  loss:  0.0002484298311173916
Batch  71  loss:  0.000299432867905125
Batch  81  loss:  0.0003059175214730203
Batch  91  loss:  0.00021389932953752577
Batch  101  loss:  0.00015751061437185854
Batch  111  loss:  0.00018158223247155547
Batch  121  loss:  0.0002925277512986213
Batch  131  loss:  0.0003869440406560898
Batch  141  loss:  0.000286029651761055
Batch  151  loss:  0.000450074061518535
Batch  161  loss:  0.00020965175644960254
Batch  171  loss:  0.00022796790290158242
Batch  181  loss:  0.00019682201673276722
Batch  191  loss:  0.000278170860838145
Validation on real data: 
LOSS supervised-train 0.0002766366356809158, valid 0.0002964386949315667
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00034766955650411546
Batch  11  loss:  0.0002631265379022807
Batch  21  loss:  0.00031096141901798546
Batch  31  loss:  0.00021350692259147763
Batch  41  loss:  0.0004762138705700636
Batch  51  loss:  0.0001953181199496612
Batch  61  loss:  0.0002755605964921415
Batch  71  loss:  0.0003066317003685981
Batch  81  loss:  0.00029417057521641254
Batch  91  loss:  0.00026632071239873767
Batch  101  loss:  0.00017896095232572407
Batch  111  loss:  0.00020674504048656672
Batch  121  loss:  0.00027459539705887437
Batch  131  loss:  0.00035139708779752254
Batch  141  loss:  0.00030689206323586404
Batch  151  loss:  0.000413242494687438
Batch  161  loss:  0.0002918120881076902
Batch  171  loss:  0.00025881349574774504
Batch  181  loss:  0.00021356367506086826
Batch  191  loss:  0.0003479309962131083
Validation on real data: 
LOSS supervised-train 0.00027354991107131355, valid 0.00020913491607643664
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0005166304181329906
Batch  11  loss:  0.0002772256266325712
Batch  21  loss:  0.000385812483727932
Batch  31  loss:  0.00017756166926119477
Batch  41  loss:  0.0003931068640667945
Batch  51  loss:  0.00020530317851807922
Batch  61  loss:  0.0002740340423770249
Batch  71  loss:  0.00032689442741684616
Batch  81  loss:  0.00017983671568799764
Batch  91  loss:  0.00017949736502487212
Batch  101  loss:  0.00019561882072594017
Batch  111  loss:  0.0001597254304215312
Batch  121  loss:  0.0002273828722536564
Batch  131  loss:  0.00024409155594184995
Batch  141  loss:  0.0002130930806742981
Batch  151  loss:  0.0003616297908592969
Batch  161  loss:  0.00025565840769559145
Batch  171  loss:  0.00025180107331834733
Batch  181  loss:  0.00019932538270950317
Batch  191  loss:  0.00024501534062437713
Validation on real data: 
LOSS supervised-train 0.00026310451328754426, valid 0.00018989897216670215
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00035443666274659336
Batch  11  loss:  0.0002961326972581446
Batch  21  loss:  0.00025707235909067094
Batch  31  loss:  0.00025998352793976665
Batch  41  loss:  0.00035911001032218337
Batch  51  loss:  0.00023546961892861873
Batch  61  loss:  0.0003549949615262449
Batch  71  loss:  0.0004027447139378637
Batch  81  loss:  0.0003075282438658178
Batch  91  loss:  0.00023373770818579942
Batch  101  loss:  0.00023473522742278874
Batch  111  loss:  0.000159258910571225
Batch  121  loss:  0.00024207735259551555
Batch  131  loss:  0.00024669317645020783
Batch  141  loss:  0.0002622515894472599
Batch  151  loss:  0.0004475153109524399
Batch  161  loss:  0.0002581937878858298
Batch  171  loss:  0.00022799981525167823
Batch  181  loss:  0.0001768798683770001
Batch  191  loss:  0.00032839091727510095
Validation on real data: 
LOSS supervised-train 0.000253489456299576, valid 0.0002530209894757718
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0003514255804475397
Batch  11  loss:  0.00023380055790767074
Batch  21  loss:  0.0003140422631986439
Batch  31  loss:  0.00020476059580687433
Batch  41  loss:  0.0004421935009304434
Batch  51  loss:  0.0002126394974766299
Batch  61  loss:  0.00029383052606135607
Batch  71  loss:  0.00037620365037582815
Batch  81  loss:  0.00023248496290761977
Batch  91  loss:  0.0001868454273790121
Batch  101  loss:  0.0001767092471709475
Batch  111  loss:  0.00017125066369771957
Batch  121  loss:  0.0002362048689974472
Batch  131  loss:  0.0002542033907957375
Batch  141  loss:  0.0003111789410468191
Batch  151  loss:  0.00036407014704309404
Batch  161  loss:  0.0002471554616931826
Batch  171  loss:  0.00032875651959329844
Batch  181  loss:  0.00017540249973535538
Batch  191  loss:  0.0002814974868670106
Validation on real data: 
LOSS supervised-train 0.0002594083704025252, valid 0.00020567289902828634
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0004869096155744046
Batch  11  loss:  0.0002773710002657026
Batch  21  loss:  0.00028669583844020963
Batch  31  loss:  0.00015669230197090656
Batch  41  loss:  0.00041766290087252855
Batch  51  loss:  0.0002854755148291588
Batch  61  loss:  0.0003710721211973578
Batch  71  loss:  0.0003388410259503871
Batch  81  loss:  0.0001639671972952783
Batch  91  loss:  0.00016735887038521469
Batch  101  loss:  0.0001983996480703354
Batch  111  loss:  0.00018608669051900506
Batch  121  loss:  0.00023286398209165782
Batch  131  loss:  0.00024153538106475025
Batch  141  loss:  0.0002606386551633477
Batch  151  loss:  0.00029652396915480494
Batch  161  loss:  0.00019806739874184132
Batch  171  loss:  0.00019466581579763442
Batch  181  loss:  0.00019529364362824708
Batch  191  loss:  0.0002994765527546406
Validation on real data: 
LOSS supervised-train 0.00026061466480314265, valid 0.00018890239880420268
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0003860176366288215
Batch  11  loss:  0.00025074079167097807
Batch  21  loss:  0.0003434462705627084
Batch  31  loss:  0.00019713952497113496
Batch  41  loss:  0.0003786070446949452
Batch  51  loss:  0.00020027942082379013
Batch  61  loss:  0.000228464268730022
Batch  71  loss:  0.00040304340654984117
Batch  81  loss:  0.00019466386584099382
Batch  91  loss:  0.00018694195023272187
Batch  101  loss:  0.0001692934602033347
Batch  111  loss:  0.0001628311292733997
Batch  121  loss:  0.0002321226056665182
Batch  131  loss:  0.00020145352755207568
Batch  141  loss:  0.00019830773817375302
Batch  151  loss:  0.00047792380792088807
Batch  161  loss:  0.00028381141601130366
Batch  171  loss:  0.00019267317838966846
Batch  181  loss:  0.00017880032828543335
Batch  191  loss:  0.0002735443995334208
Validation on real data: 
LOSS supervised-train 0.0002460121009789873, valid 0.0002076178789138794
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00037128495750948787
Batch  11  loss:  0.00025165817351080477
Batch  21  loss:  0.000332863099174574
Batch  31  loss:  0.0001763390755513683
Batch  41  loss:  0.0003202819498255849
Batch  51  loss:  0.0001666724419919774
Batch  61  loss:  0.00026212839293293655
Batch  71  loss:  0.0002734159934334457
Batch  81  loss:  0.0002719464828260243
Batch  91  loss:  0.00017967533494811505
Batch  101  loss:  0.00017398643831256777
Batch  111  loss:  0.00016979529755190015
Batch  121  loss:  0.0002566186885815114
Batch  131  loss:  0.0002340441569685936
Batch  141  loss:  0.00021412463684100658
Batch  151  loss:  0.000411551765864715
Batch  161  loss:  0.0003304552228655666
Batch  171  loss:  0.0002130570646841079
Batch  181  loss:  0.00021281573572196066
Batch  191  loss:  0.0002969796769320965
Validation on real data: 
LOSS supervised-train 0.00024228261267126073, valid 0.00019560792134143412
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00027646272792480886
Batch  11  loss:  0.00026416475884616375
Batch  21  loss:  0.00030983571195974946
Batch  31  loss:  0.00017222327005583793
Batch  41  loss:  0.00042215606663376093
Batch  51  loss:  0.00020990624034311622
Batch  61  loss:  0.00020095694344490767
Batch  71  loss:  0.0002810387231875211
Batch  81  loss:  0.00022151954181026667
Batch  91  loss:  0.00016843908815644681
Batch  101  loss:  0.00022561452351510525
Batch  111  loss:  0.00020377672626636922
Batch  121  loss:  0.0002285012014908716
Batch  131  loss:  0.00022854532289784402
Batch  141  loss:  0.00020894718181807548
Batch  151  loss:  0.00032970315078273416
Batch  161  loss:  0.00022135436302050948
Batch  171  loss:  0.00016336730914190412
Batch  181  loss:  0.0001984068367164582
Batch  191  loss:  0.00030401666299439967
Validation on real data: 
LOSS supervised-train 0.00024379574846534525, valid 0.000248921278398484
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00031516130547970533
Batch  11  loss:  0.00022404923220165074
Batch  21  loss:  0.00017518020467832685
Batch  31  loss:  0.00018602031923364848
Batch  41  loss:  0.0003648308920674026
Batch  51  loss:  0.0002809877914842218
Batch  61  loss:  0.0002824790426529944
Batch  71  loss:  0.0002555253158789128
Batch  81  loss:  0.00022408826043829322
Batch  91  loss:  0.00023254512052517384
Batch  101  loss:  0.0002145799226127565
Batch  111  loss:  0.00018004371668212116
Batch  121  loss:  0.00031884273630566895
Batch  131  loss:  0.00017966849554795772
Batch  141  loss:  0.000251909950748086
Batch  151  loss:  0.0003627265978138894
Batch  161  loss:  0.0003542568883858621
Batch  171  loss:  0.00024263268278446048
Batch  181  loss:  0.00021982475300319493
Batch  191  loss:  0.00025980366626754403
Validation on real data: 
LOSS supervised-train 0.00024659728216647637, valid 0.00021331143216229975
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00044983610860072076
Batch  11  loss:  0.00020125308947172016
Batch  21  loss:  0.0003030405205208808
Batch  31  loss:  0.0001364563504466787
Batch  41  loss:  0.00036191835533827543
Batch  51  loss:  0.0002170779334846884
Batch  61  loss:  0.00028454349376261234
Batch  71  loss:  0.00026793297729454935
Batch  81  loss:  0.00016204929852392524
Batch  91  loss:  0.00017388153355568647
Batch  101  loss:  0.00021081321756355464
Batch  111  loss:  0.00016874709399417043
Batch  121  loss:  0.00024186514201574028
Batch  131  loss:  0.0001595593203091994
Batch  141  loss:  0.00021149031817913055
Batch  151  loss:  0.00033152123796753585
Batch  161  loss:  0.0002518677501939237
Batch  171  loss:  0.00021901167929172516
Batch  181  loss:  0.0001778538862708956
Batch  191  loss:  0.0002757939510047436
Validation on real data: 
LOSS supervised-train 0.00024024543199629989, valid 0.0002258306194562465
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00040867552161216736
Batch  11  loss:  0.00026967236772179604
Batch  21  loss:  0.00022078990878071636
Batch  31  loss:  0.00023469312873203307
Batch  41  loss:  0.0005030948086641729
Batch  51  loss:  0.00021694721363019198
Batch  61  loss:  0.00021225881937425584
Batch  71  loss:  0.00027644759393297136
Batch  81  loss:  0.00021231602295301855
Batch  91  loss:  0.00017556115926709026
Batch  101  loss:  0.00019446051737759262
Batch  111  loss:  0.00017218843277078122
Batch  121  loss:  0.00027318060165271163
Batch  131  loss:  0.000272281322395429
Batch  141  loss:  0.00019290884665679187
Batch  151  loss:  0.00031963136279955506
Batch  161  loss:  0.00019938533660024405
Batch  171  loss:  0.00019412401888985187
Batch  181  loss:  0.00018608990649227053
Batch  191  loss:  0.00038116960786283016
Validation on real data: 
LOSS supervised-train 0.0002365558560632053, valid 0.0002638468286022544
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.00041606667218729854
Batch  11  loss:  0.0002103718143189326
Batch  21  loss:  0.00021112916874699295
Batch  31  loss:  0.000145972371683456
Batch  41  loss:  0.00046379517880268395
Batch  51  loss:  0.0002000630192924291
Batch  61  loss:  0.000274204823654145
Batch  71  loss:  0.0002912464551627636
Batch  81  loss:  0.00021450397616717964
Batch  91  loss:  0.00016513210721313953
Batch  101  loss:  0.00021831445337738842
Batch  111  loss:  0.00021135236602276564
Batch  121  loss:  0.00022558326600119472
Batch  131  loss:  0.00021274664322845638
Batch  141  loss:  0.0001791971444617957
Batch  151  loss:  0.00038155593210831285
Batch  161  loss:  0.00028248660964891315
Batch  171  loss:  0.00027929811039939523
Batch  181  loss:  0.0001950449077412486
Batch  191  loss:  0.00026109229656867683
Validation on real data: 
LOSS supervised-train 0.00023133028393203858, valid 0.00020676830899901688
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0004466716491151601
Batch  11  loss:  0.0002508559264242649
Batch  21  loss:  0.00023874764156062156
Batch  31  loss:  0.00014809012645855546
Batch  41  loss:  0.0003243683895561844
Batch  51  loss:  0.00020694619161076844
Batch  61  loss:  0.00023912571487016976
Batch  71  loss:  0.0002541648573242128
Batch  81  loss:  0.00018083200848195702
Batch  91  loss:  0.00017312179261352867
Batch  101  loss:  0.0002020825195359066
Batch  111  loss:  0.00019305919704493135
Batch  121  loss:  0.00021384663705248386
Batch  131  loss:  0.00024878053227439523
Batch  141  loss:  0.0002517153334338218
Batch  151  loss:  0.00036122428718954325
Batch  161  loss:  0.00023844465613365173
Batch  171  loss:  0.00025820330483838916
Batch  181  loss:  0.00021828860917594284
Batch  191  loss:  0.00037100756890140474
Validation on real data: 
LOSS supervised-train 0.0002351038285996765, valid 0.0002453619963489473
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0003271980385761708
Batch  11  loss:  0.00034150833380408585
Batch  21  loss:  0.0003012496163137257
Batch  31  loss:  0.00015809093019925058
Batch  41  loss:  0.00038092295289970934
Batch  51  loss:  0.0001765172928571701
Batch  61  loss:  0.00031644239788874984
Batch  71  loss:  0.0002343904197914526
Batch  81  loss:  0.0002279952313983813
Batch  91  loss:  0.0001588365266798064
Batch  101  loss:  0.000163162563694641
Batch  111  loss:  0.0001887543621705845
Batch  121  loss:  0.0002662764163687825
Batch  131  loss:  0.00013952724111732095
Batch  141  loss:  0.00019531618454493582
Batch  151  loss:  0.000406623468734324
Batch  161  loss:  0.0002772623847704381
Batch  171  loss:  0.00017931894399225712
Batch  181  loss:  0.00014894279593136162
Batch  191  loss:  0.00022579982760362327
Validation on real data: 
LOSS supervised-train 0.000226083527195442, valid 0.00018858473049476743
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00032186947646550834
Batch  11  loss:  0.00031124791712500155
Batch  21  loss:  0.00024005759041756392
Batch  31  loss:  0.00015945194172672927
Batch  41  loss:  0.00029622120200656354
Batch  51  loss:  0.00017154053784906864
Batch  61  loss:  0.00038537263753823936
Batch  71  loss:  0.00029722912586294115
Batch  81  loss:  0.00020373851293697953
Batch  91  loss:  0.00022327124315779656
Batch  101  loss:  0.00022490948322229087
Batch  111  loss:  0.00014785959501750767
Batch  121  loss:  0.00028764299349859357
Batch  131  loss:  0.00020680413581430912
Batch  141  loss:  0.00017654552357271314
Batch  151  loss:  0.0003319362585898489
Batch  161  loss:  0.0002636211575008929
Batch  171  loss:  0.00018779715173877776
Batch  181  loss:  0.00018010141502600163
Batch  191  loss:  0.0003947055374737829
Validation on real data: 
LOSS supervised-train 0.00022736942970368545, valid 0.00021464064775500447
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0003881902666762471
Batch  11  loss:  0.0003232910530641675
Batch  21  loss:  0.00021184259094297886
Batch  31  loss:  0.00013175256026443094
Batch  41  loss:  0.00029396271565929055
Batch  51  loss:  0.00022761299624107778
Batch  61  loss:  0.00022384787735063583
Batch  71  loss:  0.0002850428281817585
Batch  81  loss:  0.00019040600454900414
Batch  91  loss:  0.00019566318951547146
Batch  101  loss:  0.0002238988090539351
Batch  111  loss:  0.000205165590159595
Batch  121  loss:  0.00024384567223023623
Batch  131  loss:  0.00021257498883642256
Batch  141  loss:  0.0001597399532329291
Batch  151  loss:  0.0003200625942554325
Batch  161  loss:  0.00019116787007078528
Batch  171  loss:  0.00018709631694946438
Batch  181  loss:  0.00017929259047377855
Batch  191  loss:  0.00026578421238809824
Validation on real data: 
LOSS supervised-train 0.00023054109198710648, valid 0.0001885566598502919
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00036804197588935494
Batch  11  loss:  0.0002038548409473151
Batch  21  loss:  0.00021776552603114396
Batch  31  loss:  0.00015562526823487133
Batch  41  loss:  0.00038276705890893936
Batch  51  loss:  0.00021283701062202454
Batch  61  loss:  0.00026312368572689593
Batch  71  loss:  0.0001899373164633289
Batch  81  loss:  0.00022099961643107235
Batch  91  loss:  0.00025894405553117394
Batch  101  loss:  0.00021276743791531771
Batch  111  loss:  0.00016066458192653954
Batch  121  loss:  0.00018022675067186356
Batch  131  loss:  0.00023756643349770457
Batch  141  loss:  0.00025085359811782837
Batch  151  loss:  0.0003124515642412007
Batch  161  loss:  0.00023628068447578698
Batch  171  loss:  0.0001642361021367833
Batch  181  loss:  0.00016847331426106393
Batch  191  loss:  0.0002098598051816225
Validation on real data: 
LOSS supervised-train 0.00022445069687819342, valid 0.0001960157387657091
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00034541537752375007
Batch  11  loss:  0.0001982030807994306
Batch  21  loss:  0.0002979601267725229
Batch  31  loss:  0.00019279583648312837
Batch  41  loss:  0.0003660184156615287
Batch  51  loss:  0.00016497842443641275
Batch  61  loss:  0.00023283516929950565
Batch  71  loss:  0.00032951642060652375
Batch  81  loss:  0.0002238783927168697
Batch  91  loss:  0.0002213669358752668
Batch  101  loss:  0.0001683850568952039
Batch  111  loss:  0.00017626203771214932
Batch  121  loss:  0.00022797838028054684
Batch  131  loss:  0.00015159927716013044
Batch  141  loss:  0.0001326925994362682
Batch  151  loss:  0.0002778758353088051
Batch  161  loss:  0.000189411046449095
Batch  171  loss:  0.00018181680934503675
Batch  181  loss:  0.00014891814498696476
Batch  191  loss:  0.0002458830422256142
Validation on real data: 
LOSS supervised-train 0.00021679796591342893, valid 0.00024266078253276646
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00042661651968955994
Batch  11  loss:  0.00029468582943081856
Batch  21  loss:  0.00033254543086513877
Batch  31  loss:  0.00016617204528301954
Batch  41  loss:  0.00035589627805165946
Batch  51  loss:  0.0001748446375131607
Batch  61  loss:  0.00024952288367785513
Batch  71  loss:  0.0002562688896432519
Batch  81  loss:  0.00019506647367961705
Batch  91  loss:  0.00018188939429819584
Batch  101  loss:  0.00013296361430548131
Batch  111  loss:  0.0001333261316176504
Batch  121  loss:  0.00030812149634584785
Batch  131  loss:  0.00017542640853207558
Batch  141  loss:  0.0002094196097459644
Batch  151  loss:  0.00024427688913419843
Batch  161  loss:  0.0003090118698310107
Batch  171  loss:  0.00021023544832132757
Batch  181  loss:  0.0002118022384820506
Batch  191  loss:  0.00022462307242676616
Validation on real data: 
LOSS supervised-train 0.0002162417412182549, valid 0.00021827203454449773
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00031365014729090035
Batch  11  loss:  0.00022467499366030097
Batch  21  loss:  0.00023956206860020757
Batch  31  loss:  0.00018550576351117343
Batch  41  loss:  0.0003220760845579207
Batch  51  loss:  0.0001963666727533564
Batch  61  loss:  0.00019787285418715328
Batch  71  loss:  0.00024405460862908512
Batch  81  loss:  0.00016565955593250692
Batch  91  loss:  0.00022285952582024038
Batch  101  loss:  0.00023120448167901486
Batch  111  loss:  0.000140495685627684
Batch  121  loss:  0.0002285588125232607
Batch  131  loss:  0.00026682831230573356
Batch  141  loss:  0.00019411624816711992
Batch  151  loss:  0.0003148033865727484
Batch  161  loss:  0.00022657321824226528
Batch  171  loss:  0.00020680803572759032
Batch  181  loss:  0.00015667978732381016
Batch  191  loss:  0.00026501534739509225
Validation on real data: 
LOSS supervised-train 0.00021148320429347222, valid 0.0001544168044347316
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0003810925700236112
Batch  11  loss:  0.00018764621927402914
Batch  21  loss:  0.0003454152902122587
Batch  31  loss:  0.00021455844398587942
Batch  41  loss:  0.0002693699498195201
Batch  51  loss:  0.00019819136650767177
Batch  61  loss:  0.00015125353820621967
Batch  71  loss:  0.00025528701371513307
Batch  81  loss:  0.00016944948583841324
Batch  91  loss:  0.0001938062923727557
Batch  101  loss:  0.00018402596469968557
Batch  111  loss:  0.00018728167924564332
Batch  121  loss:  0.00024708276032470167
Batch  131  loss:  0.00020756128651555628
Batch  141  loss:  0.00022075165179558098
Batch  151  loss:  0.0002686193329282105
Batch  161  loss:  0.0002150239160982892
Batch  171  loss:  0.0002334229211555794
Batch  181  loss:  0.0002034141798503697
Batch  191  loss:  0.00023234593390952796
Validation on real data: 
LOSS supervised-train 0.00021984188242640812, valid 0.00020568772742990404
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0002637886500451714
Batch  11  loss:  0.00027769128791987896
Batch  21  loss:  0.0003681609232444316
Batch  31  loss:  0.0001669457124080509
Batch  41  loss:  0.00028260445105843246
Batch  51  loss:  0.0001527780113974586
Batch  61  loss:  0.0002201954193878919
Batch  71  loss:  0.00024157171719707549
Batch  81  loss:  0.00017637701239436865
Batch  91  loss:  0.00019298508414067328
Batch  101  loss:  0.00015256440383382142
Batch  111  loss:  0.00013574620243161917
Batch  121  loss:  0.00022040068870410323
Batch  131  loss:  0.00017353294242639095
Batch  141  loss:  0.00015483537572436035
Batch  151  loss:  0.00026497896760702133
Batch  161  loss:  0.00024338799994438887
Batch  171  loss:  0.00020907387079205364
Batch  181  loss:  0.00017786126409191638
Batch  191  loss:  0.00022722828725818545
Validation on real data: 
LOSS supervised-train 0.0002082977983809542, valid 0.0001880345807876438
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00033778577926568687
Batch  11  loss:  0.00021854361693840474
Batch  21  loss:  0.00022704846924170852
Batch  31  loss:  0.00018151193216908723
Batch  41  loss:  0.00029255429399199784
Batch  51  loss:  0.00022144976537674665
Batch  61  loss:  0.0002981917350552976
Batch  71  loss:  0.00023961448459886014
Batch  81  loss:  0.00020404934184625745
Batch  91  loss:  0.00017778378969524056
Batch  101  loss:  0.00018776493379846215
Batch  111  loss:  0.00015641830395907164
Batch  121  loss:  0.00021021082648076117
Batch  131  loss:  0.00018324439588468522
Batch  141  loss:  0.00020578104886226356
Batch  151  loss:  0.0003740480460692197
Batch  161  loss:  0.0001667937176534906
Batch  171  loss:  0.0002002314868150279
Batch  181  loss:  0.0001543338585179299
Batch  191  loss:  0.00024139251036103815
Validation on real data: 
LOSS supervised-train 0.00020618906695744953, valid 0.00016338657587766647
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0003579965850804001
Batch  11  loss:  0.00025008656666614115
Batch  21  loss:  0.0003267517895437777
Batch  31  loss:  0.00011415425251470879
Batch  41  loss:  0.00038754596607759595
Batch  51  loss:  0.00015633388829883188
Batch  61  loss:  0.00019525371317286044
Batch  71  loss:  0.0002690865658223629
Batch  81  loss:  0.00026701914612203836
Batch  91  loss:  0.00015086487110238522
Batch  101  loss:  0.0001671605568844825
Batch  111  loss:  0.00018164834182243794
Batch  121  loss:  0.00026667103520594537
Batch  131  loss:  0.00016948733536992222
Batch  141  loss:  0.00016532525478396565
Batch  151  loss:  0.0003336502122692764
Batch  161  loss:  0.00017725217912811786
Batch  171  loss:  0.00014187894703354686
Batch  181  loss:  0.00015780865214765072
Batch  191  loss:  0.00030672643333673477
Validation on real data: 
LOSS supervised-train 0.00021140076005394804, valid 0.00018516313866712153
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00029989605536684394
Batch  11  loss:  0.00019097310723736882
Batch  21  loss:  0.0002745172241702676
Batch  31  loss:  0.00011731051927199587
Batch  41  loss:  0.00029253438697196543
Batch  51  loss:  0.00016698733088560402
Batch  61  loss:  0.00019458917086012661
Batch  71  loss:  0.0002677903394214809
Batch  81  loss:  0.00019440446340013295
Batch  91  loss:  0.00013963156379759312
Batch  101  loss:  0.00021887157345190644
Batch  111  loss:  0.00017360913625452667
Batch  121  loss:  0.00021707842824980617
Batch  131  loss:  0.00017651506641414016
Batch  141  loss:  0.00016353573300875723
Batch  151  loss:  0.00031884247437119484
Batch  161  loss:  0.00020529210451059043
Batch  171  loss:  0.0001949423021869734
Batch  181  loss:  0.00011960018309764564
Batch  191  loss:  0.0002496179367881268
Validation on real data: 
LOSS supervised-train 0.00020042713218572318, valid 0.00017887612921185791
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0002421128301648423
Batch  11  loss:  0.0002316485915798694
Batch  21  loss:  0.0002469396567903459
Batch  31  loss:  0.00016386687639169395
Batch  41  loss:  0.0002948587934952229
Batch  51  loss:  0.0001708989148028195
Batch  61  loss:  0.00021672795992344618
Batch  71  loss:  0.0002053221978712827
Batch  81  loss:  0.00019303456065244973
Batch  91  loss:  0.00016129376308526844
Batch  101  loss:  0.00017619047139305621
Batch  111  loss:  0.00015207170508801937
Batch  121  loss:  0.000239611224969849
Batch  131  loss:  0.00012904773757327348
Batch  141  loss:  0.00017170279170386493
Batch  151  loss:  0.00034951194538734853
Batch  161  loss:  0.00024581726756878197
Batch  171  loss:  0.00016191607574000955
Batch  181  loss:  0.00016726783360354602
Batch  191  loss:  0.00031248299637809396
Validation on real data: 
LOSS supervised-train 0.0002032410560059361, valid 0.0001440529158571735
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00034404578036628664
Batch  11  loss:  0.0002496838860679418
Batch  21  loss:  0.00024118204601109028
Batch  31  loss:  0.00015930287190712988
Batch  41  loss:  0.0003138211905024946
Batch  51  loss:  0.00022402542526833713
Batch  61  loss:  0.00019219152454752475
Batch  71  loss:  0.00027680135099217296
Batch  81  loss:  0.00014303797797765583
Batch  91  loss:  0.00014615801046602428
Batch  101  loss:  0.00016086956020444632
Batch  111  loss:  0.00021917695994488895
Batch  121  loss:  0.00020639292779378593
Batch  131  loss:  0.00014534263755194843
Batch  141  loss:  0.00013715162640437484
Batch  151  loss:  0.00030945707112550735
Batch  161  loss:  0.00019669758330564946
Batch  171  loss:  0.00017829961143434048
Batch  181  loss:  0.00021005012968089432
Batch  191  loss:  0.00024574302369728684
Validation on real data: 
LOSS supervised-train 0.00020479605278524106, valid 0.00023803135263733566
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0002960581914521754
Batch  11  loss:  0.00020703610789496452
Batch  21  loss:  0.00025233483756892383
Batch  31  loss:  0.0001636764354771003
Batch  41  loss:  0.0002962693979497999
Batch  51  loss:  0.0001885398814920336
Batch  61  loss:  0.00023226087796501815
Batch  71  loss:  0.00023028561554383487
Batch  81  loss:  0.00018312591419089586
Batch  91  loss:  0.00015595318109262735
Batch  101  loss:  0.00015735070337541401
Batch  111  loss:  0.00020877382485195994
Batch  121  loss:  0.00028026761719956994
Batch  131  loss:  0.00020513194613158703
Batch  141  loss:  0.00020228086214046925
Batch  151  loss:  0.00030017230892553926
Batch  161  loss:  0.0002004220150411129
Batch  171  loss:  0.00020411114383023232
Batch  181  loss:  0.00019989778229501098
Batch  191  loss:  0.00026848792913369834
Validation on real data: 
LOSS supervised-train 0.00021175539870455396, valid 0.00022428430384024978
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00046840921277180314
Batch  11  loss:  0.00019950953719671816
Batch  21  loss:  0.00025697657838463783
Batch  31  loss:  0.0002200874441768974
Batch  41  loss:  0.0002544499875511974
Batch  51  loss:  0.00020904075063299388
Batch  61  loss:  0.0001822460035327822
Batch  71  loss:  0.00028575255419127643
Batch  81  loss:  0.00012654552119784057
Batch  91  loss:  0.00019905598310288042
Batch  101  loss:  0.00022800522856414318
Batch  111  loss:  0.0001590922474861145
Batch  121  loss:  0.00023482766118831933
Batch  131  loss:  0.000212141836527735
Batch  141  loss:  0.00017863858374767005
Batch  151  loss:  0.00021235358144622296
Batch  161  loss:  0.0003287316649220884
Batch  171  loss:  0.00018821578123606741
Batch  181  loss:  0.00017542633577249944
Batch  191  loss:  0.00022394811094272882
Validation on real data: 
LOSS supervised-train 0.00021091003702167655, valid 0.00020634382963180542
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0002655584830790758
Batch  11  loss:  0.00019194006745237857
Batch  21  loss:  0.00031660913373343647
Batch  31  loss:  0.00018839295080397278
Batch  41  loss:  0.00024465631577186286
Batch  51  loss:  0.00018701153749134392
Batch  61  loss:  0.00016131757001858205
Batch  71  loss:  0.00018828170141205192
Batch  81  loss:  0.00017176836263388395
Batch  91  loss:  0.00018363840354140848
Batch  101  loss:  0.00021753954933956265
Batch  111  loss:  0.0002148593484889716
Batch  121  loss:  0.0001540222583571449
Batch  131  loss:  0.0002044233842752874
Batch  141  loss:  0.00018473704403731972
Batch  151  loss:  0.00029398134211078286
Batch  161  loss:  0.00022119234199635684
Batch  171  loss:  0.00015099353913683444
Batch  181  loss:  0.00016776446136645973
Batch  191  loss:  0.00017819732602220029
Validation on real data: 
LOSS supervised-train 0.00019500387341395253, valid 0.00023412908194586635
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00032466280390508473
Batch  11  loss:  0.00017540305270813406
Batch  21  loss:  0.00023862805392127484
Batch  31  loss:  0.00011288251698715612
Batch  41  loss:  0.0002968701592180878
Batch  51  loss:  0.00016936096653807908
Batch  61  loss:  0.00022413849364966154
Batch  71  loss:  0.00026551054907031357
Batch  81  loss:  0.00013239987310953438
Batch  91  loss:  0.00021699849457945675
Batch  101  loss:  0.00025590808945707977
Batch  111  loss:  0.00018894340610131621
Batch  121  loss:  0.00023616632097400725
Batch  131  loss:  0.00018134087440557778
Batch  141  loss:  0.00016487501852679998
Batch  151  loss:  0.00032653368543833494
Batch  161  loss:  0.00020522199338302016
Batch  171  loss:  0.0001502086961409077
Batch  181  loss:  0.00011874364281538874
Batch  191  loss:  0.00020375374879222363
Validation on real data: 
LOSS supervised-train 0.00019708279505721294, valid 0.00016294067609123886
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0003226909029763192
Batch  11  loss:  0.0001455548917874694
Batch  21  loss:  0.00023644759494345635
Batch  31  loss:  0.00011467551667010412
Batch  41  loss:  0.00026433251332491636
Batch  51  loss:  0.00016614887863397598
Batch  61  loss:  0.0002407752617727965
Batch  71  loss:  0.00022819533478468657
Batch  81  loss:  0.00013078797201160342
Batch  91  loss:  0.00013861202751286328
Batch  101  loss:  0.00018440412532072514
Batch  111  loss:  0.00023534383217338473
Batch  121  loss:  0.00026818327023647726
Batch  131  loss:  0.00032272961107082665
Batch  141  loss:  0.00016579982184339315
Batch  151  loss:  0.0003296000068075955
Batch  161  loss:  0.00015847916074562818
Batch  171  loss:  0.00013657363888341933
Batch  181  loss:  0.0001548218569951132
Batch  191  loss:  0.0001950272126123309
Validation on real data: 
LOSS supervised-train 0.00019788778921792983, valid 0.00018039540736936033
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0002968679182231426
Batch  11  loss:  0.00018514256225898862
Batch  21  loss:  0.00024617495364509523
Batch  31  loss:  0.00013077280891593546
Batch  41  loss:  0.0003391061909496784
Batch  51  loss:  0.0002094011870212853
Batch  61  loss:  0.00022017474111635238
Batch  71  loss:  0.0002823064278345555
Batch  81  loss:  0.00016461613995488733
Batch  91  loss:  0.00028234333149157465
Batch  101  loss:  0.00014756235759705305
Batch  111  loss:  0.00025009072851389647
Batch  121  loss:  0.00018262390221934766
Batch  131  loss:  0.0001359868620056659
Batch  141  loss:  0.00014200853183865547
Batch  151  loss:  0.00021392654161900282
Batch  161  loss:  0.00016739107377361506
Batch  171  loss:  0.00015654433809686452
Batch  181  loss:  0.00016326432523783296
Batch  191  loss:  0.00024290330475196242
Validation on real data: 
LOSS supervised-train 0.00019656718883197754, valid 0.00017421989468857646
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00020646110351663083
Batch  11  loss:  0.0002254288992844522
Batch  21  loss:  0.00019867751689162105
Batch  31  loss:  0.00017180769646074623
Batch  41  loss:  0.00031107262475416064
Batch  51  loss:  0.00020062840485479683
Batch  61  loss:  0.00024994785781018436
Batch  71  loss:  0.0001795872231014073
Batch  81  loss:  0.0002025649300776422
Batch  91  loss:  0.00019316363614052534
Batch  101  loss:  0.00014654341794084758
Batch  111  loss:  0.00015556845755781978
Batch  121  loss:  0.0002434093039482832
Batch  131  loss:  0.00024586942163296044
Batch  141  loss:  0.00019421470642555505
Batch  151  loss:  0.0002047721645794809
Batch  161  loss:  0.00017892538744490594
Batch  171  loss:  0.00015392278146464378
Batch  181  loss:  0.00014567424659617245
Batch  191  loss:  0.0002903242420870811
Validation on real data: 
LOSS supervised-train 0.000193950961183873, valid 0.00025748444022610784
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00034877605503425
Batch  11  loss:  0.00029700101003982127
Batch  21  loss:  0.0003045179764740169
Batch  31  loss:  0.00015385550796054304
Batch  41  loss:  0.0002851918980013579
Batch  51  loss:  0.00020560220582410693
Batch  61  loss:  0.0002441813994664699
Batch  71  loss:  0.00021728596766479313
Batch  81  loss:  0.00017530554032418877
Batch  91  loss:  0.0001914247841341421
Batch  101  loss:  0.00013370202213991433
Batch  111  loss:  0.00016995322948787361
Batch  121  loss:  0.00024125806521624327
Batch  131  loss:  0.00020450133888516575
Batch  141  loss:  0.0001823304919525981
Batch  151  loss:  0.00025768703198991716
Batch  161  loss:  0.00021763055701740086
Batch  171  loss:  0.00018308970902580768
Batch  181  loss:  0.0001623012503841892
Batch  191  loss:  0.00023506325669586658
Validation on real data: 
LOSS supervised-train 0.00019538247470336502, valid 0.0002769863640423864
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0002896348596550524
Batch  11  loss:  0.0001700029824860394
Batch  21  loss:  0.0002716366434469819
Batch  31  loss:  0.0001180603212560527
Batch  41  loss:  0.00023299506574403495
Batch  51  loss:  0.00015250688011292368
Batch  61  loss:  0.0002021992113441229
Batch  71  loss:  0.00018804011051543057
Batch  81  loss:  0.0001889242703327909
Batch  91  loss:  0.00019281418644823134
Batch  101  loss:  0.00013214386126492172
Batch  111  loss:  0.00013257868704386055
Batch  121  loss:  0.00020903497352264822
Batch  131  loss:  0.0001644566364120692
Batch  141  loss:  0.00014707725495100021
Batch  151  loss:  0.000303253618767485
Batch  161  loss:  0.000203195697395131
Batch  171  loss:  0.0001605001016287133
Batch  181  loss:  0.00015257768973242491
Batch  191  loss:  0.0001706567854853347
Validation on real data: 
LOSS supervised-train 0.00018627616813319037, valid 0.00023303626221604645
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00033460103441029787
Batch  11  loss:  0.0002333597803954035
Batch  21  loss:  0.00016957831394392997
Batch  31  loss:  0.00012896073167212307
Batch  41  loss:  0.00026201840955764055
Batch  51  loss:  0.00019167884602211416
Batch  61  loss:  0.0002710582921281457
Batch  71  loss:  0.00020917385700158775
Batch  81  loss:  0.0001481948420405388
Batch  91  loss:  0.000195011860341765
Batch  101  loss:  0.0001547753345221281
Batch  111  loss:  0.0001216201635543257
Batch  121  loss:  0.00018376561638433486
Batch  131  loss:  0.00015288400754798204
Batch  141  loss:  0.00013969671272207052
Batch  151  loss:  0.0002539988490752876
Batch  161  loss:  0.00020552295609377325
Batch  171  loss:  0.0001279095304198563
Batch  181  loss:  0.0001203325082315132
Batch  191  loss:  0.00017145324090961367
Validation on real data: 
LOSS supervised-train 0.00019019213876163123, valid 0.00020951684564352036
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00022757785336580127
Batch  11  loss:  0.00019885387155227363
Batch  21  loss:  0.0002103549923049286
Batch  31  loss:  0.000156892798258923
Batch  41  loss:  0.00033238163450732827
Batch  51  loss:  0.00017184171883855015
Batch  61  loss:  0.00019569159485399723
Batch  71  loss:  0.0002347938425373286
Batch  81  loss:  0.00017262685287278146
Batch  91  loss:  0.0002094148803735152
Batch  101  loss:  0.0001854386500781402
Batch  111  loss:  0.0001891079155029729
Batch  121  loss:  0.0002092578652082011
Batch  131  loss:  0.00020797776232939214
Batch  141  loss:  0.00016256075468845665
Batch  151  loss:  0.00021198013564571738
Batch  161  loss:  0.00021513781393878162
Batch  171  loss:  0.00015832186909392476
Batch  181  loss:  0.00015641409845557064
Batch  191  loss:  0.0002610898809507489
Validation on real data: 
LOSS supervised-train 0.00018723184242844582, valid 0.00021952565293759108
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00028785664471797645
Batch  11  loss:  0.00019190696184523404
Batch  21  loss:  0.00022711411293130368
Batch  31  loss:  0.00012660394713748246
Batch  41  loss:  0.0002784438256639987
Batch  51  loss:  0.00019757721747737378
Batch  61  loss:  0.00018441113934386522
Batch  71  loss:  0.00022073133732192218
Batch  81  loss:  0.000164704819326289
Batch  91  loss:  0.00018155727593693882
Batch  101  loss:  0.00020587972539942712
Batch  111  loss:  0.0001804708590498194
Batch  121  loss:  0.00021825352450832725
Batch  131  loss:  0.0003049224615097046
Batch  141  loss:  0.0001995089987758547
Batch  151  loss:  0.0003563434584066272
Batch  161  loss:  0.00021352148905862123
Batch  171  loss:  0.0001586975995451212
Batch  181  loss:  0.0002135165559593588
Batch  191  loss:  0.0002355273172724992
Validation on real data: 
LOSS supervised-train 0.00019514854939188807, valid 0.00021469310740940273
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00027861000853590667
Batch  11  loss:  0.00017915347416419536
Batch  21  loss:  0.0003426318580750376
Batch  31  loss:  0.00015364005230367184
Batch  41  loss:  0.00029836175963282585
Batch  51  loss:  0.00016867149679455906
Batch  61  loss:  0.00017523507995065302
Batch  71  loss:  0.00018997145525645465
Batch  81  loss:  0.00020024599507451057
Batch  91  loss:  0.00015360656834673136
Batch  101  loss:  0.00010458017641212791
Batch  111  loss:  0.00015224590606521815
Batch  121  loss:  0.00017973801004700363
Batch  131  loss:  0.00021636731980834156
Batch  141  loss:  0.00016498267359565943
Batch  151  loss:  0.00022131772129796445
Batch  161  loss:  0.00021362440020311624
Batch  171  loss:  0.00016933880397118628
Batch  181  loss:  0.00017916677461471409
Batch  191  loss:  0.00018817692762240767
Validation on real data: 
LOSS supervised-train 0.00018499445359339007, valid 0.0002478526730556041
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00026008865097537637
Batch  11  loss:  0.00019774898828472942
Batch  21  loss:  0.00015053740935400128
Batch  31  loss:  0.0001248558983206749
Batch  41  loss:  0.0002697138988878578
Batch  51  loss:  0.00012785861326847225
Batch  61  loss:  0.00018319360970053822
Batch  71  loss:  0.00021691303118132055
Batch  81  loss:  0.0001298313873121515
Batch  91  loss:  0.00021402012498583645
Batch  101  loss:  0.00016274655354209244
Batch  111  loss:  0.00017533890786580741
Batch  121  loss:  0.000196418899577111
Batch  131  loss:  0.0002277298190165311
Batch  141  loss:  0.00015130727842915803
Batch  151  loss:  0.00027466649771668017
Batch  161  loss:  0.00014576215471606702
Batch  171  loss:  0.00014905181888025254
Batch  181  loss:  0.0001964453695109114
Batch  191  loss:  0.0002446891739964485
Validation on real data: 
LOSS supervised-train 0.00018225830295705238, valid 0.00016925527597777545
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00031614876934327185
Batch  11  loss:  0.0002818003995344043
Batch  21  loss:  0.0002962442231364548
Batch  31  loss:  0.00016677069652359933
Batch  41  loss:  0.00026813134900294244
Batch  51  loss:  0.00013642710109706968
Batch  61  loss:  0.00020922653493471444
Batch  71  loss:  0.0001916056062327698
Batch  81  loss:  0.0002481522678863257
Batch  91  loss:  0.0001661582791712135
Batch  101  loss:  0.00016162903921213
Batch  111  loss:  0.00017227786884177476
Batch  121  loss:  0.0002259325556224212
Batch  131  loss:  0.00013932998990640044
Batch  141  loss:  0.00014536033268086612
Batch  151  loss:  0.00025110383285209537
Batch  161  loss:  0.00019990428700111806
Batch  171  loss:  0.0001378610759275034
Batch  181  loss:  0.0001237443502759561
Batch  191  loss:  0.00017803437367547303
Validation on real data: 
LOSS supervised-train 0.00018846031533030329, valid 0.0002041373518295586
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00024445378221571445
Batch  11  loss:  0.0002173928078263998
Batch  21  loss:  0.00018027573241852224
Batch  31  loss:  0.00014068686868995428
Batch  41  loss:  0.00026415998581796885
Batch  51  loss:  0.0001401501940563321
Batch  61  loss:  0.0002341226499993354
Batch  71  loss:  0.0001904862729134038
Batch  81  loss:  0.0003029135405085981
Batch  91  loss:  0.00014835834736004472
Batch  101  loss:  0.00015537541185040027
Batch  111  loss:  0.00018044507305603474
Batch  121  loss:  0.00019633643387351185
Batch  131  loss:  0.00016229090397246182
Batch  141  loss:  0.00013496445899363607
Batch  151  loss:  0.0002317197941010818
Batch  161  loss:  0.00020196179684717208
Batch  171  loss:  0.00018168659880757332
Batch  181  loss:  0.0001332983374595642
Batch  191  loss:  0.00019093485025223345
Validation on real data: 
LOSS supervised-train 0.00017731997144437628, valid 0.00018646742682904005
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00031475789728574455
Batch  11  loss:  0.0001794209238141775
Batch  21  loss:  0.0002519851259421557
Batch  31  loss:  0.00011065250873798504
Batch  41  loss:  0.00030032620998099446
Batch  51  loss:  0.0001932987943291664
Batch  61  loss:  0.0002424234990030527
Batch  71  loss:  0.0002181979943998158
Batch  81  loss:  0.00019125576363876462
Batch  91  loss:  0.00018791081674862653
Batch  101  loss:  0.00016097354819066823
Batch  111  loss:  0.0001381259353365749
Batch  121  loss:  0.0001985343697015196
Batch  131  loss:  0.00017226363706868142
Batch  141  loss:  0.00011620097211562097
Batch  151  loss:  0.00026887032436206937
Batch  161  loss:  0.0001857077149907127
Batch  171  loss:  0.00014054086932446808
Batch  181  loss:  0.00012818376126233488
Batch  191  loss:  0.00023727136431261897
Validation on real data: 
LOSS supervised-train 0.0001798902631708188, valid 0.0002546522009652108
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0002951278875116259
Batch  11  loss:  0.00018492300296202302
Batch  21  loss:  0.00020900725212413818
Batch  31  loss:  0.00013296565157361329
Batch  41  loss:  0.0002539471606723964
Batch  51  loss:  0.00012767163570970297
Batch  61  loss:  0.00021265681425575167
Batch  71  loss:  0.00021629704860970378
Batch  81  loss:  0.00019572376913856715
Batch  91  loss:  0.00019270560005679727
Batch  101  loss:  0.00011260865721851587
Batch  111  loss:  0.00019098128541372716
Batch  121  loss:  0.000164529075846076
Batch  131  loss:  0.00022818292200099677
Batch  141  loss:  0.0001705547038000077
Batch  151  loss:  0.0002672080881893635
Batch  161  loss:  0.00018526996427681297
Batch  171  loss:  0.00017656742420513183
Batch  181  loss:  0.0001397229643771425
Batch  191  loss:  0.0002276024897582829
Validation on real data: 
LOSS supervised-train 0.00018631479342730018, valid 0.00022111051657702774
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0002562756126280874
Batch  11  loss:  0.00017958095122594386
Batch  21  loss:  0.00023792525462340564
Batch  31  loss:  0.00015925844490993768
Batch  41  loss:  0.00021643760555889457
Batch  51  loss:  0.00023287975636776537
Batch  61  loss:  0.0001949635916389525
Batch  71  loss:  0.00021306412236299366
Batch  81  loss:  0.00017979741096496582
Batch  91  loss:  0.000148788167280145
Batch  101  loss:  0.00017779147310648113
Batch  111  loss:  0.00010866332740988582
Batch  121  loss:  0.0001934061583597213
Batch  131  loss:  0.00014273611304815859
Batch  141  loss:  0.00012249196879565716
Batch  151  loss:  0.00025504958466626704
Batch  161  loss:  0.00014030360034666955
Batch  171  loss:  0.00012510127271525562
Batch  181  loss:  0.0001500501239206642
Batch  191  loss:  0.0001590326428413391
Validation on real data: 
LOSS supervised-train 0.00017895361423143186, valid 0.00020790116104763
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0002745864912867546
Batch  11  loss:  0.0001988895528484136
Batch  21  loss:  0.00025251737679354846
Batch  31  loss:  0.00013801947352476418
Batch  41  loss:  0.00028488715179264545
Batch  51  loss:  0.00017408993153367192
Batch  61  loss:  0.00019285368034616113
Batch  71  loss:  0.00015668566629756242
Batch  81  loss:  0.00018228890257887542
Batch  91  loss:  0.00016890755796339363
Batch  101  loss:  0.00016853473789524287
Batch  111  loss:  0.0001812929258449003
Batch  121  loss:  0.0001768032816471532
Batch  131  loss:  0.00016158667858690023
Batch  141  loss:  0.00016059514018706977
Batch  151  loss:  0.00025742300204001367
Batch  161  loss:  0.00019164744298905134
Batch  171  loss:  0.00020111196499783546
Batch  181  loss:  0.0002180328156100586
Batch  191  loss:  0.00021416935487650335
Validation on real data: 
LOSS supervised-train 0.00017881862517242551, valid 0.00021751878375653177
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00017296230362262577
Batch  11  loss:  0.0002316990285180509
Batch  21  loss:  0.0002469559258315712
Batch  31  loss:  0.00015909734065644443
Batch  41  loss:  0.00022122044174466282
Batch  51  loss:  0.00019349114154465497
Batch  61  loss:  0.00020966544980183244
Batch  71  loss:  0.00018719557556323707
Batch  81  loss:  0.00016421593318227679
Batch  91  loss:  0.00017603136075194925
Batch  101  loss:  0.00017711221880745143
Batch  111  loss:  0.00015806820010766387
Batch  121  loss:  0.00015475084364879876
Batch  131  loss:  0.000174629531102255
Batch  141  loss:  0.00013485307863447815
Batch  151  loss:  0.0002823305840138346
Batch  161  loss:  0.0001371437538182363
Batch  171  loss:  0.00016981642693281174
Batch  181  loss:  0.00016519999189767987
Batch  191  loss:  0.00017275044228881598
Validation on real data: 
LOSS supervised-train 0.00017550601398397703, valid 0.00016843514458741993
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00021667855617124587
Batch  11  loss:  0.00017742927593644708
Batch  21  loss:  0.00016961540677584708
Batch  31  loss:  0.00015816837549209595
Batch  41  loss:  0.0002186149504268542
Batch  51  loss:  0.0001490830909460783
Batch  61  loss:  0.00021266813564579934
Batch  71  loss:  0.00018793433264363557
Batch  81  loss:  0.00019006257934961468
Batch  91  loss:  0.00015523750334978104
Batch  101  loss:  0.0001512277958681807
Batch  111  loss:  0.0001496002805652097
Batch  121  loss:  0.0001537311909487471
Batch  131  loss:  0.00018701102817431092
Batch  141  loss:  0.00015805262955836952
Batch  151  loss:  0.0002460611576680094
Batch  161  loss:  0.00016428284288849682
Batch  171  loss:  0.0001289047213504091
Batch  181  loss:  0.0001407623931299895
Batch  191  loss:  0.00017810959252528846
Validation on real data: 
LOSS supervised-train 0.0001722206550766714, valid 0.00016969849821180105
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00018353882478550076
Batch  11  loss:  0.0001637965178815648
Batch  21  loss:  0.000172186570125632
Batch  31  loss:  0.0001158912418759428
Batch  41  loss:  0.000300566665828228
Batch  51  loss:  0.0001562066754559055
Batch  61  loss:  0.00018463190644979477
Batch  71  loss:  0.0001902504445752129
Batch  81  loss:  0.00014892732724547386
Batch  91  loss:  0.00016236129158642143
Batch  101  loss:  0.000155923655256629
Batch  111  loss:  0.0001297598355449736
Batch  121  loss:  0.00017559922707732767
Batch  131  loss:  0.00017463475523982197
Batch  141  loss:  0.00016617111396044493
Batch  151  loss:  0.0002051493793260306
Batch  161  loss:  0.0001573230983922258
Batch  171  loss:  0.0001337978319497779
Batch  181  loss:  0.00017163886514026672
Batch  191  loss:  0.00016029321704991162
Validation on real data: 
LOSS supervised-train 0.00016981905901047867, valid 0.0002215646527474746
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.00024782546097412705
Batch  11  loss:  0.00021257917978800833
Batch  21  loss:  0.00015975788119249046
Batch  31  loss:  8.773631998337805e-05
Batch  41  loss:  0.00025279982946813107
Batch  51  loss:  0.00016135122859850526
Batch  61  loss:  0.0001549888402223587
Batch  71  loss:  0.00016179043450392783
Batch  81  loss:  0.00013868777023162693
Batch  91  loss:  0.00012457690900191665
Batch  101  loss:  0.00016528381092939526
Batch  111  loss:  0.00017153966473415494
Batch  121  loss:  0.00020478836086113006
Batch  131  loss:  0.00015247375995386392
Batch  141  loss:  0.00014276082220021635
Batch  151  loss:  0.00017961577395908535
Batch  161  loss:  0.00018503305909689516
Batch  171  loss:  0.00019898288883268833
Batch  181  loss:  0.00017842939996626228
Batch  191  loss:  0.00016863080963958055
Validation on real data: 
LOSS supervised-train 0.00017238390661077573, valid 0.00016219068493228406
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00021554790146183223
Batch  11  loss:  0.00022025014914106578
Batch  21  loss:  0.00020843499805778265
Batch  31  loss:  0.00014319116598926485
Batch  41  loss:  0.00024726567789912224
Batch  51  loss:  0.0001245366729563102
Batch  61  loss:  0.00019848629017360508
Batch  71  loss:  0.0002617201243992895
Batch  81  loss:  0.00010452057176735252
Batch  91  loss:  0.0001846875238697976
Batch  101  loss:  0.0001470515999244526
Batch  111  loss:  0.00014506257139146328
Batch  121  loss:  0.00015783225535415113
Batch  131  loss:  0.00019567894923966378
Batch  141  loss:  0.0001341675961157307
Batch  151  loss:  0.0002239266032120213
Batch  161  loss:  0.00016495965246576816
Batch  171  loss:  0.00013256109377834946
Batch  181  loss:  0.0001281739241676405
Batch  191  loss:  0.00015544990310445428
Validation on real data: 
LOSS supervised-train 0.00016494644372869516, valid 0.0001396127772750333
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00022771164367441088
Batch  11  loss:  0.00019583478569984436
Batch  21  loss:  0.0002158787101507187
Batch  31  loss:  0.00017757888417690992
Batch  41  loss:  0.00023467007849831134
Batch  51  loss:  0.00017060706159099936
Batch  61  loss:  0.0001678031840128824
Batch  71  loss:  0.0002207675133831799
Batch  81  loss:  0.00011575669486774132
Batch  91  loss:  0.00016514721210114658
Batch  101  loss:  0.00014232737885322422
Batch  111  loss:  0.00012568061356432736
Batch  121  loss:  0.00013794888218399137
Batch  131  loss:  0.00021728813590016216
Batch  141  loss:  0.00017059833044186234
Batch  151  loss:  0.00020652631064876914
Batch  161  loss:  0.00014033759362064302
Batch  171  loss:  9.445152682019398e-05
Batch  181  loss:  0.00015451502986252308
Batch  191  loss:  0.00018723012180998921
Validation on real data: 
LOSS supervised-train 0.00016434330609627068, valid 0.00019751368381548673
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00025186428683809936
Batch  11  loss:  0.00019239442190155387
Batch  21  loss:  0.00018268749408889562
Batch  31  loss:  0.00022470798285212368
Batch  41  loss:  0.00026141313719563186
Batch  51  loss:  0.00012422508734744042
Batch  61  loss:  0.0001788601657608524
Batch  71  loss:  0.00017034464690368623
Batch  81  loss:  0.00012070426600985229
Batch  91  loss:  0.00013847295485902578
Batch  101  loss:  0.0001443106884835288
Batch  111  loss:  0.0001572157343616709
Batch  121  loss:  0.0001509033318143338
Batch  131  loss:  0.00025721665588207543
Batch  141  loss:  0.00014680797175969929
Batch  151  loss:  0.00024351570755243301
Batch  161  loss:  0.00024002014833968133
Batch  171  loss:  0.00013289166963659227
Batch  181  loss:  0.00012657266051974148
Batch  191  loss:  0.0002329748822376132
Validation on real data: 
LOSS supervised-train 0.0001685897117931745, valid 0.00018441891006659716
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0001979916269192472
Batch  11  loss:  0.0001691701472736895
Batch  21  loss:  0.00020957687229383737
Batch  31  loss:  0.0001097178683266975
Batch  41  loss:  0.0002604382752906531
Batch  51  loss:  0.00013328077329788357
Batch  61  loss:  0.00019199351663701236
Batch  71  loss:  0.0001435380836483091
Batch  81  loss:  0.00011434048064984381
Batch  91  loss:  0.0001589114690432325
Batch  101  loss:  0.00015365093713626266
Batch  111  loss:  0.000147821701830253
Batch  121  loss:  0.0001555356284370646
Batch  131  loss:  0.00013627004227600992
Batch  141  loss:  0.00015694816829636693
Batch  151  loss:  0.00021575528080575168
Batch  161  loss:  0.00019378584693185985
Batch  171  loss:  0.00014328115503303707
Batch  181  loss:  0.00014550115156453103
Batch  191  loss:  0.0001534674083814025
Validation on real data: 
LOSS supervised-train 0.00016404710007918765, valid 0.00016828073421493173
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00023586078896187246
Batch  11  loss:  0.00018628776888363063
Batch  21  loss:  0.0001655274973018095
Batch  31  loss:  9.267514542443678e-05
Batch  41  loss:  0.00020205711189191788
Batch  51  loss:  0.00013934291200712323
Batch  61  loss:  0.00018761615501716733
Batch  71  loss:  0.00016268152103293687
Batch  81  loss:  0.00013490323908627033
Batch  91  loss:  0.00012772569607477635
Batch  101  loss:  0.00011678683949867263
Batch  111  loss:  0.00011861553502967581
Batch  121  loss:  0.00015596575394738466
Batch  131  loss:  0.00013883762585464865
Batch  141  loss:  0.00013611117901746184
Batch  151  loss:  0.00024033605586737394
Batch  161  loss:  0.00017256710270885378
Batch  171  loss:  0.00015995214926078916
Batch  181  loss:  0.0001793665433069691
Batch  191  loss:  0.00018907879712060094
Validation on real data: 
LOSS supervised-train 0.00016399031916080275, valid 0.00019558760686777532
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00018788724264595658
Batch  11  loss:  0.00017090719484258443
Batch  21  loss:  0.00018052624363917857
Batch  31  loss:  0.00014904752606526017
Batch  41  loss:  0.0002387329441262409
Batch  51  loss:  0.00012679901556111872
Batch  61  loss:  0.0002002580586122349
Batch  71  loss:  0.00016548823623452336
Batch  81  loss:  0.00017085301806218922
Batch  91  loss:  0.00014516680676024407
Batch  101  loss:  0.00015465343312826008
Batch  111  loss:  0.00017853594908956438
Batch  121  loss:  0.00018502511375118047
Batch  131  loss:  0.00012039727880619466
Batch  141  loss:  0.00014844576071482152
Batch  151  loss:  0.00027422161656431854
Batch  161  loss:  0.0001525379193481058
Batch  171  loss:  0.0001299990399274975
Batch  181  loss:  0.00013709694030694664
Batch  191  loss:  0.0001228981273015961
Validation on real data: 
LOSS supervised-train 0.0001607023155884235, valid 0.00015899096615612507
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00025151591398753226
Batch  11  loss:  0.00015320730744861066
Batch  21  loss:  0.00017558423860464245
Batch  31  loss:  0.00010366290371166542
Batch  41  loss:  0.00019562769739422947
Batch  51  loss:  0.0001464537635911256
Batch  61  loss:  0.00010915689199464396
Batch  71  loss:  0.00019762567535508424
Batch  81  loss:  0.00014432263560593128
Batch  91  loss:  0.0001411987468600273
Batch  101  loss:  0.00013309629866853356
Batch  111  loss:  0.0001403884234605357
Batch  121  loss:  0.00019505400268826634
Batch  131  loss:  0.00012714401236735284
Batch  141  loss:  0.00012404560402501374
Batch  151  loss:  0.00020627419871743768
Batch  161  loss:  0.00016316014807671309
Batch  171  loss:  0.0001716037659207359
Batch  181  loss:  0.00019092342699877918
Batch  191  loss:  0.000181552954018116
Validation on real data: 
LOSS supervised-train 0.00016572687760344707, valid 0.00016234509530477226
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0001869227853603661
Batch  11  loss:  0.00020453293109312654
Batch  21  loss:  0.00014031580940354615
Batch  31  loss:  0.00014463832485489547
Batch  41  loss:  0.00020656788547057658
Batch  51  loss:  0.00015574076678603888
Batch  61  loss:  0.0001516048505436629
Batch  71  loss:  0.00022293321671895683
Batch  81  loss:  0.0001273556990781799
Batch  91  loss:  0.0001155607751570642
Batch  101  loss:  0.00016349014185834676
Batch  111  loss:  0.00015968974912539124
Batch  121  loss:  0.0001330315280938521
Batch  131  loss:  0.00021488532365765423
Batch  141  loss:  0.0001380588801112026
Batch  151  loss:  0.000236636507906951
Batch  161  loss:  0.00019599478400778025
Batch  171  loss:  0.00014096852100919932
Batch  181  loss:  0.0001672990620136261
Batch  191  loss:  0.00018032948719337583
Validation on real data: 
LOSS supervised-train 0.00016537601324671415, valid 0.0001179249447886832
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00024914168170653284
Batch  11  loss:  0.000181335853994824
Batch  21  loss:  0.00017929977911990136
Batch  31  loss:  0.00011500803520902991
Batch  41  loss:  0.00027121370658278465
Batch  51  loss:  0.00013853651762474328
Batch  61  loss:  0.0001511922018835321
Batch  71  loss:  0.00017583473527338356
Batch  81  loss:  0.0001192964191432111
Batch  91  loss:  0.00011231280950596556
Batch  101  loss:  0.00016173343465197831
Batch  111  loss:  0.0001009130064630881
Batch  121  loss:  0.00016156543279066682
Batch  131  loss:  0.0001452148426324129
Batch  141  loss:  0.00016555030015297234
Batch  151  loss:  0.00024925291654653847
Batch  161  loss:  0.00013197991938795894
Batch  171  loss:  0.00011815754260169342
Batch  181  loss:  0.00011179423017892987
Batch  191  loss:  0.00015874142991378903
Validation on real data: 
LOSS supervised-train 0.00015874961241934215, valid 0.00013719272101297975
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00019843055633828044
Batch  11  loss:  0.00013583489635493606
Batch  21  loss:  0.00022391298261936754
Batch  31  loss:  0.00013352202950045466
Batch  41  loss:  0.00022836943389847875
Batch  51  loss:  0.0001407779345754534
Batch  61  loss:  0.00012221021461300552
Batch  71  loss:  0.0001551652530906722
Batch  81  loss:  0.0001193970674648881
Batch  91  loss:  0.0001600961695658043
Batch  101  loss:  0.00012835001689381897
Batch  111  loss:  0.00012656413309741765
Batch  121  loss:  0.000148106919368729
Batch  131  loss:  0.00013634850620292127
Batch  141  loss:  0.00010342134919483215
Batch  151  loss:  0.0002583328459877521
Batch  161  loss:  0.00014883826952427626
Batch  171  loss:  0.00010321031004423276
Batch  181  loss:  0.00014721222396474332
Batch  191  loss:  0.00018599681789055467
Validation on real data: 
LOSS supervised-train 0.00015899466474365907, valid 0.00022803954198025167
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00021802930859848857
Batch  11  loss:  0.00019549451826605946
Batch  21  loss:  0.0002891116600949317
Batch  31  loss:  0.0001485506072640419
Batch  41  loss:  0.0001805385109037161
Batch  51  loss:  0.00012856991088483483
Batch  61  loss:  0.00018225920211989433
Batch  71  loss:  0.0001887998660095036
Batch  81  loss:  0.00012402092397678643
Batch  91  loss:  0.0001355756539851427
Batch  101  loss:  0.0001331154053332284
Batch  111  loss:  0.00012025207979604602
Batch  121  loss:  0.00013568038411904126
Batch  131  loss:  0.00017479150847066194
Batch  141  loss:  0.00014495539653580636
Batch  151  loss:  0.00021799681417178363
Batch  161  loss:  0.00018900108989328146
Batch  171  loss:  0.00013074689195491374
Batch  181  loss:  0.00010676596139091998
Batch  191  loss:  0.000203477029572241
Validation on real data: 
LOSS supervised-train 0.00016113991168822394, valid 0.0002020220854319632
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0002109285705955699
Batch  11  loss:  0.00019034869910683483
Batch  21  loss:  0.00021950843802187592
Batch  31  loss:  0.00011147803161293268
Batch  41  loss:  0.00021254480816423893
Batch  51  loss:  0.0001760601589921862
Batch  61  loss:  0.00019166966376360506
Batch  71  loss:  0.00020003382815048099
Batch  81  loss:  0.00013650173787027597
Batch  91  loss:  0.00014043960254639387
Batch  101  loss:  0.00015324822743423283
Batch  111  loss:  0.0001980217348318547
Batch  121  loss:  0.00018711324082687497
Batch  131  loss:  0.0001544533297419548
Batch  141  loss:  0.00012011879152851179
Batch  151  loss:  0.00018317515787202865
Batch  161  loss:  0.00013961942750029266
Batch  171  loss:  0.00016410875832661986
Batch  181  loss:  0.00013615885109174997
Batch  191  loss:  0.00016598105139564723
Validation on real data: 
LOSS supervised-train 0.0001585227972100256, valid 0.00016977242194116116
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0001823081256588921
Batch  11  loss:  0.00015136809088289738
Batch  21  loss:  0.00016672511992510408
Batch  31  loss:  0.0001349332887912169
Batch  41  loss:  0.00020887322898488492
Batch  51  loss:  0.0001859778567450121
Batch  61  loss:  0.00016777189739514142
Batch  71  loss:  0.00021320635278243572
Batch  81  loss:  0.00014060194371268153
Batch  91  loss:  0.00013700390991289169
Batch  101  loss:  0.00013555372424889356
Batch  111  loss:  0.0001440148480469361
Batch  121  loss:  0.0001585069257998839
Batch  131  loss:  0.00016041593335103244
Batch  141  loss:  0.00010250839841319248
Batch  151  loss:  0.00023942616826388985
Batch  161  loss:  0.00012895431427750736
Batch  171  loss:  0.00012347129813861102
Batch  181  loss:  0.0001587842416483909
Batch  191  loss:  0.00016329613572452217
Validation on real data: 
LOSS supervised-train 0.00016534896290977487, valid 0.00015461737348232418
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00020678454893641174
Batch  11  loss:  0.00017435546033084393
Batch  21  loss:  0.0001505792315583676
Batch  31  loss:  0.0001406356313964352
Batch  41  loss:  0.0002357709890929982
Batch  51  loss:  0.00015744073607493192
Batch  61  loss:  0.00018640344205778092
Batch  71  loss:  0.00017030839808285236
Batch  81  loss:  0.00010163556726183742
Batch  91  loss:  0.0001528400316601619
Batch  101  loss:  0.00013480571215040982
Batch  111  loss:  0.0001557861833134666
Batch  121  loss:  0.00017525139264762402
Batch  131  loss:  0.00015428285405505449
Batch  141  loss:  0.00015100989548955113
Batch  151  loss:  0.00025157388881780207
Batch  161  loss:  0.00018130525131709874
Batch  171  loss:  0.0001354032283416018
Batch  181  loss:  0.00011638973228400573
Batch  191  loss:  0.00015341260586865246
Validation on real data: 
LOSS supervised-train 0.0001576828620454762, valid 0.00016196415526792407
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  motorcycle ; Model ID: 481f7a57a12517e0fe1b9fad6c90c7bf
--------------------
Training baseline regression model:  2022-03-30 07:24:01.879855
Detector:  point_transformer
Object:  motorcycle
--------------------
device is  cuda
--------------------
Number of trainable parameters:  900778
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.11482129991054535
Batch  11  loss:  0.07807286083698273
Batch  21  loss:  0.06414753943681717
Batch  31  loss:  0.037766359746456146
Batch  41  loss:  0.030907249078154564
Batch  51  loss:  0.05120300129055977
Batch  61  loss:  0.019433483481407166
Batch  71  loss:  0.01367203425616026
Batch  81  loss:  0.020255552604794502
Batch  91  loss:  0.03275963291525841
Batch  101  loss:  0.02551286667585373
Batch  111  loss:  0.030928080901503563
Batch  121  loss:  0.01053119357675314
Batch  131  loss:  0.008200305514037609
Batch  141  loss:  0.00774572417140007
Batch  151  loss:  0.005858903285115957
Batch  161  loss:  0.009459184482693672
Batch  171  loss:  0.006361824925988913
Batch  181  loss:  0.004286095034331083
Batch  191  loss:  0.005828877445310354
Validation on real data: 
LOSS supervised-train 0.023958125454373657, valid 0.0024962874595075846
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.004744098987430334
Batch  11  loss:  0.0021829770412296057
Batch  21  loss:  0.0018694326281547546
Batch  31  loss:  0.0016634559724479914
Batch  41  loss:  0.0033311049919575453
Batch  51  loss:  0.001855561276897788
Batch  61  loss:  0.002495397115126252
Batch  71  loss:  0.001983022317290306
Batch  81  loss:  0.003430092940106988
Batch  91  loss:  0.0025535474997013807
Batch  101  loss:  0.004653098527342081
Batch  111  loss:  0.004777468740940094
Batch  121  loss:  0.0038319809827953577
Batch  131  loss:  0.00527408579364419
Batch  141  loss:  0.0024671361315995455
Batch  151  loss:  0.004426960367709398
Batch  161  loss:  0.006410903763025999
Batch  171  loss:  0.00258132116869092
Batch  181  loss:  0.002529712626710534
Batch  191  loss:  0.002977781230583787
Validation on real data: 
LOSS supervised-train 0.0033582891640253367, valid 0.002387452404946089
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0021880108397454023
Batch  11  loss:  0.0021505081094801426
Batch  21  loss:  0.00262815342284739
Batch  31  loss:  0.0017608542693778872
Batch  41  loss:  0.0025859910529106855
Batch  51  loss:  0.001622116775251925
Batch  61  loss:  0.0019109544809907675
Batch  71  loss:  0.0012107652146369219
Batch  81  loss:  0.002994778100401163
Batch  91  loss:  0.002611504402011633
Batch  101  loss:  0.004459725692868233
Batch  111  loss:  0.0032534548081457615
Batch  121  loss:  0.002742229728028178
Batch  131  loss:  0.0033835230860859156
Batch  141  loss:  0.0012791642220690846
Batch  151  loss:  0.0029971867334097624
Batch  161  loss:  0.0032771422993391752
Batch  171  loss:  0.0018190705450251698
Batch  181  loss:  0.002670097863301635
Batch  191  loss:  0.0018391424091532826
Validation on real data: 
LOSS supervised-train 0.0023047269726521337, valid 0.0018582276534289122
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0019631944596767426
Batch  11  loss:  0.0014365115202963352
Batch  21  loss:  0.0015841611893847585
Batch  31  loss:  0.0014573074877262115
Batch  41  loss:  0.0015191280981525779
Batch  51  loss:  0.0017455917550250888
Batch  61  loss:  0.001454177312552929
Batch  71  loss:  0.0009331844048574567
Batch  81  loss:  0.002530557569116354
Batch  91  loss:  0.002167175756767392
Batch  101  loss:  0.004213020671159029
Batch  111  loss:  0.0025114917661994696
Batch  121  loss:  0.002409195527434349
Batch  131  loss:  0.0026608421467244625
Batch  141  loss:  0.0015279350336641073
Batch  151  loss:  0.002052299678325653
Batch  161  loss:  0.0024037002585828304
Batch  171  loss:  0.0015362455742433667
Batch  181  loss:  0.0018004286102950573
Batch  191  loss:  0.0016156935598701239
Validation on real data: 
LOSS supervised-train 0.0018254738708492369, valid 0.0014891878236085176
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0014364152448251843
Batch  11  loss:  0.001107630436308682
Batch  21  loss:  0.0013740254798904061
Batch  31  loss:  0.0009970578830689192
Batch  41  loss:  0.0012960124295204878
Batch  51  loss:  0.0012891951482743025
Batch  61  loss:  0.000900905579328537
Batch  71  loss:  0.001013097702525556
Batch  81  loss:  0.0022305361926555634
Batch  91  loss:  0.0019000034080818295
Batch  101  loss:  0.0032953107729554176
Batch  111  loss:  0.0019881525076925755
Batch  121  loss:  0.002168689388781786
Batch  131  loss:  0.0023947448935359716
Batch  141  loss:  0.00151776522397995
Batch  151  loss:  0.0013946780236437917
Batch  161  loss:  0.0017791488207876682
Batch  171  loss:  0.0009797838283702731
Batch  181  loss:  0.0011431362945586443
Batch  191  loss:  0.0012597271706908941
Validation on real data: 
LOSS supervised-train 0.0014838322282594164, valid 0.0008583713788539171
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0013971306616440415
Batch  11  loss:  0.0009015274117700756
Batch  21  loss:  0.0008551522623747587
Batch  31  loss:  0.000830718083307147
Batch  41  loss:  0.0013738804263994098
Batch  51  loss:  0.0011098121758550406
Batch  61  loss:  0.0008005506824702024
Batch  71  loss:  0.0007499956991523504
Batch  81  loss:  0.0020522756967693567
Batch  91  loss:  0.0014166850596666336
Batch  101  loss:  0.0031452467665076256
Batch  111  loss:  0.0019644214771687984
Batch  121  loss:  0.001749240094795823
Batch  131  loss:  0.0023958394303917885
Batch  141  loss:  0.00109215232077986
Batch  151  loss:  0.0013990747975185513
Batch  161  loss:  0.001895574270747602
Batch  171  loss:  0.0007059723720885813
Batch  181  loss:  0.0012491577072069049
Batch  191  loss:  0.001355850719846785
Validation on real data: 
LOSS supervised-train 0.0013615087245125324, valid 0.001132096047513187
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0012139041209593415
Batch  11  loss:  0.0009664356475695968
Batch  21  loss:  0.0008933123899623752
Batch  31  loss:  0.0007380343740805984
Batch  41  loss:  0.0012443698942661285
Batch  51  loss:  0.0016766160260885954
Batch  61  loss:  0.0010026507079601288
Batch  71  loss:  0.0006772139458917081
Batch  81  loss:  0.001429361873306334
Batch  91  loss:  0.001803465886041522
Batch  101  loss:  0.002802058355882764
Batch  111  loss:  0.0015394502552226186
Batch  121  loss:  0.0016015296569094062
Batch  131  loss:  0.001627791440114379
Batch  141  loss:  0.001010304782539606
Batch  151  loss:  0.001118403160944581
Batch  161  loss:  0.0014898702502250671
Batch  171  loss:  0.0008727117674425244
Batch  181  loss:  0.001308602630160749
Batch  191  loss:  0.0012732924660667777
Validation on real data: 
LOSS supervised-train 0.0012070111322100275, valid 0.000964030739851296
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0012722272658720613
Batch  11  loss:  0.0009222033550031483
Batch  21  loss:  0.0008190007065422833
Batch  31  loss:  0.0007204819121398032
Batch  41  loss:  0.0008971623610705137
Batch  51  loss:  0.0009243760141544044
Batch  61  loss:  0.000692973320838064
Batch  71  loss:  0.0005777587648481131
Batch  81  loss:  0.0016207239823415875
Batch  91  loss:  0.0015371900517493486
Batch  101  loss:  0.0022099034395068884
Batch  111  loss:  0.0012600724585354328
Batch  121  loss:  0.0011629294604063034
Batch  131  loss:  0.001404234440997243
Batch  141  loss:  0.0008449803572148085
Batch  151  loss:  0.0011147240875288844
Batch  161  loss:  0.0013362424215301871
Batch  171  loss:  0.0006463384488597512
Batch  181  loss:  0.0007762231980450451
Batch  191  loss:  0.001049574464559555
Validation on real data: 
LOSS supervised-train 0.0010643933816754725, valid 0.0006941881729289889
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0008013548795133829
Batch  11  loss:  0.0005211722454987466
Batch  21  loss:  0.000793480547145009
Batch  31  loss:  0.0005164960166439414
Batch  41  loss:  0.0011675457935780287
Batch  51  loss:  0.0009921641321852803
Batch  61  loss:  0.000663230603095144
Batch  71  loss:  0.0007297363481484354
Batch  81  loss:  0.0011308185057714581
Batch  91  loss:  0.0012350914767012
Batch  101  loss:  0.002243645954877138
Batch  111  loss:  0.0013143076794221997
Batch  121  loss:  0.0010558136273175478
Batch  131  loss:  0.0013286700705066323
Batch  141  loss:  0.0007657560636289418
Batch  151  loss:  0.0009626527898944914
Batch  161  loss:  0.0010235781082883477
Batch  171  loss:  0.0007114728214219213
Batch  181  loss:  0.0009746327996253967
Batch  191  loss:  0.0011949966428801417
Validation on real data: 
LOSS supervised-train 0.0009868142212508247, valid 0.0006973535637371242
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0009360056137666106
Batch  11  loss:  0.0005859686643816531
Batch  21  loss:  0.0007052540895529091
Batch  31  loss:  0.0007101811352185905
Batch  41  loss:  0.0008673104457557201
Batch  51  loss:  0.0008923965506255627
Batch  61  loss:  0.0005602638120763004
Batch  71  loss:  0.0005519535043276846
Batch  81  loss:  0.0008649065857753158
Batch  91  loss:  0.0014843522803857923
Batch  101  loss:  0.002268551615998149
Batch  111  loss:  0.000916356744710356
Batch  121  loss:  0.000981789082288742
Batch  131  loss:  0.0010284617310389876
Batch  141  loss:  0.0010136286728084087
Batch  151  loss:  0.0009005841566249728
Batch  161  loss:  0.0009535268764011562
Batch  171  loss:  0.0007324653561227024
Batch  181  loss:  0.0009327164152637124
Batch  191  loss:  0.0008491274784319103
Validation on real data: 
LOSS supervised-train 0.0008792457744129933, valid 0.0005271196132525802
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0009082779288291931
Batch  11  loss:  0.0005823202664032578
Batch  21  loss:  0.0004999953089281917
Batch  31  loss:  0.0004982599057257175
Batch  41  loss:  0.0006528851808980107
Batch  51  loss:  0.001015646499581635
Batch  61  loss:  0.000571135024074465
Batch  71  loss:  0.0005190381198190153
Batch  81  loss:  0.0008005159324966371
Batch  91  loss:  0.001084266696125269
Batch  101  loss:  0.0017614088719710708
Batch  111  loss:  0.0010943097295239568
Batch  121  loss:  0.0010795950656756759
Batch  131  loss:  0.0011018362129107118
Batch  141  loss:  0.0006971274851821363
Batch  151  loss:  0.0007015499286353588
Batch  161  loss:  0.0011402183445170522
Batch  171  loss:  0.0006113880081102252
Batch  181  loss:  0.0006711279274895787
Batch  191  loss:  0.0010387160582467914
Validation on real data: 
LOSS supervised-train 0.0008216867336886935, valid 0.0003827245091088116
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.000689354375936091
Batch  11  loss:  0.0005309635307639837
Batch  21  loss:  0.0005577102419920266
Batch  31  loss:  0.0004882381472270936
Batch  41  loss:  0.0005923060816712677
Batch  51  loss:  0.0007512843585573137
Batch  61  loss:  0.0006056557758711278
Batch  71  loss:  0.00048433264601044357
Batch  81  loss:  0.0009627669933252037
Batch  91  loss:  0.00105192419141531
Batch  101  loss:  0.0016865871148183942
Batch  111  loss:  0.0008347815019078553
Batch  121  loss:  0.0009340901742689312
Batch  131  loss:  0.0008454397902823985
Batch  141  loss:  0.0005787687259726226
Batch  151  loss:  0.0007487671100534499
Batch  161  loss:  0.0010879779001697898
Batch  171  loss:  0.0005118380067870021
Batch  181  loss:  0.0005933787324465811
Batch  191  loss:  0.0007767046918161213
Validation on real data: 
LOSS supervised-train 0.0007648090149450582, valid 0.0004357719444669783
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0007001629564911127
Batch  11  loss:  0.0003826111205853522
Batch  21  loss:  0.0007391538238152862
Batch  31  loss:  0.00035245882463641465
Batch  41  loss:  0.000562684319447726
Batch  51  loss:  0.0006705499836243689
Batch  61  loss:  0.0004419024335220456
Batch  71  loss:  0.0005236990982666612
Batch  81  loss:  0.0007554562180303037
Batch  91  loss:  0.0011841192608699203
Batch  101  loss:  0.0016543668461963534
Batch  111  loss:  0.0009853249648585916
Batch  121  loss:  0.0007922707591205835
Batch  131  loss:  0.0006869303761050105
Batch  141  loss:  0.0006800341652706265
Batch  151  loss:  0.0009401278221048415
Batch  161  loss:  0.0008830667939037085
Batch  171  loss:  0.0006383074796758592
Batch  181  loss:  0.0005278713069856167
Batch  191  loss:  0.0006041977903805673
Validation on real data: 
LOSS supervised-train 0.0007054410192358773, valid 0.0004014349542558193
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00050794129492715
Batch  11  loss:  0.0003993386635556817
Batch  21  loss:  0.0005226829671300948
Batch  31  loss:  0.0004420946061145514
Batch  41  loss:  0.0006508580408990383
Batch  51  loss:  0.0007391740218736231
Batch  61  loss:  0.00036923561128787696
Batch  71  loss:  0.00048229499952867627
Batch  81  loss:  0.0006845575990155339
Batch  91  loss:  0.001255273586139083
Batch  101  loss:  0.0017219254514202476
Batch  111  loss:  0.0007830364047549665
Batch  121  loss:  0.0007297447300516069
Batch  131  loss:  0.0008782637887634337
Batch  141  loss:  0.000749834522139281
Batch  151  loss:  0.0006293394253589213
Batch  161  loss:  0.0009364810539409518
Batch  171  loss:  0.0004960670485161245
Batch  181  loss:  0.0005458682426251471
Batch  191  loss:  0.0008183285244740546
Validation on real data: 
LOSS supervised-train 0.000685418517969083, valid 0.0004913496086373925
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0005716841551475227
Batch  11  loss:  0.00045957250404171646
Batch  21  loss:  0.0004678717232309282
Batch  31  loss:  0.0006150457775220275
Batch  41  loss:  0.00044034194434061646
Batch  51  loss:  0.0007003897335380316
Batch  61  loss:  0.0003469693474471569
Batch  71  loss:  0.0003993030113633722
Batch  81  loss:  0.0006634123274125159
Batch  91  loss:  0.00086969044059515
Batch  101  loss:  0.001697047962807119
Batch  111  loss:  0.0007991985185071826
Batch  121  loss:  0.0008300135377794504
Batch  131  loss:  0.0008694560383446515
Batch  141  loss:  0.0006509778322651982
Batch  151  loss:  0.0005528004840016365
Batch  161  loss:  0.0010377595899626613
Batch  171  loss:  0.0005385363474488258
Batch  181  loss:  0.000546638504602015
Batch  191  loss:  0.000714680936653167
Validation on real data: 
LOSS supervised-train 0.0006381078663980588, valid 0.0004172135959379375
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0005709460820071399
Batch  11  loss:  0.0005178888095542789
Batch  21  loss:  0.0005463758134283125
Batch  31  loss:  0.00046336441300809383
Batch  41  loss:  0.00042818396468646824
Batch  51  loss:  0.0007287245243787766
Batch  61  loss:  0.0004212696512695402
Batch  71  loss:  0.0004017382161691785
Batch  81  loss:  0.0006790042971260846
Batch  91  loss:  0.0008788771810941398
Batch  101  loss:  0.0014756452292203903
Batch  111  loss:  0.0006306967115961015
Batch  121  loss:  0.000732948537915945
Batch  131  loss:  0.0005663912743330002
Batch  141  loss:  0.0005695144063793123
Batch  151  loss:  0.0006005094619467854
Batch  161  loss:  0.0008393297903239727
Batch  171  loss:  0.0006375944358296692
Batch  181  loss:  0.00044022200745530427
Batch  191  loss:  0.0005304478690959513
Validation on real data: 
LOSS supervised-train 0.0005921824908000417, valid 0.0003918140719179064
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0005303614307194948
Batch  11  loss:  0.00041008475818671286
Batch  21  loss:  0.0004953983589075506
Batch  31  loss:  0.00048232704284600914
Batch  41  loss:  0.00042794537148438394
Batch  51  loss:  0.0006889299256727099
Batch  61  loss:  0.0003951406688429415
Batch  71  loss:  0.0004314216203056276
Batch  81  loss:  0.0006886394112370908
Batch  91  loss:  0.0008298095781356096
Batch  101  loss:  0.0014030277961865067
Batch  111  loss:  0.00083321169950068
Batch  121  loss:  0.0006668834248557687
Batch  131  loss:  0.0005025675054639578
Batch  141  loss:  0.0005442440160550177
Batch  151  loss:  0.0006046274793334305
Batch  161  loss:  0.0008416383061558008
Batch  171  loss:  0.0004894481389783323
Batch  181  loss:  0.0008302981150336564
Batch  191  loss:  0.0005380814545787871
Validation on real data: 
LOSS supervised-train 0.0005753531499067321, valid 0.0003688201541081071
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.00047852183342911303
Batch  11  loss:  0.00043204257963225245
Batch  21  loss:  0.00048029772005975246
Batch  31  loss:  0.0004451627901289612
Batch  41  loss:  0.0005501820123754442
Batch  51  loss:  0.0006512069376185536
Batch  61  loss:  0.00032318843295797706
Batch  71  loss:  0.00028120778733864427
Batch  81  loss:  0.0004374329000711441
Batch  91  loss:  0.0008762857178226113
Batch  101  loss:  0.0013698205584660172
Batch  111  loss:  0.000616871751844883
Batch  121  loss:  0.0006953777046874166
Batch  131  loss:  0.0007007846143096685
Batch  141  loss:  0.0005552337388508022
Batch  151  loss:  0.0006210614810697734
Batch  161  loss:  0.0006776080117560923
Batch  171  loss:  0.0005189228104427457
Batch  181  loss:  0.0005355371977202594
Batch  191  loss:  0.0004971445887349546
Validation on real data: 
LOSS supervised-train 0.0005724987333815079, valid 0.0003934405976906419
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.000550033466424793
Batch  11  loss:  0.0003647634293884039
Batch  21  loss:  0.000448456295998767
Batch  31  loss:  0.0004943813546560705
Batch  41  loss:  0.00041542432154528797
Batch  51  loss:  0.000511470134370029
Batch  61  loss:  0.0003105835057795048
Batch  71  loss:  0.0004229848855175078
Batch  81  loss:  0.0005100333364680409
Batch  91  loss:  0.0007320350268855691
Batch  101  loss:  0.0012549973325803876
Batch  111  loss:  0.0005521017010323703
Batch  121  loss:  0.0006050005904398859
Batch  131  loss:  0.0003364933072589338
Batch  141  loss:  0.0004732227826025337
Batch  151  loss:  0.0005660108872689307
Batch  161  loss:  0.0007127870921976864
Batch  171  loss:  0.0006192618748173118
Batch  181  loss:  0.00044672677177004516
Batch  191  loss:  0.00036119139986112714
Validation on real data: 
LOSS supervised-train 0.0005245914740953595, valid 0.0003250068984925747
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0004417516174726188
Batch  11  loss:  0.0004183401179034263
Batch  21  loss:  0.00046076028957031667
Batch  31  loss:  0.0004842638736590743
Batch  41  loss:  0.0003418254782445729
Batch  51  loss:  0.0006783065036870539
Batch  61  loss:  0.0003892567765433341
Batch  71  loss:  0.00036159317824058235
Batch  81  loss:  0.0007164768758229911
Batch  91  loss:  0.0007472672732546926
Batch  101  loss:  0.0012060720473527908
Batch  111  loss:  0.0006078791921027005
Batch  121  loss:  0.0005720056942664087
Batch  131  loss:  0.000315987563226372
Batch  141  loss:  0.0004909601411782205
Batch  151  loss:  0.000545640243217349
Batch  161  loss:  0.0008736389572732151
Batch  171  loss:  0.0003701713285408914
Batch  181  loss:  0.0004448426770977676
Batch  191  loss:  0.00042380517697893083
Validation on real data: 
LOSS supervised-train 0.0005169051791017409, valid 0.0002659846795722842
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0004228192556183785
Batch  11  loss:  0.00038995116483420134
Batch  21  loss:  0.00042886531446129084
Batch  31  loss:  0.0003084845084231347
Batch  41  loss:  0.00039170810487121344
Batch  51  loss:  0.0005468245362862945
Batch  61  loss:  0.0002969057240989059
Batch  71  loss:  0.00032856903271749616
Batch  81  loss:  0.00048029489698819816
Batch  91  loss:  0.0007855768781155348
Batch  101  loss:  0.0012736152857542038
Batch  111  loss:  0.0005797726335003972
Batch  121  loss:  0.0005913445493206382
Batch  131  loss:  0.0004253335064277053
Batch  141  loss:  0.0004112249589525163
Batch  151  loss:  0.0003921280731447041
Batch  161  loss:  0.0007132818573154509
Batch  171  loss:  0.0004892260185442865
Batch  181  loss:  0.0003782629792112857
Batch  191  loss:  0.0005979606648907065
Validation on real data: 
LOSS supervised-train 0.0004787186795147136, valid 0.0003389440826140344
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0004831327823922038
Batch  11  loss:  0.0002999509160872549
Batch  21  loss:  0.00037248944863677025
Batch  31  loss:  0.0004011168493889272
Batch  41  loss:  0.00039933077641762793
Batch  51  loss:  0.00045377807691693306
Batch  61  loss:  0.0003136983432341367
Batch  71  loss:  0.0003556849842425436
Batch  81  loss:  0.0005690305260941386
Batch  91  loss:  0.0008297137683257461
Batch  101  loss:  0.0014091968769207597
Batch  111  loss:  0.0004788307414855808
Batch  121  loss:  0.0005767448456026614
Batch  131  loss:  0.0003275957133155316
Batch  141  loss:  0.0004982450627721846
Batch  151  loss:  0.000444701814558357
Batch  161  loss:  0.0006491102976724505
Batch  171  loss:  0.0005109960329718888
Batch  181  loss:  0.00033201795304194093
Batch  191  loss:  0.00038558716187253594
Validation on real data: 
LOSS supervised-train 0.0004659156679554144, valid 0.00026368239196017385
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0003761312400456518
Batch  11  loss:  0.00028508156538009644
Batch  21  loss:  0.0004033597360830754
Batch  31  loss:  0.00037471629912033677
Batch  41  loss:  0.0003538834280334413
Batch  51  loss:  0.0004978749202564359
Batch  61  loss:  0.0003190786228515208
Batch  71  loss:  0.00031242892146110535
Batch  81  loss:  0.0004665135929826647
Batch  91  loss:  0.000749366357922554
Batch  101  loss:  0.0011034009512513876
Batch  111  loss:  0.0005761319189332426
Batch  121  loss:  0.000560108688659966
Batch  131  loss:  0.00032812118297442794
Batch  141  loss:  0.00038855260936543345
Batch  151  loss:  0.0006610736600123346
Batch  161  loss:  0.0006682854727841914
Batch  171  loss:  0.0006256974884308875
Batch  181  loss:  0.00037878856528550386
Batch  191  loss:  0.00042021472472697496
Validation on real data: 
LOSS supervised-train 0.00045109835067705715, valid 0.0003214136231690645
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00037841618177480996
Batch  11  loss:  0.0003075799031648785
Batch  21  loss:  0.0003730624448508024
Batch  31  loss:  0.00023627493646927178
Batch  41  loss:  0.0003904280602000654
Batch  51  loss:  0.00046956061851233244
Batch  61  loss:  0.000278969993814826
Batch  71  loss:  0.0003368270699866116
Batch  81  loss:  0.0003973544226028025
Batch  91  loss:  0.0005667724180966616
Batch  101  loss:  0.0010254039661958814
Batch  111  loss:  0.0005640725139528513
Batch  121  loss:  0.00044547728612087667
Batch  131  loss:  0.00035170617047697306
Batch  141  loss:  0.0005043511628173292
Batch  151  loss:  0.0003932096005883068
Batch  161  loss:  0.0006726451683789492
Batch  171  loss:  0.00036442518467083573
Batch  181  loss:  0.0003268995205871761
Batch  191  loss:  0.0004164517449680716
Validation on real data: 
LOSS supervised-train 0.0004204740221030079, valid 0.00033224729122594
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.00039513930096291006
Batch  11  loss:  0.0004971580929122865
Batch  21  loss:  0.00033208070090040565
Batch  31  loss:  0.00022839124721940607
Batch  41  loss:  0.000389735127100721
Batch  51  loss:  0.0004532817692961544
Batch  61  loss:  0.00043894199188798666
Batch  71  loss:  0.00038633885560557246
Batch  81  loss:  0.00047531037125736475
Batch  91  loss:  0.0005059366812929511
Batch  101  loss:  0.0013713481603190303
Batch  111  loss:  0.0008326247334480286
Batch  121  loss:  0.0004947131383232772
Batch  131  loss:  0.0003301902033854276
Batch  141  loss:  0.00039449206087738276
Batch  151  loss:  0.0005369220743887126
Batch  161  loss:  0.0006889667711220682
Batch  171  loss:  0.0004173686320427805
Batch  181  loss:  0.0004034138983115554
Batch  191  loss:  0.0004016704042442143
Validation on real data: 
LOSS supervised-train 0.0004403714515501633, valid 0.00028978585032746196
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.00031928703538142145
Batch  11  loss:  0.0003474772674962878
Batch  21  loss:  0.0002622935571707785
Batch  31  loss:  0.0002834799524862319
Batch  41  loss:  0.00030809242161922157
Batch  51  loss:  0.00045982375741004944
Batch  61  loss:  0.00029356739833019674
Batch  71  loss:  0.0002535661624278873
Batch  81  loss:  0.00033504367456771433
Batch  91  loss:  0.00041398598114028573
Batch  101  loss:  0.0012570760445669293
Batch  111  loss:  0.00033762454404495656
Batch  121  loss:  0.0004121601232327521
Batch  131  loss:  0.00025815010303631425
Batch  141  loss:  0.00038724608020856977
Batch  151  loss:  0.00044642447028309107
Batch  161  loss:  0.0008068508468568325
Batch  171  loss:  0.00040437778807245195
Batch  181  loss:  0.000333606731146574
Batch  191  loss:  0.0003325316938571632
Validation on real data: 
LOSS supervised-train 0.00040771476735244503, valid 0.00029795491718687117
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0004577317740768194
Batch  11  loss:  0.00029991823248565197
Batch  21  loss:  0.00038972898619249463
Batch  31  loss:  0.00030958474962972105
Batch  41  loss:  0.00028319976991042495
Batch  51  loss:  0.0004822144110221416
Batch  61  loss:  0.00023745314683765173
Batch  71  loss:  0.00026921965763904154
Batch  81  loss:  0.0005024769925512373
Batch  91  loss:  0.0006342384149320424
Batch  101  loss:  0.00093397794989869
Batch  111  loss:  0.00046779404510743916
Batch  121  loss:  0.000373101735021919
Batch  131  loss:  0.00024482887238264084
Batch  141  loss:  0.00035479143843986094
Batch  151  loss:  0.0004051111463923007
Batch  161  loss:  0.0004890431882813573
Batch  171  loss:  0.0004928415874019265
Batch  181  loss:  0.00028652010951191187
Batch  191  loss:  0.0003002548182848841
Validation on real data: 
LOSS supervised-train 0.0003963077946536941, valid 0.0003665749682113528
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.00044356557191349566
Batch  11  loss:  0.00027282460359856486
Batch  21  loss:  0.0002779537462629378
Batch  31  loss:  0.00028197275241836905
Batch  41  loss:  0.0004395686846692115
Batch  51  loss:  0.0005384096875786781
Batch  61  loss:  0.00031311222119256854
Batch  71  loss:  0.00025360489962622523
Batch  81  loss:  0.0004393030540086329
Batch  91  loss:  0.0006616664468310773
Batch  101  loss:  0.0011830870062112808
Batch  111  loss:  0.0004064799868501723
Batch  121  loss:  0.00045939942356199026
Batch  131  loss:  0.0003153795260004699
Batch  141  loss:  0.00045006221625953913
Batch  151  loss:  0.000379304081434384
Batch  161  loss:  0.0005488520255312324
Batch  171  loss:  0.00033379331580363214
Batch  181  loss:  0.00033091771183535457
Batch  191  loss:  0.0004218806861899793
Validation on real data: 
LOSS supervised-train 0.00038491784158395604, valid 0.00026517038349993527
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0003679584769997746
Batch  11  loss:  0.0003756735532078892
Batch  21  loss:  0.0002580821455921978
Batch  31  loss:  0.00029232780798338354
Batch  41  loss:  0.0002964898303616792
Batch  51  loss:  0.00036960840225219727
Batch  61  loss:  0.00024626628146506846
Batch  71  loss:  0.00026829371927306056
Batch  81  loss:  0.00034820064320228994
Batch  91  loss:  0.0005427702562883496
Batch  101  loss:  0.0008494514040648937
Batch  111  loss:  0.00035449847928248346
Batch  121  loss:  0.000369152839994058
Batch  131  loss:  0.0002736095921136439
Batch  141  loss:  0.00029639771673828363
Batch  151  loss:  0.0004549064615275711
Batch  161  loss:  0.0006145074730738997
Batch  171  loss:  0.0003954250132665038
Batch  181  loss:  0.00027120718732476234
Batch  191  loss:  0.00036461837589740753
Validation on real data: 
LOSS supervised-train 0.0003649217177007813, valid 0.00033178782905451953
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0003530499234329909
Batch  11  loss:  0.0003049682127311826
Batch  21  loss:  0.0002880119427572936
Batch  31  loss:  0.00031860359013080597
Batch  41  loss:  0.0003548953973222524
Batch  51  loss:  0.00038537787622772157
Batch  61  loss:  0.0003127623931504786
Batch  71  loss:  0.0002865280257537961
Batch  81  loss:  0.0003169535775668919
Batch  91  loss:  0.0004869178228545934
Batch  101  loss:  0.0006948122172616422
Batch  111  loss:  0.0004593266930896789
Batch  121  loss:  0.00037162526859901845
Batch  131  loss:  0.000414627866121009
Batch  141  loss:  0.0003719966043718159
Batch  151  loss:  0.0003883731260430068
Batch  161  loss:  0.00033655628794804215
Batch  171  loss:  0.00028347159968689084
Batch  181  loss:  0.00030538818100467324
Batch  191  loss:  0.00046283993287943304
Validation on real data: 
LOSS supervised-train 0.0003736087985453196, valid 0.00020410612341947854
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0003906766651198268
Batch  11  loss:  0.0004001273773610592
Batch  21  loss:  0.00024227864923886955
Batch  31  loss:  0.00019337191770318896
Batch  41  loss:  0.0002688873209990561
Batch  51  loss:  0.00038389532710425556
Batch  61  loss:  0.0002021419641096145
Batch  71  loss:  0.00022277998505160213
Batch  81  loss:  0.00042271806159988046
Batch  91  loss:  0.00040793372318148613
Batch  101  loss:  0.0010385254863649607
Batch  111  loss:  0.00047372965491376817
Batch  121  loss:  0.00041608663741499186
Batch  131  loss:  0.0003106764343101531
Batch  141  loss:  0.0003230615984648466
Batch  151  loss:  0.00034654574119485915
Batch  161  loss:  0.00045190355740487576
Batch  171  loss:  0.0003268946602474898
Batch  181  loss:  0.00019963020167779177
Batch  191  loss:  0.00045549377682618797
Validation on real data: 
LOSS supervised-train 0.00035627243691124024, valid 0.00033300992799922824
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0003138017491437495
Batch  11  loss:  0.00027944170869886875
Batch  21  loss:  0.00031703582499176264
Batch  31  loss:  0.00023456748749595135
Batch  41  loss:  0.0003431994409766048
Batch  51  loss:  0.0004445511440280825
Batch  61  loss:  0.0002836917992681265
Batch  71  loss:  0.0003174688608851284
Batch  81  loss:  0.00034336349926888943
Batch  91  loss:  0.0005297320894896984
Batch  101  loss:  0.0008796533802524209
Batch  111  loss:  0.00039125539478845894
Batch  121  loss:  0.0004341585736256093
Batch  131  loss:  0.0003031219821423292
Batch  141  loss:  0.00031127160764299333
Batch  151  loss:  0.00030001599225215614
Batch  161  loss:  0.0005328464321792126
Batch  171  loss:  0.00031150112044997513
Batch  181  loss:  0.0003203196683898568
Batch  191  loss:  0.0004198031092528254
Validation on real data: 
LOSS supervised-train 0.00035582097007136326, valid 0.00018667909898795187
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00029014868778176606
Batch  11  loss:  0.00023926216817926615
Batch  21  loss:  0.0002499493712093681
Batch  31  loss:  0.00032266348716802895
Batch  41  loss:  0.0003333382192067802
Batch  51  loss:  0.00043083130731247365
Batch  61  loss:  0.0002000808308366686
Batch  71  loss:  0.0002512643695808947
Batch  81  loss:  0.00031775690149515867
Batch  91  loss:  0.0004913350567221642
Batch  101  loss:  0.0007885867380537093
Batch  111  loss:  0.00041487326961942017
Batch  121  loss:  0.0003802792343776673
Batch  131  loss:  0.00027734923060052097
Batch  141  loss:  0.0002671830588951707
Batch  151  loss:  0.000379721139324829
Batch  161  loss:  0.0006213085725903511
Batch  171  loss:  0.00029314879793673754
Batch  181  loss:  0.00032410494168289006
Batch  191  loss:  0.00048054108629003167
Validation on real data: 
LOSS supervised-train 0.0003383203010162106, valid 0.000273057259619236
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00028253166237846017
Batch  11  loss:  0.0002601817832328379
Batch  21  loss:  0.00024410936748608947
Batch  31  loss:  0.00021255947649478912
Batch  41  loss:  0.00026635246467776597
Batch  51  loss:  0.00039015981019474566
Batch  61  loss:  0.0002169376239180565
Batch  71  loss:  0.0002158370625693351
Batch  81  loss:  0.0002235920837847516
Batch  91  loss:  0.0004407878150232136
Batch  101  loss:  0.0007897559553384781
Batch  111  loss:  0.00044220624840818346
Batch  121  loss:  0.00034249629243277013
Batch  131  loss:  0.0002717047755140811
Batch  141  loss:  0.0003174467710778117
Batch  151  loss:  0.0002971647190861404
Batch  161  loss:  0.0005463743000291288
Batch  171  loss:  0.0002459720417391509
Batch  181  loss:  0.0002641177561599761
Batch  191  loss:  0.0003747125738300383
Validation on real data: 
LOSS supervised-train 0.0003332712504925439, valid 0.00030406040605157614
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.00035327544901520014
Batch  11  loss:  0.00021755273337475955
Batch  21  loss:  0.00034225377021357417
Batch  31  loss:  0.0002613418619148433
Batch  41  loss:  0.0003111666301265359
Batch  51  loss:  0.00039047852624207735
Batch  61  loss:  0.00024718602071516216
Batch  71  loss:  0.00028830047813244164
Batch  81  loss:  0.00034153315937146544
Batch  91  loss:  0.00045647937804460526
Batch  101  loss:  0.0008648946532048285
Batch  111  loss:  0.00039394106715917587
Batch  121  loss:  0.00036419546813704073
Batch  131  loss:  0.0002943400468211621
Batch  141  loss:  0.00023471059103030711
Batch  151  loss:  0.00023030034208204597
Batch  161  loss:  0.000571972515899688
Batch  171  loss:  0.0002901160914916545
Batch  181  loss:  0.00022809955407865345
Batch  191  loss:  0.00032435424509458244
Validation on real data: 
LOSS supervised-train 0.000337356267918949, valid 0.0002024567365879193
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00027631345437839627
Batch  11  loss:  0.00023906461137812585
Batch  21  loss:  0.0003338586539030075
Batch  31  loss:  0.00025832030223682523
Batch  41  loss:  0.00025759966229088604
Batch  51  loss:  0.00031754691735841334
Batch  61  loss:  0.00021797147928737104
Batch  71  loss:  0.00021027977345511317
Batch  81  loss:  0.0002660819736775011
Batch  91  loss:  0.00048485747538506985
Batch  101  loss:  0.000743262586183846
Batch  111  loss:  0.0003051225212402642
Batch  121  loss:  0.0003006254555657506
Batch  131  loss:  0.00023239405709318817
Batch  141  loss:  0.00032260004081763327
Batch  151  loss:  0.0003261579549871385
Batch  161  loss:  0.00046329552424140275
Batch  171  loss:  0.0003717747167684138
Batch  181  loss:  0.00023513085034210235
Batch  191  loss:  0.00047567879664711654
Validation on real data: 
LOSS supervised-train 0.0003193885603832314, valid 0.00035724681220017374
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0003272283065598458
Batch  11  loss:  0.00019790377700701356
Batch  21  loss:  0.00024111247330438346
Batch  31  loss:  0.0002665831707417965
Batch  41  loss:  0.00026214210083708167
Batch  51  loss:  0.00037091909325681627
Batch  61  loss:  0.00020494313503149897
Batch  71  loss:  0.00026283261831849813
Batch  81  loss:  0.00038782780757173896
Batch  91  loss:  0.00045039787073619664
Batch  101  loss:  0.000802867638412863
Batch  111  loss:  0.0004037820908706635
Batch  121  loss:  0.0003127907984890044
Batch  131  loss:  0.00029616596293635666
Batch  141  loss:  0.0002939024125225842
Batch  151  loss:  0.00035227215266786516
Batch  161  loss:  0.0007482440560124815
Batch  171  loss:  0.00029409676790237427
Batch  181  loss:  0.00023732424597255886
Batch  191  loss:  0.0003315151552669704
Validation on real data: 
LOSS supervised-train 0.0003269546139927115, valid 0.0002820197551045567
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0002709669934120029
Batch  11  loss:  0.00020043255062773824
Batch  21  loss:  0.000246602256083861
Batch  31  loss:  0.00020433503959793597
Batch  41  loss:  0.00023797839821781963
Batch  51  loss:  0.00039493347867392004
Batch  61  loss:  0.00026805305969901383
Batch  71  loss:  0.00029082217952236533
Batch  81  loss:  0.00036864264984615147
Batch  91  loss:  0.0004715991672128439
Batch  101  loss:  0.0007883703801780939
Batch  111  loss:  0.00045668258098885417
Batch  121  loss:  0.0003218615020159632
Batch  131  loss:  0.00022941677889321
Batch  141  loss:  0.0002458537055645138
Batch  151  loss:  0.0002884537971112877
Batch  161  loss:  0.0005673765554092824
Batch  171  loss:  0.0002522224676795304
Batch  181  loss:  0.000235526284086518
Batch  191  loss:  0.00032192165963351727
Validation on real data: 
LOSS supervised-train 0.0003030708590085851, valid 0.0002456707588862628
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0003151859564241022
Batch  11  loss:  0.00021471476065926254
Batch  21  loss:  0.0002701188495848328
Batch  31  loss:  0.00015497715503443033
Batch  41  loss:  0.00022786702902521938
Batch  51  loss:  0.0003322500560898334
Batch  61  loss:  0.00021369136811699718
Batch  71  loss:  0.0002693992864806205
Batch  81  loss:  0.00028289726469665766
Batch  91  loss:  0.0004641906125470996
Batch  101  loss:  0.0008455229108221829
Batch  111  loss:  0.0002892065094783902
Batch  121  loss:  0.0002651753311511129
Batch  131  loss:  0.00020251449313946068
Batch  141  loss:  0.00030723042436875403
Batch  151  loss:  0.0002792963641695678
Batch  161  loss:  0.0005492572672665119
Batch  171  loss:  0.00034326541936025023
Batch  181  loss:  0.00018916871340479702
Batch  191  loss:  0.00036625718348659575
Validation on real data: 
LOSS supervised-train 0.0002959423837455688, valid 0.00017057622608263046
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.00028334642411209643
Batch  11  loss:  0.0002918293175753206
Batch  21  loss:  0.00019561387307476252
Batch  31  loss:  0.00021299815853126347
Batch  41  loss:  0.00033240087213926017
Batch  51  loss:  0.00044702074956148863
Batch  61  loss:  0.00021708350686822087
Batch  71  loss:  0.00026012599118985236
Batch  81  loss:  0.00029313654522411525
Batch  91  loss:  0.00034979861811734736
Batch  101  loss:  0.0009238044731318951
Batch  111  loss:  0.00024276731710415334
Batch  121  loss:  0.00032294343691319227
Batch  131  loss:  0.00026358431205153465
Batch  141  loss:  0.0002810546138789505
Batch  151  loss:  0.00026768865063786507
Batch  161  loss:  0.00035721465246751904
Batch  171  loss:  0.00022950007405597717
Batch  181  loss:  0.00026581960264593363
Batch  191  loss:  0.00032554834615439177
Validation on real data: 
LOSS supervised-train 0.0002976453071460128, valid 0.00022132316371425986
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0002550882054492831
Batch  11  loss:  0.00024043690063990653
Batch  21  loss:  0.0002444775600451976
Batch  31  loss:  0.00022110945428721607
Batch  41  loss:  0.00032613708754070103
Batch  51  loss:  0.00031860944000072777
Batch  61  loss:  0.00030018354300409555
Batch  71  loss:  0.00027718290220946074
Batch  81  loss:  0.00025543360970914364
Batch  91  loss:  0.0005059785908088088
Batch  101  loss:  0.000527508498635143
Batch  111  loss:  0.0003721513203345239
Batch  121  loss:  0.0002730082778725773
Batch  131  loss:  0.0001969543081941083
Batch  141  loss:  0.00021838958491571248
Batch  151  loss:  0.00030144056654535234
Batch  161  loss:  0.00041588846943341196
Batch  171  loss:  0.0002327773254364729
Batch  181  loss:  0.00019477093883324414
Batch  191  loss:  0.0003215186297893524
Validation on real data: 
LOSS supervised-train 0.00028260560393391645, valid 0.00027610844699665904
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00031869791564531624
Batch  11  loss:  0.0002294755249749869
Batch  21  loss:  0.00021301252127159387
Batch  31  loss:  0.000169281309354119
Batch  41  loss:  0.0002283601788803935
Batch  51  loss:  0.0002850781020242721
Batch  61  loss:  0.00022179032384883612
Batch  71  loss:  0.00016289479390252382
Batch  81  loss:  0.00019960859208367765
Batch  91  loss:  0.0003820618148893118
Batch  101  loss:  0.0004987489082850516
Batch  111  loss:  0.00027031669742427766
Batch  121  loss:  0.0002782077935989946
Batch  131  loss:  0.0002460292598698288
Batch  141  loss:  0.0002378499775659293
Batch  151  loss:  0.00028507981915026903
Batch  161  loss:  0.00046201294753700495
Batch  171  loss:  0.00023378380865324289
Batch  181  loss:  0.0002215712593169883
Batch  191  loss:  0.00037920280010439456
Validation on real data: 
LOSS supervised-train 0.00028090360639907884, valid 0.00019224261632189155
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.000302827829727903
Batch  11  loss:  0.0002682963095139712
Batch  21  loss:  0.0002700852637644857
Batch  31  loss:  0.0002071311610052362
Batch  41  loss:  0.00029813076253049076
Batch  51  loss:  0.00025902135530486703
Batch  61  loss:  0.00023883899848442525
Batch  71  loss:  0.00022953796724323183
Batch  81  loss:  0.0002731529821176082
Batch  91  loss:  0.0003814913216046989
Batch  101  loss:  0.0006323999841697514
Batch  111  loss:  0.0002732278953772038
Batch  121  loss:  0.00029035721672698855
Batch  131  loss:  0.00022756218095310032
Batch  141  loss:  0.00022072673891671002
Batch  151  loss:  0.000224898976739496
Batch  161  loss:  0.0004389554960653186
Batch  171  loss:  0.0002690588589757681
Batch  181  loss:  0.0002616118872538209
Batch  191  loss:  0.00035804364597424865
Validation on real data: 
LOSS supervised-train 0.00027636479688226246, valid 0.00018672968144528568
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00034441021853126585
Batch  11  loss:  0.00023050527670420706
Batch  21  loss:  0.00023024406982585788
Batch  31  loss:  0.00017571491480339319
Batch  41  loss:  0.00021273677702993155
Batch  51  loss:  0.00032260853913612664
Batch  61  loss:  0.00019437746959738433
Batch  71  loss:  0.0002308798866579309
Batch  81  loss:  0.0002484516298864037
Batch  91  loss:  0.0004967119311913848
Batch  101  loss:  0.0007673409418202937
Batch  111  loss:  0.00038275300175882876
Batch  121  loss:  0.00029614436789415777
Batch  131  loss:  0.00019492882711347193
Batch  141  loss:  0.0002341974904993549
Batch  151  loss:  0.00022913033899385482
Batch  161  loss:  0.00048312844592146575
Batch  171  loss:  0.00027316168416291475
Batch  181  loss:  0.00021608136012218893
Batch  191  loss:  0.00025366191403009
Validation on real data: 
LOSS supervised-train 0.00027697170393366834, valid 0.00014454429037868977
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.00022204170818440616
Batch  11  loss:  0.00021815607033204287
Batch  21  loss:  0.00026461679954081774
Batch  31  loss:  0.00025789032224565744
Batch  41  loss:  0.0002733863366302103
Batch  51  loss:  0.00027492435765452683
Batch  61  loss:  0.00016752250667195767
Batch  71  loss:  0.00018522584286984056
Batch  81  loss:  0.00018233961600344628
Batch  91  loss:  0.00040036506834439933
Batch  101  loss:  0.0006014399696141481
Batch  111  loss:  0.00033065781462937593
Batch  121  loss:  0.00027807921287603676
Batch  131  loss:  0.0002762189833447337
Batch  141  loss:  0.00030626996885985136
Batch  151  loss:  0.0002744778466876596
Batch  161  loss:  0.0004091447626706213
Batch  171  loss:  0.0002736134338192642
Batch  181  loss:  0.00030876294476911426
Batch  191  loss:  0.0003225772816222161
Validation on real data: 
LOSS supervised-train 0.00027691072442394217, valid 0.0001874547597253695
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00034174497704952955
Batch  11  loss:  0.0002105708554154262
Batch  21  loss:  0.00025138541241176426
Batch  31  loss:  0.00020845401741098613
Batch  41  loss:  0.0002778127964120358
Batch  51  loss:  0.0003984920622315258
Batch  61  loss:  0.0001810297108022496
Batch  71  loss:  0.0001702002773527056
Batch  81  loss:  0.00023418442287947983
Batch  91  loss:  0.0003793446230702102
Batch  101  loss:  0.00043848378118127584
Batch  111  loss:  0.0002468878810759634
Batch  121  loss:  0.0002904361463151872
Batch  131  loss:  0.00016885946388356388
Batch  141  loss:  0.0002310680429218337
Batch  151  loss:  0.00026023294776678085
Batch  161  loss:  0.0003645711694844067
Batch  171  loss:  0.00023121658887248486
Batch  181  loss:  0.0002800275688059628
Batch  191  loss:  0.00033956419792957604
Validation on real data: 
LOSS supervised-train 0.0002604646547115408, valid 0.0002015258651226759
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.00026983246789313853
Batch  11  loss:  0.00025006290525197983
Batch  21  loss:  0.00027378316735848784
Batch  31  loss:  0.00023357111786026508
Batch  41  loss:  0.0002649172965902835
Batch  51  loss:  0.000322389998473227
Batch  61  loss:  0.0002215247368440032
Batch  71  loss:  0.00017480418318882585
Batch  81  loss:  0.0002041441184701398
Batch  91  loss:  0.00042215725989080966
Batch  101  loss:  0.0009207350667566061
Batch  111  loss:  0.00029413349693641067
Batch  121  loss:  0.00022985586838331074
Batch  131  loss:  0.0002648477384354919
Batch  141  loss:  0.00022678551613353193
Batch  151  loss:  0.0002205691416747868
Batch  161  loss:  0.00035510832094587386
Batch  171  loss:  0.00019031325064133853
Batch  181  loss:  0.00024962416500784457
Batch  191  loss:  0.00027150195091962814
Validation on real data: 
LOSS supervised-train 0.00025591719429939985, valid 0.0001696232648100704
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0002431550237815827
Batch  11  loss:  0.00018109376833308488
Batch  21  loss:  0.0001727888302411884
Batch  31  loss:  0.00027281357324682176
Batch  41  loss:  0.00018789754540193826
Batch  51  loss:  0.0003063891490455717
Batch  61  loss:  0.00019354540563654155
Batch  71  loss:  0.00021085012122057378
Batch  81  loss:  0.0001873892906587571
Batch  91  loss:  0.0005288404645398259
Batch  101  loss:  0.0005141957080923021
Batch  111  loss:  0.0003043343313038349
Batch  121  loss:  0.00028518150793388486
Batch  131  loss:  0.0001806052605388686
Batch  141  loss:  0.00023503803822677583
Batch  151  loss:  0.0002690272231120616
Batch  161  loss:  0.000532172794919461
Batch  171  loss:  0.0002341631770832464
Batch  181  loss:  0.0004761323216371238
Batch  191  loss:  0.00033535275724716485
Validation on real data: 
LOSS supervised-train 0.0002586653037724318, valid 0.0002849704469554126
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.00022198825899977237
Batch  11  loss:  0.00019306808826513588
Batch  21  loss:  0.0001977997162612155
Batch  31  loss:  0.0002198471047449857
Batch  41  loss:  0.00020223183673806489
Batch  51  loss:  0.0002995930553879589
Batch  61  loss:  0.00015059270663186908
Batch  71  loss:  0.00017637920973356813
Batch  81  loss:  0.0002188763755839318
Batch  91  loss:  0.0003850905632134527
Batch  101  loss:  0.00048170684021897614
Batch  111  loss:  0.0003523172636050731
Batch  121  loss:  0.00024001830024644732
Batch  131  loss:  0.00018228444969281554
Batch  141  loss:  0.0002568695927038789
Batch  151  loss:  0.00022805291519034654
Batch  161  loss:  0.0003385314194019884
Batch  171  loss:  0.0002819531364366412
Batch  181  loss:  0.0003141859779134393
Batch  191  loss:  0.0003647590347100049
Validation on real data: 
LOSS supervised-train 0.0002518686974508455, valid 0.0002608580398373306
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00023633318778593093
Batch  11  loss:  0.00017563559231348336
Batch  21  loss:  0.00022915733279660344
Batch  31  loss:  0.0001785023050615564
Batch  41  loss:  0.0002663616614881903
Batch  51  loss:  0.00022937401081435382
Batch  61  loss:  0.00019178273214492947
Batch  71  loss:  0.00024187154485844076
Batch  81  loss:  0.00018277020717505366
Batch  91  loss:  0.000394846050767228
Batch  101  loss:  0.0005559514975175261
Batch  111  loss:  0.00018891347281169146
Batch  121  loss:  0.000148632942000404
Batch  131  loss:  0.00021387367451097816
Batch  141  loss:  0.0002588046481832862
Batch  151  loss:  0.0002665476349648088
Batch  161  loss:  0.0005083207506686449
Batch  171  loss:  0.00020385095558594912
Batch  181  loss:  0.0001980899105546996
Batch  191  loss:  0.0002923677093349397
Validation on real data: 
LOSS supervised-train 0.0002475118325673975, valid 0.00018206419190391898
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0002914919750764966
Batch  11  loss:  0.00018841482233256102
Batch  21  loss:  0.000181297175004147
Batch  31  loss:  0.00019545727991499007
Batch  41  loss:  0.0002362831583013758
Batch  51  loss:  0.00021475728135555983
Batch  61  loss:  0.00015005760360509157
Batch  71  loss:  0.00015522565809078515
Batch  81  loss:  0.00023353096912615
Batch  91  loss:  0.0004529141588136554
Batch  101  loss:  0.0005414568586274981
Batch  111  loss:  0.00035355225554667413
Batch  121  loss:  0.0003044091281481087
Batch  131  loss:  0.0001678087719483301
Batch  141  loss:  0.00022601989621762186
Batch  151  loss:  0.00028673498309217393
Batch  161  loss:  0.00048820488154888153
Batch  171  loss:  0.00023612554650753736
Batch  181  loss:  0.00021387345623224974
Batch  191  loss:  0.00026358733884990215
Validation on real data: 
LOSS supervised-train 0.00024825437743857036, valid 0.00017966391169466078
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0003667620476335287
Batch  11  loss:  0.00016456510638818145
Batch  21  loss:  0.0002144775353372097
Batch  31  loss:  0.00018190518312621862
Batch  41  loss:  0.00022891380649525672
Batch  51  loss:  0.00030228670220822096
Batch  61  loss:  0.0002074877847917378
Batch  71  loss:  0.00021677016047760844
Batch  81  loss:  0.0002416857605567202
Batch  91  loss:  0.0003691597667057067
Batch  101  loss:  0.000440398434875533
Batch  111  loss:  0.00029943662229925394
Batch  121  loss:  0.0002501026028767228
Batch  131  loss:  0.0001850309781730175
Batch  141  loss:  0.00022198313672561198
Batch  151  loss:  0.00025115738390013576
Batch  161  loss:  0.0004551538440864533
Batch  171  loss:  0.0001739485451253131
Batch  181  loss:  0.00022188427101355046
Batch  191  loss:  0.00029834682936780155
Validation on real data: 
LOSS supervised-train 0.00023940111503179651, valid 0.00024640842457301915
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00027983784093521535
Batch  11  loss:  0.00020633317762985826
Batch  21  loss:  0.00020765229419339448
Batch  31  loss:  0.0001794213749235496
Batch  41  loss:  0.00020903242693748325
Batch  51  loss:  0.0002600692387204617
Batch  61  loss:  0.00019906758097931743
Batch  71  loss:  0.00024526604101993144
Batch  81  loss:  0.00019835236889775842
Batch  91  loss:  0.0003102820774074644
Batch  101  loss:  0.0005713244900107384
Batch  111  loss:  0.0003646664263214916
Batch  121  loss:  0.00024228048278018832
Batch  131  loss:  0.00017218930588569492
Batch  141  loss:  0.0002453396446071565
Batch  151  loss:  0.0002366527187405154
Batch  161  loss:  0.0003690274606924504
Batch  171  loss:  0.00020054630294907838
Batch  181  loss:  0.00022092073049861938
Batch  191  loss:  0.0003483449690975249
Validation on real data: 
LOSS supervised-train 0.0002391750922834035, valid 0.00018379178072791547
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00029158408869989216
Batch  11  loss:  0.00016787713684607297
Batch  21  loss:  0.00016186652646865696
Batch  31  loss:  0.00017156820103991777
Batch  41  loss:  0.0002014409692492336
Batch  51  loss:  0.0002223686024080962
Batch  61  loss:  0.0001631353807169944
Batch  71  loss:  0.00021855669911019504
Batch  81  loss:  0.00023824900563340634
Batch  91  loss:  0.00033797803916968405
Batch  101  loss:  0.0005646903882734478
Batch  111  loss:  0.00020952921477146447
Batch  121  loss:  0.0002917203528340906
Batch  131  loss:  0.00019099695782642812
Batch  141  loss:  0.00018292252207174897
Batch  151  loss:  0.00026513487682677805
Batch  161  loss:  0.0004464194062165916
Batch  171  loss:  0.0002213607367593795
Batch  181  loss:  0.0002190059021813795
Batch  191  loss:  0.00025991842267103493
Validation on real data: 
LOSS supervised-train 0.00023831583253922873, valid 0.000150718551594764
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.00024567742366343737
Batch  11  loss:  0.00017079940880648792
Batch  21  loss:  0.00011028946028091013
Batch  31  loss:  0.00014126914902590215
Batch  41  loss:  0.00018375314539298415
Batch  51  loss:  0.0002272474957862869
Batch  61  loss:  0.00013657168892677873
Batch  71  loss:  0.00016646794392727315
Batch  81  loss:  0.0001432291610399261
Batch  91  loss:  0.000354718416929245
Batch  101  loss:  0.00042775063775479794
Batch  111  loss:  0.0002046766021521762
Batch  121  loss:  0.00018120511958841234
Batch  131  loss:  0.0001960027584573254
Batch  141  loss:  0.0002116033574566245
Batch  151  loss:  0.00015709298895671964
Batch  161  loss:  0.0003902322787325829
Batch  171  loss:  0.00015909256762824953
Batch  181  loss:  0.00021579703025054187
Batch  191  loss:  0.00027433334616944194
Validation on real data: 
LOSS supervised-train 0.00022339984549034852, valid 0.00013187600416131318
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00021357428340706974
Batch  11  loss:  0.00016468807007186115
Batch  21  loss:  0.00018465462198946625
Batch  31  loss:  0.00018624038784764707
Batch  41  loss:  0.00019919054466299713
Batch  51  loss:  0.00020022757234983146
Batch  61  loss:  0.00013105067773722112
Batch  71  loss:  0.00016945959941949695
Batch  81  loss:  0.00016845710342749953
Batch  91  loss:  0.00036802320391871035
Batch  101  loss:  0.0006124397041276097
Batch  111  loss:  0.0003116933221463114
Batch  121  loss:  0.00023269961820915341
Batch  131  loss:  0.00015827662718947977
Batch  141  loss:  0.00027068189228884876
Batch  151  loss:  0.0002590965887065977
Batch  161  loss:  0.00037195906043052673
Batch  171  loss:  0.0001471370633225888
Batch  181  loss:  0.00018495923723094165
Batch  191  loss:  0.0003263486723881215
Validation on real data: 
LOSS supervised-train 0.00023580930072057528, valid 0.00026611227076500654
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0001794383570086211
Batch  11  loss:  0.00017707761435303837
Batch  21  loss:  0.00014164653839543462
Batch  31  loss:  0.00014459210797213018
Batch  41  loss:  0.0002370780275668949
Batch  51  loss:  0.00030536105623468757
Batch  61  loss:  0.0001613335043657571
Batch  71  loss:  0.0001903576048789546
Batch  81  loss:  0.00014967071183491498
Batch  91  loss:  0.00027463494916446507
Batch  101  loss:  0.0005372613086365163
Batch  111  loss:  0.00025699951220303774
Batch  121  loss:  0.0002174443652620539
Batch  131  loss:  0.0001780412276275456
Batch  141  loss:  0.00024350330932065845
Batch  151  loss:  0.00021243702212814242
Batch  161  loss:  0.0003371936618350446
Batch  171  loss:  0.00017488643061369658
Batch  181  loss:  0.000213175211683847
Batch  191  loss:  0.0003134864382445812
Validation on real data: 
LOSS supervised-train 0.00021691596331947948, valid 0.0002123872545780614
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00019262987188994884
Batch  11  loss:  0.0001811122492654249
Batch  21  loss:  0.00015498936409130692
Batch  31  loss:  0.00015572267875541002
Batch  41  loss:  0.00018939637811854482
Batch  51  loss:  0.00021736303460784256
Batch  61  loss:  0.00014972964709158987
Batch  71  loss:  0.00018565542995929718
Batch  81  loss:  0.00014559653936885297
Batch  91  loss:  0.00041652077925391495
Batch  101  loss:  0.0004503986856434494
Batch  111  loss:  0.00020074074564035982
Batch  121  loss:  0.00020564469741657376
Batch  131  loss:  0.00017449675942771137
Batch  141  loss:  0.00020267195941414684
Batch  151  loss:  0.00019573158351704478
Batch  161  loss:  0.0003284811391495168
Batch  171  loss:  0.00023449606669601053
Batch  181  loss:  0.00019977573538199067
Batch  191  loss:  0.0002789851860143244
Validation on real data: 
LOSS supervised-train 0.00021421310266305226, valid 0.00017428785213269293
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.00018022514996118844
Batch  11  loss:  0.0001378772285534069
Batch  21  loss:  0.000163756194524467
Batch  31  loss:  0.00026799869374372065
Batch  41  loss:  0.00017160635616164654
Batch  51  loss:  0.00023059267550706863
Batch  61  loss:  0.00016622307884972543
Batch  71  loss:  0.0002397668140474707
Batch  81  loss:  0.00021756780915893614
Batch  91  loss:  0.00028795041725970805
Batch  101  loss:  0.0006158770411275327
Batch  111  loss:  0.00025679258396849036
Batch  121  loss:  0.00020540159312076867
Batch  131  loss:  0.0001709843782009557
Batch  141  loss:  0.00023731353576295078
Batch  151  loss:  0.0002794705505948514
Batch  161  loss:  0.0003436214756220579
Batch  171  loss:  0.00023114898067433387
Batch  181  loss:  0.00018366995209362358
Batch  191  loss:  0.0003196681500412524
Validation on real data: 
LOSS supervised-train 0.0002170759691944113, valid 0.00018261748482473195
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00022406745119951665
Batch  11  loss:  0.00010882748028961942
Batch  21  loss:  0.00016390852397307754
Batch  31  loss:  0.00012886464537587017
Batch  41  loss:  0.0001454094599466771
Batch  51  loss:  0.00017771571583580226
Batch  61  loss:  0.00016948979464359581
Batch  71  loss:  0.00018623536743689328
Batch  81  loss:  0.00017906216089613736
Batch  91  loss:  0.00033885359880514443
Batch  101  loss:  0.0005100853159092367
Batch  111  loss:  0.00024201306223403662
Batch  121  loss:  0.00022439176973421127
Batch  131  loss:  0.0001763518521329388
Batch  141  loss:  0.00021160395408514887
Batch  151  loss:  0.0002493236097507179
Batch  161  loss:  0.00033814265043474734
Batch  171  loss:  0.00020110934565309435
Batch  181  loss:  0.00021333980839699507
Batch  191  loss:  0.00027849635807797313
Validation on real data: 
LOSS supervised-train 0.0002182194641500246, valid 0.00021941888553556055
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.00018002986325882375
Batch  11  loss:  0.00015475129475817084
Batch  21  loss:  0.00020213193784002215
Batch  31  loss:  0.00018478660786058754
Batch  41  loss:  0.00018227731925435364
Batch  51  loss:  0.00020001086522825062
Batch  61  loss:  0.00015201294445432723
Batch  71  loss:  0.00016427587252110243
Batch  81  loss:  0.0002468717284500599
Batch  91  loss:  0.0003002957091666758
Batch  101  loss:  0.00045544601744040847
Batch  111  loss:  0.00020392898295540363
Batch  121  loss:  0.0001834499416872859
Batch  131  loss:  0.00015275151235982776
Batch  141  loss:  0.00017360845231451094
Batch  151  loss:  0.00017227699572686106
Batch  161  loss:  0.00028864984051324427
Batch  171  loss:  0.00017490095342509449
Batch  181  loss:  0.00019734870875254273
Batch  191  loss:  0.0002766300749499351
Validation on real data: 
LOSS supervised-train 0.00020957205913873622, valid 0.00015886835171841085
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00023256275744643062
Batch  11  loss:  0.000141108306706883
Batch  21  loss:  0.0001114521364797838
Batch  31  loss:  0.00015395991795230657
Batch  41  loss:  0.0001609720929991454
Batch  51  loss:  0.0002443287812639028
Batch  61  loss:  0.00015030473878141493
Batch  71  loss:  0.00017344695515930653
Batch  81  loss:  0.00014837081835139543
Batch  91  loss:  0.0003363410651218146
Batch  101  loss:  0.00042337572085671127
Batch  111  loss:  0.0002737017930485308
Batch  121  loss:  0.00018409700714983046
Batch  131  loss:  0.00014127418398857117
Batch  141  loss:  0.0002117780240951106
Batch  151  loss:  0.00022221452672965825
Batch  161  loss:  0.00028235471108928323
Batch  171  loss:  0.00017210810619872063
Batch  181  loss:  0.000195053216884844
Batch  191  loss:  0.00022081396309658885
Validation on real data: 
LOSS supervised-train 0.00020853176265518415, valid 0.00013684410077985376
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00021473373635672033
Batch  11  loss:  0.00012369641626719385
Batch  21  loss:  0.00014648410433437675
Batch  31  loss:  0.00010826849029399455
Batch  41  loss:  0.00016464748478028923
Batch  51  loss:  0.00017586583271622658
Batch  61  loss:  0.0001655642263358459
Batch  71  loss:  0.0001451912976335734
Batch  81  loss:  0.00018305430421605706
Batch  91  loss:  0.00033465668093413115
Batch  101  loss:  0.0004669589689001441
Batch  111  loss:  0.00022820691810920835
Batch  121  loss:  0.0001737530983518809
Batch  131  loss:  0.00016365229384973645
Batch  141  loss:  0.00018734710465651006
Batch  151  loss:  0.00017317892343271524
Batch  161  loss:  0.00036211544647812843
Batch  171  loss:  0.00014518700481858104
Batch  181  loss:  0.00016955885803326964
Batch  191  loss:  0.000282465189229697
Validation on real data: 
LOSS supervised-train 0.00020824502567847958, valid 0.00017901149112731218
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0001812855334719643
Batch  11  loss:  0.00016378365398850292
Batch  21  loss:  0.000128521875012666
Batch  31  loss:  0.00014413081225939095
Batch  41  loss:  0.00017879593360703439
Batch  51  loss:  0.00023076268553268164
Batch  61  loss:  0.0001538148062536493
Batch  71  loss:  0.00014103302964940667
Batch  81  loss:  0.00017365420353598893
Batch  91  loss:  0.0003304341807961464
Batch  101  loss:  0.0003380889247637242
Batch  111  loss:  0.0002680355974007398
Batch  121  loss:  0.00020477957150433213
Batch  131  loss:  0.00016368980868719518
Batch  141  loss:  0.00016508385306224227
Batch  151  loss:  0.0002293900033691898
Batch  161  loss:  0.00030386768048629165
Batch  171  loss:  0.00018639127665665
Batch  181  loss:  0.00017517957894597203
Batch  191  loss:  0.00022168214491102844
Validation on real data: 
LOSS supervised-train 0.00020135691949690226, valid 0.00016251124907284975
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00017603269952815026
Batch  11  loss:  0.00013785318878944963
Batch  21  loss:  0.0001387861411785707
Batch  31  loss:  0.0001608124002814293
Batch  41  loss:  0.00013942150690127164
Batch  51  loss:  0.00019406620413064957
Batch  61  loss:  0.0002190585364587605
Batch  71  loss:  0.00019058478937949985
Batch  81  loss:  0.00017159440903924406
Batch  91  loss:  0.00031613415922038257
Batch  101  loss:  0.0005165957263670862
Batch  111  loss:  0.0002455046633258462
Batch  121  loss:  0.00014030029706191272
Batch  131  loss:  0.00024093050160445273
Batch  141  loss:  0.00021523293980862945
Batch  151  loss:  0.00019257435633335263
Batch  161  loss:  0.00028319406555965543
Batch  171  loss:  0.00013206589210312814
Batch  181  loss:  0.00018372114573139697
Batch  191  loss:  0.0003037833666894585
Validation on real data: 
LOSS supervised-train 0.00020166268303000833, valid 0.00017553940415382385
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00022469056420959532
Batch  11  loss:  0.0001935834006872028
Batch  21  loss:  0.000142079807119444
Batch  31  loss:  0.00013555561599787325
Batch  41  loss:  0.00019100088684353977
Batch  51  loss:  0.0002540006535127759
Batch  61  loss:  0.0001233417569892481
Batch  71  loss:  0.00020850069995503873
Batch  81  loss:  0.00019895459990948439
Batch  91  loss:  0.0003359714464750141
Batch  101  loss:  0.0004465988895390183
Batch  111  loss:  0.00027202494675293565
Batch  121  loss:  0.00020728236995637417
Batch  131  loss:  0.00019983755191788077
Batch  141  loss:  0.00020348120597191155
Batch  151  loss:  0.00013983120152261108
Batch  161  loss:  0.0003138455213047564
Batch  171  loss:  0.00018531516252551228
Batch  181  loss:  0.00017095604562200606
Batch  191  loss:  0.0003705392009578645
Validation on real data: 
LOSS supervised-train 0.0002062817008481943, valid 0.00017636969278100878
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00017289133393205702
Batch  11  loss:  0.00014611850201617926
Batch  21  loss:  0.00015023812011349946
Batch  31  loss:  0.0001775959535734728
Batch  41  loss:  0.00015238256310112774
Batch  51  loss:  0.00018506651395000517
Batch  61  loss:  0.0001870830892585218
Batch  71  loss:  0.0001493911986472085
Batch  81  loss:  0.00023649803188163787
Batch  91  loss:  0.00024052300432231277
Batch  101  loss:  0.0003911806852556765
Batch  111  loss:  0.00017396637122146785
Batch  121  loss:  0.00013348026550374925
Batch  131  loss:  0.00017703711637295783
Batch  141  loss:  0.0001575431233504787
Batch  151  loss:  0.0002193333493778482
Batch  161  loss:  0.0002239748282590881
Batch  171  loss:  0.0001607681333553046
Batch  181  loss:  0.00018307864957023412
Batch  191  loss:  0.00020331950508989394
Validation on real data: 
LOSS supervised-train 0.00019337319194164592, valid 0.00018966893549077213
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00023593594960402697
Batch  11  loss:  0.0001023572840495035
Batch  21  loss:  0.00013436457084026188
Batch  31  loss:  0.000218571862205863
Batch  41  loss:  0.00019211843027733266
Batch  51  loss:  0.00022991876176092774
Batch  61  loss:  0.00014966011804062873
Batch  71  loss:  0.00014852678577881306
Batch  81  loss:  0.00016677609528414905
Batch  91  loss:  0.0002654456766322255
Batch  101  loss:  0.0003476242709439248
Batch  111  loss:  0.0001724050525808707
Batch  121  loss:  0.00019468147365842015
Batch  131  loss:  0.00016165470879059285
Batch  141  loss:  0.00022146008268464357
Batch  151  loss:  0.00012097961734980345
Batch  161  loss:  0.00024832962662912905
Batch  171  loss:  0.00014756536984350532
Batch  181  loss:  0.00020738963212352246
Batch  191  loss:  0.0001904499513329938
Validation on real data: 
LOSS supervised-train 0.0001959335906940396, valid 0.0001226494205184281
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00026225668261758983
Batch  11  loss:  0.00011877810175064951
Batch  21  loss:  0.0002004358684644103
Batch  31  loss:  0.00013179444067645818
Batch  41  loss:  0.00018747719877865165
Batch  51  loss:  0.00019052460265811533
Batch  61  loss:  0.00014785378880333155
Batch  71  loss:  0.0001290130749111995
Batch  81  loss:  0.00012356499792076647
Batch  91  loss:  0.0003066417411901057
Batch  101  loss:  0.00042492232751101255
Batch  111  loss:  0.00023644461180083454
Batch  121  loss:  0.00017805669631343335
Batch  131  loss:  0.00016855669673532248
Batch  141  loss:  0.00023872617748565972
Batch  151  loss:  0.000261355860857293
Batch  161  loss:  0.00034429228981025517
Batch  171  loss:  0.00017119194671977311
Batch  181  loss:  0.00018526236817706376
Batch  191  loss:  0.0002199332375312224
Validation on real data: 
LOSS supervised-train 0.00019530910612957085, valid 0.0001382963382638991
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00016162771498784423
Batch  11  loss:  0.00015416719543281943
Batch  21  loss:  0.00014144254964776337
Batch  31  loss:  0.00013541325461119413
Batch  41  loss:  0.00015262170927599072
Batch  51  loss:  0.00022275310766417533
Batch  61  loss:  0.0001566903811180964
Batch  71  loss:  0.00018865425954572856
Batch  81  loss:  0.000170665152836591
Batch  91  loss:  0.0003542131744325161
Batch  101  loss:  0.00040441006422042847
Batch  111  loss:  0.00018871185602620244
Batch  121  loss:  0.00020953746570739895
Batch  131  loss:  0.0001276915572816506
Batch  141  loss:  0.000148092964082025
Batch  151  loss:  0.00017389559070579708
Batch  161  loss:  0.00037584101664833724
Batch  171  loss:  0.00012095965212211013
Batch  181  loss:  0.0001546096900710836
Batch  191  loss:  0.0002629939990583807
Validation on real data: 
LOSS supervised-train 0.00020166071615676629, valid 0.00017111722263507545
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00020266787032596767
Batch  11  loss:  0.00015505673945881426
Batch  21  loss:  0.00013900188787374645
Batch  31  loss:  0.00012495969713199884
Batch  41  loss:  0.0001523609389550984
Batch  51  loss:  0.00020359206246212125
Batch  61  loss:  0.00012183067883597687
Batch  71  loss:  0.00016425213834736496
Batch  81  loss:  0.0002029367460636422
Batch  91  loss:  0.00027061704895459116
Batch  101  loss:  0.00041893578600138426
Batch  111  loss:  0.00022421192261390388
Batch  121  loss:  0.00019439957395661622
Batch  131  loss:  0.00015427534526679665
Batch  141  loss:  0.0001766815548762679
Batch  151  loss:  0.00017075556388590485
Batch  161  loss:  0.0003387370961718261
Batch  171  loss:  0.00014731254486832768
Batch  181  loss:  0.00021135556744411588
Batch  191  loss:  0.00024545035557821393
Validation on real data: 
LOSS supervised-train 0.0001878587772807805, valid 0.00022553966846317053
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00018699887732509524
Batch  11  loss:  0.00016196972865145653
Batch  21  loss:  0.00013859722821507603
Batch  31  loss:  0.00014776935859117657
Batch  41  loss:  0.0001245429739356041
Batch  51  loss:  0.00019412461551837623
Batch  61  loss:  0.00016065264935605228
Batch  71  loss:  0.0001853700669016689
Batch  81  loss:  0.00013655620568897575
Batch  91  loss:  0.0002817991771735251
Batch  101  loss:  0.00038339823368005455
Batch  111  loss:  0.00020467153808567673
Batch  121  loss:  0.00017765471420716494
Batch  131  loss:  0.00016026690718717873
Batch  141  loss:  0.00018589847604744136
Batch  151  loss:  0.00016415846766903996
Batch  161  loss:  0.000290767929982394
Batch  171  loss:  0.00016283888544421643
Batch  181  loss:  0.00018045831529889256
Batch  191  loss:  0.0002387419663136825
Validation on real data: 
LOSS supervised-train 0.00019074486215686192, valid 0.00013627625594381243
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00019273038196843117
Batch  11  loss:  0.00011566190369194373
Batch  21  loss:  0.00011208603245904669
Batch  31  loss:  0.00020243829931132495
Batch  41  loss:  0.00017232961545232683
Batch  51  loss:  0.00024373071209993213
Batch  61  loss:  0.00011085349979111925
Batch  71  loss:  0.0001958753855433315
Batch  81  loss:  0.00016389417578466237
Batch  91  loss:  0.0002173263783333823
Batch  101  loss:  0.00044003373477607965
Batch  111  loss:  0.00019216346845496446
Batch  121  loss:  0.00015071964298840612
Batch  131  loss:  0.00016726240573916584
Batch  141  loss:  0.00017287503578700125
Batch  151  loss:  0.0001628041354706511
Batch  161  loss:  0.0003055745910387486
Batch  171  loss:  0.00017027690773829818
Batch  181  loss:  0.00021720527729485184
Batch  191  loss:  0.0002412519243080169
Validation on real data: 
LOSS supervised-train 0.0001856438527465798, valid 0.00011927091691177338
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00019616873760242015
Batch  11  loss:  0.0001542174577480182
Batch  21  loss:  0.00014641978486906737
Batch  31  loss:  0.00018654126324690878
Batch  41  loss:  0.00017361495702061802
Batch  51  loss:  0.00016922471695579588
Batch  61  loss:  0.00015895847172942013
Batch  71  loss:  0.0001273103553103283
Batch  81  loss:  0.00012577392044477165
Batch  91  loss:  0.00026895542396232486
Batch  101  loss:  0.00045323825906962156
Batch  111  loss:  0.000247118528932333
Batch  121  loss:  0.00015931409143377095
Batch  131  loss:  0.00012121820327593014
Batch  141  loss:  0.00018523338076192886
Batch  151  loss:  0.00016049851546995342
Batch  161  loss:  0.0003357621608301997
Batch  171  loss:  0.00019125982362311333
Batch  181  loss:  0.00020324232173152268
Batch  191  loss:  0.00029694646946154535
Validation on real data: 
LOSS supervised-train 0.00018904111115261913, valid 0.00024761358508840203
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00016857647278811783
Batch  11  loss:  0.00014594402455259115
Batch  21  loss:  0.00013332722301129252
Batch  31  loss:  0.00011894369526999071
Batch  41  loss:  0.00019472998974379152
Batch  51  loss:  0.00020415824837982655
Batch  61  loss:  0.00013165228301659226
Batch  71  loss:  0.00020145814050920308
Batch  81  loss:  0.00019390418310649693
Batch  91  loss:  0.00021944769832771271
Batch  101  loss:  0.0004396940639708191
Batch  111  loss:  0.00016119034262374043
Batch  121  loss:  0.00017184208263643086
Batch  131  loss:  0.00016430638788733631
Batch  141  loss:  0.00016480348131153733
Batch  151  loss:  0.00018279283540323377
Batch  161  loss:  0.00028761810972355306
Batch  171  loss:  0.00016502552898600698
Batch  181  loss:  0.00014841236406937242
Batch  191  loss:  0.0002811569138430059
Validation on real data: 
LOSS supervised-train 0.00017834438931458864, valid 0.0001636784872971475
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0001767097710398957
Batch  11  loss:  0.00010628167365211993
Batch  21  loss:  0.0002098668337566778
Batch  31  loss:  0.0001599689421709627
Batch  41  loss:  0.00014100351836532354
Batch  51  loss:  0.00022357073612511158
Batch  61  loss:  0.00020984570437576622
Batch  71  loss:  0.0001848769752541557
Batch  81  loss:  0.00019350052752997726
Batch  91  loss:  0.000330642651533708
Batch  101  loss:  0.00040640076622366905
Batch  111  loss:  0.00019797343702521175
Batch  121  loss:  0.0001642478018766269
Batch  131  loss:  0.00015068754146341234
Batch  141  loss:  0.00017909969028551131
Batch  151  loss:  0.00020725395006593317
Batch  161  loss:  0.000252780708251521
Batch  171  loss:  0.00013198665692470968
Batch  181  loss:  0.00017857614147942513
Batch  191  loss:  0.00018677584012039006
Validation on real data: 
LOSS supervised-train 0.00018440523137542187, valid 0.00017663621110841632
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00019728526240214705
Batch  11  loss:  0.00011906383588211611
Batch  21  loss:  0.00010933259909506887
Batch  31  loss:  0.00012652981968130916
Batch  41  loss:  0.00015828647883608937
Batch  51  loss:  0.00019312444783281535
Batch  61  loss:  0.0001050948048941791
Batch  71  loss:  0.00017302784544881433
Batch  81  loss:  0.00017088877211790532
Batch  91  loss:  0.00025340996216982603
Batch  101  loss:  0.00041688012424856424
Batch  111  loss:  0.00019385735504329205
Batch  121  loss:  0.00018216478929389268
Batch  131  loss:  0.00011629810614977032
Batch  141  loss:  0.00025318050757050514
Batch  151  loss:  0.00015134016575757414
Batch  161  loss:  0.00023575822706334293
Batch  171  loss:  0.0001605842262506485
Batch  181  loss:  0.00013304271851666272
Batch  191  loss:  0.0002647268702276051
Validation on real data: 
LOSS supervised-train 0.00017265982234675902, valid 0.00015595703735016286
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00019451286061666906
Batch  11  loss:  0.00015905825421214104
Batch  21  loss:  0.00013906684762332588
Batch  31  loss:  0.0001350830716546625
Batch  41  loss:  0.00021078178542666137
Batch  51  loss:  0.00019698460528161377
Batch  61  loss:  0.00011575183452805504
Batch  71  loss:  0.0001158695449703373
Batch  81  loss:  0.00010686043242458254
Batch  91  loss:  0.0002493740466888994
Batch  101  loss:  0.00034209611476399004
Batch  111  loss:  0.00017996290989685804
Batch  121  loss:  0.00016454208525829017
Batch  131  loss:  0.00011239454033784568
Batch  141  loss:  0.0001436521124560386
Batch  151  loss:  0.00015678639465477318
Batch  161  loss:  0.00022617318609263748
Batch  171  loss:  0.00017144507728517056
Batch  181  loss:  0.0002003221889026463
Batch  191  loss:  0.00026463926769793034
Validation on real data: 
LOSS supervised-train 0.0001709360979293706, valid 0.00014988382463343441
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00018147460650652647
Batch  11  loss:  9.501942986389622e-05
Batch  21  loss:  0.00014057430962566286
Batch  31  loss:  0.00012093033728888258
Batch  41  loss:  0.00012532503751572222
Batch  51  loss:  0.00026222955784760416
Batch  61  loss:  0.00015919901488814503
Batch  71  loss:  0.0001574059424456209
Batch  81  loss:  0.00013932399451732635
Batch  91  loss:  0.0003101020120084286
Batch  101  loss:  0.0003690957964863628
Batch  111  loss:  0.00016375425911974162
Batch  121  loss:  0.00012386139133013785
Batch  131  loss:  0.00015166863158810884
Batch  141  loss:  0.0001826093503041193
Batch  151  loss:  0.0001339795853709802
Batch  161  loss:  0.00031282706186175346
Batch  171  loss:  0.0001640361442696303
Batch  181  loss:  0.00018564575293567032
Batch  191  loss:  0.00022143345267977566
Validation on real data: 
LOSS supervised-train 0.00017414386740711052, valid 0.00020169616618659347
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0001945322728715837
Batch  11  loss:  0.00014041349641047418
Batch  21  loss:  0.000123585487017408
Batch  31  loss:  0.00010495133756194264
Batch  41  loss:  0.00013512246368918568
Batch  51  loss:  0.00022351935331244022
Batch  61  loss:  0.000133847031975165
Batch  71  loss:  0.0001547085412312299
Batch  81  loss:  0.00013529675197787583
Batch  91  loss:  0.0002859927772078663
Batch  101  loss:  0.00034685939317569137
Batch  111  loss:  0.0002526161842979491
Batch  121  loss:  0.00017207843484357
Batch  131  loss:  0.00014576147077605128
Batch  141  loss:  0.0001378899469273165
Batch  151  loss:  0.00018270527652930468
Batch  161  loss:  0.00029498562798835337
Batch  171  loss:  0.00015965825878083706
Batch  181  loss:  0.00019707826140802354
Batch  191  loss:  0.0001754834083840251
Validation on real data: 
LOSS supervised-train 0.0001740619676638744, valid 0.00011383337550796568
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00016958892229013145
Batch  11  loss:  0.00015999523748178035
Batch  21  loss:  0.00011280886246822774
Batch  31  loss:  0.00014427276619244367
Batch  41  loss:  0.00014133279910311103
Batch  51  loss:  0.0001965752599062398
Batch  61  loss:  0.00013514187594410032
Batch  71  loss:  0.00016692023200448602
Batch  81  loss:  0.00013804946502204984
Batch  91  loss:  0.0002563163870945573
Batch  101  loss:  0.0003532604896463454
Batch  111  loss:  0.00013794662663713098
Batch  121  loss:  0.00014071438636165112
Batch  131  loss:  0.00013752887025475502
Batch  141  loss:  0.00017262257460970432
Batch  151  loss:  0.00016047488315962255
Batch  161  loss:  0.00021100240701343864
Batch  171  loss:  0.00019643630366772413
Batch  181  loss:  0.0001915056345751509
Batch  191  loss:  0.0002348362177144736
Validation on real data: 
LOSS supervised-train 0.00016975903050479247, valid 0.00017687698709778488
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00016788560606073588
Batch  11  loss:  0.00011682321928674355
Batch  21  loss:  0.00016177773068193346
Batch  31  loss:  0.00015183821960818022
Batch  41  loss:  0.00016827255603857338
Batch  51  loss:  0.00018623284995555878
Batch  61  loss:  0.000155409230501391
Batch  71  loss:  0.00016780746227595955
Batch  81  loss:  0.00018229610577691346
Batch  91  loss:  0.0002811063313856721
Batch  101  loss:  0.0003697555512189865
Batch  111  loss:  0.0002310692798346281
Batch  121  loss:  0.00014388702402357012
Batch  131  loss:  0.00016596783825661987
Batch  141  loss:  0.00016938516637310386
Batch  151  loss:  0.0001647278986638412
Batch  161  loss:  0.0002585133188404143
Batch  171  loss:  0.00014818842464592308
Batch  181  loss:  0.0001663855800870806
Batch  191  loss:  0.00024359137751162052
Validation on real data: 
LOSS supervised-train 0.0001776262231214787, valid 0.00014639818982686847
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00013758736895397305
Batch  11  loss:  0.0001829663960961625
Batch  21  loss:  0.00011345056555001065
Batch  31  loss:  0.0001221951679326594
Batch  41  loss:  0.00012541624892037362
Batch  51  loss:  0.00016583003161940724
Batch  61  loss:  0.00011174574319738895
Batch  71  loss:  0.00013906098320148885
Batch  81  loss:  0.00016197902732528746
Batch  91  loss:  0.00022795780387241393
Batch  101  loss:  0.00032885215478017926
Batch  111  loss:  0.00016506511019542813
Batch  121  loss:  0.00014230558008421212
Batch  131  loss:  0.00011552366777323186
Batch  141  loss:  0.00017739069880917668
Batch  151  loss:  0.00018847953469958156
Batch  161  loss:  0.0002759179042186588
Batch  171  loss:  0.00014112339704297483
Batch  181  loss:  0.00015309208538383245
Batch  191  loss:  0.00019028931274078786
Validation on real data: 
LOSS supervised-train 0.00017203369658091105, valid 0.00012823620636481792
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00018950826779473573
Batch  11  loss:  9.484570182394236e-05
Batch  21  loss:  0.00010936721810139716
Batch  31  loss:  0.00012013169907731935
Batch  41  loss:  0.0001557529321871698
Batch  51  loss:  0.00019227535813115537
Batch  61  loss:  0.00012813834473490715
Batch  71  loss:  0.00013770094665233046
Batch  81  loss:  9.67755404417403e-05
Batch  91  loss:  0.0002440555690554902
Batch  101  loss:  0.00036415993236005306
Batch  111  loss:  0.00021423905855044723
Batch  121  loss:  0.0001671865175012499
Batch  131  loss:  0.00012743727711495012
Batch  141  loss:  0.0001475715107517317
Batch  151  loss:  0.00015242995868902653
Batch  161  loss:  0.00029558668029494584
Batch  171  loss:  0.00014284499047789723
Batch  181  loss:  0.00018704170361161232
Batch  191  loss:  0.00024637338356114924
Validation on real data: 
LOSS supervised-train 0.00016645321727992268, valid 0.0001349137455690652
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00016072379366960377
Batch  11  loss:  0.00011370767606422305
Batch  21  loss:  0.0001515729381935671
Batch  31  loss:  0.00014063695562072098
Batch  41  loss:  0.0001769464579410851
Batch  51  loss:  0.00017695773567538708
Batch  61  loss:  0.0001162087864940986
Batch  71  loss:  0.00016865369980223477
Batch  81  loss:  0.000132511617266573
Batch  91  loss:  0.00031908287201076746
Batch  101  loss:  0.0003320048563182354
Batch  111  loss:  0.00020023930119350553
Batch  121  loss:  0.0001227475149789825
Batch  131  loss:  0.000150531719555147
Batch  141  loss:  0.00018778443336486816
Batch  151  loss:  0.0001655940868658945
Batch  161  loss:  0.00022764995810575783
Batch  171  loss:  0.00013254344230517745
Batch  181  loss:  0.0001867996179498732
Batch  191  loss:  0.00020678203145507723
Validation on real data: 
LOSS supervised-train 0.00017106745384808163, valid 0.00013760264846496284
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.00015458620327990502
Batch  11  loss:  0.00010527230915613472
Batch  21  loss:  0.00012974184937775135
Batch  31  loss:  0.00012456199328880757
Batch  41  loss:  0.00017790762649383396
Batch  51  loss:  0.00021755755005870014
Batch  61  loss:  0.0001269806089112535
Batch  71  loss:  0.00018833327339962125
Batch  81  loss:  0.00012510725355241448
Batch  91  loss:  0.00026051924214698374
Batch  101  loss:  0.0002648024819791317
Batch  111  loss:  0.00015238585183396935
Batch  121  loss:  0.0001434142468497157
Batch  131  loss:  0.00012942020839545876
Batch  141  loss:  0.0001442624779883772
Batch  151  loss:  0.00013556239719036967
Batch  161  loss:  0.000248396914685145
Batch  171  loss:  0.00015603167412336916
Batch  181  loss:  0.00010972734889946878
Batch  191  loss:  0.0001856026065070182
Validation on real data: 
LOSS supervised-train 0.0001711385872476967, valid 0.0001273380476050079
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00016258176765404642
Batch  11  loss:  0.00011869069567183033
Batch  21  loss:  0.00010149913578061387
Batch  31  loss:  0.00013942885561846197
Batch  41  loss:  0.0001698995183687657
Batch  51  loss:  0.00017878586368169636
Batch  61  loss:  0.0001273010711884126
Batch  71  loss:  0.00012350491306278855
Batch  81  loss:  0.0001712784869596362
Batch  91  loss:  0.00018348486628383398
Batch  101  loss:  0.00037462718319147825
Batch  111  loss:  0.00016309809871017933
Batch  121  loss:  0.00017502857372164726
Batch  131  loss:  0.0001202608400490135
Batch  141  loss:  0.00017940127872861922
Batch  151  loss:  0.00021454013767652214
Batch  161  loss:  0.0002626146888360381
Batch  171  loss:  0.00012416102981660515
Batch  181  loss:  0.00016366178169846535
Batch  191  loss:  0.0002030423638643697
Validation on real data: 
LOSS supervised-train 0.00015997049711586442, valid 0.00014682061737403274
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00013556172780226916
Batch  11  loss:  0.00016471896378789097
Batch  21  loss:  0.00015716171765234321
Batch  31  loss:  0.00014781422214582562
Batch  41  loss:  0.0001316824200330302
Batch  51  loss:  0.0002013987977989018
Batch  61  loss:  0.00013059198681730777
Batch  71  loss:  0.00012144932406954467
Batch  81  loss:  0.0001291921507799998
Batch  91  loss:  0.00026778149185702205
Batch  101  loss:  0.0003496861318126321
Batch  111  loss:  0.00017194257816299796
Batch  121  loss:  0.00012241271906532347
Batch  131  loss:  0.00015260827785823494
Batch  141  loss:  0.00016594264889135957
Batch  151  loss:  0.0001555653871037066
Batch  161  loss:  0.0002304389636265114
Batch  171  loss:  0.00011843072570627555
Batch  181  loss:  0.00016851852706167847
Batch  191  loss:  0.00024095166008919477
Validation on real data: 
LOSS supervised-train 0.00016335233292920747, valid 0.00014630742953158915
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00019610353047028184
Batch  11  loss:  0.0001744847686495632
Batch  21  loss:  0.0001100134541047737
Batch  31  loss:  0.0001250871573574841
Batch  41  loss:  0.00010594945342745632
Batch  51  loss:  0.00017947684682440013
Batch  61  loss:  0.00012896402040496469
Batch  71  loss:  0.0001456293393857777
Batch  81  loss:  0.00013421732001006603
Batch  91  loss:  0.00020624087483156472
Batch  101  loss:  0.00035037510679103434
Batch  111  loss:  0.00016794740804471076
Batch  121  loss:  0.00011975685629295185
Batch  131  loss:  9.948883234756067e-05
Batch  141  loss:  0.0001766763161867857
Batch  151  loss:  0.0001502792874816805
Batch  161  loss:  0.0002693235292099416
Batch  171  loss:  0.00015015294775366783
Batch  181  loss:  0.00016427219088654965
Batch  191  loss:  0.00022269159671850502
Validation on real data: 
LOSS supervised-train 0.00016233611218922307, valid 0.00014961298438720405
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00018172524869441986
Batch  11  loss:  0.0001786016655387357
Batch  21  loss:  0.00010738024138845503
Batch  31  loss:  9.332156332675368e-05
Batch  41  loss:  0.00014744293002877384
Batch  51  loss:  0.0001561007957207039
Batch  61  loss:  0.00013796308485325426
Batch  71  loss:  0.000132037399453111
Batch  81  loss:  0.00012032809172524139
Batch  91  loss:  0.00020080518152099103
Batch  101  loss:  0.00045778139610774815
Batch  111  loss:  0.00013424009375739843
Batch  121  loss:  0.00014997941616456956
Batch  131  loss:  0.0001801805483410135
Batch  141  loss:  0.0001503730600234121
Batch  151  loss:  0.00015265830734279007
Batch  161  loss:  0.00023664817854296416
Batch  171  loss:  0.0001398140739183873
Batch  181  loss:  0.00015539172454737127
Batch  191  loss:  0.00019147242710459977
Validation on real data: 
LOSS supervised-train 0.00015668803938751808, valid 0.00010782881872728467
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00016208781744353473
Batch  11  loss:  9.86065570032224e-05
Batch  21  loss:  0.00011323833314236253
Batch  31  loss:  0.00010158109944313765
Batch  41  loss:  0.00012445244647096843
Batch  51  loss:  0.00020218953432049602
Batch  61  loss:  0.00011054118658648804
Batch  71  loss:  0.0001522145321359858
Batch  81  loss:  0.00013122096424922347
Batch  91  loss:  0.00026624088059179485
Batch  101  loss:  0.00039174946141429245
Batch  111  loss:  0.00015716197958681732
Batch  121  loss:  0.0001370598911307752
Batch  131  loss:  0.00011159523273818195
Batch  141  loss:  0.00013121566735208035
Batch  151  loss:  0.00012219669588375837
Batch  161  loss:  0.00023498479276895523
Batch  171  loss:  0.00011352110595908016
Batch  181  loss:  0.0001499230565968901
Batch  191  loss:  0.0001992200850509107
Validation on real data: 
LOSS supervised-train 0.00015806211191375042, valid 0.00013537336781155318
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00016045318625401706
Batch  11  loss:  0.00010030178964370862
Batch  21  loss:  0.0001376038126181811
Batch  31  loss:  0.00010499562631594017
Batch  41  loss:  0.00016883474017959088
Batch  51  loss:  0.00014096141967456788
Batch  61  loss:  0.00012452788359951228
Batch  71  loss:  0.00012490758672356606
Batch  81  loss:  0.00013538972416426986
Batch  91  loss:  0.00025525561068207026
Batch  101  loss:  0.0003115955041721463
Batch  111  loss:  0.00011597853153944016
Batch  121  loss:  0.00016535788017790765
Batch  131  loss:  0.0001117384890676476
Batch  141  loss:  0.0001248128537554294
Batch  151  loss:  0.00015895692922640592
Batch  161  loss:  0.00022913249267730862
Batch  171  loss:  0.00015391728084068745
Batch  181  loss:  0.00019754293316509575
Batch  191  loss:  0.0001909252896439284
Validation on real data: 
LOSS supervised-train 0.00015820031636394562, valid 0.00014770847337786108
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00017555677914060652
Batch  11  loss:  8.331827848451212e-05
Batch  21  loss:  0.00013630534522235394
Batch  31  loss:  0.00011155007086927071
Batch  41  loss:  0.00015020139107946306
Batch  51  loss:  0.00016444522771053016
Batch  61  loss:  0.0001407279632985592
Batch  71  loss:  0.0001456864847568795
Batch  81  loss:  0.00013672788918484002
Batch  91  loss:  0.00023282795154955238
Batch  101  loss:  0.00025598780484870076
Batch  111  loss:  0.00016247403982561082
Batch  121  loss:  0.00011220892338315025
Batch  131  loss:  0.00014490130706690252
Batch  141  loss:  0.00016771741502452642
Batch  151  loss:  0.00015742647519800812
Batch  161  loss:  0.00018264386744704098
Batch  171  loss:  0.00011692668340401724
Batch  181  loss:  0.0001272142253583297
Batch  191  loss:  0.00022728490876033902
Validation on real data: 
LOSS supervised-train 0.00015549942596408074, valid 0.00014234146510716528
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00018852550419978797
Batch  11  loss:  0.00010793341061798856
Batch  21  loss:  0.00012123239866923541
Batch  31  loss:  0.00011370461288606748
Batch  41  loss:  0.00012775871437042952
Batch  51  loss:  0.00014818542695138603
Batch  61  loss:  0.0001144669295172207
Batch  71  loss:  0.00013096885231789201
Batch  81  loss:  0.00010591356840450317
Batch  91  loss:  0.00023007436539046466
Batch  101  loss:  0.000311926007270813
Batch  111  loss:  0.00013395966379903257
Batch  121  loss:  0.00016158247308339924
Batch  131  loss:  0.00012655749742407352
Batch  141  loss:  0.00017025410488713533
Batch  151  loss:  0.00016517228505108505
Batch  161  loss:  0.00020233358372934163
Batch  171  loss:  0.0001343015319434926
Batch  181  loss:  0.00017336715245619416
Batch  191  loss:  0.0001901939103845507
Validation on real data: 
LOSS supervised-train 0.00015401235530589473, valid 0.0001117617212003097
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00015842943685129285
Batch  11  loss:  8.708719542482868e-05
Batch  21  loss:  0.00014744272630196065
Batch  31  loss:  0.00016191735630854964
Batch  41  loss:  0.00012627916294150054
Batch  51  loss:  0.00019386141502764076
Batch  61  loss:  0.00014254335837904364
Batch  71  loss:  0.00016739066631998867
Batch  81  loss:  0.00010368911898694932
Batch  91  loss:  0.00021570069657173008
Batch  101  loss:  0.00029770637047477067
Batch  111  loss:  0.0001778917940100655
Batch  121  loss:  0.00019587318820413202
Batch  131  loss:  0.0001172349220723845
Batch  141  loss:  0.0001424723886884749
Batch  151  loss:  0.00014655022823717445
Batch  161  loss:  0.0001814052084228024
Batch  171  loss:  0.00011326804815325886
Batch  181  loss:  0.00014874710177537054
Batch  191  loss:  0.00024373119231313467
Validation on real data: 
LOSS supervised-train 0.00016039244354033145, valid 0.00012184116349089891
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00015476744738407433
Batch  11  loss:  9.403136937180534e-05
Batch  21  loss:  0.00011230090603930876
Batch  31  loss:  0.0001845942751970142
Batch  41  loss:  0.0001611746265552938
Batch  51  loss:  0.00015310871822293848
Batch  61  loss:  0.00014631435624323785
Batch  71  loss:  0.0001682154106674716
Batch  81  loss:  0.0001428562100045383
Batch  91  loss:  0.00021521108283195645
Batch  101  loss:  0.00028383839526213706
Batch  111  loss:  0.0001505696855019778
Batch  121  loss:  0.00016688868345227093
Batch  131  loss:  9.937589493347332e-05
Batch  141  loss:  0.00016937650798354298
Batch  151  loss:  0.00014674632984679192
Batch  161  loss:  0.00018892220396082848
Batch  171  loss:  0.00011221588647458702
Batch  181  loss:  0.0001542116078780964
Batch  191  loss:  0.00017665751511231065
Validation on real data: 
LOSS supervised-train 0.00014887809025822209, valid 0.00014167575864121318
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00012444482126738876
Batch  11  loss:  0.00014892182662151754
Batch  21  loss:  9.637765469960868e-05
Batch  31  loss:  0.0001235360832652077
Batch  41  loss:  0.00016839898307807744
Batch  51  loss:  0.000180130431544967
Batch  61  loss:  0.00012015791435260326
Batch  71  loss:  0.00013517001934815198
Batch  81  loss:  0.00014604837633669376
Batch  91  loss:  0.00020491561735980213
Batch  101  loss:  0.00037113961298018694
Batch  111  loss:  0.00017700041644275188
Batch  121  loss:  0.00013975870388094336
Batch  131  loss:  0.00014109666517470032
Batch  141  loss:  0.00013524982205126435
Batch  151  loss:  0.00011750309204217046
Batch  161  loss:  0.00016000453615561128
Batch  171  loss:  0.00016385555500164628
Batch  181  loss:  0.00019258550310041755
Batch  191  loss:  0.00019774449174292386
Validation on real data: 
LOSS supervised-train 0.00015128112194361164, valid 0.00011452587932581082
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00013488611148204654
Batch  11  loss:  9.406750177731737e-05
Batch  21  loss:  0.00012384752335492522
Batch  31  loss:  0.00013538463099393994
Batch  41  loss:  0.00014605672913603485
Batch  51  loss:  0.0001750734227243811
Batch  61  loss:  9.71103654592298e-05
Batch  71  loss:  0.00013744019088335335
Batch  81  loss:  9.966942889150232e-05
Batch  91  loss:  0.00018739793449640274
Batch  101  loss:  0.00044547728612087667
Batch  111  loss:  0.00015208609693218023
Batch  121  loss:  0.00013108189159538597
Batch  131  loss:  0.00011625210754573345
Batch  141  loss:  0.00017605701577849686
Batch  151  loss:  0.00012047243217239156
Batch  161  loss:  0.00035856023896485567
Batch  171  loss:  0.0001424095535185188
Batch  181  loss:  0.0003464761539362371
Batch  191  loss:  0.0002080924023175612
Validation on real data: 
LOSS supervised-train 0.00015089604286913527, valid 0.0001273911038879305
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.00016777038399595767
Batch  11  loss:  0.0001338358997600153
Batch  21  loss:  0.00010949999705189839
Batch  31  loss:  0.00014306652883533388
Batch  41  loss:  0.0001157593505922705
Batch  51  loss:  0.00014272378757596016
Batch  61  loss:  0.0001031857609632425
Batch  71  loss:  0.00012272103049326688
Batch  81  loss:  0.00011855318734887987
Batch  91  loss:  0.00028652363107539713
Batch  101  loss:  0.00024819077225402
Batch  111  loss:  0.00015357095981016755
Batch  121  loss:  0.00013601648970507085
Batch  131  loss:  0.00012893331586383283
Batch  141  loss:  0.00012161752238171175
Batch  151  loss:  0.00010666794696589932
Batch  161  loss:  0.0002233644190710038
Batch  171  loss:  0.00010890494013438001
Batch  181  loss:  0.0001519596262369305
Batch  191  loss:  0.0001587782462593168
Validation on real data: 
LOSS supervised-train 0.00014382726956682745, valid 0.00012216220784466714
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00016497235628776252
Batch  11  loss:  8.280677866423503e-05
Batch  21  loss:  0.00014883399126119912
Batch  31  loss:  0.00011554247612366453
Batch  41  loss:  0.0001371713005937636
Batch  51  loss:  0.0001530325534986332
Batch  61  loss:  0.0001363565243082121
Batch  71  loss:  0.00017027757712639868
Batch  81  loss:  0.00011135807289974764
Batch  91  loss:  0.00018965564959216863
Batch  101  loss:  0.00031587007106281817
Batch  111  loss:  0.00014454239862971008
Batch  121  loss:  0.00012510236410889775
Batch  131  loss:  0.00014485212159343064
Batch  141  loss:  0.00013676000526174903
Batch  151  loss:  0.00013477295578923076
Batch  161  loss:  0.00018462061416357756
Batch  171  loss:  0.00010464480146765709
Batch  181  loss:  0.00011205431655980647
Batch  191  loss:  0.00020254946139175445
Validation on real data: 
LOSS supervised-train 0.00014934833150618944, valid 0.00015060427540447563
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  mug ; Model ID: f3a7f8198cc50c225f5e789acd4d1122
--------------------
Training baseline regression model:  2022-03-30 08:27:43.321203
Detector:  point_transformer
Object:  mug
--------------------
device is  cuda
--------------------
Number of trainable parameters:  896161
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.12848441302776337
Batch  11  loss:  0.026409931480884552
Batch  21  loss:  0.03151686489582062
Batch  31  loss:  0.02158237434923649
Batch  41  loss:  0.015443737618625164
Batch  51  loss:  0.013599664904177189
Batch  61  loss:  0.007704916875809431
Batch  71  loss:  0.013994834385812283
Batch  81  loss:  0.009622789919376373
Batch  91  loss:  0.009830066002905369
Batch  101  loss:  0.00875247921794653
Batch  111  loss:  0.008385277353227139
Batch  121  loss:  0.010277152061462402
Batch  131  loss:  0.004990819375962019
Batch  141  loss:  0.008590246550738811
Batch  151  loss:  0.006544430740177631
Batch  161  loss:  0.006426803767681122
Batch  171  loss:  0.005669300444424152
Batch  181  loss:  0.004473784938454628
Batch  191  loss:  0.004984228406101465
Validation on real data: 
LOSS supervised-train 0.015865541255334393, valid 0.011744052171707153
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.009714452549815178
Batch  11  loss:  0.008230322040617466
Batch  21  loss:  0.005365383345633745
Batch  31  loss:  0.004378947429358959
Batch  41  loss:  0.0027054452802985907
Batch  51  loss:  0.003851136891171336
Batch  61  loss:  0.007204958237707615
Batch  71  loss:  0.004024221561849117
Batch  81  loss:  0.0032740002498030663
Batch  91  loss:  0.004135767929255962
Batch  101  loss:  0.0042358338832855225
Batch  111  loss:  0.0032901265658438206
Batch  121  loss:  0.006190968211740255
Batch  131  loss:  0.0033381113316863775
Batch  141  loss:  0.005868901032954454
Batch  151  loss:  0.005182279273867607
Batch  161  loss:  0.003745023161172867
Batch  171  loss:  0.005783286411315203
Batch  181  loss:  0.002202121540904045
Batch  191  loss:  0.0032135124783962965
Validation on real data: 
LOSS supervised-train 0.0052625607285881415, valid 0.014946251176297665
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0079090790823102
Batch  11  loss:  0.00803857110440731
Batch  21  loss:  0.005886687431484461
Batch  31  loss:  0.003525539068505168
Batch  41  loss:  0.0023991332855075598
Batch  51  loss:  0.0030572356190532446
Batch  61  loss:  0.005956864450126886
Batch  71  loss:  0.0024756970815360546
Batch  81  loss:  0.002243833150714636
Batch  91  loss:  0.0036289638374000788
Batch  101  loss:  0.002628104295581579
Batch  111  loss:  0.0030873168725520372
Batch  121  loss:  0.005015593487769365
Batch  131  loss:  0.0028700607363134623
Batch  141  loss:  0.0038567485753446817
Batch  151  loss:  0.0035845416132360697
Batch  161  loss:  0.0029617680702358484
Batch  171  loss:  0.0036827432923018932
Batch  181  loss:  0.001659016590565443
Batch  191  loss:  0.002837508451193571
Validation on real data: 
LOSS supervised-train 0.004081844682805241, valid 0.007639561779797077
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.006288156379014254
Batch  11  loss:  0.007088192738592625
Batch  21  loss:  0.004852644167840481
Batch  31  loss:  0.0022424885537475348
Batch  41  loss:  0.0016691854689270258
Batch  51  loss:  0.002247641561552882
Batch  61  loss:  0.00548101169988513
Batch  71  loss:  0.0020833080634474754
Batch  81  loss:  0.002390774665400386
Batch  91  loss:  0.0027111107483506203
Batch  101  loss:  0.001858660252764821
Batch  111  loss:  0.002417245414108038
Batch  121  loss:  0.0037860232405364513
Batch  131  loss:  0.0020500021055340767
Batch  141  loss:  0.0030543473549187183
Batch  151  loss:  0.0032070144079625607
Batch  161  loss:  0.002145647071301937
Batch  171  loss:  0.0043881856836378574
Batch  181  loss:  0.0016042334027588367
Batch  191  loss:  0.0023256721906363964
Validation on real data: 
LOSS supervised-train 0.0032720093795796856, valid 0.0047530559822916985
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.004882794339209795
Batch  11  loss:  0.006096620578318834
Batch  21  loss:  0.002602471737191081
Batch  31  loss:  0.0017998021794483066
Batch  41  loss:  0.0014357749605551362
Batch  51  loss:  0.0021678952034562826
Batch  61  loss:  0.004482792690396309
Batch  71  loss:  0.0018066226039081812
Batch  81  loss:  0.0021790729369968176
Batch  91  loss:  0.002118388656526804
Batch  101  loss:  0.0012018090346828103
Batch  111  loss:  0.0021287165582180023
Batch  121  loss:  0.0037221750244498253
Batch  131  loss:  0.0018884459277614951
Batch  141  loss:  0.0026021762751042843
Batch  151  loss:  0.0029986232984811068
Batch  161  loss:  0.0017706104554235935
Batch  171  loss:  0.0034523506183177233
Batch  181  loss:  0.0013511914294213057
Batch  191  loss:  0.002298110630363226
Validation on real data: 
LOSS supervised-train 0.002701122279104311, valid 0.0034590563736855984
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0034864312037825584
Batch  11  loss:  0.0056171421892941
Batch  21  loss:  0.0026614712551236153
Batch  31  loss:  0.0014405000256374478
Batch  41  loss:  0.0012664790265262127
Batch  51  loss:  0.002038719365373254
Batch  61  loss:  0.003586286911740899
Batch  71  loss:  0.0016675848746672273
Batch  81  loss:  0.0020177923142910004
Batch  91  loss:  0.0020923083648085594
Batch  101  loss:  0.0013062426587566733
Batch  111  loss:  0.0017026378773152828
Batch  121  loss:  0.0032572548370808363
Batch  131  loss:  0.0017893260810524225
Batch  141  loss:  0.0023714755661785603
Batch  151  loss:  0.0021953412797302008
Batch  161  loss:  0.0015288788126781583
Batch  171  loss:  0.003387357108294964
Batch  181  loss:  0.0008943700813688338
Batch  191  loss:  0.002115535782650113
Validation on real data: 
LOSS supervised-train 0.0023425713714095766, valid 0.0012782535050064325
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.004159305710345507
Batch  11  loss:  0.004465813748538494
Batch  21  loss:  0.0019651080947369337
Batch  31  loss:  0.0012825444573536515
Batch  41  loss:  0.001431745127774775
Batch  51  loss:  0.00190945144277066
Batch  61  loss:  0.0032438135240226984
Batch  71  loss:  0.0015265175607055426
Batch  81  loss:  0.002248310251161456
Batch  91  loss:  0.001470650197006762
Batch  101  loss:  0.0010387594811618328
Batch  111  loss:  0.0016318245325237513
Batch  121  loss:  0.0026787645183503628
Batch  131  loss:  0.0014539381954818964
Batch  141  loss:  0.0025179290678352118
Batch  151  loss:  0.0018835937371477485
Batch  161  loss:  0.0011728362878784537
Batch  171  loss:  0.0023624456953257322
Batch  181  loss:  0.0012301073875278234
Batch  191  loss:  0.0026649695355445147
Validation on real data: 
LOSS supervised-train 0.0021663620762410575, valid 0.00500619038939476
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0027444460429251194
Batch  11  loss:  0.003725645365193486
Batch  21  loss:  0.0016052420251071453
Batch  31  loss:  0.001414733356796205
Batch  41  loss:  0.001307449652813375
Batch  51  loss:  0.0021730903536081314
Batch  61  loss:  0.0028626909479498863
Batch  71  loss:  0.0013158705551177263
Batch  81  loss:  0.0017351302085444331
Batch  91  loss:  0.0012249305145815015
Batch  101  loss:  0.0008967352914623916
Batch  111  loss:  0.0010815002024173737
Batch  121  loss:  0.002376857679337263
Batch  131  loss:  0.0011443889234215021
Batch  141  loss:  0.0016137075144797564
Batch  151  loss:  0.0018175170989707112
Batch  161  loss:  0.001282653771340847
Batch  171  loss:  0.002495991764590144
Batch  181  loss:  0.001222207909449935
Batch  191  loss:  0.0022571966983377934
Validation on real data: 
LOSS supervised-train 0.0019513456273125484, valid 0.002995816757902503
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0032940253149718046
Batch  11  loss:  0.0034963723737746477
Batch  21  loss:  0.0013640152756124735
Batch  31  loss:  0.0014642216265201569
Batch  41  loss:  0.001115337130613625
Batch  51  loss:  0.0014505541184917092
Batch  61  loss:  0.003094190265983343
Batch  71  loss:  0.0016168388538062572
Batch  81  loss:  0.001758397207595408
Batch  91  loss:  0.0012950142845511436
Batch  101  loss:  0.000645087449811399
Batch  111  loss:  0.0009530826937407255
Batch  121  loss:  0.0023317658342421055
Batch  131  loss:  0.0014431281015276909
Batch  141  loss:  0.001502776867710054
Batch  151  loss:  0.0016332798404619098
Batch  161  loss:  0.0010830133687704802
Batch  171  loss:  0.0024951272644102573
Batch  181  loss:  0.0010779425501823425
Batch  191  loss:  0.002340619219467044
Validation on real data: 
LOSS supervised-train 0.0018546193605288863, valid 0.002286454662680626
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0029983643908053637
Batch  11  loss:  0.00348416855558753
Batch  21  loss:  0.0015285058179870248
Batch  31  loss:  0.0012996678706258535
Batch  41  loss:  0.0013382683973759413
Batch  51  loss:  0.001608681515790522
Batch  61  loss:  0.002218917477875948
Batch  71  loss:  0.001127540715970099
Batch  81  loss:  0.002257436281070113
Batch  91  loss:  0.0013673040084540844
Batch  101  loss:  0.0007483328809030354
Batch  111  loss:  0.001047733472660184
Batch  121  loss:  0.0017941308906301856
Batch  131  loss:  0.0013072279980406165
Batch  141  loss:  0.0018960753222927451
Batch  151  loss:  0.0016843074699863791
Batch  161  loss:  0.00130537711083889
Batch  171  loss:  0.0021355741191655397
Batch  181  loss:  0.000984667451120913
Batch  191  loss:  0.002445308491587639
Validation on real data: 
LOSS supervised-train 0.0017288100885343737, valid 0.003040507435798645
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0022419332526624203
Batch  11  loss:  0.0025295838713645935
Batch  21  loss:  0.0013044832739979029
Batch  31  loss:  0.001044599455781281
Batch  41  loss:  0.001178433420136571
Batch  51  loss:  0.0011166419135406613
Batch  61  loss:  0.003090070327743888
Batch  71  loss:  0.001377583248540759
Batch  81  loss:  0.0019944512750953436
Batch  91  loss:  0.001378734246827662
Batch  101  loss:  0.0007146866992115974
Batch  111  loss:  0.0008068628958426416
Batch  121  loss:  0.001773545052856207
Batch  131  loss:  0.0016790599329397082
Batch  141  loss:  0.0015262754168361425
Batch  151  loss:  0.0017158869886770844
Batch  161  loss:  0.0011831269366666675
Batch  171  loss:  0.002186810364946723
Batch  181  loss:  0.0007219722028821707
Batch  191  loss:  0.0024467913899570704
Validation on real data: 
LOSS supervised-train 0.001638693129643798, valid 0.002127491869032383
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0022093146108090878
Batch  11  loss:  0.0024788486771285534
Batch  21  loss:  0.0012116003781557083
Batch  31  loss:  0.001127789611928165
Batch  41  loss:  0.000962425721809268
Batch  51  loss:  0.0009249115246348083
Batch  61  loss:  0.0027377877850085497
Batch  71  loss:  0.0014389163115993142
Batch  81  loss:  0.0013456392334774137
Batch  91  loss:  0.0012999491300433874
Batch  101  loss:  0.0007161942194215953
Batch  111  loss:  0.0008465147693641484
Batch  121  loss:  0.0022765689063817263
Batch  131  loss:  0.0010375060373917222
Batch  141  loss:  0.0013963575474917889
Batch  151  loss:  0.001502581755630672
Batch  161  loss:  0.0015488469507545233
Batch  171  loss:  0.0018953150138258934
Batch  181  loss:  0.0008238540613092482
Batch  191  loss:  0.00215765368193388
Validation on real data: 
LOSS supervised-train 0.00150394560652785, valid 0.003757365746423602
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0026976540684700012
Batch  11  loss:  0.0029672961682081223
Batch  21  loss:  0.0011502989800646901
Batch  31  loss:  0.0009440410067327321
Batch  41  loss:  0.0010567818535491824
Batch  51  loss:  0.001363180112093687
Batch  61  loss:  0.0025483916979283094
Batch  71  loss:  0.0016131618758663535
Batch  81  loss:  0.0015540398890152574
Batch  91  loss:  0.001321718329563737
Batch  101  loss:  0.0006705136620439589
Batch  111  loss:  0.0005704111536033452
Batch  121  loss:  0.0015624427469447255
Batch  131  loss:  0.0011829703580588102
Batch  141  loss:  0.0013966044643893838
Batch  151  loss:  0.001465433626435697
Batch  161  loss:  0.0010063429363071918
Batch  171  loss:  0.0017764422809705138
Batch  181  loss:  0.0009495801059529185
Batch  191  loss:  0.0019784492906183004
Validation on real data: 
LOSS supervised-train 0.0014242629329964983, valid 0.0034409710206091404
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0025240741670131683
Batch  11  loss:  0.002772304695099592
Batch  21  loss:  0.0012205878738313913
Batch  31  loss:  0.0009176810854114592
Batch  41  loss:  0.0009853626834228635
Batch  51  loss:  0.0009172455174848437
Batch  61  loss:  0.002065123524516821
Batch  71  loss:  0.001131247729063034
Batch  81  loss:  0.0013916557654738426
Batch  91  loss:  0.0012577715096995234
Batch  101  loss:  0.0004954356700181961
Batch  111  loss:  0.0011237653670832515
Batch  121  loss:  0.0017416193149983883
Batch  131  loss:  0.0007528266287408769
Batch  141  loss:  0.0017880286322906613
Batch  151  loss:  0.0010279638227075338
Batch  161  loss:  0.000955230847466737
Batch  171  loss:  0.0013784375041723251
Batch  181  loss:  0.0010027565294876695
Batch  191  loss:  0.002085576532408595
Validation on real data: 
LOSS supervised-train 0.0013542681903345511, valid 0.0028069321997463703
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0016610822640359402
Batch  11  loss:  0.0026044934056699276
Batch  21  loss:  0.0010534239700064063
Batch  31  loss:  0.0012720003724098206
Batch  41  loss:  0.0008476422517560422
Batch  51  loss:  0.00141244288533926
Batch  61  loss:  0.0019110512221232057
Batch  71  loss:  0.0011837866622954607
Batch  81  loss:  0.0014133625663816929
Batch  91  loss:  0.0012424918822944164
Batch  101  loss:  0.00045527517795562744
Batch  111  loss:  0.0008914163336157799
Batch  121  loss:  0.0018144975183531642
Batch  131  loss:  0.0012303601251915097
Batch  141  loss:  0.0013826910872012377
Batch  151  loss:  0.0010508187115192413
Batch  161  loss:  0.0009316515643149614
Batch  171  loss:  0.0015262026572600007
Batch  181  loss:  0.0008588212658651173
Batch  191  loss:  0.001210864051245153
Validation on real data: 
LOSS supervised-train 0.0012618192247464321, valid 0.00290221581235528
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.001684053335338831
Batch  11  loss:  0.0017486929427832365
Batch  21  loss:  0.0012281315866857767
Batch  31  loss:  0.0011402135714888573
Batch  41  loss:  0.0009402122814208269
Batch  51  loss:  0.0011917139636352658
Batch  61  loss:  0.00190928450319916
Batch  71  loss:  0.0011295551666989923
Batch  81  loss:  0.0010297391563653946
Batch  91  loss:  0.0009478373685851693
Batch  101  loss:  0.00038368444074876606
Batch  111  loss:  0.0007620156975463033
Batch  121  loss:  0.0017060637474060059
Batch  131  loss:  0.0008550934144295752
Batch  141  loss:  0.0013299890561029315
Batch  151  loss:  0.0010514785535633564
Batch  161  loss:  0.001255011884495616
Batch  171  loss:  0.001443097018636763
Batch  181  loss:  0.0006012142403051257
Batch  191  loss:  0.0021269985008984804
Validation on real data: 
LOSS supervised-train 0.0012163770607730839, valid 0.001816837815567851
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.002213286003097892
Batch  11  loss:  0.002067356603220105
Batch  21  loss:  0.0008329228148795664
Batch  31  loss:  0.0011904757702723145
Batch  41  loss:  0.0009542633197270334
Batch  51  loss:  0.0007462569046765566
Batch  61  loss:  0.0017211431404575706
Batch  71  loss:  0.0010773215908557177
Batch  81  loss:  0.0014554390218108892
Batch  91  loss:  0.0011170660145580769
Batch  101  loss:  0.00046894836123101413
Batch  111  loss:  0.0007019849144853652
Batch  121  loss:  0.0012402728898450732
Batch  131  loss:  0.0007396636647172272
Batch  141  loss:  0.0017249704105779529
Batch  151  loss:  0.0009659251081757247
Batch  161  loss:  0.0010145074920728803
Batch  171  loss:  0.0017642374150454998
Batch  181  loss:  0.0005608702194876969
Batch  191  loss:  0.0016247197054326534
Validation on real data: 
LOSS supervised-train 0.0011731522926129402, valid 0.002509572310373187
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.001324250129982829
Batch  11  loss:  0.001972275087609887
Batch  21  loss:  0.0008911722688935697
Batch  31  loss:  0.0011760630877688527
Batch  41  loss:  0.000678040087223053
Batch  51  loss:  0.0008011934696696699
Batch  61  loss:  0.0024091913364827633
Batch  71  loss:  0.0011339084012433887
Batch  81  loss:  0.0009729029843583703
Batch  91  loss:  0.0007311758236028254
Batch  101  loss:  0.0005028517916798592
Batch  111  loss:  0.0007159470696933568
Batch  121  loss:  0.0015196306630969048
Batch  131  loss:  0.000698358635418117
Batch  141  loss:  0.0012350082397460938
Batch  151  loss:  0.0009224274544976652
Batch  161  loss:  0.0012033410603180528
Batch  171  loss:  0.0018450352363288403
Batch  181  loss:  0.0005696308217011392
Batch  191  loss:  0.0013529506977647543
Validation on real data: 
LOSS supervised-train 0.0011169372717267834, valid 0.0016211746260523796
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.001293802517466247
Batch  11  loss:  0.001223861938342452
Batch  21  loss:  0.0008083027205429971
Batch  31  loss:  0.0010631661862134933
Batch  41  loss:  0.0008454099879600108
Batch  51  loss:  0.0008747383253648877
Batch  61  loss:  0.0017175349639728665
Batch  71  loss:  0.0009728700970299542
Batch  81  loss:  0.001256088726222515
Batch  91  loss:  0.0009049218497239053
Batch  101  loss:  0.0003956306609325111
Batch  111  loss:  0.0006481403834186494
Batch  121  loss:  0.0015817449893802404
Batch  131  loss:  0.0009051900706253946
Batch  141  loss:  0.0009990998078137636
Batch  151  loss:  0.0009262802777811885
Batch  161  loss:  0.0009140509064309299
Batch  171  loss:  0.001351568498648703
Batch  181  loss:  0.0006603567744605243
Batch  191  loss:  0.0016141094965860248
Validation on real data: 
LOSS supervised-train 0.0010506205853016582, valid 0.0006893986137583852
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.001431865617632866
Batch  11  loss:  0.0011587898479774594
Batch  21  loss:  0.0008000697707757354
Batch  31  loss:  0.0009848965564742684
Batch  41  loss:  0.0009409714257344604
Batch  51  loss:  0.0008495330112054944
Batch  61  loss:  0.0013811294920742512
Batch  71  loss:  0.0009678350179456174
Batch  81  loss:  0.0012047009076923132
Batch  91  loss:  0.0008977261022664607
Batch  101  loss:  0.0005713385762646794
Batch  111  loss:  0.0005165485199540854
Batch  121  loss:  0.0011825404362753034
Batch  131  loss:  0.0006442799349315464
Batch  141  loss:  0.001274461392313242
Batch  151  loss:  0.0008065563160926104
Batch  161  loss:  0.00092862214660272
Batch  171  loss:  0.0009943965123966336
Batch  181  loss:  0.0005804735119454563
Batch  191  loss:  0.0014325941447168589
Validation on real data: 
LOSS supervised-train 0.0010373712600267027, valid 0.001513853669166565
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0016996451886370778
Batch  11  loss:  0.0018132576951757073
Batch  21  loss:  0.0007103942334651947
Batch  31  loss:  0.0012411935022100806
Batch  41  loss:  0.0009033387759700418
Batch  51  loss:  0.0011422737734392285
Batch  61  loss:  0.001259987591765821
Batch  71  loss:  0.0008410057052969933
Batch  81  loss:  0.0009332492481917143
Batch  91  loss:  0.0008945701993070543
Batch  101  loss:  0.00042786807171069086
Batch  111  loss:  0.0007096462068147957
Batch  121  loss:  0.0015970490640029311
Batch  131  loss:  0.0008545894525013864
Batch  141  loss:  0.0010949556017294526
Batch  151  loss:  0.0008869071025401354
Batch  161  loss:  0.000679870368912816
Batch  171  loss:  0.0011505244765430689
Batch  181  loss:  0.0007622367120347917
Batch  191  loss:  0.0015897438861429691
Validation on real data: 
LOSS supervised-train 0.0010113822837593033, valid 0.0010922880610451102
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.001521823345683515
Batch  11  loss:  0.0014011738821864128
Batch  21  loss:  0.0008529144106432796
Batch  31  loss:  0.000669051893055439
Batch  41  loss:  0.0006946009234525263
Batch  51  loss:  0.000687546213157475
Batch  61  loss:  0.001248104264959693
Batch  71  loss:  0.0007853051647543907
Batch  81  loss:  0.0012820756528526545
Batch  91  loss:  0.0009711792226880789
Batch  101  loss:  0.0005127767217345536
Batch  111  loss:  0.00048139551654458046
Batch  121  loss:  0.0010674723889678717
Batch  131  loss:  0.0005927591118961573
Batch  141  loss:  0.0011496860533952713
Batch  151  loss:  0.0008747021201997995
Batch  161  loss:  0.0009216328035108745
Batch  171  loss:  0.001064435695298016
Batch  181  loss:  0.0007444223738275468
Batch  191  loss:  0.0015673370799049735
Validation on real data: 
LOSS supervised-train 0.0009377340473292861, valid 0.0010049049742519855
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0013272359501570463
Batch  11  loss:  0.0016194904455915093
Batch  21  loss:  0.0007919942145235837
Batch  31  loss:  0.0007159935776144266
Batch  41  loss:  0.001005283324047923
Batch  51  loss:  0.0009389754268340766
Batch  61  loss:  0.001203089952468872
Batch  71  loss:  0.0007930300780571997
Batch  81  loss:  0.0009608126129023731
Batch  91  loss:  0.0006744237616658211
Batch  101  loss:  0.0004669982008635998
Batch  111  loss:  0.000733370310626924
Batch  121  loss:  0.0013583065010607243
Batch  131  loss:  0.0004995821509510279
Batch  141  loss:  0.0008627669303677976
Batch  151  loss:  0.0007847354863770306
Batch  161  loss:  0.0007890551351010799
Batch  171  loss:  0.000895067525561899
Batch  181  loss:  0.0004450695123523474
Batch  191  loss:  0.0009637072216719389
Validation on real data: 
LOSS supervised-train 0.0008736158999090548, valid 0.0012151787523180246
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0013885658700019121
Batch  11  loss:  0.001018134644255042
Batch  21  loss:  0.0008006325806491077
Batch  31  loss:  0.000990477274172008
Batch  41  loss:  0.0008313260041177273
Batch  51  loss:  0.000658519915305078
Batch  61  loss:  0.0012530866079032421
Batch  71  loss:  0.0007706739124841988
Batch  81  loss:  0.001036033732816577
Batch  91  loss:  0.001036265166476369
Batch  101  loss:  0.0003291016328148544
Batch  111  loss:  0.0007576460484415293
Batch  121  loss:  0.000958525575697422
Batch  131  loss:  0.000464438198832795
Batch  141  loss:  0.0012917983112856746
Batch  151  loss:  0.0007830215618014336
Batch  161  loss:  0.0007880902267061174
Batch  171  loss:  0.0013103081146255136
Batch  181  loss:  0.0006224822718650103
Batch  191  loss:  0.0008628155919723213
Validation on real data: 
LOSS supervised-train 0.0008842562102654483, valid 0.0011068829335272312
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.001226817024871707
Batch  11  loss:  0.0011876480421051383
Batch  21  loss:  0.0009608333348296583
Batch  31  loss:  0.0010171785252168775
Batch  41  loss:  0.0006870904471725225
Batch  51  loss:  0.0009190464043058455
Batch  61  loss:  0.001034483895637095
Batch  71  loss:  0.0009200419299304485
Batch  81  loss:  0.0011260720202699304
Batch  91  loss:  0.00074969936395064
Batch  101  loss:  0.00033534655813127756
Batch  111  loss:  0.0006159942713566124
Batch  121  loss:  0.001101684058085084
Batch  131  loss:  0.0005305444938130677
Batch  141  loss:  0.0012982806656509638
Batch  151  loss:  0.0008118252153508365
Batch  161  loss:  0.0010416100267320871
Batch  171  loss:  0.0012295139022171497
Batch  181  loss:  0.0006836004904471338
Batch  191  loss:  0.0011539910919964314
Validation on real data: 
LOSS supervised-train 0.0008594460251333658, valid 0.001204141415655613
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0010916331084445119
Batch  11  loss:  0.0009457264095544815
Batch  21  loss:  0.0008371651056222618
Batch  31  loss:  0.0008259094902314246
Batch  41  loss:  0.0006315862410701811
Batch  51  loss:  0.0006021381122991443
Batch  61  loss:  0.0008898150990717113
Batch  71  loss:  0.000775812310166657
Batch  81  loss:  0.00081609096378088
Batch  91  loss:  0.0007184140267781913
Batch  101  loss:  0.0003787899622693658
Batch  111  loss:  0.0006081186002120376
Batch  121  loss:  0.0011557270772755146
Batch  131  loss:  0.00043520869803614914
Batch  141  loss:  0.0011696484871208668
Batch  151  loss:  0.00048192558460868895
Batch  161  loss:  0.0006854949751868844
Batch  171  loss:  0.000936219235882163
Batch  181  loss:  0.00042712638969533145
Batch  191  loss:  0.0012451924849301577
Validation on real data: 
LOSS supervised-train 0.0008249415302998386, valid 0.0009277116041630507
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0009251324809156358
Batch  11  loss:  0.0006938970764167607
Batch  21  loss:  0.0009272666065953672
Batch  31  loss:  0.0006283530965447426
Batch  41  loss:  0.0007409377139993012
Batch  51  loss:  0.0007445094524882734
Batch  61  loss:  0.0014109342591837049
Batch  71  loss:  0.000643885403405875
Batch  81  loss:  0.0008981673163361847
Batch  91  loss:  0.0007920622592791915
Batch  101  loss:  0.0005113245570100844
Batch  111  loss:  0.0006923032342456281
Batch  121  loss:  0.0008480473188683391
Batch  131  loss:  0.0004832417762372643
Batch  141  loss:  0.0009557215380482376
Batch  151  loss:  0.0005595923867076635
Batch  161  loss:  0.0006490520900115371
Batch  171  loss:  0.0008414781768806279
Batch  181  loss:  0.000410129054216668
Batch  191  loss:  0.0010231251362711191
Validation on real data: 
LOSS supervised-train 0.0007863435021135956, valid 0.001137771992944181
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0010786160128191113
Batch  11  loss:  0.0008103924337774515
Batch  21  loss:  0.0006700812373310328
Batch  31  loss:  0.0009261976811103523
Batch  41  loss:  0.0005793753080070019
Batch  51  loss:  0.0009049815707840025
Batch  61  loss:  0.0006705377600155771
Batch  71  loss:  0.0006135549047030509
Batch  81  loss:  0.0011355075985193253
Batch  91  loss:  0.0009639469790272415
Batch  101  loss:  0.00030311645241454244
Batch  111  loss:  0.0005711662815883756
Batch  121  loss:  0.0009802154963836074
Batch  131  loss:  0.0005118160624988377
Batch  141  loss:  0.0010063936933875084
Batch  151  loss:  0.0007104912074282765
Batch  161  loss:  0.0006734172347933054
Batch  171  loss:  0.0011644347105175257
Batch  181  loss:  0.0006506135687232018
Batch  191  loss:  0.0015818518586456776
Validation on real data: 
LOSS supervised-train 0.0007868598120694514, valid 0.000851561373565346
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0008543632575310767
Batch  11  loss:  0.001024309080094099
Batch  21  loss:  0.000906418717931956
Batch  31  loss:  0.0007538101635873318
Batch  41  loss:  0.0007553585455752909
Batch  51  loss:  0.000823146547190845
Batch  61  loss:  0.001101263682357967
Batch  71  loss:  0.0006976017029955983
Batch  81  loss:  0.0006821698043495417
Batch  91  loss:  0.0007932177977636456
Batch  101  loss:  0.00037729539326392114
Batch  111  loss:  0.000659410550724715
Batch  121  loss:  0.0010552360909059644
Batch  131  loss:  0.0005212961696088314
Batch  141  loss:  0.0009533414267934859
Batch  151  loss:  0.00047240001731552184
Batch  161  loss:  0.00048690559924580157
Batch  171  loss:  0.0011720061302185059
Batch  181  loss:  0.000432353321230039
Batch  191  loss:  0.0012896459084004164
Validation on real data: 
LOSS supervised-train 0.000783891573373694, valid 0.0012698888313025236
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0008490462205372751
Batch  11  loss:  0.0007205418078228831
Batch  21  loss:  0.0008384467219002545
Batch  31  loss:  0.0007597533985972404
Batch  41  loss:  0.0005602599703706801
Batch  51  loss:  0.0006182860233820975
Batch  61  loss:  0.0008185815531760454
Batch  71  loss:  0.0005639825249090791
Batch  81  loss:  0.000934650597628206
Batch  91  loss:  0.0005794484750367701
Batch  101  loss:  0.0004151736793573946
Batch  111  loss:  0.0005606846534647048
Batch  121  loss:  0.0009199969936162233
Batch  131  loss:  0.0004463501973077655
Batch  141  loss:  0.0009771022014319897
Batch  151  loss:  0.0005589005304500461
Batch  161  loss:  0.000631899805739522
Batch  171  loss:  0.000803453556727618
Batch  181  loss:  0.0004220532428007573
Batch  191  loss:  0.0011689640814438462
Validation on real data: 
LOSS supervised-train 0.0007573999236046802, valid 0.0007841007318347692
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0006574179278686643
Batch  11  loss:  0.000690885295625776
Batch  21  loss:  0.0007776044076308608
Batch  31  loss:  0.0007244718726724386
Batch  41  loss:  0.0005973028019070625
Batch  51  loss:  0.0007526751724071801
Batch  61  loss:  0.0008450919413007796
Batch  71  loss:  0.0008495688089169562
Batch  81  loss:  0.0010766386985778809
Batch  91  loss:  0.0005736661842092872
Batch  101  loss:  0.00035505203413777053
Batch  111  loss:  0.00048407932627014816
Batch  121  loss:  0.0009961070027202368
Batch  131  loss:  0.0005093166255392134
Batch  141  loss:  0.0010560783557593822
Batch  151  loss:  0.000580654654186219
Batch  161  loss:  0.0006784391007386148
Batch  171  loss:  0.0010352646932005882
Batch  181  loss:  0.0004430720000527799
Batch  191  loss:  0.0009800418047234416
Validation on real data: 
LOSS supervised-train 0.0007282262758235447, valid 0.0011054353090003133
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0009284643456339836
Batch  11  loss:  0.0010003882925957441
Batch  21  loss:  0.0006723040714859962
Batch  31  loss:  0.0008495284710079432
Batch  41  loss:  0.0005637976573780179
Batch  51  loss:  0.0008113211370073259
Batch  61  loss:  0.0010521678486838937
Batch  71  loss:  0.0006766627775505185
Batch  81  loss:  0.001058728201314807
Batch  91  loss:  0.00049992447020486
Batch  101  loss:  0.00030112251988612115
Batch  111  loss:  0.0006102388142608106
Batch  121  loss:  0.0009292971226386726
Batch  131  loss:  0.0004443276848178357
Batch  141  loss:  0.0008819277281872928
Batch  151  loss:  0.0007855963776819408
Batch  161  loss:  0.00058437988627702
Batch  171  loss:  0.0009657428017817438
Batch  181  loss:  0.0004144401755183935
Batch  191  loss:  0.0007176354411058128
Validation on real data: 
LOSS supervised-train 0.0007435577154683415, valid 0.0009021223522722721
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0009767700685188174
Batch  11  loss:  0.0006043503526598215
Batch  21  loss:  0.0006738249794580042
Batch  31  loss:  0.0008967952453531325
Batch  41  loss:  0.000478043919429183
Batch  51  loss:  0.000783093273639679
Batch  61  loss:  0.0007379996823146939
Batch  71  loss:  0.0005487377056851983
Batch  81  loss:  0.0009021892910823226
Batch  91  loss:  0.0008567474433220923
Batch  101  loss:  0.00034578292979858816
Batch  111  loss:  0.0006456071278080344
Batch  121  loss:  0.0010286951437592506
Batch  131  loss:  0.0004379206511657685
Batch  141  loss:  0.0006454082322306931
Batch  151  loss:  0.000525058654602617
Batch  161  loss:  0.00045669759856536984
Batch  171  loss:  0.0006597003084607422
Batch  181  loss:  0.0005061512347310781
Batch  191  loss:  0.0010291648795828223
Validation on real data: 
LOSS supervised-train 0.0006967687320138794, valid 0.000982940662652254
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0007312881643883884
Batch  11  loss:  0.0006972653791308403
Batch  21  loss:  0.0004830168909393251
Batch  31  loss:  0.0006115692667663097
Batch  41  loss:  0.0004080092767253518
Batch  51  loss:  0.000617576006334275
Batch  61  loss:  0.0009661049698479474
Batch  71  loss:  0.0005816261982545257
Batch  81  loss:  0.0012965473579242826
Batch  91  loss:  0.00074772845255211
Batch  101  loss:  0.00035896431654691696
Batch  111  loss:  0.0006162712234072387
Batch  121  loss:  0.0010440708138048649
Batch  131  loss:  0.0004628005553968251
Batch  141  loss:  0.0007380497409030795
Batch  151  loss:  0.0007350455271080136
Batch  161  loss:  0.0006082398467697203
Batch  171  loss:  0.0008664785418659449
Batch  181  loss:  0.000510151672642678
Batch  191  loss:  0.0007969748694449663
Validation on real data: 
LOSS supervised-train 0.0006659394854796119, valid 0.000708408304490149
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0007938758935779333
Batch  11  loss:  0.0007501186919398606
Batch  21  loss:  0.0005393229075707495
Batch  31  loss:  0.0006106459768489003
Batch  41  loss:  0.0005657275905832648
Batch  51  loss:  0.0007341988384723663
Batch  61  loss:  0.0006818558904342353
Batch  71  loss:  0.000737262365873903
Batch  81  loss:  0.0010935029713436961
Batch  91  loss:  0.00055648572742939
Batch  101  loss:  0.000293942226562649
Batch  111  loss:  0.000832653371617198
Batch  121  loss:  0.0007152555044740438
Batch  131  loss:  0.0003809974296018481
Batch  141  loss:  0.0010677723912522197
Batch  151  loss:  0.0007602247642353177
Batch  161  loss:  0.0007279125275090337
Batch  171  loss:  0.0007467569084838033
Batch  181  loss:  0.0004108352877665311
Batch  191  loss:  0.0009908800711855292
Validation on real data: 
LOSS supervised-train 0.0006699183779710438, valid 0.001485400483943522
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0007467122050002217
Batch  11  loss:  0.0007465023663826287
Batch  21  loss:  0.0006447057239711285
Batch  31  loss:  0.0005830672453157604
Batch  41  loss:  0.0006518899463117123
Batch  51  loss:  0.0007968798745423555
Batch  61  loss:  0.0007455780869349837
Batch  71  loss:  0.000953709299210459
Batch  81  loss:  0.0010001366026699543
Batch  91  loss:  0.0008388556307181716
Batch  101  loss:  0.00038636516546830535
Batch  111  loss:  0.0005473982309922576
Batch  121  loss:  0.0010435977019369602
Batch  131  loss:  0.0006491084350273013
Batch  141  loss:  0.0008750433335080743
Batch  151  loss:  0.0003923046460840851
Batch  161  loss:  0.00045408098958432674
Batch  171  loss:  0.0006326528382487595
Batch  181  loss:  0.0005129093187861145
Batch  191  loss:  0.0011815231991931796
Validation on real data: 
LOSS supervised-train 0.0006949502012867015, valid 0.0018816090887412429
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0006133389542810619
Batch  11  loss:  0.0006376231322064996
Batch  21  loss:  0.0006328544113785028
Batch  31  loss:  0.0005191856762394309
Batch  41  loss:  0.0005635747802443802
Batch  51  loss:  0.0005627658683806658
Batch  61  loss:  0.0005553719820454717
Batch  71  loss:  0.0006151289562694728
Batch  81  loss:  0.0011838923674076796
Batch  91  loss:  0.0007243852014653385
Batch  101  loss:  0.00040510762482881546
Batch  111  loss:  0.0006635868339799345
Batch  121  loss:  0.0007956065237522125
Batch  131  loss:  0.0004138726508244872
Batch  141  loss:  0.0007519347709603608
Batch  151  loss:  0.0006419610581360757
Batch  161  loss:  0.00042970760841853917
Batch  171  loss:  0.0007183782872743905
Batch  181  loss:  0.0005540630081668496
Batch  191  loss:  0.0008099066908471286
Validation on real data: 
LOSS supervised-train 0.0006530379720061319, valid 0.0008832868188619614
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0005785184330306947
Batch  11  loss:  0.0006103147170506418
Batch  21  loss:  0.0007215301739051938
Batch  31  loss:  0.0005850528832525015
Batch  41  loss:  0.0005460767424665391
Batch  51  loss:  0.0006052057724446058
Batch  61  loss:  0.0005198176950216293
Batch  71  loss:  0.0006653828895650804
Batch  81  loss:  0.001222369959577918
Batch  91  loss:  0.0004681383725255728
Batch  101  loss:  0.00040196164627559483
Batch  111  loss:  0.0006932207616046071
Batch  121  loss:  0.0010207107989117503
Batch  131  loss:  0.0004504366952460259
Batch  141  loss:  0.0009514398407191038
Batch  151  loss:  0.0006499543669633567
Batch  161  loss:  0.0004351393727120012
Batch  171  loss:  0.0009477704297751188
Batch  181  loss:  0.0005107166361995041
Batch  191  loss:  0.0007804441265761852
Validation on real data: 
LOSS supervised-train 0.0006445425868150778, valid 0.0023239715956151485
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0004960832884535193
Batch  11  loss:  0.0002496723609510809
Batch  21  loss:  0.0005529579939320683
Batch  31  loss:  0.0006613691803067923
Batch  41  loss:  0.00039928240585140884
Batch  51  loss:  0.0005868215812370181
Batch  61  loss:  0.0006366369780153036
Batch  71  loss:  0.0005948483594693244
Batch  81  loss:  0.0009735056082718074
Batch  91  loss:  0.0006707300199195743
Batch  101  loss:  0.0002214854903286323
Batch  111  loss:  0.000687816587742418
Batch  121  loss:  0.0009900328004732728
Batch  131  loss:  0.0003523188061080873
Batch  141  loss:  0.0006637599435634911
Batch  151  loss:  0.00046642072265967727
Batch  161  loss:  0.0007084800745360553
Batch  171  loss:  0.000801026588305831
Batch  181  loss:  0.0004183208220638335
Batch  191  loss:  0.0007797629805281758
Validation on real data: 
LOSS supervised-train 0.0006145594693953171, valid 0.0031764530576765537
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0006822008290328085
Batch  11  loss:  0.0004164116980973631
Batch  21  loss:  0.0005795651813969016
Batch  31  loss:  0.0005386130069382489
Batch  41  loss:  0.0002907381276600063
Batch  51  loss:  0.00047876581083983183
Batch  61  loss:  0.0006656501209363341
Batch  71  loss:  0.0004861511988565326
Batch  81  loss:  0.0011754754232242703
Batch  91  loss:  0.0006624165689572692
Batch  101  loss:  0.00031349502387456596
Batch  111  loss:  0.0006299133528955281
Batch  121  loss:  0.0009176676976494491
Batch  131  loss:  0.00037429388612508774
Batch  141  loss:  0.0011450393358245492
Batch  151  loss:  0.000511227292008698
Batch  161  loss:  0.0008012704201973975
Batch  171  loss:  0.0009328958112746477
Batch  181  loss:  0.00044065056135877967
Batch  191  loss:  0.0008935609366744757
Validation on real data: 
LOSS supervised-train 0.0006078272570448461, valid 0.0006139762117527425
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0005542482831515372
Batch  11  loss:  0.0007031376590020955
Batch  21  loss:  0.0006348086171783507
Batch  31  loss:  0.0006162194185890257
Batch  41  loss:  0.0004253572260495275
Batch  51  loss:  0.0005206404021009803
Batch  61  loss:  0.0005699202883988619
Batch  71  loss:  0.0006193232256919146
Batch  81  loss:  0.001097727450542152
Batch  91  loss:  0.0005169727955944836
Batch  101  loss:  0.0002624920743983239
Batch  111  loss:  0.0005913843051530421
Batch  121  loss:  0.0007822811603546143
Batch  131  loss:  0.0003380530688446015
Batch  141  loss:  0.0009034390677697957
Batch  151  loss:  0.00044930714648216963
Batch  161  loss:  0.0005102168070152402
Batch  171  loss:  0.0008097013924270868
Batch  181  loss:  0.00033366100979037583
Batch  191  loss:  0.0007475726306438446
Validation on real data: 
LOSS supervised-train 0.0006176843518915121, valid 0.0006884464528411627
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00055771938059479
Batch  11  loss:  0.000766137964092195
Batch  21  loss:  0.0007446514791809022
Batch  31  loss:  0.0007464605150744319
Batch  41  loss:  0.0005942223942838609
Batch  51  loss:  0.00040390901267528534
Batch  61  loss:  0.0006778289098292589
Batch  71  loss:  0.0006513649132102728
Batch  81  loss:  0.0009102291660383344
Batch  91  loss:  0.0006276955245994031
Batch  101  loss:  0.00036560214357450604
Batch  111  loss:  0.0006511618266813457
Batch  121  loss:  0.0009904078906401992
Batch  131  loss:  0.0004259439592715353
Batch  141  loss:  0.0009740900713950396
Batch  151  loss:  0.00048745935782790184
Batch  161  loss:  0.0005474905483424664
Batch  171  loss:  0.0006689022993668914
Batch  181  loss:  0.0003740637330338359
Batch  191  loss:  0.000799819827079773
Validation on real data: 
LOSS supervised-train 0.0006176669197157026, valid 0.002461903728544712
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.000766330398619175
Batch  11  loss:  0.00038827062235213816
Batch  21  loss:  0.0006611585267819464
Batch  31  loss:  0.0006955612334422767
Batch  41  loss:  0.0004779686569236219
Batch  51  loss:  0.0003696497587952763
Batch  61  loss:  0.0005555183161050081
Batch  71  loss:  0.0006029176292940974
Batch  81  loss:  0.0008850193698890507
Batch  91  loss:  0.0006335938815027475
Batch  101  loss:  0.0002692182024475187
Batch  111  loss:  0.0005763443186879158
Batch  121  loss:  0.0008235627319663763
Batch  131  loss:  0.00033356575295329094
Batch  141  loss:  0.0006998994504101574
Batch  151  loss:  0.0005506921443156898
Batch  161  loss:  0.0005034244386479259
Batch  171  loss:  0.0007108281133696437
Batch  181  loss:  0.00034949398832395673
Batch  191  loss:  0.0007473989971913397
Validation on real data: 
LOSS supervised-train 0.0005892783045419492, valid 0.0009274393087252975
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.000711696338839829
Batch  11  loss:  0.000416953262174502
Batch  21  loss:  0.0005158489802852273
Batch  31  loss:  0.0004886066308245063
Batch  41  loss:  0.00044468219857662916
Batch  51  loss:  0.0005225846543908119
Batch  61  loss:  0.0005301585188135505
Batch  71  loss:  0.00040391526999883354
Batch  81  loss:  0.0009432734805159271
Batch  91  loss:  0.000425678095780313
Batch  101  loss:  0.00035713985562324524
Batch  111  loss:  0.0004536496999207884
Batch  121  loss:  0.0005933439824730158
Batch  131  loss:  0.00035143716377206147
Batch  141  loss:  0.0007528424030169845
Batch  151  loss:  0.0005496575613506138
Batch  161  loss:  0.00042673127609305084
Batch  171  loss:  0.0009572089766152203
Batch  181  loss:  0.0003388918994460255
Batch  191  loss:  0.0007187037845142186
Validation on real data: 
LOSS supervised-train 0.0005593106221931521, valid 0.0007056769682094455
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0008846271084621549
Batch  11  loss:  0.0003068338264711201
Batch  21  loss:  0.0006990246474742889
Batch  31  loss:  0.0007769151125103235
Batch  41  loss:  0.000593994976952672
Batch  51  loss:  0.0004388033994473517
Batch  61  loss:  0.0005127958138473332
Batch  71  loss:  0.00040739375981502235
Batch  81  loss:  0.0009230577852576971
Batch  91  loss:  0.00041799023165367544
Batch  101  loss:  0.00034962117206305265
Batch  111  loss:  0.0006366244633682072
Batch  121  loss:  0.0006221431540325284
Batch  131  loss:  0.0004263493465259671
Batch  141  loss:  0.0010742611484602094
Batch  151  loss:  0.0003999949258286506
Batch  161  loss:  0.0004914972232654691
Batch  171  loss:  0.0008008171571418643
Batch  181  loss:  0.00038666604086756706
Batch  191  loss:  0.0006681865779682994
Validation on real data: 
LOSS supervised-train 0.0005674158666079165, valid 0.0008639387087896466
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0005583951715379953
Batch  11  loss:  0.00039013189962133765
Batch  21  loss:  0.0008655505953356624
Batch  31  loss:  0.00049020373262465
Batch  41  loss:  0.000441946554929018
Batch  51  loss:  0.0006816726527176797
Batch  61  loss:  0.0004121004312764853
Batch  71  loss:  0.0004830036195926368
Batch  81  loss:  0.0009152346756309271
Batch  91  loss:  0.0005829181172885001
Batch  101  loss:  0.0002500312984921038
Batch  111  loss:  0.0005541093414649367
Batch  121  loss:  0.0009237820631824434
Batch  131  loss:  0.0003351581108290702
Batch  141  loss:  0.000612603616900742
Batch  151  loss:  0.0004587220901157707
Batch  161  loss:  0.0005562532460317016
Batch  171  loss:  0.0006146293599158525
Batch  181  loss:  0.0003502032777760178
Batch  191  loss:  0.0004558647342491895
Validation on real data: 
LOSS supervised-train 0.0005607223681727191, valid 0.0013157224748283625
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0005991315119899809
Batch  11  loss:  0.0002701806661207229
Batch  21  loss:  0.0006743219564668834
Batch  31  loss:  0.0005741887143813074
Batch  41  loss:  0.000500866153743118
Batch  51  loss:  0.0004542641108855605
Batch  61  loss:  0.0004946009139530361
Batch  71  loss:  0.0003866774495691061
Batch  81  loss:  0.0011719235917553306
Batch  91  loss:  0.00027398456586524844
Batch  101  loss:  0.0002402838581474498
Batch  111  loss:  0.0005815972108393908
Batch  121  loss:  0.0006758531089872122
Batch  131  loss:  0.0003178435727022588
Batch  141  loss:  0.0008198326104320586
Batch  151  loss:  0.00038265070179477334
Batch  161  loss:  0.0005514771328307688
Batch  171  loss:  0.0005450195167213678
Batch  181  loss:  0.00033095566323027015
Batch  191  loss:  0.0006200949428603053
Validation on real data: 
LOSS supervised-train 0.0005246232415811392, valid 0.0009754174388945103
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0005743600195273757
Batch  11  loss:  0.00036684132646769285
Batch  21  loss:  0.0006971658440306783
Batch  31  loss:  0.00045734166633337736
Batch  41  loss:  0.0003846102044917643
Batch  51  loss:  0.0007108639110811055
Batch  61  loss:  0.0006952015683054924
Batch  71  loss:  0.0005809168214909732
Batch  81  loss:  0.0007058407063595951
Batch  91  loss:  0.0005668406956829131
Batch  101  loss:  0.00035328418016433716
Batch  111  loss:  0.0006557298474945128
Batch  121  loss:  0.0007440586341544986
Batch  131  loss:  0.000423685007262975
Batch  141  loss:  0.0005237286677584052
Batch  151  loss:  0.0004880630876868963
Batch  161  loss:  0.0005267373635433614
Batch  171  loss:  0.0006136818556115031
Batch  181  loss:  0.00024556255084462464
Batch  191  loss:  0.0004642262647394091
Validation on real data: 
LOSS supervised-train 0.0005355255283939187, valid 0.0010838453890755773
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0004906069370917976
Batch  11  loss:  0.0003865544276777655
Batch  21  loss:  0.0005516418605111539
Batch  31  loss:  0.0004107356071472168
Batch  41  loss:  0.0004727696068584919
Batch  51  loss:  0.0007017808966338634
Batch  61  loss:  0.0006477910792455077
Batch  71  loss:  0.0005134930252097547
Batch  81  loss:  0.0009546445216983557
Batch  91  loss:  0.0005081418203189969
Batch  101  loss:  0.00030541830346919596
Batch  111  loss:  0.0006318161031231284
Batch  121  loss:  0.0006427092012017965
Batch  131  loss:  0.0003193439915776253
Batch  141  loss:  0.0008131896611303091
Batch  151  loss:  0.00035665184259414673
Batch  161  loss:  0.00034020032035186887
Batch  171  loss:  0.0007905696402303874
Batch  181  loss:  0.00042935373494401574
Batch  191  loss:  0.000703271187376231
Validation on real data: 
LOSS supervised-train 0.0005364472077781102, valid 0.0007694977102801204
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0004479594936128706
Batch  11  loss:  0.00044800780597142875
Batch  21  loss:  0.0004938672645948827
Batch  31  loss:  0.00041620052070356905
Batch  41  loss:  0.00036098482087254524
Batch  51  loss:  0.00044633535435423255
Batch  61  loss:  0.0005471105687320232
Batch  71  loss:  0.0004358144651632756
Batch  81  loss:  0.0008998548146337271
Batch  91  loss:  0.0004920198116451502
Batch  101  loss:  0.0002868406299967319
Batch  111  loss:  0.0004541542730294168
Batch  121  loss:  0.0007821305189281702
Batch  131  loss:  0.00030702550429850817
Batch  141  loss:  0.0007492029690183699
Batch  151  loss:  0.00035699416184797883
Batch  161  loss:  0.0003606587997637689
Batch  171  loss:  0.0006422569276764989
Batch  181  loss:  0.0003565917431842536
Batch  191  loss:  0.0004206149314995855
Validation on real data: 
LOSS supervised-train 0.0005011277252197033, valid 0.00039772200398147106
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0008004619157873094
Batch  11  loss:  0.0003584811638575047
Batch  21  loss:  0.0004768434155266732
Batch  31  loss:  0.0005355132161639631
Batch  41  loss:  0.0005121309077367187
Batch  51  loss:  0.000557712628506124
Batch  61  loss:  0.000668270280584693
Batch  71  loss:  0.00038137147203087807
Batch  81  loss:  0.0007165159331634641
Batch  91  loss:  0.0003854216483887285
Batch  101  loss:  0.00031042337650433183
Batch  111  loss:  0.0005267756641842425
Batch  121  loss:  0.0006680286605842412
Batch  131  loss:  0.00024220507475547493
Batch  141  loss:  0.0006555079598911107
Batch  151  loss:  0.00036594303674064577
Batch  161  loss:  0.00034336704993620515
Batch  171  loss:  0.0005274609429761767
Batch  181  loss:  0.0004071940784342587
Batch  191  loss:  0.0006829039775766432
Validation on real data: 
LOSS supervised-train 0.000509341873403173, valid 0.0008510996121913195
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0004521582159213722
Batch  11  loss:  0.00043434477993287146
Batch  21  loss:  0.00058812863426283
Batch  31  loss:  0.0005422930116765201
Batch  41  loss:  0.0002702154451981187
Batch  51  loss:  0.0006112416740506887
Batch  61  loss:  0.0005565996398217976
Batch  71  loss:  0.00041225089807994664
Batch  81  loss:  0.0010565663687884808
Batch  91  loss:  0.00035602605203166604
Batch  101  loss:  0.00022542636725120246
Batch  111  loss:  0.0004891492426395416
Batch  121  loss:  0.0007910315762273967
Batch  131  loss:  0.0002631187089718878
Batch  141  loss:  0.0010178706143051386
Batch  151  loss:  0.0004139898519497365
Batch  161  loss:  0.0005708425305783749
Batch  171  loss:  0.0005787082482129335
Batch  181  loss:  0.00031164783285930753
Batch  191  loss:  0.0005529904155991971
Validation on real data: 
LOSS supervised-train 0.0005300974901911105, valid 0.0007053613080643117
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00043416296830400825
Batch  11  loss:  0.00032952288165688515
Batch  21  loss:  0.0005549938650801778
Batch  31  loss:  0.0005748188123106956
Batch  41  loss:  0.00035882717929780483
Batch  51  loss:  0.0005109233316034079
Batch  61  loss:  0.0004655470547731966
Batch  71  loss:  0.0003417443367652595
Batch  81  loss:  0.0010014829458668828
Batch  91  loss:  0.0003912215179298073
Batch  101  loss:  0.00022297438408713788
Batch  111  loss:  0.0004519474459812045
Batch  121  loss:  0.0006897638668306172
Batch  131  loss:  0.00039962949813343585
Batch  141  loss:  0.0008303520153276622
Batch  151  loss:  0.00047378920135088265
Batch  161  loss:  0.0003317647788207978
Batch  171  loss:  0.0006650107097811997
Batch  181  loss:  0.0003976112639065832
Batch  191  loss:  0.0006428811466321349
Validation on real data: 
LOSS supervised-train 0.0004804430442163721, valid 0.0013910892885178328
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00064773706253618
Batch  11  loss:  0.00031581547227688134
Batch  21  loss:  0.0005244498606771231
Batch  31  loss:  0.0004746287304442376
Batch  41  loss:  0.0005727895768359303
Batch  51  loss:  0.0004519550420809537
Batch  61  loss:  0.0005168089410290122
Batch  71  loss:  0.00043164342059753835
Batch  81  loss:  0.0007168036536313593
Batch  91  loss:  0.000351352384313941
Batch  101  loss:  0.00035200483398512006
Batch  111  loss:  0.0004675059753935784
Batch  121  loss:  0.0005183135508559644
Batch  131  loss:  0.0003654227184597403
Batch  141  loss:  0.0007512982119806111
Batch  151  loss:  0.00036348964204080403
Batch  161  loss:  0.0004243411822244525
Batch  171  loss:  0.0005136598483659327
Batch  181  loss:  0.00029044991242699325
Batch  191  loss:  0.0005949800834059715
Validation on real data: 
LOSS supervised-train 0.0004895199551538099, valid 0.0012551757972687483
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.000459238508483395
Batch  11  loss:  0.00028135886532254517
Batch  21  loss:  0.0004418271710164845
Batch  31  loss:  0.0005239163874648511
Batch  41  loss:  0.0003538489399943501
Batch  51  loss:  0.000589295697864145
Batch  61  loss:  0.0005151815130375326
Batch  71  loss:  0.0004244732845108956
Batch  81  loss:  0.0007202522247098386
Batch  91  loss:  0.0003709707234520465
Batch  101  loss:  0.000353820068994537
Batch  111  loss:  0.0004924983368255198
Batch  121  loss:  0.0005993418744765222
Batch  131  loss:  0.00030587700894102454
Batch  141  loss:  0.0008166545303538442
Batch  151  loss:  0.00040391646325588226
Batch  161  loss:  0.000422432494815439
Batch  171  loss:  0.0005248925881460309
Batch  181  loss:  0.0002865514252334833
Batch  191  loss:  0.0007196732331067324
Validation on real data: 
LOSS supervised-train 0.0004833196397521533, valid 0.0005958855617791414
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0004616179212462157
Batch  11  loss:  0.0002568503259681165
Batch  21  loss:  0.00042435649083927274
Batch  31  loss:  0.0007143837283365428
Batch  41  loss:  0.0003129629767499864
Batch  51  loss:  0.0005050353356637061
Batch  61  loss:  0.0004083318926859647
Batch  71  loss:  0.00027315132319927216
Batch  81  loss:  0.0008549236226826906
Batch  91  loss:  0.0004446796083357185
Batch  101  loss:  0.000275869999313727
Batch  111  loss:  0.00045724757364951074
Batch  121  loss:  0.0006049773073755205
Batch  131  loss:  0.00025668356101959944
Batch  141  loss:  0.0006985762738622725
Batch  151  loss:  0.000338061829097569
Batch  161  loss:  0.00046684654080308974
Batch  171  loss:  0.0004631371411960572
Batch  181  loss:  0.00033806910505518317
Batch  191  loss:  0.0005827381974086165
Validation on real data: 
LOSS supervised-train 0.00047391014493769034, valid 0.000836510444059968
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0004910029820166528
Batch  11  loss:  0.00017488450976088643
Batch  21  loss:  0.0004950609290972352
Batch  31  loss:  0.0006218662019819021
Batch  41  loss:  0.00025336502585560083
Batch  51  loss:  0.0005687186494469643
Batch  61  loss:  0.00037098024040460587
Batch  71  loss:  0.0003954322310164571
Batch  81  loss:  0.0009442638256587088
Batch  91  loss:  0.0005285005318000913
Batch  101  loss:  0.00028553392621688545
Batch  111  loss:  0.000479999027447775
Batch  121  loss:  0.0005523039144463837
Batch  131  loss:  0.0003790496557485312
Batch  141  loss:  0.0006021556328050792
Batch  151  loss:  0.00038320638122968376
Batch  161  loss:  0.0004966648411937058
Batch  171  loss:  0.0005047855665907264
Batch  181  loss:  0.00031241949182003736
Batch  191  loss:  0.0004160075623076409
Validation on real data: 
LOSS supervised-train 0.00047464672112255355, valid 0.00192394875921309
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0004644561850000173
Batch  11  loss:  0.0003590145497582853
Batch  21  loss:  0.0005309616681188345
Batch  31  loss:  0.0003246998821850866
Batch  41  loss:  0.0002469794999342412
Batch  51  loss:  0.00031029645469971
Batch  61  loss:  0.0004198683600407094
Batch  71  loss:  0.0005250330432318151
Batch  81  loss:  0.0009610464330762625
Batch  91  loss:  0.00033670628909021616
Batch  101  loss:  0.000257026229519397
Batch  111  loss:  0.0005733390571549535
Batch  121  loss:  0.0005223338375799358
Batch  131  loss:  0.0003547025262378156
Batch  141  loss:  0.0010157340439036489
Batch  151  loss:  0.0006241618539206684
Batch  161  loss:  0.00045981831499375403
Batch  171  loss:  0.0007051011780276895
Batch  181  loss:  0.00023207132471725345
Batch  191  loss:  0.0006456597475335002
Validation on real data: 
LOSS supervised-train 0.0004796372384589631, valid 0.0008470992906950414
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0006042692111805081
Batch  11  loss:  0.0002342876687180251
Batch  21  loss:  0.0006054750992916524
Batch  31  loss:  0.000336582976160571
Batch  41  loss:  0.000406510109314695
Batch  51  loss:  0.00029886100674048066
Batch  61  loss:  0.0002989484928548336
Batch  71  loss:  0.0003320326504763216
Batch  81  loss:  0.0007980496739037335
Batch  91  loss:  0.0006854121456854045
Batch  101  loss:  0.00035239607677794993
Batch  111  loss:  0.0006738420925103128
Batch  121  loss:  0.0005630251835100353
Batch  131  loss:  0.00030345507548190653
Batch  141  loss:  0.0006062242900952697
Batch  151  loss:  0.00036179355811327696
Batch  161  loss:  0.0003591927816160023
Batch  171  loss:  0.000726324156858027
Batch  181  loss:  0.0002343490341445431
Batch  191  loss:  0.0005611714441329241
Validation on real data: 
LOSS supervised-train 0.0004637475214258302, valid 0.001367238350212574
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0006390299531631172
Batch  11  loss:  0.0002994934911839664
Batch  21  loss:  0.0005226600915193558
Batch  31  loss:  0.0003189897397533059
Batch  41  loss:  0.00027991324895992875
Batch  51  loss:  0.00046847661724314094
Batch  61  loss:  0.00040420450386591256
Batch  71  loss:  0.0003688350843731314
Batch  81  loss:  0.00062460673507303
Batch  91  loss:  0.0002577406994532794
Batch  101  loss:  0.00023640542349312454
Batch  111  loss:  0.00032731040846556425
Batch  121  loss:  0.0005058247479610145
Batch  131  loss:  0.00035448037669993937
Batch  141  loss:  0.0008233700646087527
Batch  151  loss:  0.0003182640648446977
Batch  161  loss:  0.00036989850923419
Batch  171  loss:  0.00043625725083984435
Batch  181  loss:  0.00036311440635472536
Batch  191  loss:  0.00040318220271728933
Validation on real data: 
LOSS supervised-train 0.0004692843864177121, valid 0.0019063565414398909
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.00030820234678685665
Batch  11  loss:  0.00038164496072568
Batch  21  loss:  0.0006038950523361564
Batch  31  loss:  0.0005356900510378182
Batch  41  loss:  0.0005514010554179549
Batch  51  loss:  0.0004796029534190893
Batch  61  loss:  0.00044075213372707367
Batch  71  loss:  0.00042929797200486064
Batch  81  loss:  0.0004899728810414672
Batch  91  loss:  0.00022417157015297562
Batch  101  loss:  0.0002916616213042289
Batch  111  loss:  0.00036901887506246567
Batch  121  loss:  0.00039791164454072714
Batch  131  loss:  0.00033442603307776153
Batch  141  loss:  0.000767600373364985
Batch  151  loss:  0.0005845126579515636
Batch  161  loss:  0.00038777015288360417
Batch  171  loss:  0.0005539589910767972
Batch  181  loss:  0.000348387606209144
Batch  191  loss:  0.0005022063269279897
Validation on real data: 
LOSS supervised-train 0.0004422070484724827, valid 0.0008959620026871562
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0005132824298925698
Batch  11  loss:  0.00032884321990422904
Batch  21  loss:  0.0004989237058907747
Batch  31  loss:  0.0005456234212033451
Batch  41  loss:  0.00025027617812156677
Batch  51  loss:  0.00046897708671167493
Batch  61  loss:  0.0003306747239548713
Batch  71  loss:  0.0004083389067091048
Batch  81  loss:  0.0007670515333302319
Batch  91  loss:  0.00047061240184120834
Batch  101  loss:  0.0002649977686814964
Batch  111  loss:  0.00045198845327831805
Batch  121  loss:  0.00062482402427122
Batch  131  loss:  0.00027904953458346426
Batch  141  loss:  0.0004571835452225059
Batch  151  loss:  0.00037901668110862374
Batch  161  loss:  0.0003468785434961319
Batch  171  loss:  0.000629028188996017
Batch  181  loss:  0.00019604794215410948
Batch  191  loss:  0.000602816988248378
Validation on real data: 
LOSS supervised-train 0.0004308871331886621, valid 0.0011056234361603856
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0003810846828855574
Batch  11  loss:  0.0003793486102949828
Batch  21  loss:  0.0004699564597103745
Batch  31  loss:  0.00043855508556589484
Batch  41  loss:  0.00023380530183203518
Batch  51  loss:  0.0004308828210923821
Batch  61  loss:  0.0003700608212966472
Batch  71  loss:  0.0003904040204361081
Batch  81  loss:  0.0011084044817835093
Batch  91  loss:  0.0004540100635495037
Batch  101  loss:  0.0003983538772445172
Batch  111  loss:  0.0005085017764940858
Batch  121  loss:  0.0004920404171571136
Batch  131  loss:  0.0003249576548114419
Batch  141  loss:  0.0006095799617469311
Batch  151  loss:  0.000561380700673908
Batch  161  loss:  0.00031284071155823767
Batch  171  loss:  0.0003724543785210699
Batch  181  loss:  0.00024070314248092473
Batch  191  loss:  0.0004687175096478313
Validation on real data: 
LOSS supervised-train 0.00044003786679240874, valid 0.0010845867218449712
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0005851031164638698
Batch  11  loss:  0.00031756144016981125
Batch  21  loss:  0.0004820370813831687
Batch  31  loss:  0.00044831502600573003
Batch  41  loss:  0.00030504606547765434
Batch  51  loss:  0.0006802937132306397
Batch  61  loss:  0.000454815017292276
Batch  71  loss:  0.0003895810223184526
Batch  81  loss:  0.0008294386207126081
Batch  91  loss:  0.00047887032269500196
Batch  101  loss:  0.0003037679416593164
Batch  111  loss:  0.00040924191125668585
Batch  121  loss:  0.00046242368989624083
Batch  131  loss:  0.00032115232897922397
Batch  141  loss:  0.0006994706345722079
Batch  151  loss:  0.0004051715659443289
Batch  161  loss:  0.00037974800216034055
Batch  171  loss:  0.0004597975348588079
Batch  181  loss:  0.0002739794726949185
Batch  191  loss:  0.0004642251005861908
Validation on real data: 
LOSS supervised-train 0.00044600791778066196, valid 0.0019058060133829713
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0005472442135214806
Batch  11  loss:  0.0003365228185430169
Batch  21  loss:  0.0005379710346460342
Batch  31  loss:  0.0004482119402382523
Batch  41  loss:  0.000261892331764102
Batch  51  loss:  0.00045371768646873534
Batch  61  loss:  0.0003212050360161811
Batch  71  loss:  0.0003405906318221241
Batch  81  loss:  0.0007887453539296985
Batch  91  loss:  0.0003439391730353236
Batch  101  loss:  0.0002664781059138477
Batch  111  loss:  0.00046280911192297935
Batch  121  loss:  0.0005461674300022423
Batch  131  loss:  0.00030816090293228626
Batch  141  loss:  0.0006615510210394859
Batch  151  loss:  0.0003184877277817577
Batch  161  loss:  0.00042656989535316825
Batch  171  loss:  0.0004319074214436114
Batch  181  loss:  0.00023662102466914803
Batch  191  loss:  0.0004693597147706896
Validation on real data: 
LOSS supervised-train 0.0004409462373587303, valid 0.0015922670718282461
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00031281023984774947
Batch  11  loss:  0.0001992706093005836
Batch  21  loss:  0.0002943859435617924
Batch  31  loss:  0.0004951253649778664
Batch  41  loss:  0.0002418200601823628
Batch  51  loss:  0.0003841954458039254
Batch  61  loss:  0.00023879591026343405
Batch  71  loss:  0.000359200726961717
Batch  81  loss:  0.0011520413681864738
Batch  91  loss:  0.0004232279898133129
Batch  101  loss:  0.0003478755825199187
Batch  111  loss:  0.0005538534023799002
Batch  121  loss:  0.000532160687725991
Batch  131  loss:  0.00026796411839313805
Batch  141  loss:  0.0007632391061633825
Batch  151  loss:  0.00028324697632342577
Batch  161  loss:  0.00038113698246888816
Batch  171  loss:  0.0003832790825981647
Batch  181  loss:  0.0002513988583814353
Batch  191  loss:  0.0008651406387798488
Validation on real data: 
LOSS supervised-train 0.00042790962572325953, valid 0.0009590828558430076
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0004356401041150093
Batch  11  loss:  0.00027408794267103076
Batch  21  loss:  0.00044894905295223
Batch  31  loss:  0.0003275149501860142
Batch  41  loss:  0.00032409446430392563
Batch  51  loss:  0.0004697377735283226
Batch  61  loss:  0.00033090016222558916
Batch  71  loss:  0.0003535090363584459
Batch  81  loss:  0.000746605102904141
Batch  91  loss:  0.00044499951764009893
Batch  101  loss:  0.00027565049822442234
Batch  111  loss:  0.00042196051799692214
Batch  121  loss:  0.0004053514276165515
Batch  131  loss:  0.0002765727986115962
Batch  141  loss:  0.0006731111207045615
Batch  151  loss:  0.0002905363799072802
Batch  161  loss:  0.00035608813050203025
Batch  171  loss:  0.0004179025418125093
Batch  181  loss:  0.0002894549397751689
Batch  191  loss:  0.0008967668982222676
Validation on real data: 
LOSS supervised-train 0.00042663562308007383, valid 0.00040876417187973857
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00043420601286925375
Batch  11  loss:  0.0003149045805912465
Batch  21  loss:  0.0004052002332173288
Batch  31  loss:  0.0006199234630912542
Batch  41  loss:  0.000335186516167596
Batch  51  loss:  0.000472685118438676
Batch  61  loss:  0.0002559192944318056
Batch  71  loss:  0.00030114816036075354
Batch  81  loss:  0.0006595159647986293
Batch  91  loss:  0.00040331511991098523
Batch  101  loss:  0.00021541863679885864
Batch  111  loss:  0.0003939807938877493
Batch  121  loss:  0.0004369184607639909
Batch  131  loss:  0.00029783902573399246
Batch  141  loss:  0.0006247111014090478
Batch  151  loss:  0.00033535933471284807
Batch  161  loss:  0.00029940615058876574
Batch  171  loss:  0.000495627464260906
Batch  181  loss:  0.00025583081878721714
Batch  191  loss:  0.0005452499608509243
Validation on real data: 
LOSS supervised-train 0.0004123341893136967, valid 0.001420831773430109
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0002774920139927417
Batch  11  loss:  0.0003050064842682332
Batch  21  loss:  0.0004523862444330007
Batch  31  loss:  0.0003934670821763575
Batch  41  loss:  0.0002371553855482489
Batch  51  loss:  0.0004833879938814789
Batch  61  loss:  0.00029955097124911845
Batch  71  loss:  0.0003310130850877613
Batch  81  loss:  0.0011293536517769098
Batch  91  loss:  0.00026579576660878956
Batch  101  loss:  0.00025754800299182534
Batch  111  loss:  0.0002911863848567009
Batch  121  loss:  0.0005259545869193971
Batch  131  loss:  0.00024178577587008476
Batch  141  loss:  0.00059439119650051
Batch  151  loss:  0.0002944334701169282
Batch  161  loss:  0.00027306374977342784
Batch  171  loss:  0.0004241963033564389
Batch  181  loss:  0.00019285091548226774
Batch  191  loss:  0.000441409123595804
Validation on real data: 
LOSS supervised-train 0.00039421247762220444, valid 0.002560769207775593
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0003485450579319149
Batch  11  loss:  0.000265234149992466
Batch  21  loss:  0.0005002701072953641
Batch  31  loss:  0.0005428515141829848
Batch  41  loss:  0.00026624518795870245
Batch  51  loss:  0.0004402056219987571
Batch  61  loss:  0.0003344605793245137
Batch  71  loss:  0.00037228743894957006
Batch  81  loss:  0.0007301445002667606
Batch  91  loss:  0.00038594609941355884
Batch  101  loss:  0.00021576612198259681
Batch  111  loss:  0.0003810484486166388
Batch  121  loss:  0.0005583336460404098
Batch  131  loss:  0.00023893969773780555
Batch  141  loss:  0.0005239457241259515
Batch  151  loss:  0.00039205787470564246
Batch  161  loss:  0.0003639007918536663
Batch  171  loss:  0.00042061455314978957
Batch  181  loss:  0.00027165087522007525
Batch  191  loss:  0.0004652400966733694
Validation on real data: 
LOSS supervised-train 0.00040992450522026047, valid 0.0012990627437829971
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0004330593510530889
Batch  11  loss:  0.00021809614554513246
Batch  21  loss:  0.00037537372554652393
Batch  31  loss:  0.0003814177471213043
Batch  41  loss:  0.0002954778610728681
Batch  51  loss:  0.0004531190497800708
Batch  61  loss:  0.000297621067147702
Batch  71  loss:  0.0004382544429972768
Batch  81  loss:  0.0008015885832719505
Batch  91  loss:  0.0003403321316000074
Batch  101  loss:  0.00023617401893716305
Batch  111  loss:  0.00038079748628661036
Batch  121  loss:  0.00036559044383466244
Batch  131  loss:  0.0002878440427593887
Batch  141  loss:  0.0005238475278019905
Batch  151  loss:  0.0003587501705624163
Batch  161  loss:  0.0002909469767473638
Batch  171  loss:  0.00042139238212257624
Batch  181  loss:  0.0002957400865852833
Batch  191  loss:  0.0005016063805669546
Validation on real data: 
LOSS supervised-train 0.00039938070098287425, valid 0.0024310597218573093
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0004195640794932842
Batch  11  loss:  0.0002863623376470059
Batch  21  loss:  0.0005075784283690155
Batch  31  loss:  0.0005213572876527905
Batch  41  loss:  0.00030370225431397557
Batch  51  loss:  0.0003641100774984807
Batch  61  loss:  0.0004567679716274142
Batch  71  loss:  0.0002897462109103799
Batch  81  loss:  0.0006723916158080101
Batch  91  loss:  0.0003891598607879132
Batch  101  loss:  0.0002897143713198602
Batch  111  loss:  0.000393202732084319
Batch  121  loss:  0.0005220835446380079
Batch  131  loss:  0.0002459804236423224
Batch  141  loss:  0.0004897567559964955
Batch  151  loss:  0.0004025810630992055
Batch  161  loss:  0.0001922214578371495
Batch  171  loss:  0.0004358321602921933
Batch  181  loss:  0.0002236245636595413
Batch  191  loss:  0.0006285537383519113
Validation on real data: 
LOSS supervised-train 0.00040126728810719215, valid 0.0005526031600311399
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00040367484325543046
Batch  11  loss:  0.00033008441096171737
Batch  21  loss:  0.00038720856537111104
Batch  31  loss:  0.0003765131696127355
Batch  41  loss:  0.0002757514303084463
Batch  51  loss:  0.0006995151634328067
Batch  61  loss:  0.00029667565831914544
Batch  71  loss:  0.00033132752287201583
Batch  81  loss:  0.000506359850987792
Batch  91  loss:  0.0003166205424349755
Batch  101  loss:  0.00019480813352856785
Batch  111  loss:  0.0005406057462096214
Batch  121  loss:  0.0003445929614827037
Batch  131  loss:  0.00028185511473566294
Batch  141  loss:  0.0006630413117818534
Batch  151  loss:  0.0004075972246937454
Batch  161  loss:  0.00041107041761279106
Batch  171  loss:  0.000668693333864212
Batch  181  loss:  0.00023905180569272488
Batch  191  loss:  0.0003826400497928262
Validation on real data: 
LOSS supervised-train 0.00040503337724658196, valid 0.000779500522185117
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00047727811033837497
Batch  11  loss:  0.00025275585358031094
Batch  21  loss:  0.0004195315414108336
Batch  31  loss:  0.0005495927180163562
Batch  41  loss:  0.0002720535558182746
Batch  51  loss:  0.0005290169501677155
Batch  61  loss:  0.0005514283548109233
Batch  71  loss:  0.0003677991626318544
Batch  81  loss:  0.0006624370580539107
Batch  91  loss:  0.0004954271716997027
Batch  101  loss:  0.0003220950602553785
Batch  111  loss:  0.0005576421390287578
Batch  121  loss:  0.0004919071798212826
Batch  131  loss:  0.00022742871078662574
Batch  141  loss:  0.0006196349859237671
Batch  151  loss:  0.0005582906305789948
Batch  161  loss:  0.00038186192978173494
Batch  171  loss:  0.0004003717622254044
Batch  181  loss:  0.00013859836326446384
Batch  191  loss:  0.0006961343460716307
Validation on real data: 
LOSS supervised-train 0.00041841401332931127, valid 0.0009092363761737943
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00042736611794680357
Batch  11  loss:  0.00015302620886359364
Batch  21  loss:  0.00044885356328450143
Batch  31  loss:  0.0003669876605272293
Batch  41  loss:  0.00021894674864597619
Batch  51  loss:  0.0005267515080049634
Batch  61  loss:  0.00023775875160936266
Batch  71  loss:  0.0003243558167014271
Batch  81  loss:  0.0007333705434575677
Batch  91  loss:  0.0005086971214041114
Batch  101  loss:  0.0002397697971900925
Batch  111  loss:  0.0005874930648133159
Batch  121  loss:  0.00042288846452720463
Batch  131  loss:  0.0003739912644959986
Batch  141  loss:  0.0007440446061082184
Batch  151  loss:  0.0002874571073334664
Batch  161  loss:  0.0002733426517806947
Batch  171  loss:  0.0004947042325511575
Batch  181  loss:  0.00024345758720301092
Batch  191  loss:  0.000566645700018853
Validation on real data: 
LOSS supervised-train 0.0004012621288711671, valid 0.000603141204919666
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0003135555307380855
Batch  11  loss:  0.00025225747958756983
Batch  21  loss:  0.00035478093195706606
Batch  31  loss:  0.0002609200309962034
Batch  41  loss:  0.00024769341689534485
Batch  51  loss:  0.0003576792951207608
Batch  61  loss:  0.0003460851148702204
Batch  71  loss:  0.00029727508081123233
Batch  81  loss:  0.0008172441157512367
Batch  91  loss:  0.0004304785979911685
Batch  101  loss:  0.0002803192473948002
Batch  111  loss:  0.0004519537033047527
Batch  121  loss:  0.0004494261520449072
Batch  131  loss:  0.00024957788991741836
Batch  141  loss:  0.0005288620595820248
Batch  151  loss:  0.00047760369488969445
Batch  161  loss:  0.0003359469701536
Batch  171  loss:  0.0004800773167517036
Batch  181  loss:  0.0001716920523904264
Batch  191  loss:  0.000676811090670526
Validation on real data: 
LOSS supervised-train 0.00039144344576925506, valid 0.0007625676225870848
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00044977510697208345
Batch  11  loss:  0.0003550246183294803
Batch  21  loss:  0.0003560594341251999
Batch  31  loss:  0.00047091947635635734
Batch  41  loss:  0.00020901535754092038
Batch  51  loss:  0.00027123335166834295
Batch  61  loss:  0.00030113148386590183
Batch  71  loss:  0.00028173383907414973
Batch  81  loss:  0.0005441895918920636
Batch  91  loss:  0.0003542292397469282
Batch  101  loss:  0.0002735448069870472
Batch  111  loss:  0.00048212020192295313
Batch  121  loss:  0.00040556443855166435
Batch  131  loss:  0.000285762595012784
Batch  141  loss:  0.00045869051245972514
Batch  151  loss:  0.0003005489706993103
Batch  161  loss:  0.00023591640638187528
Batch  171  loss:  0.0005256197764538229
Batch  181  loss:  0.00022525034728460014
Batch  191  loss:  0.0003357386449351907
Validation on real data: 
LOSS supervised-train 0.000385684853172279, valid 0.00028031208785250783
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0004250964557286352
Batch  11  loss:  0.0002325113455299288
Batch  21  loss:  0.0002767420664895326
Batch  31  loss:  0.0003174664743710309
Batch  41  loss:  0.00022339409042615443
Batch  51  loss:  0.0005293511785566807
Batch  61  loss:  0.00029146697488613427
Batch  71  loss:  0.0002779291826300323
Batch  81  loss:  0.0007682321593165398
Batch  91  loss:  0.0005582482554018497
Batch  101  loss:  0.0002783213567454368
Batch  111  loss:  0.0004393146373331547
Batch  121  loss:  0.00039826109423302114
Batch  131  loss:  0.0002826653071679175
Batch  141  loss:  0.0006016591214574873
Batch  151  loss:  0.00023530458565801382
Batch  161  loss:  0.00036366115091368556
Batch  171  loss:  0.0004034144221805036
Batch  181  loss:  0.0002104878512909636
Batch  191  loss:  0.0005402208189480007
Validation on real data: 
LOSS supervised-train 0.0003828669025097042, valid 0.0004096187185496092
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.000249999575316906
Batch  11  loss:  0.00021170148102100939
Batch  21  loss:  0.00044370131217874587
Batch  31  loss:  0.00042856723302975297
Batch  41  loss:  0.00024054123787209392
Batch  51  loss:  0.000269322277745232
Batch  61  loss:  0.00027830421458929777
Batch  71  loss:  0.0002436765207676217
Batch  81  loss:  0.0007340937736444175
Batch  91  loss:  0.0004208242171443999
Batch  101  loss:  0.00024560093879699707
Batch  111  loss:  0.00030797714134678245
Batch  121  loss:  0.0004231894272379577
Batch  131  loss:  0.0002083793660858646
Batch  141  loss:  0.0007759195286780596
Batch  151  loss:  0.000340573547873646
Batch  161  loss:  0.0002847032737918198
Batch  171  loss:  0.0003783845459111035
Batch  181  loss:  0.00020000275981146842
Batch  191  loss:  0.0004414869472384453
Validation on real data: 
LOSS supervised-train 0.0003705269285273971, valid 0.0006692649330943823
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0003512044786475599
Batch  11  loss:  0.00016076327301561832
Batch  21  loss:  0.0004806230717804283
Batch  31  loss:  0.00040376544347964227
Batch  41  loss:  0.000192087100003846
Batch  51  loss:  0.00037707321462221444
Batch  61  loss:  0.0002602399908937514
Batch  71  loss:  0.0002096461394103244
Batch  81  loss:  0.0004780714516527951
Batch  91  loss:  0.00043719448149204254
Batch  101  loss:  0.0003397368418518454
Batch  111  loss:  0.0004103660467080772
Batch  121  loss:  0.00031266940641216934
Batch  131  loss:  0.0002651796967256814
Batch  141  loss:  0.0004790200327988714
Batch  151  loss:  0.00042322269291616976
Batch  161  loss:  0.0004580143722705543
Batch  171  loss:  0.0004672028007917106
Batch  181  loss:  0.0002936730161309242
Batch  191  loss:  0.0003407718031667173
Validation on real data: 
LOSS supervised-train 0.00037587793085549495, valid 0.0005148342461325228
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0005306053790263832
Batch  11  loss:  0.00044279827852733433
Batch  21  loss:  0.00039091534563340247
Batch  31  loss:  0.0003787581517826766
Batch  41  loss:  0.0003422078734729439
Batch  51  loss:  0.00030094402609393
Batch  61  loss:  0.00027229770785197616
Batch  71  loss:  0.000247025367571041
Batch  81  loss:  0.00048154196701943874
Batch  91  loss:  0.00030854149372316897
Batch  101  loss:  0.00019627365691121668
Batch  111  loss:  0.00027117435820400715
Batch  121  loss:  0.00046019128058105707
Batch  131  loss:  0.0002554388775024563
Batch  141  loss:  0.0007095433538779616
Batch  151  loss:  0.00018544304475653917
Batch  161  loss:  0.00037371128564700484
Batch  171  loss:  0.00047278249985538423
Batch  181  loss:  0.00023425377730745822
Batch  191  loss:  0.000503145216498524
Validation on real data: 
LOSS supervised-train 0.00036475254673860034, valid 0.0006516810390166938
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0003705556155182421
Batch  11  loss:  0.00023948022862896323
Batch  21  loss:  0.00036798647488467395
Batch  31  loss:  0.0004703192971646786
Batch  41  loss:  0.00037646092823706567
Batch  51  loss:  0.0005581508739851415
Batch  61  loss:  0.0002118982665706426
Batch  71  loss:  0.0002988117921631783
Batch  81  loss:  0.0009454974788241088
Batch  91  loss:  0.0004957960336469114
Batch  101  loss:  0.0002435087226331234
Batch  111  loss:  0.0004094195901416242
Batch  121  loss:  0.0003844200982712209
Batch  131  loss:  0.0002863587869796902
Batch  141  loss:  0.0004153048212174326
Batch  151  loss:  0.00031416892306879163
Batch  161  loss:  0.00035434309393167496
Batch  171  loss:  0.000553901947569102
Batch  181  loss:  0.00018290782463736832
Batch  191  loss:  0.0004418501921463758
Validation on real data: 
LOSS supervised-train 0.0003787216264026938, valid 0.0008612382807768881
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0005618536961264908
Batch  11  loss:  0.00041928174323402345
Batch  21  loss:  0.00035498879151418805
Batch  31  loss:  0.0005161441513337195
Batch  41  loss:  0.0002941162674687803
Batch  51  loss:  0.0003812094801105559
Batch  61  loss:  0.000289452844299376
Batch  71  loss:  0.00030328764114528894
Batch  81  loss:  0.0006295490893535316
Batch  91  loss:  0.0004779525625053793
Batch  101  loss:  0.0002396587369730696
Batch  111  loss:  0.00032345749787054956
Batch  121  loss:  0.0003706748830154538
Batch  131  loss:  0.0002562357112765312
Batch  141  loss:  0.0006389517802745104
Batch  151  loss:  0.00026994236395694315
Batch  161  loss:  0.0005569871864281595
Batch  171  loss:  0.0006470304797403514
Batch  181  loss:  0.0001570265885675326
Batch  191  loss:  0.00042645703069865704
Validation on real data: 
LOSS supervised-train 0.00038365795146091844, valid 0.0017259129090234637
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00031886581564322114
Batch  11  loss:  0.0002567990741226822
Batch  21  loss:  0.00031586168915964663
Batch  31  loss:  0.00038671205402351916
Batch  41  loss:  0.0002999642747454345
Batch  51  loss:  0.0004575130296871066
Batch  61  loss:  0.0003830442437902093
Batch  71  loss:  0.00030547170899808407
Batch  81  loss:  0.0007578427903354168
Batch  91  loss:  0.00028571594157256186
Batch  101  loss:  0.00028428566292859614
Batch  111  loss:  0.00026828519185073674
Batch  121  loss:  0.0002900662075262517
Batch  131  loss:  0.00019522084039635956
Batch  141  loss:  0.000602246611379087
Batch  151  loss:  0.00030219065956771374
Batch  161  loss:  0.0002364572137594223
Batch  171  loss:  0.00036569571238942444
Batch  181  loss:  0.00016446241352241486
Batch  191  loss:  0.0002757212787400931
Validation on real data: 
LOSS supervised-train 0.0003529806873484631, valid 0.0012481892481446266
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0003313833731226623
Batch  11  loss:  0.0003234227770008147
Batch  21  loss:  0.000436656380770728
Batch  31  loss:  0.0006110167596489191
Batch  41  loss:  0.00025562039809301496
Batch  51  loss:  0.0004548283468466252
Batch  61  loss:  0.00030206923838704824
Batch  71  loss:  0.00023452001914847642
Batch  81  loss:  0.0010247015161439776
Batch  91  loss:  0.00031137524638324976
Batch  101  loss:  0.0002994054520968348
Batch  111  loss:  0.0003373241634108126
Batch  121  loss:  0.00031084599322639406
Batch  131  loss:  0.0003294039925094694
Batch  141  loss:  0.0005552096990868449
Batch  151  loss:  0.00023198190319817513
Batch  161  loss:  0.00025010446552187204
Batch  171  loss:  0.00041748196235857904
Batch  181  loss:  0.000232400736422278
Batch  191  loss:  0.00037741317646577954
Validation on real data: 
LOSS supervised-train 0.0003694832496694289, valid 0.0008279519388452172
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0003248883585911244
Batch  11  loss:  0.0002999862190335989
Batch  21  loss:  0.00041886200779117644
Batch  31  loss:  0.0004391859984025359
Batch  41  loss:  0.00026115099899470806
Batch  51  loss:  0.00032360712066292763
Batch  61  loss:  0.0005164144095033407
Batch  71  loss:  0.00029372709104791284
Batch  81  loss:  0.0005035296198911965
Batch  91  loss:  0.0003699297085404396
Batch  101  loss:  0.0002609373186714947
Batch  111  loss:  0.00035499283694662154
Batch  121  loss:  0.0003436848346609622
Batch  131  loss:  0.0002154344692826271
Batch  141  loss:  0.0006781158153899014
Batch  151  loss:  0.000280672189546749
Batch  161  loss:  0.0003010902728419751
Batch  171  loss:  0.000296426413115114
Batch  181  loss:  0.0002743661461863667
Batch  191  loss:  0.0003795790544245392
Validation on real data: 
LOSS supervised-train 0.0003821298554976238, valid 0.0011254886630922556
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0003804097359534353
Batch  11  loss:  0.00021581698092631996
Batch  21  loss:  0.00026927387807518244
Batch  31  loss:  0.00033305204124189913
Batch  41  loss:  0.00043081812327727675
Batch  51  loss:  0.0002677550946827978
Batch  61  loss:  0.0003205479006282985
Batch  71  loss:  0.0003374905209057033
Batch  81  loss:  0.0008327320101670921
Batch  91  loss:  0.00039314417517744005
Batch  101  loss:  0.00026546689332462847
Batch  111  loss:  0.0003700275265146047
Batch  121  loss:  0.0003820292477030307
Batch  131  loss:  0.00021340760577004403
Batch  141  loss:  0.0004060966311953962
Batch  151  loss:  0.0001936682965606451
Batch  161  loss:  0.00025944746448658407
Batch  171  loss:  0.00048596670967526734
Batch  181  loss:  0.00018010061467066407
Batch  191  loss:  0.00039939055568538606
Validation on real data: 
LOSS supervised-train 0.00036265658782212994, valid 0.0013612658949568868
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0003466767375357449
Batch  11  loss:  0.00033317989436909556
Batch  21  loss:  0.00027090750518254936
Batch  31  loss:  0.0004202724958304316
Batch  41  loss:  0.0003038918075617403
Batch  51  loss:  0.000497077708132565
Batch  61  loss:  0.00034829240757972
Batch  71  loss:  0.0002621407329570502
Batch  81  loss:  0.0009171577985398471
Batch  91  loss:  0.00038633993244729936
Batch  101  loss:  0.00028405539342202246
Batch  111  loss:  0.00042044170550070703
Batch  121  loss:  0.0002737597096711397
Batch  131  loss:  0.0003414834209252149
Batch  141  loss:  0.00037318706745281816
Batch  151  loss:  0.0002506855526007712
Batch  161  loss:  0.0003321327967569232
Batch  171  loss:  0.00043911533430218697
Batch  181  loss:  0.00022431995603255928
Batch  191  loss:  0.0006477179122157395
Validation on real data: 
LOSS supervised-train 0.0003622858135349816, valid 0.0006564625073224306
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0003664011019282043
Batch  11  loss:  0.00018739311781246215
Batch  21  loss:  0.0002638031728565693
Batch  31  loss:  0.00046775329974479973
Batch  41  loss:  0.00021111700334586203
Batch  51  loss:  0.00027838218375109136
Batch  61  loss:  0.0002477492089383304
Batch  71  loss:  0.00030918861739337444
Batch  81  loss:  0.0005880130338482559
Batch  91  loss:  0.0003969755780417472
Batch  101  loss:  0.00019370917289052159
Batch  111  loss:  0.0003006551996804774
Batch  121  loss:  0.00051033467752859
Batch  131  loss:  0.00021720273070968688
Batch  141  loss:  0.000485403259517625
Batch  151  loss:  0.00022231589537113905
Batch  161  loss:  0.0005443764966912568
Batch  171  loss:  0.0004493098531384021
Batch  181  loss:  0.00022158666979521513
Batch  191  loss:  0.0005346167599782348
Validation on real data: 
LOSS supervised-train 0.0003578287806158187, valid 0.000562977627851069
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00041198282269760966
Batch  11  loss:  0.00023163204605225474
Batch  21  loss:  0.0004135776835028082
Batch  31  loss:  0.0002819339861162007
Batch  41  loss:  0.0002697927411645651
Batch  51  loss:  0.0003305845893919468
Batch  61  loss:  0.0002817982458509505
Batch  71  loss:  0.0003203880332875997
Batch  81  loss:  0.0010085990652441978
Batch  91  loss:  0.0005394438630901277
Batch  101  loss:  0.00023836559557821602
Batch  111  loss:  0.00032233851379714906
Batch  121  loss:  0.0004294115351513028
Batch  131  loss:  0.00020286417566239834
Batch  141  loss:  0.0004919660859741271
Batch  151  loss:  0.0003023560275323689
Batch  161  loss:  0.000337320554535836
Batch  171  loss:  0.00033242334029637277
Batch  181  loss:  0.000238191889366135
Batch  191  loss:  0.0005021290853619576
Validation on real data: 
LOSS supervised-train 0.0003529533686378272, valid 0.00030325126135721803
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00027678965125232935
Batch  11  loss:  0.0002050092734862119
Batch  21  loss:  0.00029478545184247196
Batch  31  loss:  0.0008365726098418236
Batch  41  loss:  0.0004105204716324806
Batch  51  loss:  0.0004102864768356085
Batch  61  loss:  0.0003576060989871621
Batch  71  loss:  0.0002479138784110546
Batch  81  loss:  0.0004945921245962381
Batch  91  loss:  0.0003595033485908061
Batch  101  loss:  0.00034726414014585316
Batch  111  loss:  0.0003412970108911395
Batch  121  loss:  0.00039990065852180123
Batch  131  loss:  0.00024540774757042527
Batch  141  loss:  0.0003538415185175836
Batch  151  loss:  0.0002240713802166283
Batch  161  loss:  0.00027099711587652564
Batch  171  loss:  0.0005836673662997782
Batch  181  loss:  0.00023603092995472252
Batch  191  loss:  0.0003205487155355513
Validation on real data: 
LOSS supervised-train 0.00035076222662610234, valid 0.0010378772858530283
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0002564018068369478
Batch  11  loss:  0.00023302702174987644
Batch  21  loss:  0.0002971069479826838
Batch  31  loss:  0.0005102685536257923
Batch  41  loss:  0.0002974306116811931
Batch  51  loss:  0.00035781218321062624
Batch  61  loss:  0.00020580341515596956
Batch  71  loss:  0.00044705666368827224
Batch  81  loss:  0.0010800502495840192
Batch  91  loss:  0.0004092534654773772
Batch  101  loss:  0.0001982141548069194
Batch  111  loss:  0.00033582188189029694
Batch  121  loss:  0.00028529230621643364
Batch  131  loss:  0.00026678465656004846
Batch  141  loss:  0.00041102312388829887
Batch  151  loss:  0.00034889066591858864
Batch  161  loss:  0.0003062219766434282
Batch  171  loss:  0.0004351664683781564
Batch  181  loss:  0.00017389305867254734
Batch  191  loss:  0.00028740993002429605
Validation on real data: 
LOSS supervised-train 0.0003681049694569083, valid 0.0013137271162122488
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00025982651277445257
Batch  11  loss:  0.00031814881367608905
Batch  21  loss:  0.0002923040883615613
Batch  31  loss:  0.0003088840749114752
Batch  41  loss:  0.00020814937306568027
Batch  51  loss:  0.00033030661870725453
Batch  61  loss:  0.0001897173497127369
Batch  71  loss:  0.0003552158013917506
Batch  81  loss:  0.0005587075720541179
Batch  91  loss:  0.00021612306591123343
Batch  101  loss:  0.00019758257258217782
Batch  111  loss:  0.000350120069924742
Batch  121  loss:  0.0004719601129181683
Batch  131  loss:  0.0003432977246120572
Batch  141  loss:  0.0005736923776566982
Batch  151  loss:  0.00022684592113364488
Batch  161  loss:  0.0002796918561216444
Batch  171  loss:  0.00038205733289942145
Batch  181  loss:  0.00019259117834735662
Batch  191  loss:  0.0002718377800192684
Validation on real data: 
LOSS supervised-train 0.00034586735775519626, valid 0.0014052585465833545
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00040109269320964813
Batch  11  loss:  0.00029154252842999995
Batch  21  loss:  0.00041791045805439353
Batch  31  loss:  0.00036311536678113043
Batch  41  loss:  0.00022745381284039468
Batch  51  loss:  0.0003854143142234534
Batch  61  loss:  0.00024433733779005706
Batch  71  loss:  0.00040566883399151266
Batch  81  loss:  0.0010123081738129258
Batch  91  loss:  0.00034865393536165357
Batch  101  loss:  0.00016470890841446817
Batch  111  loss:  0.0002600854495540261
Batch  121  loss:  0.0006439437856897712
Batch  131  loss:  0.00015982432523742318
Batch  141  loss:  0.0003305459686089307
Batch  151  loss:  0.0002938436809927225
Batch  161  loss:  0.00041079981019720435
Batch  171  loss:  0.0003412453515920788
Batch  181  loss:  0.0002276039303978905
Batch  191  loss:  0.0004130270972382277
Validation on real data: 
LOSS supervised-train 0.0003532062612066511, valid 0.0006670195143669844
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0003345151199027896
Batch  11  loss:  0.0003228207351639867
Batch  21  loss:  0.00039409720920957625
Batch  31  loss:  0.00044237839756533504
Batch  41  loss:  0.0002084037841996178
Batch  51  loss:  0.0003846382023766637
Batch  61  loss:  0.0001833843271015212
Batch  71  loss:  0.0003638699708972126
Batch  81  loss:  0.001136138685978949
Batch  91  loss:  0.0004174807108938694
Batch  101  loss:  0.0002596817212179303
Batch  111  loss:  0.00029198636184446514
Batch  121  loss:  0.00045853861956857145
Batch  131  loss:  0.00019478714966680855
Batch  141  loss:  0.00048174604307860136
Batch  151  loss:  0.00032390301930718124
Batch  161  loss:  0.00024062962620519102
Batch  171  loss:  0.00031700311228632927
Batch  181  loss:  0.00028091727290302515
Batch  191  loss:  0.0006018251879140735
Validation on real data: 
LOSS supervised-train 0.00039073672058293597, valid 0.000766660668887198
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00036342302337288857
Batch  11  loss:  0.000360718957381323
Batch  21  loss:  0.0005140339490026236
Batch  31  loss:  0.00048736578901298344
Batch  41  loss:  0.00022820192680228502
Batch  51  loss:  0.0003093789564445615
Batch  61  loss:  0.00013567888527177274
Batch  71  loss:  0.000317192665534094
Batch  81  loss:  0.0006648723501712084
Batch  91  loss:  0.0004324423789512366
Batch  101  loss:  0.0001798840385163203
Batch  111  loss:  0.00028357747942209244
Batch  121  loss:  0.00048255862202495337
Batch  131  loss:  0.00024131206737365574
Batch  141  loss:  0.00045566094922833145
Batch  151  loss:  0.000179898357600905
Batch  161  loss:  0.00027563306502997875
Batch  171  loss:  0.0003770063049159944
Batch  181  loss:  0.000262659159488976
Batch  191  loss:  0.00043562587234191597
Validation on real data: 
LOSS supervised-train 0.00035408125375397504, valid 0.0009733078186400235
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0004468576516956091
Batch  11  loss:  0.0002102539292536676
Batch  21  loss:  0.00041301100281998515
Batch  31  loss:  0.00025969045236706734
Batch  41  loss:  0.0002468574093654752
Batch  51  loss:  0.00023217494890559465
Batch  61  loss:  0.0002498863614164293
Batch  71  loss:  0.0003875730908475816
Batch  81  loss:  0.0005727431271225214
Batch  91  loss:  0.00036481558345258236
Batch  101  loss:  0.0002516217646189034
Batch  111  loss:  0.0002847819996532053
Batch  121  loss:  0.00038755941204726696
Batch  131  loss:  0.00024619358009658754
Batch  141  loss:  0.00038115851930342615
Batch  151  loss:  0.0002034103381447494
Batch  161  loss:  0.0002419298398308456
Batch  171  loss:  0.0003036724519915879
Batch  181  loss:  0.000314208009513095
Batch  191  loss:  0.0004810064274352044
Validation on real data: 
LOSS supervised-train 0.0003420471186109353, valid 0.0009266887791454792
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00042365616536699235
Batch  11  loss:  0.0003741849213838577
Batch  21  loss:  0.00041129972669295967
Batch  31  loss:  0.0003476231067907065
Batch  41  loss:  0.0003203567466698587
Batch  51  loss:  0.00028456561267375946
Batch  61  loss:  0.0003730374446604401
Batch  71  loss:  0.0004402451741043478
Batch  81  loss:  0.0008876703213900328
Batch  91  loss:  0.00032233877573162317
Batch  101  loss:  0.0002553476660978049
Batch  111  loss:  0.0002508315665181726
Batch  121  loss:  0.00035583067801781
Batch  131  loss:  0.00017072490300051868
Batch  141  loss:  0.0005157603300176561
Batch  151  loss:  0.0003364580334164202
Batch  161  loss:  0.00024070650397334248
Batch  171  loss:  0.0003741422260645777
Batch  181  loss:  0.00018995317805092782
Batch  191  loss:  0.0003245807019993663
Validation on real data: 
LOSS supervised-train 0.0003448174582445063, valid 0.0005496349185705185
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.00044898089254274964
Batch  11  loss:  0.00022728189651388675
Batch  21  loss:  0.00028406435740180314
Batch  31  loss:  0.0004100502992514521
Batch  41  loss:  0.0003162848006468266
Batch  51  loss:  0.0003554744180291891
Batch  61  loss:  0.0002313214063178748
Batch  71  loss:  0.0003729943127837032
Batch  81  loss:  0.0007848654640838504
Batch  91  loss:  0.000371301342966035
Batch  101  loss:  0.00022062876087147743
Batch  111  loss:  0.00031658916850574315
Batch  121  loss:  0.0004542770911939442
Batch  131  loss:  0.0002076106466120109
Batch  141  loss:  0.0004413035057950765
Batch  151  loss:  0.0001864208170445636
Batch  161  loss:  0.00023802698706276715
Batch  171  loss:  0.00037222972605377436
Batch  181  loss:  0.00026521238032728434
Batch  191  loss:  0.00047090172301977873
Validation on real data: 
LOSS supervised-train 0.00033694304249365814, valid 0.0004109534784220159
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00036719575291499496
Batch  11  loss:  0.000458589376648888
Batch  21  loss:  0.0004187383456155658
Batch  31  loss:  0.00032474438194185495
Batch  41  loss:  0.00026173159130848944
Batch  51  loss:  0.000365716201486066
Batch  61  loss:  0.00020399055210873485
Batch  71  loss:  0.0003588253166526556
Batch  81  loss:  0.0008468982996419072
Batch  91  loss:  0.0004298619460314512
Batch  101  loss:  0.00021918017591815442
Batch  111  loss:  0.0002460487012285739
Batch  121  loss:  0.00044935173355042934
Batch  131  loss:  0.00030123177566565573
Batch  141  loss:  0.00039174064295366406
Batch  151  loss:  0.00022499411716125906
Batch  161  loss:  0.0002798288769554347
Batch  171  loss:  0.0003528100496623665
Batch  181  loss:  0.0001802221522666514
Batch  191  loss:  0.00039547900087200105
Validation on real data: 
LOSS supervised-train 0.0003584185766521841, valid 0.0023346852976828814
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  skateboard ; Model ID: 98222a1e5f59f2098745e78dbc45802e
--------------------
Training baseline regression model:  2022-03-30 09:30:42.208545
Detector:  point_transformer
Object:  skateboard
--------------------
device is  cuda
--------------------
Number of trainable parameters:  894622
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.17059271037578583
Batch  11  loss:  0.13609828054904938
Batch  21  loss:  0.08553633838891983
Batch  31  loss:  0.031388066709041595
Batch  41  loss:  0.03064870275557041
Batch  51  loss:  0.036215852946043015
Batch  61  loss:  0.016298988834023476
Batch  71  loss:  0.031084712594747543
Batch  81  loss:  0.013425571843981743
Batch  91  loss:  0.027029361575841904
Batch  101  loss:  0.009012673981487751
Batch  111  loss:  0.012731267139315605
Batch  121  loss:  0.012075728736817837
Batch  131  loss:  0.017434118315577507
Batch  141  loss:  0.019252443686127663
Batch  151  loss:  0.005900768097490072
Batch  161  loss:  0.019518038257956505
Batch  171  loss:  0.008352790959179401
Batch  181  loss:  0.0059773302637040615
Batch  191  loss:  0.004832340870052576
Validation on real data: 
LOSS supervised-train 0.03167740456410684, valid 0.005573820788413286
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.01138778030872345
Batch  11  loss:  0.008460751734673977
Batch  21  loss:  0.003163241082802415
Batch  31  loss:  0.006699258927255869
Batch  41  loss:  0.005716749466955662
Batch  51  loss:  0.004927765112370253
Batch  61  loss:  0.006371836643666029
Batch  71  loss:  0.009108195081353188
Batch  81  loss:  0.0036497418768703938
Batch  91  loss:  0.0024527625646442175
Batch  101  loss:  0.0022982913069427013
Batch  111  loss:  0.0024236165918409824
Batch  121  loss:  0.004852812737226486
Batch  131  loss:  0.0051320577040314674
Batch  141  loss:  0.006149460561573505
Batch  151  loss:  0.002724205842241645
Batch  161  loss:  0.008098457008600235
Batch  171  loss:  0.0050973654724657536
Batch  181  loss:  0.003380157984793186
Batch  191  loss:  0.0029222723096609116
Validation on real data: 
LOSS supervised-train 0.004422315407427959, valid 0.0020337174646556377
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.006702157203108072
Batch  11  loss:  0.004681697580963373
Batch  21  loss:  0.0012246333062648773
Batch  31  loss:  0.002633263124153018
Batch  41  loss:  0.0025738300755620003
Batch  51  loss:  0.005099020898342133
Batch  61  loss:  0.003392130136489868
Batch  71  loss:  0.006301290821284056
Batch  81  loss:  0.0020402593072503805
Batch  91  loss:  0.0013530642027035356
Batch  101  loss:  0.002001081593334675
Batch  111  loss:  0.0021622488275170326
Batch  121  loss:  0.0033505032770335674
Batch  131  loss:  0.003643832402303815
Batch  141  loss:  0.005678147543221712
Batch  151  loss:  0.002056875266134739
Batch  161  loss:  0.005923554301261902
Batch  171  loss:  0.002082815393805504
Batch  181  loss:  0.002041972940787673
Batch  191  loss:  0.0014924105489626527
Validation on real data: 
LOSS supervised-train 0.0028439908020664005, valid 0.0016948748379945755
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.003920906223356724
Batch  11  loss:  0.003975844010710716
Batch  21  loss:  0.0009775173384696245
Batch  31  loss:  0.0013513718731701374
Batch  41  loss:  0.002292271703481674
Batch  51  loss:  0.003499183338135481
Batch  61  loss:  0.0027150586247444153
Batch  71  loss:  0.0043798163533210754
Batch  81  loss:  0.0013562351232394576
Batch  91  loss:  0.0007377969450317323
Batch  101  loss:  0.0016761020524427295
Batch  111  loss:  0.001453423872590065
Batch  121  loss:  0.0024932355154305696
Batch  131  loss:  0.0023029104340821505
Batch  141  loss:  0.003910391125828028
Batch  151  loss:  0.0014328638790175319
Batch  161  loss:  0.003555077826604247
Batch  171  loss:  0.001701537985354662
Batch  181  loss:  0.0014375507598742843
Batch  191  loss:  0.0012872593943029642
Validation on real data: 
LOSS supervised-train 0.002078048287949059, valid 0.0012889859499409795
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0028634988702833652
Batch  11  loss:  0.00213751383125782
Batch  21  loss:  0.00097154505783692
Batch  31  loss:  0.001053539803251624
Batch  41  loss:  0.0014769203262403607
Batch  51  loss:  0.002615715144202113
Batch  61  loss:  0.0020876650232821703
Batch  71  loss:  0.002703380770981312
Batch  81  loss:  0.001020620227791369
Batch  91  loss:  0.0007840428152121603
Batch  101  loss:  0.0011205653427168727
Batch  111  loss:  0.0011749457335099578
Batch  121  loss:  0.0018230881541967392
Batch  131  loss:  0.0018338914960622787
Batch  141  loss:  0.0029005424585193396
Batch  151  loss:  0.0011789569398388267
Batch  161  loss:  0.002750282408669591
Batch  171  loss:  0.0013871133560314775
Batch  181  loss:  0.0013113287277519703
Batch  191  loss:  0.001442486303858459
Validation on real data: 
LOSS supervised-train 0.0015730112724122592, valid 0.0008173128589987755
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.001789427944459021
Batch  11  loss:  0.001936210785061121
Batch  21  loss:  0.0008560347487218678
Batch  31  loss:  0.0008542415453121066
Batch  41  loss:  0.001332247513346374
Batch  51  loss:  0.001687948708422482
Batch  61  loss:  0.0014533508801832795
Batch  71  loss:  0.0019811242818832397
Batch  81  loss:  0.0009360731928609312
Batch  91  loss:  0.0005174177931621671
Batch  101  loss:  0.0010911301942542195
Batch  111  loss:  0.001059419009834528
Batch  121  loss:  0.0012528365477919579
Batch  131  loss:  0.001469351351261139
Batch  141  loss:  0.0022633220069110394
Batch  151  loss:  0.0009354741778224707
Batch  161  loss:  0.0019433993147686124
Batch  171  loss:  0.0011315473821014166
Batch  181  loss:  0.0008924461435526609
Batch  191  loss:  0.0009541100007481873
Validation on real data: 
LOSS supervised-train 0.0012352793832542375, valid 0.0005935935769230127
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0016532731242477894
Batch  11  loss:  0.0014415363548323512
Batch  21  loss:  0.0006491260137408972
Batch  31  loss:  0.000951232563238591
Batch  41  loss:  0.0010480991331860423
Batch  51  loss:  0.0019770730286836624
Batch  61  loss:  0.0012400862760841846
Batch  71  loss:  0.0015578600578010082
Batch  81  loss:  0.0008682053885422647
Batch  91  loss:  0.000460591894807294
Batch  101  loss:  0.0007318704156205058
Batch  111  loss:  0.0010199769167229533
Batch  121  loss:  0.0011857426725327969
Batch  131  loss:  0.001381568261422217
Batch  141  loss:  0.0017729944083839655
Batch  151  loss:  0.0008358266786672175
Batch  161  loss:  0.0018028038321062922
Batch  171  loss:  0.0011776257306337357
Batch  181  loss:  0.0010934144956991076
Batch  191  loss:  0.0009402985451743007
Validation on real data: 
LOSS supervised-train 0.0010461778879107442, valid 0.0007134656189009547
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0014038719236850739
Batch  11  loss:  0.001205409411340952
Batch  21  loss:  0.0006464096950367093
Batch  31  loss:  0.0008257040171884
Batch  41  loss:  0.0009741972316987813
Batch  51  loss:  0.0014717855956405401
Batch  61  loss:  0.0011049427557736635
Batch  71  loss:  0.001294944086112082
Batch  81  loss:  0.0008689241949468851
Batch  91  loss:  0.000489395868498832
Batch  101  loss:  0.0007332563982345164
Batch  111  loss:  0.0008407634450122714
Batch  121  loss:  0.0010185119463130832
Batch  131  loss:  0.0010417840676382184
Batch  141  loss:  0.0013174749910831451
Batch  151  loss:  0.0006523492629639804
Batch  161  loss:  0.0014895382337272167
Batch  171  loss:  0.000864351459313184
Batch  181  loss:  0.0008838608046062291
Batch  191  loss:  0.0009564018109813333
Validation on real data: 
LOSS supervised-train 0.0009532576039782726, valid 0.000508886412717402
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0011025272542610765
Batch  11  loss:  0.0009393906220793724
Batch  21  loss:  0.0007601758698001504
Batch  31  loss:  0.0005177987040951848
Batch  41  loss:  0.0007757549756206572
Batch  51  loss:  0.0011932876659557223
Batch  61  loss:  0.0009735278435982764
Batch  71  loss:  0.0009624059894122183
Batch  81  loss:  0.0006161736091598868
Batch  91  loss:  0.0004208628088235855
Batch  101  loss:  0.000787978176958859
Batch  111  loss:  0.0006946800276637077
Batch  121  loss:  0.0008287038654088974
Batch  131  loss:  0.0008852094179019332
Batch  141  loss:  0.001268960302695632
Batch  151  loss:  0.0009387010359205306
Batch  161  loss:  0.0015341576654464006
Batch  171  loss:  0.0009228282142430544
Batch  181  loss:  0.000732945918571204
Batch  191  loss:  0.0007291534566320479
Validation on real data: 
LOSS supervised-train 0.0008307181934651453, valid 0.0004755284171551466
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0010094143217429519
Batch  11  loss:  0.0008639015723019838
Batch  21  loss:  0.0006337625673040748
Batch  31  loss:  0.0005117169348523021
Batch  41  loss:  0.0007433881401084363
Batch  51  loss:  0.0012095506535843015
Batch  61  loss:  0.0008722016937099397
Batch  71  loss:  0.0008621866581961513
Batch  81  loss:  0.0004708500928245485
Batch  91  loss:  0.00039110268699005246
Batch  101  loss:  0.0006547594675794244
Batch  111  loss:  0.0006949403905309737
Batch  121  loss:  0.0007344500045292079
Batch  131  loss:  0.000747921469155699
Batch  141  loss:  0.0009720083326101303
Batch  151  loss:  0.000588507333304733
Batch  161  loss:  0.0012457078555598855
Batch  171  loss:  0.0008180771837942302
Batch  181  loss:  0.0005541178397834301
Batch  191  loss:  0.0009593178401701152
Validation on real data: 
LOSS supervised-train 0.0007561877596890553, valid 0.00048808311112225056
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.00089336431119591
Batch  11  loss:  0.0008685342036187649
Batch  21  loss:  0.0005679888417944312
Batch  31  loss:  0.0006899532745592296
Batch  41  loss:  0.0007180575048550963
Batch  51  loss:  0.0007620059768669307
Batch  61  loss:  0.0007859112229198217
Batch  71  loss:  0.0009063144098035991
Batch  81  loss:  0.0006500653689727187
Batch  91  loss:  0.0004151304601691663
Batch  101  loss:  0.0006509214290417731
Batch  111  loss:  0.0007985886768437922
Batch  121  loss:  0.0005671713151969016
Batch  131  loss:  0.000798931228928268
Batch  141  loss:  0.0010461756028234959
Batch  151  loss:  0.0006269456353038549
Batch  161  loss:  0.0010804267367348075
Batch  171  loss:  0.0006976482109166682
Batch  181  loss:  0.0005365833058021963
Batch  191  loss:  0.0008002104586921632
Validation on real data: 
LOSS supervised-train 0.0006868044385919347, valid 0.0004272176302038133
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0008235929999500513
Batch  11  loss:  0.0006711164023727179
Batch  21  loss:  0.0005342751392163336
Batch  31  loss:  0.000568532501347363
Batch  41  loss:  0.0006418207194656134
Batch  51  loss:  0.000844764756038785
Batch  61  loss:  0.0006953124538995326
Batch  71  loss:  0.0006812008796259761
Batch  81  loss:  0.0005299009499140084
Batch  91  loss:  0.00041911378502845764
Batch  101  loss:  0.0006225393735803664
Batch  111  loss:  0.0006426316103897989
Batch  121  loss:  0.000638021738268435
Batch  131  loss:  0.0007597871008329093
Batch  141  loss:  0.0008138042758218944
Batch  151  loss:  0.0006274597253650427
Batch  161  loss:  0.000989944557659328
Batch  171  loss:  0.0006172545254230499
Batch  181  loss:  0.00046316400403156877
Batch  191  loss:  0.0007785403286106884
Validation on real data: 
LOSS supervised-train 0.0006341197190340609, valid 0.00045909633627161384
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0007204916910268366
Batch  11  loss:  0.0006321747205220163
Batch  21  loss:  0.0004851685371249914
Batch  31  loss:  0.00048579982831142843
Batch  41  loss:  0.0005912960623390973
Batch  51  loss:  0.000786066462751478
Batch  61  loss:  0.000620450999122113
Batch  71  loss:  0.0006786830490455031
Batch  81  loss:  0.00044663171865977347
Batch  91  loss:  0.0003755663346964866
Batch  101  loss:  0.0004851456615142524
Batch  111  loss:  0.000716920243576169
Batch  121  loss:  0.0004898378392681479
Batch  131  loss:  0.0005972803919576108
Batch  141  loss:  0.0009291897295042872
Batch  151  loss:  0.0005003660917282104
Batch  161  loss:  0.0009436577092856169
Batch  171  loss:  0.0006779912509955466
Batch  181  loss:  0.0005059560062363744
Batch  191  loss:  0.0006414348026737571
Validation on real data: 
LOSS supervised-train 0.0006088729118346236, valid 0.0004235542146489024
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0007226318120956421
Batch  11  loss:  0.0006736415671184659
Batch  21  loss:  0.0005005322746001184
Batch  31  loss:  0.0005744847585447133
Batch  41  loss:  0.0005940491682849824
Batch  51  loss:  0.0008088400936685503
Batch  61  loss:  0.0006361716659739614
Batch  71  loss:  0.0006575332372449338
Batch  81  loss:  0.000487127952510491
Batch  91  loss:  0.0004667172906920314
Batch  101  loss:  0.0006481083109974861
Batch  111  loss:  0.0005400402587838471
Batch  121  loss:  0.0005235375137999654
Batch  131  loss:  0.00048043308197520673
Batch  141  loss:  0.0007972640451043844
Batch  151  loss:  0.0005181795568205416
Batch  161  loss:  0.0007843641215004027
Batch  171  loss:  0.0004213504143990576
Batch  181  loss:  0.0005829279543831944
Batch  191  loss:  0.0006085637724027038
Validation on real data: 
LOSS supervised-train 0.0005678301051375457, valid 0.0003971429541707039
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0005280771874822676
Batch  11  loss:  0.0005920930998399854
Batch  21  loss:  0.0005427906289696693
Batch  31  loss:  0.0005550811183638871
Batch  41  loss:  0.0005938928225077689
Batch  51  loss:  0.00046034768456593156
Batch  61  loss:  0.0004728833446279168
Batch  71  loss:  0.0006933483527973294
Batch  81  loss:  0.0004850847471971065
Batch  91  loss:  0.00035220803692936897
Batch  101  loss:  0.00048369914293289185
Batch  111  loss:  0.0005825445405207574
Batch  121  loss:  0.0004501122166402638
Batch  131  loss:  0.0004935442702844739
Batch  141  loss:  0.0005922485725022852
Batch  151  loss:  0.0005317226168699563
Batch  161  loss:  0.0007046946557238698
Batch  171  loss:  0.0005671207909472287
Batch  181  loss:  0.0005733745056204498
Batch  191  loss:  0.0005187209462746978
Validation on real data: 
LOSS supervised-train 0.000531774701521499, valid 0.0003067318466491997
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0005754088633693755
Batch  11  loss:  0.0006303857080638409
Batch  21  loss:  0.0004900588537566364
Batch  31  loss:  0.00040305970469489694
Batch  41  loss:  0.0005536839016713202
Batch  51  loss:  0.00046330291661433876
Batch  61  loss:  0.0005219142767600715
Batch  71  loss:  0.0005011411849409342
Batch  81  loss:  0.00044535862980410457
Batch  91  loss:  0.0003437072446104139
Batch  101  loss:  0.00041790379327721894
Batch  111  loss:  0.0006286731222644448
Batch  121  loss:  0.00040313860517926514
Batch  131  loss:  0.0005388139979913831
Batch  141  loss:  0.0005986763280816376
Batch  151  loss:  0.000506969983689487
Batch  161  loss:  0.0007743194582872093
Batch  171  loss:  0.0006625120877288282
Batch  181  loss:  0.0006127453525550663
Batch  191  loss:  0.0005871960893273354
Validation on real data: 
LOSS supervised-train 0.000513922271784395, valid 0.0003329794271849096
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0005427352152764797
Batch  11  loss:  0.00048828008584678173
Batch  21  loss:  0.00043554967851378024
Batch  31  loss:  0.0004686764150392264
Batch  41  loss:  0.0005066043813712895
Batch  51  loss:  0.0007983024115674198
Batch  61  loss:  0.0004369205853436142
Batch  71  loss:  0.0005792912561446428
Batch  81  loss:  0.0003502598265185952
Batch  91  loss:  0.0002979577402584255
Batch  101  loss:  0.000525223440490663
Batch  111  loss:  0.0004448923282325268
Batch  121  loss:  0.0003734016790986061
Batch  131  loss:  0.0004360864986665547
Batch  141  loss:  0.0005469801835715771
Batch  151  loss:  0.0004847421369049698
Batch  161  loss:  0.0007949410937726498
Batch  171  loss:  0.0006590879638679326
Batch  181  loss:  0.0005411482998169959
Batch  191  loss:  0.0004996905918233097
Validation on real data: 
LOSS supervised-train 0.0004961200567777269, valid 0.00028749913326464593
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.000553859630599618
Batch  11  loss:  0.0004217955283820629
Batch  21  loss:  0.00043437848216854036
Batch  31  loss:  0.0004884567460976541
Batch  41  loss:  0.00046907231444492936
Batch  51  loss:  0.0005971068749204278
Batch  61  loss:  0.00046419951831921935
Batch  71  loss:  0.000564289279282093
Batch  81  loss:  0.0003238825884182006
Batch  91  loss:  0.00039325514808297157
Batch  101  loss:  0.00043595119495876133
Batch  111  loss:  0.00039243619539774954
Batch  121  loss:  0.0003487095527816564
Batch  131  loss:  0.0004104946565348655
Batch  141  loss:  0.0005906881997361779
Batch  151  loss:  0.00047258264385163784
Batch  161  loss:  0.0008663410553708673
Batch  171  loss:  0.0006338781677186489
Batch  181  loss:  0.0005297977477312088
Batch  191  loss:  0.0005138683482073247
Validation on real data: 
LOSS supervised-train 0.0004808132170001045, valid 0.00032487313728779554
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0005649780505336821
Batch  11  loss:  0.0005403759423643351
Batch  21  loss:  0.0004319676663726568
Batch  31  loss:  0.00035532168112695217
Batch  41  loss:  0.0005302310455590487
Batch  51  loss:  0.0005180472508072853
Batch  61  loss:  0.00046995774027891457
Batch  71  loss:  0.00038901384687051177
Batch  81  loss:  0.00036205790820531547
Batch  91  loss:  0.00035390531411394477
Batch  101  loss:  0.0003957490553148091
Batch  111  loss:  0.00039564460166729987
Batch  121  loss:  0.0003745125432033092
Batch  131  loss:  0.00039857253432273865
Batch  141  loss:  0.0005368858692236245
Batch  151  loss:  0.000378957309294492
Batch  161  loss:  0.0005893579218536615
Batch  171  loss:  0.0004118693177588284
Batch  181  loss:  0.0004516637709457427
Batch  191  loss:  0.0006190863205119967
Validation on real data: 
LOSS supervised-train 0.00045627203988260587, valid 0.0003498902078717947
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.00045744291855953634
Batch  11  loss:  0.0004448923282325268
Batch  21  loss:  0.0003692499885801226
Batch  31  loss:  0.0003598318144213408
Batch  41  loss:  0.00041431887075304985
Batch  51  loss:  0.0005250746035017073
Batch  61  loss:  0.0003931229584850371
Batch  71  loss:  0.0005127868498675525
Batch  81  loss:  0.0003550119581632316
Batch  91  loss:  0.0003333356580697
Batch  101  loss:  0.0004291459044907242
Batch  111  loss:  0.0004893156001344323
Batch  121  loss:  0.00038416628376580775
Batch  131  loss:  0.0003661088994704187
Batch  141  loss:  0.0005079144611954689
Batch  151  loss:  0.0004080418439116329
Batch  161  loss:  0.0006571984267793596
Batch  171  loss:  0.0003715909260790795
Batch  181  loss:  0.0003899119037669152
Batch  191  loss:  0.00047295770491473377
Validation on real data: 
LOSS supervised-train 0.00043971670558676126, valid 0.00034818873973563313
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0004415696894284338
Batch  11  loss:  0.00047299679135903716
Batch  21  loss:  0.000430700252763927
Batch  31  loss:  0.00044142117258161306
Batch  41  loss:  0.0005048623424954712
Batch  51  loss:  0.00041104588308371603
Batch  61  loss:  0.0004080542712472379
Batch  71  loss:  0.0004242461873218417
Batch  81  loss:  0.00033910266938619316
Batch  91  loss:  0.0003327555605210364
Batch  101  loss:  0.00037647574208676815
Batch  111  loss:  0.00047815858852118254
Batch  121  loss:  0.00032264995388686657
Batch  131  loss:  0.00039557917625643313
Batch  141  loss:  0.0005172910168766975
Batch  151  loss:  0.00041524096741341054
Batch  161  loss:  0.0005476213991641998
Batch  171  loss:  0.0004527737037278712
Batch  181  loss:  0.0004966618726029992
Batch  191  loss:  0.00039560667937621474
Validation on real data: 
LOSS supervised-train 0.00042869290700764395, valid 0.0003172206343151629
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.000539990549441427
Batch  11  loss:  0.0004667383327614516
Batch  21  loss:  0.0003445966576691717
Batch  31  loss:  0.00031820955337025225
Batch  41  loss:  0.0005291196866892278
Batch  51  loss:  0.00047468021512031555
Batch  61  loss:  0.00041510671144351363
Batch  71  loss:  0.0003387063043192029
Batch  81  loss:  0.00032304300111718476
Batch  91  loss:  0.00030548026552423835
Batch  101  loss:  0.00040346747846342623
Batch  111  loss:  0.0004019253537990153
Batch  121  loss:  0.0002933433570433408
Batch  131  loss:  0.0003662894305307418
Batch  141  loss:  0.0004930691793560982
Batch  151  loss:  0.0003960722533520311
Batch  161  loss:  0.000608760688919574
Batch  171  loss:  0.00047205149894580245
Batch  181  loss:  0.00033579787123017013
Batch  191  loss:  0.00045146196498535573
Validation on real data: 
LOSS supervised-train 0.0004121128212136682, valid 0.0002761580399237573
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0003244167601224035
Batch  11  loss:  0.00028386645135469735
Batch  21  loss:  0.00034805454197339714
Batch  31  loss:  0.00036213509156368673
Batch  41  loss:  0.00036200135946273804
Batch  51  loss:  0.0005652125109918416
Batch  61  loss:  0.00034406347549520433
Batch  71  loss:  0.0004845856165047735
Batch  81  loss:  0.00032751858816482127
Batch  91  loss:  0.0003573838621377945
Batch  101  loss:  0.0003251746529713273
Batch  111  loss:  0.0004220573464408517
Batch  121  loss:  0.0003136963932774961
Batch  131  loss:  0.0003468744398560375
Batch  141  loss:  0.00038066404522396624
Batch  151  loss:  0.000424028403358534
Batch  161  loss:  0.0004679977719206363
Batch  171  loss:  0.00037339149275794625
Batch  181  loss:  0.00042935259989462793
Batch  191  loss:  0.00046240127994678915
Validation on real data: 
LOSS supervised-train 0.0003997492956114002, valid 0.00024300950462929904
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00038573655183427036
Batch  11  loss:  0.0003953084524255246
Batch  21  loss:  0.0003356861707288772
Batch  31  loss:  0.00038999810931272805
Batch  41  loss:  0.00038928474532440305
Batch  51  loss:  0.0004070269351359457
Batch  61  loss:  0.0003120371256954968
Batch  71  loss:  0.0004195129731670022
Batch  81  loss:  0.0002816789783537388
Batch  91  loss:  0.0003433142846915871
Batch  101  loss:  0.0003104172646999359
Batch  111  loss:  0.00038054853212088346
Batch  121  loss:  0.000304987101117149
Batch  131  loss:  0.0003233867173548788
Batch  141  loss:  0.0003522679035086185
Batch  151  loss:  0.00031438484438695014
Batch  161  loss:  0.0005682729533873498
Batch  171  loss:  0.0005138441338203847
Batch  181  loss:  0.0003035525151062757
Batch  191  loss:  0.0004159462114330381
Validation on real data: 
LOSS supervised-train 0.0003884876416850602, valid 0.0002700667828321457
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0003483490436337888
Batch  11  loss:  0.00040918897138908505
Batch  21  loss:  0.0003629613493103534
Batch  31  loss:  0.0002784427488222718
Batch  41  loss:  0.0004436612653080374
Batch  51  loss:  0.00043378351256251335
Batch  61  loss:  0.00033405848080292344
Batch  71  loss:  0.00045714862062595785
Batch  81  loss:  0.00028640576056204736
Batch  91  loss:  0.00036892169737257063
Batch  101  loss:  0.00036779753281734884
Batch  111  loss:  0.0003691684396471828
Batch  121  loss:  0.00030098282149992883
Batch  131  loss:  0.00025194353656843305
Batch  141  loss:  0.0003646786790341139
Batch  151  loss:  0.0004171052423771471
Batch  161  loss:  0.0005041273543611169
Batch  171  loss:  0.0004405322251841426
Batch  181  loss:  0.00041578151285648346
Batch  191  loss:  0.0005131984944455326
Validation on real data: 
LOSS supervised-train 0.0003793469574884512, valid 0.0002903307613451034
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.00030751098529435694
Batch  11  loss:  0.0003814071824308485
Batch  21  loss:  0.00032891303999349475
Batch  31  loss:  0.00038748508086428046
Batch  41  loss:  0.0004452306602615863
Batch  51  loss:  0.00038072920870035887
Batch  61  loss:  0.0003273475158493966
Batch  71  loss:  0.00032759690657258034
Batch  81  loss:  0.00034759161644615233
Batch  91  loss:  0.0003243913524784148
Batch  101  loss:  0.0003539695462677628
Batch  111  loss:  0.0003269146545790136
Batch  121  loss:  0.0002593782846815884
Batch  131  loss:  0.0003147733223158866
Batch  141  loss:  0.00046838424168527126
Batch  151  loss:  0.00031539343763142824
Batch  161  loss:  0.0004896555910818279
Batch  171  loss:  0.0004063033265992999
Batch  181  loss:  0.00030359855736605823
Batch  191  loss:  0.00041929809958674014
Validation on real data: 
LOSS supervised-train 0.0003565867814177182, valid 0.00027732306625694036
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.00030448747565969825
Batch  11  loss:  0.00038896052865311503
Batch  21  loss:  0.0004482753574848175
Batch  31  loss:  0.00031275421497412026
Batch  41  loss:  0.00038361188489943743
Batch  51  loss:  0.0003626607940532267
Batch  61  loss:  0.00035741194733418524
Batch  71  loss:  0.0005308386753313243
Batch  81  loss:  0.0003227917477488518
Batch  91  loss:  0.0003420035354793072
Batch  101  loss:  0.0003405867319088429
Batch  111  loss:  0.00028166105039417744
Batch  121  loss:  0.00029334513237699866
Batch  131  loss:  0.0003039306029677391
Batch  141  loss:  0.00027135678101330996
Batch  151  loss:  0.00033555051777511835
Batch  161  loss:  0.0004466147511266172
Batch  171  loss:  0.00033407090813852847
Batch  181  loss:  0.00034223764669150114
Batch  191  loss:  0.0003850736247841269
Validation on real data: 
LOSS supervised-train 0.00035840169017319567, valid 0.00020978122483938932
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.00031041225884109735
Batch  11  loss:  0.0003581592463888228
Batch  21  loss:  0.000325565692037344
Batch  31  loss:  0.00036845263093709946
Batch  41  loss:  0.0003343270509503782
Batch  51  loss:  0.00032137639936991036
Batch  61  loss:  0.00037107913522049785
Batch  71  loss:  0.00031941456836648285
Batch  81  loss:  0.0002599027066025883
Batch  91  loss:  0.0003068737278226763
Batch  101  loss:  0.00037301055272109807
Batch  111  loss:  0.0004108563589397818
Batch  121  loss:  0.0002671889087650925
Batch  131  loss:  0.00026646157493814826
Batch  141  loss:  0.00037028922815807164
Batch  151  loss:  0.0003423125308472663
Batch  161  loss:  0.0004908171249553561
Batch  171  loss:  0.00031912903068587184
Batch  181  loss:  0.00033362043905071914
Batch  191  loss:  0.00041583285201340914
Validation on real data: 
LOSS supervised-train 0.0003561355634155916, valid 0.00022875017020851374
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0003665088734123856
Batch  11  loss:  0.00038376115844585
Batch  21  loss:  0.00033865682780742645
Batch  31  loss:  0.00028480953187681735
Batch  41  loss:  0.00033031037310138345
Batch  51  loss:  0.0004611594195012003
Batch  61  loss:  0.0003229551948606968
Batch  71  loss:  0.0004121199017390609
Batch  81  loss:  0.0002971766225527972
Batch  91  loss:  0.0003038770519196987
Batch  101  loss:  0.00035456864861771464
Batch  111  loss:  0.0003561577759683132
Batch  121  loss:  0.0003535933792591095
Batch  131  loss:  0.00035160823608748615
Batch  141  loss:  0.00036465152516029775
Batch  151  loss:  0.00024264625972136855
Batch  161  loss:  0.0005428752047009766
Batch  171  loss:  0.0002789995342027396
Batch  181  loss:  0.00040613359306007624
Batch  191  loss:  0.00037984034861437976
Validation on real data: 
LOSS supervised-train 0.00034045318367134316, valid 0.0002721712808124721
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0003393243532627821
Batch  11  loss:  0.0003341399133205414
Batch  21  loss:  0.0003337872039992362
Batch  31  loss:  0.00027666339883580804
Batch  41  loss:  0.00031821857555769384
Batch  51  loss:  0.0005527085741050541
Batch  61  loss:  0.0002906929876189679
Batch  71  loss:  0.00039046257734298706
Batch  81  loss:  0.0002952198265120387
Batch  91  loss:  0.0002386805572314188
Batch  101  loss:  0.0003679441870190203
Batch  111  loss:  0.0003517451696097851
Batch  121  loss:  0.000237350381212309
Batch  131  loss:  0.0002716501767281443
Batch  141  loss:  0.0003867075138259679
Batch  151  loss:  0.0003228013520129025
Batch  161  loss:  0.00046829841448925436
Batch  171  loss:  0.00032855282188393176
Batch  181  loss:  0.00032175437081605196
Batch  191  loss:  0.00036238200846128166
Validation on real data: 
LOSS supervised-train 0.0003343157527706353, valid 0.00021416028903331608
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00032363354694098234
Batch  11  loss:  0.000329635658999905
Batch  21  loss:  0.000287918170215562
Batch  31  loss:  0.0002696521987672895
Batch  41  loss:  0.0003597655158955604
Batch  51  loss:  0.00043025618651881814
Batch  61  loss:  0.0002723618526943028
Batch  71  loss:  0.00034594369935803115
Batch  81  loss:  0.00022968371922615916
Batch  91  loss:  0.00022373729734681547
Batch  101  loss:  0.000348790199495852
Batch  111  loss:  0.00031035946449264884
Batch  121  loss:  0.00027061623404733837
Batch  131  loss:  0.00027320891967974603
Batch  141  loss:  0.00027325109113007784
Batch  151  loss:  0.0003435532271396369
Batch  161  loss:  0.0005569605273194611
Batch  171  loss:  0.0003790515474975109
Batch  181  loss:  0.00028506986564025283
Batch  191  loss:  0.00036245890078134835
Validation on real data: 
LOSS supervised-train 0.00032313270567101425, valid 0.00023767558741383255
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.00027615652652457356
Batch  11  loss:  0.000265951530309394
Batch  21  loss:  0.00037548772525042295
Batch  31  loss:  0.0002522323338780552
Batch  41  loss:  0.00037976852036081254
Batch  51  loss:  0.0003101633337792009
Batch  61  loss:  0.0002709362015593797
Batch  71  loss:  0.00033348999568261206
Batch  81  loss:  0.0002474613138474524
Batch  91  loss:  0.0002443649573251605
Batch  101  loss:  0.00030181644251570106
Batch  111  loss:  0.0002559411805123091
Batch  121  loss:  0.00022900996555108577
Batch  131  loss:  0.00024428448523394763
Batch  141  loss:  0.00029865658143535256
Batch  151  loss:  0.0002836301573552191
Batch  161  loss:  0.0006291637546382844
Batch  171  loss:  0.0003543626517057419
Batch  181  loss:  0.0004051560827065259
Batch  191  loss:  0.0003229804278817028
Validation on real data: 
LOSS supervised-train 0.00032051110923930537, valid 0.00020108559692744166
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0003231753362342715
Batch  11  loss:  0.00033675532904453576
Batch  21  loss:  0.0003223073435947299
Batch  31  loss:  0.000254241720540449
Batch  41  loss:  0.0003615935565903783
Batch  51  loss:  0.0003602108627092093
Batch  61  loss:  0.0002607824862934649
Batch  71  loss:  0.0004272241494618356
Batch  81  loss:  0.00020459650841075927
Batch  91  loss:  0.00028514195582829416
Batch  101  loss:  0.00036385576822794974
Batch  111  loss:  0.0003056236309930682
Batch  121  loss:  0.000299411010928452
Batch  131  loss:  0.0002625040360726416
Batch  141  loss:  0.00026909520966000855
Batch  151  loss:  0.0002696521405596286
Batch  161  loss:  0.0004428317188285291
Batch  171  loss:  0.000330688722897321
Batch  181  loss:  0.00031144393142312765
Batch  191  loss:  0.0003304654383100569
Validation on real data: 
LOSS supervised-train 0.00031568737933412194, valid 0.0002456284419167787
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0002767496043816209
Batch  11  loss:  0.0002937742101494223
Batch  21  loss:  0.0003317692026030272
Batch  31  loss:  0.0002732369175646454
Batch  41  loss:  0.0003501394239719957
Batch  51  loss:  0.0002798921486828476
Batch  61  loss:  0.000216949891182594
Batch  71  loss:  0.00037505768705159426
Batch  81  loss:  0.00023544993018731475
Batch  91  loss:  0.0002483898715581745
Batch  101  loss:  0.0003553055867087096
Batch  111  loss:  0.00023001505178399384
Batch  121  loss:  0.00023857885389588773
Batch  131  loss:  0.00023993551440071315
Batch  141  loss:  0.0003096871078014374
Batch  151  loss:  0.00028626140556298196
Batch  161  loss:  0.0004198110837023705
Batch  171  loss:  0.0003333214845042676
Batch  181  loss:  0.00032378092873841524
Batch  191  loss:  0.0003426658222451806
Validation on real data: 
LOSS supervised-train 0.0003099779943295289, valid 0.00022466402151621878
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0002542845904827118
Batch  11  loss:  0.0003655953914858401
Batch  21  loss:  0.0002781672519631684
Batch  31  loss:  0.00024626366212032735
Batch  41  loss:  0.00037221104139462113
Batch  51  loss:  0.000284092704532668
Batch  61  loss:  0.0003236411139369011
Batch  71  loss:  0.00041590031469240785
Batch  81  loss:  0.0002903538115788251
Batch  91  loss:  0.00028132667648606
Batch  101  loss:  0.00030549376970157027
Batch  111  loss:  0.00028849951922893524
Batch  121  loss:  0.0002837405481841415
Batch  131  loss:  0.0002019614476012066
Batch  141  loss:  0.000319028040394187
Batch  151  loss:  0.0003010144573636353
Batch  161  loss:  0.0003765876463148743
Batch  171  loss:  0.0003222963132429868
Batch  181  loss:  0.00022690396872349083
Batch  191  loss:  0.0003355880035087466
Validation on real data: 
LOSS supervised-train 0.0002945843063935172, valid 0.00026659457944333553
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00026239274302497506
Batch  11  loss:  0.00033694415469653904
Batch  21  loss:  0.00025449029635638
Batch  31  loss:  0.000313118303893134
Batch  41  loss:  0.0003819171979557723
Batch  51  loss:  0.0003205546527169645
Batch  61  loss:  0.00025774110690690577
Batch  71  loss:  0.00032253298559226096
Batch  81  loss:  0.00023672917450312525
Batch  91  loss:  0.00035327981458976865
Batch  101  loss:  0.0003533413982950151
Batch  111  loss:  0.00030283143860287964
Batch  121  loss:  0.0002933085197582841
Batch  131  loss:  0.00022259024262893945
Batch  141  loss:  0.0003821220889221877
Batch  151  loss:  0.0002605625777505338
Batch  161  loss:  0.000364655745215714
Batch  171  loss:  0.0002831253223121166
Batch  181  loss:  0.0002382289821980521
Batch  191  loss:  0.0003023715107701719
Validation on real data: 
LOSS supervised-train 0.00029840566297934857, valid 0.00023363754735328257
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0002468319144099951
Batch  11  loss:  0.00029092442127875984
Batch  21  loss:  0.00031424767803400755
Batch  31  loss:  0.00020618825510609895
Batch  41  loss:  0.00029001731309108436
Batch  51  loss:  0.000270490680122748
Batch  61  loss:  0.0002457929076626897
Batch  71  loss:  0.00040588455158285797
Batch  81  loss:  0.0002559653948992491
Batch  91  loss:  0.000247468298766762
Batch  101  loss:  0.0002694667491596192
Batch  111  loss:  0.00038727273931726813
Batch  121  loss:  0.0002655154385138303
Batch  131  loss:  0.00025191259919665754
Batch  141  loss:  0.0002452446788083762
Batch  151  loss:  0.0002496307424735278
Batch  161  loss:  0.000359178549842909
Batch  171  loss:  0.0002979134733323008
Batch  181  loss:  0.00034560999483801425
Batch  191  loss:  0.00028650189051404595
Validation on real data: 
LOSS supervised-train 0.00029537386093579697, valid 0.00025884495698846877
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00031622848473489285
Batch  11  loss:  0.0002974474336951971
Batch  21  loss:  0.0003061690367758274
Batch  31  loss:  0.00021317292703315616
Batch  41  loss:  0.00031655473867431283
Batch  51  loss:  0.00030285364482551813
Batch  61  loss:  0.000254698796197772
Batch  71  loss:  0.00029612649814225733
Batch  81  loss:  0.0002443096600472927
Batch  91  loss:  0.0002846465213224292
Batch  101  loss:  0.0002582230663392693
Batch  111  loss:  0.00027960364241153
Batch  121  loss:  0.00023139541735872626
Batch  131  loss:  0.00022242029081098735
Batch  141  loss:  0.0002656622673384845
Batch  151  loss:  0.00027447674074210227
Batch  161  loss:  0.00032758808811195195
Batch  171  loss:  0.00024703462258912623
Batch  181  loss:  0.00026466583949513733
Batch  191  loss:  0.00031641876557841897
Validation on real data: 
LOSS supervised-train 0.0002807501470670104, valid 0.00018566142534837127
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0003334297798573971
Batch  11  loss:  0.00030889129266142845
Batch  21  loss:  0.000283944042166695
Batch  31  loss:  0.0002429694141028449
Batch  41  loss:  0.00029398317565210164
Batch  51  loss:  0.0002451307082083076
Batch  61  loss:  0.00022159579384606332
Batch  71  loss:  0.000433160865213722
Batch  81  loss:  0.0002714783186092973
Batch  91  loss:  0.0002706095692701638
Batch  101  loss:  0.0002606842608656734
Batch  111  loss:  0.0003147764364257455
Batch  121  loss:  0.00019104179227724671
Batch  131  loss:  0.0002447492443025112
Batch  141  loss:  0.0003422069421503693
Batch  151  loss:  0.0003762498090509325
Batch  161  loss:  0.0004535402113106102
Batch  171  loss:  0.0003090813697781414
Batch  181  loss:  0.00025935409939847887
Batch  191  loss:  0.0002932031056843698
Validation on real data: 
LOSS supervised-train 0.0002824240976769943, valid 0.0001498887431807816
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0002751549181994051
Batch  11  loss:  0.0002817217609845102
Batch  21  loss:  0.0002941297716461122
Batch  31  loss:  0.00021553209808189422
Batch  41  loss:  0.0002879478270187974
Batch  51  loss:  0.0004225323209539056
Batch  61  loss:  0.0002647529763635248
Batch  71  loss:  0.00032807569368742406
Batch  81  loss:  0.00022976752370595932
Batch  91  loss:  0.00026217979029752314
Batch  101  loss:  0.0003114458522759378
Batch  111  loss:  0.000246013660216704
Batch  121  loss:  0.00021455991372931749
Batch  131  loss:  0.0001920094364322722
Batch  141  loss:  0.0002483199932612479
Batch  151  loss:  0.00021469849161803722
Batch  161  loss:  0.0003899786970578134
Batch  171  loss:  0.00028572563314810395
Batch  181  loss:  0.00026354737929068506
Batch  191  loss:  0.00031451135873794556
Validation on real data: 
LOSS supervised-train 0.0002740922974044224, valid 0.00023209801292978227
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.00024371276958845556
Batch  11  loss:  0.00024275448231492192
Batch  21  loss:  0.0003564871149137616
Batch  31  loss:  0.00023897438950370997
Batch  41  loss:  0.0003576298477128148
Batch  51  loss:  0.0002915655495598912
Batch  61  loss:  0.00024088552163448185
Batch  71  loss:  0.00034560333006083965
Batch  81  loss:  0.00020104653958696872
Batch  91  loss:  0.0002811601443681866
Batch  101  loss:  0.00025590654695406556
Batch  111  loss:  0.0002798678760882467
Batch  121  loss:  0.0002576352853793651
Batch  131  loss:  0.000213993524084799
Batch  141  loss:  0.00025394809199497104
Batch  151  loss:  0.00024550684611313045
Batch  161  loss:  0.00037274835631251335
Batch  171  loss:  0.0002851681492757052
Batch  181  loss:  0.0002716183371376246
Batch  191  loss:  0.00030816587968729436
Validation on real data: 
LOSS supervised-train 0.0002804187632864341, valid 0.0001897932670544833
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0002153566456399858
Batch  11  loss:  0.00022298606927506626
Batch  21  loss:  0.0002680245670489967
Batch  31  loss:  0.00020070734899491072
Batch  41  loss:  0.0002985769824590534
Batch  51  loss:  0.00025074693257920444
Batch  61  loss:  0.0002210517559433356
Batch  71  loss:  0.0005655111162923276
Batch  81  loss:  0.00021334354823920876
Batch  91  loss:  0.0002833484031725675
Batch  101  loss:  0.00028610488516278565
Batch  111  loss:  0.00026963339769281447
Batch  121  loss:  0.00020664877956733108
Batch  131  loss:  0.00019767688354477286
Batch  141  loss:  0.0003101941547356546
Batch  151  loss:  0.0002529476478230208
Batch  161  loss:  0.0004372646799311042
Batch  171  loss:  0.00029158181860111654
Batch  181  loss:  0.0003179563209414482
Batch  191  loss:  0.000349639100022614
Validation on real data: 
LOSS supervised-train 0.0002810559136560187, valid 0.00028041432960890234
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.00024163063790183514
Batch  11  loss:  0.0002704530779737979
Batch  21  loss:  0.0003082456241827458
Batch  31  loss:  0.00020929481252096593
Batch  41  loss:  0.0002456147049088031
Batch  51  loss:  0.0003408831253182143
Batch  61  loss:  0.00025283990544267
Batch  71  loss:  0.00036831709439866245
Batch  81  loss:  0.0001908444392029196
Batch  91  loss:  0.00029511377215385437
Batch  101  loss:  0.00023072768817655742
Batch  111  loss:  0.0003336224181111902
Batch  121  loss:  0.00018979176820721477
Batch  131  loss:  0.00019149873696733266
Batch  141  loss:  0.00023343368957284838
Batch  151  loss:  0.0002026151050813496
Batch  161  loss:  0.00036217676824890077
Batch  171  loss:  0.00033106666523963213
Batch  181  loss:  0.0003130773256998509
Batch  191  loss:  0.0002727190440054983
Validation on real data: 
LOSS supervised-train 0.0002636311347305309, valid 0.0001933681487571448
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00024260522332042456
Batch  11  loss:  0.0003001120057888329
Batch  21  loss:  0.00024106327327899635
Batch  31  loss:  0.0002745609963312745
Batch  41  loss:  0.00031001027673482895
Batch  51  loss:  0.00031419648439623415
Batch  61  loss:  0.00021861415007151663
Batch  71  loss:  0.00029648345662280917
Batch  81  loss:  0.0002198124275309965
Batch  91  loss:  0.000211450838833116
Batch  101  loss:  0.00031851252424530685
Batch  111  loss:  0.00028327092877589166
Batch  121  loss:  0.00020784921071026474
Batch  131  loss:  0.00019000025349669158
Batch  141  loss:  0.000310599512886256
Batch  151  loss:  0.00020080027752555907
Batch  161  loss:  0.0004141813551541418
Batch  171  loss:  0.00020869492436759174
Batch  181  loss:  0.00022536284814123064
Batch  191  loss:  0.0002364392566960305
Validation on real data: 
LOSS supervised-train 0.000258977661433164, valid 0.00017910075257532299
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0002333709562662989
Batch  11  loss:  0.0002872426121030003
Batch  21  loss:  0.0002609683433547616
Batch  31  loss:  0.00021564069902524352
Batch  41  loss:  0.00029767004889436066
Batch  51  loss:  0.0003287805593572557
Batch  61  loss:  0.00025740149430930614
Batch  71  loss:  0.00028239956009201705
Batch  81  loss:  0.00019344274187460542
Batch  91  loss:  0.00027453264920040965
Batch  101  loss:  0.00025219537201337516
Batch  111  loss:  0.00024483303423039615
Batch  121  loss:  0.00017687595391180366
Batch  131  loss:  0.00021902214211877435
Batch  141  loss:  0.0002462741394992918
Batch  151  loss:  0.0002773441665340215
Batch  161  loss:  0.00037392129888758063
Batch  171  loss:  0.0002399146178504452
Batch  181  loss:  0.00026579180848784745
Batch  191  loss:  0.0002631597453728318
Validation on real data: 
LOSS supervised-train 0.00025864798793918454, valid 0.00020461733220145106
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00033704660017974675
Batch  11  loss:  0.0002813526662066579
Batch  21  loss:  0.00023443553072866052
Batch  31  loss:  0.00023221607261803
Batch  41  loss:  0.00034082759520970285
Batch  51  loss:  0.0002489444159436971
Batch  61  loss:  0.00024253266747109592
Batch  71  loss:  0.00022619280207436532
Batch  81  loss:  0.00024263642262667418
Batch  91  loss:  0.0002624769986141473
Batch  101  loss:  0.00018060868023894727
Batch  111  loss:  0.0002855125057976693
Batch  121  loss:  0.00021906552137807012
Batch  131  loss:  0.0002994595270138234
Batch  141  loss:  0.00028910377295687795
Batch  151  loss:  0.00027502808370627463
Batch  161  loss:  0.00039926788304001093
Batch  171  loss:  0.00022268421889748424
Batch  181  loss:  0.00020517576194833964
Batch  191  loss:  0.00023945941939018667
Validation on real data: 
LOSS supervised-train 0.00025362727988976987, valid 0.00015179695037659258
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0002642765757627785
Batch  11  loss:  0.0002349660062463954
Batch  21  loss:  0.00025920014013536274
Batch  31  loss:  0.0001767004287103191
Batch  41  loss:  0.0002754893503151834
Batch  51  loss:  0.00025795638794079423
Batch  61  loss:  0.00019559571228455752
Batch  71  loss:  0.00031587586272507906
Batch  81  loss:  0.0002278833999298513
Batch  91  loss:  0.00021419842960312963
Batch  101  loss:  0.000271064811386168
Batch  111  loss:  0.00022385215561371297
Batch  121  loss:  0.00021641618513967842
Batch  131  loss:  0.0002603993925731629
Batch  141  loss:  0.00032151356572285295
Batch  151  loss:  0.0002262909256387502
Batch  161  loss:  0.00036954216193407774
Batch  171  loss:  0.00022522002109326422
Batch  181  loss:  0.0002703030768316239
Batch  191  loss:  0.0002593577082734555
Validation on real data: 
LOSS supervised-train 0.0002561845760646975, valid 0.00020772364223375916
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00020251089881639928
Batch  11  loss:  0.0002383932442171499
Batch  21  loss:  0.00023207943013403565
Batch  31  loss:  0.0002533562364988029
Batch  41  loss:  0.00030532199889421463
Batch  51  loss:  0.0002963772276416421
Batch  61  loss:  0.00021912826923653483
Batch  71  loss:  0.0002630615490488708
Batch  81  loss:  0.00019007037917617708
Batch  91  loss:  0.00020841366495005786
Batch  101  loss:  0.00023166925529949367
Batch  111  loss:  0.00023052399046719074
Batch  121  loss:  0.0002267360978294164
Batch  131  loss:  0.00019465069635771215
Batch  141  loss:  0.00023963917919900268
Batch  151  loss:  0.00021101369929965585
Batch  161  loss:  0.00041074975160881877
Batch  171  loss:  0.0002245414798380807
Batch  181  loss:  0.00021403710707090795
Batch  191  loss:  0.00024347136786673218
Validation on real data: 
LOSS supervised-train 0.0002470536174951121, valid 0.0001853416906669736
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0002249224198749289
Batch  11  loss:  0.0002143922756658867
Batch  21  loss:  0.00033200689358636737
Batch  31  loss:  0.00021254450257401913
Batch  41  loss:  0.000284887122688815
Batch  51  loss:  0.00021324175759218633
Batch  61  loss:  0.00018363185517955571
Batch  71  loss:  0.00028173241298645735
Batch  81  loss:  0.00017029093578457832
Batch  91  loss:  0.00025097597972489893
Batch  101  loss:  0.00023373536532744765
Batch  111  loss:  0.00027278062771074474
Batch  121  loss:  0.00021266286785248667
Batch  131  loss:  0.00022969496785663068
Batch  141  loss:  0.00024569465313106775
Batch  151  loss:  0.00023893597244750708
Batch  161  loss:  0.00032646104227751493
Batch  171  loss:  0.0003015125112142414
Batch  181  loss:  0.0002581002772785723
Batch  191  loss:  0.0002002368273679167
Validation on real data: 
LOSS supervised-train 0.0002459110756899463, valid 0.00021777235087938607
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00022635844652540982
Batch  11  loss:  0.00020551783381961286
Batch  21  loss:  0.00027835959917865694
Batch  31  loss:  0.00019219408568460494
Batch  41  loss:  0.00027459958801046014
Batch  51  loss:  0.000315683166263625
Batch  61  loss:  0.0002135865215677768
Batch  71  loss:  0.00029977509984746575
Batch  81  loss:  0.00017115939408540726
Batch  91  loss:  0.00025453936541453004
Batch  101  loss:  0.0002095117379212752
Batch  111  loss:  0.00023674845579080284
Batch  121  loss:  0.00019475381122902036
Batch  131  loss:  0.00019062503997702152
Batch  141  loss:  0.00022177844948600978
Batch  151  loss:  0.00026311466353945434
Batch  161  loss:  0.0003641560615506023
Batch  171  loss:  0.0002406538260402158
Batch  181  loss:  0.00019148678984493017
Batch  191  loss:  0.000262461748206988
Validation on real data: 
LOSS supervised-train 0.00024349064122361597, valid 0.0001810471439966932
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.00024200051848310977
Batch  11  loss:  0.00022132776211947203
Batch  21  loss:  0.00023074023192748427
Batch  31  loss:  0.00018388278840575367
Batch  41  loss:  0.00023877735657151788
Batch  51  loss:  0.00026490484015084803
Batch  61  loss:  0.00018354639178141952
Batch  71  loss:  0.00033708199043758214
Batch  81  loss:  0.00017806596588343382
Batch  91  loss:  0.00019028376846108586
Batch  101  loss:  0.00022169937437865883
Batch  111  loss:  0.00024015811504796147
Batch  121  loss:  0.00017883165855892003
Batch  131  loss:  0.0002252362173749134
Batch  141  loss:  0.00021947029745206237
Batch  151  loss:  0.00023740713368169963
Batch  161  loss:  0.0002903375425375998
Batch  171  loss:  0.00021565872884821147
Batch  181  loss:  0.00020770556875504553
Batch  191  loss:  0.00019407553190831095
Validation on real data: 
LOSS supervised-train 0.00023657900601392612, valid 0.00017604208551347256
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00022439270105678588
Batch  11  loss:  0.00021560772438533604
Batch  21  loss:  0.0002477134403306991
Batch  31  loss:  0.00018635246669873595
Batch  41  loss:  0.00024092088278848678
Batch  51  loss:  0.0002698473690543324
Batch  61  loss:  0.0001987551077036187
Batch  71  loss:  0.0002354500611545518
Batch  81  loss:  0.00017928621673490852
Batch  91  loss:  0.00022118340712040663
Batch  101  loss:  0.00022720701235812157
Batch  111  loss:  0.0003034848195966333
Batch  121  loss:  0.0001794887357391417
Batch  131  loss:  0.0001735104451654479
Batch  141  loss:  0.000254400132689625
Batch  151  loss:  0.00021045439643785357
Batch  161  loss:  0.00034567766124382615
Batch  171  loss:  0.0002608958457130939
Batch  181  loss:  0.0002213704283349216
Batch  191  loss:  0.00022502968204207718
Validation on real data: 
LOSS supervised-train 0.00023422438382112888, valid 0.00017748592654243112
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.00024369321181438863
Batch  11  loss:  0.00023252118262462318
Batch  21  loss:  0.00022235102369450033
Batch  31  loss:  0.0001625040458748117
Batch  41  loss:  0.0003723402332980186
Batch  51  loss:  0.00023554681683890522
Batch  61  loss:  0.00020718161249533296
Batch  71  loss:  0.000275143189355731
Batch  81  loss:  0.00015660497592762113
Batch  91  loss:  0.000242172071011737
Batch  101  loss:  0.00019982953381258994
Batch  111  loss:  0.00026061528478749096
Batch  121  loss:  0.0001574189227540046
Batch  131  loss:  0.0002199411392211914
Batch  141  loss:  0.000269473297521472
Batch  151  loss:  0.00023424654500558972
Batch  161  loss:  0.0003017386479768902
Batch  171  loss:  0.000201446411665529
Batch  181  loss:  0.00018059066496789455
Batch  191  loss:  0.0002403862599749118
Validation on real data: 
LOSS supervised-train 0.0002321565198508324, valid 0.00019580952357500792
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00019156250345986336
Batch  11  loss:  0.0002461381664033979
Batch  21  loss:  0.00024015377857722342
Batch  31  loss:  0.0001970451994566247
Batch  41  loss:  0.000281016604276374
Batch  51  loss:  0.0002726036182139069
Batch  61  loss:  0.00019208974845241755
Batch  71  loss:  0.00034033445990644395
Batch  81  loss:  0.0001732827367959544
Batch  91  loss:  0.00020441081142053008
Batch  101  loss:  0.00022284436272457242
Batch  111  loss:  0.00025242340052500367
Batch  121  loss:  0.00015277412603609264
Batch  131  loss:  0.0001852940913522616
Batch  141  loss:  0.00021432162611745298
Batch  151  loss:  0.00022243100102059543
Batch  161  loss:  0.00026615403476171196
Batch  171  loss:  0.00020925547869410366
Batch  181  loss:  0.00020187345216982067
Batch  191  loss:  0.0002352332667214796
Validation on real data: 
LOSS supervised-train 0.00022731027696863747, valid 0.00020336330635473132
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0002069677721010521
Batch  11  loss:  0.00024311155721079558
Batch  21  loss:  0.00024307958665303886
Batch  31  loss:  0.0002016105136135593
Batch  41  loss:  0.00022351802908815444
Batch  51  loss:  0.0002541876456234604
Batch  61  loss:  0.00017621389997657388
Batch  71  loss:  0.0003484015178401023
Batch  81  loss:  0.00018183092470280826
Batch  91  loss:  0.00023960402177181095
Batch  101  loss:  0.000202163151698187
Batch  111  loss:  0.0002610422670841217
Batch  121  loss:  0.00018809719767887145
Batch  131  loss:  0.0001693029043963179
Batch  141  loss:  0.00019880633044522256
Batch  151  loss:  0.00019178363436367363
Batch  161  loss:  0.0004430831759236753
Batch  171  loss:  0.00019703958241734654
Batch  181  loss:  0.00019763325690291822
Batch  191  loss:  0.00023229769431054592
Validation on real data: 
LOSS supervised-train 0.00022532771894475444, valid 0.000145276659168303
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0002223691699327901
Batch  11  loss:  0.0002179372968384996
Batch  21  loss:  0.00021188202663324773
Batch  31  loss:  0.00023318908642977476
Batch  41  loss:  0.0003328149323351681
Batch  51  loss:  0.00027228519320487976
Batch  61  loss:  0.00016591089661233127
Batch  71  loss:  0.0002676580043043941
Batch  81  loss:  0.0002011007454711944
Batch  91  loss:  0.00021976098651066422
Batch  101  loss:  0.00023138888354878873
Batch  111  loss:  0.00024016242241486907
Batch  121  loss:  0.00014926250150892884
Batch  131  loss:  0.00020710307580884546
Batch  141  loss:  0.00018928313511423767
Batch  151  loss:  0.00020153641526121646
Batch  161  loss:  0.0003046864876523614
Batch  171  loss:  0.000214507948840037
Batch  181  loss:  0.000244075374212116
Batch  191  loss:  0.000191442682989873
Validation on real data: 
LOSS supervised-train 0.0002352880424587056, valid 0.0002181546442443505
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.00019633247575256974
Batch  11  loss:  0.00022583491227123886
Batch  21  loss:  0.00024983054026961327
Batch  31  loss:  0.0001534813636681065
Batch  41  loss:  0.00021574673883151263
Batch  51  loss:  0.00029934971826151013
Batch  61  loss:  0.00016981463704723865
Batch  71  loss:  0.00018876747344620526
Batch  81  loss:  0.00022966682445257902
Batch  91  loss:  0.0002685787039808929
Batch  101  loss:  0.00015122961485758424
Batch  111  loss:  0.0002943480503745377
Batch  121  loss:  0.00017839872452896088
Batch  131  loss:  0.0001685530151007697
Batch  141  loss:  0.00023594980302732438
Batch  151  loss:  0.00023765403602737933
Batch  161  loss:  0.0002210081001976505
Batch  171  loss:  0.00019702815916389227
Batch  181  loss:  0.0002157655544579029
Batch  191  loss:  0.0002262859052279964
Validation on real data: 
LOSS supervised-train 0.00022175419802806574, valid 0.00016141217201948166
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0002544006856624037
Batch  11  loss:  0.00025716706295497715
Batch  21  loss:  0.00019907376554328948
Batch  31  loss:  0.00017029054288286716
Batch  41  loss:  0.00027283254894427955
Batch  51  loss:  0.00028595185722224414
Batch  61  loss:  0.0001792579423636198
Batch  71  loss:  0.00025435956194996834
Batch  81  loss:  0.00020295288413763046
Batch  91  loss:  0.0002891409385483712
Batch  101  loss:  0.0002574753016233444
Batch  111  loss:  0.00023012931342236698
Batch  121  loss:  0.0001855242735473439
Batch  131  loss:  0.00020978289830964059
Batch  141  loss:  0.0002749745617620647
Batch  151  loss:  0.00020563432190101594
Batch  161  loss:  0.00029211133369244635
Batch  171  loss:  0.00021502788877114654
Batch  181  loss:  0.00018078210996463895
Batch  191  loss:  0.0001552206085762009
Validation on real data: 
LOSS supervised-train 0.0002279601903501316, valid 0.00021411350462585688
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.00018662482034415007
Batch  11  loss:  0.00022546177206095308
Batch  21  loss:  0.00021553298574872315
Batch  31  loss:  0.00014995239325799048
Batch  41  loss:  0.00028048641979694366
Batch  51  loss:  0.0002602259337436408
Batch  61  loss:  0.00015463776071555912
Batch  71  loss:  0.0002816215856000781
Batch  81  loss:  0.0002002419059863314
Batch  91  loss:  0.00022440910106524825
Batch  101  loss:  0.0002006288559641689
Batch  111  loss:  0.0002237655862700194
Batch  121  loss:  0.00014825629477854818
Batch  131  loss:  0.00019667444576043636
Batch  141  loss:  0.00023519010574091226
Batch  151  loss:  0.00017275170830544084
Batch  161  loss:  0.0002661811013240367
Batch  171  loss:  0.00022753777739126235
Batch  181  loss:  0.00022314040688797832
Batch  191  loss:  0.00018758066289592534
Validation on real data: 
LOSS supervised-train 0.00021286802031681874, valid 0.00020611914806067944
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00022138975327834487
Batch  11  loss:  0.00023676913406234235
Batch  21  loss:  0.0002266620722366497
Batch  31  loss:  0.00016498408513143659
Batch  41  loss:  0.00021036430553067476
Batch  51  loss:  0.0002319273626198992
Batch  61  loss:  0.00019962413352914155
Batch  71  loss:  0.0002986870240420103
Batch  81  loss:  0.00020455459889490157
Batch  91  loss:  0.00020102865528315306
Batch  101  loss:  0.0001983718539122492
Batch  111  loss:  0.0002015988720813766
Batch  121  loss:  0.00018030821229331195
Batch  131  loss:  0.00015380626427941024
Batch  141  loss:  0.00019723695004358888
Batch  151  loss:  0.00021083081082906574
Batch  161  loss:  0.0003607088583521545
Batch  171  loss:  0.00022855508723296225
Batch  181  loss:  0.00016548500570934266
Batch  191  loss:  0.0001630173937883228
Validation on real data: 
LOSS supervised-train 0.00021513318879442522, valid 0.00017892652249429375
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0002419474330963567
Batch  11  loss:  0.00020176786347292364
Batch  21  loss:  0.00024383481650147587
Batch  31  loss:  0.0002001978427870199
Batch  41  loss:  0.00023707117361482233
Batch  51  loss:  0.00019941726350225508
Batch  61  loss:  0.00019085104577243328
Batch  71  loss:  0.00034819840220734477
Batch  81  loss:  0.00017769435362424701
Batch  91  loss:  0.00024278531782329082
Batch  101  loss:  0.00018728261056821793
Batch  111  loss:  0.0003045676276087761
Batch  121  loss:  0.00015949824592098594
Batch  131  loss:  0.0001673870865488425
Batch  141  loss:  0.0002518021210562438
Batch  151  loss:  0.00021690373250748962
Batch  161  loss:  0.0003264699480496347
Batch  171  loss:  0.00019254609651397914
Batch  181  loss:  0.0001743180473567918
Batch  191  loss:  0.00023597206745762378
Validation on real data: 
LOSS supervised-train 0.00021522867984458572, valid 0.0001430793054169044
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00017808769189286977
Batch  11  loss:  0.00018722849199548364
Batch  21  loss:  0.00020866563136223704
Batch  31  loss:  0.0001955430197995156
Batch  41  loss:  0.00028922135243192315
Batch  51  loss:  0.0002599506115075201
Batch  61  loss:  0.00015451638319063932
Batch  71  loss:  0.0003003951860591769
Batch  81  loss:  0.00017478878726251423
Batch  91  loss:  0.0001918969937833026
Batch  101  loss:  0.00020368410332594067
Batch  111  loss:  0.00024514104006811976
Batch  121  loss:  0.00019026498193852603
Batch  131  loss:  0.00018791983893606812
Batch  141  loss:  0.00017678036238066852
Batch  151  loss:  0.0002164089382858947
Batch  161  loss:  0.0003184250963386148
Batch  171  loss:  0.00023921247338876128
Batch  181  loss:  0.00024079220020212233
Batch  191  loss:  0.0002375732728978619
Validation on real data: 
LOSS supervised-train 0.00021497471090697217, valid 0.00022713822545483708
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0001832197594922036
Batch  11  loss:  0.00020681225578300655
Batch  21  loss:  0.0002357504708925262
Batch  31  loss:  0.00015906538465060294
Batch  41  loss:  0.00022749797790311277
Batch  51  loss:  0.00018155667930841446
Batch  61  loss:  0.00014360265049617738
Batch  71  loss:  0.00029891813755966723
Batch  81  loss:  0.0001886138488771394
Batch  91  loss:  0.0002288153482368216
Batch  101  loss:  0.00020846979168709368
Batch  111  loss:  0.00025188602739945054
Batch  121  loss:  0.00014642778842244297
Batch  131  loss:  0.000160044408403337
Batch  141  loss:  0.0002477720263414085
Batch  151  loss:  0.0002252004196634516
Batch  161  loss:  0.0003192790609318763
Batch  171  loss:  0.0002229052479378879
Batch  181  loss:  0.00015373167116194963
Batch  191  loss:  0.0002050343609880656
Validation on real data: 
LOSS supervised-train 0.00020829944889555918, valid 0.00015984204947017133
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00017762846255209297
Batch  11  loss:  0.00016881148621905595
Batch  21  loss:  0.00022886450460646302
Batch  31  loss:  0.00018565957725513726
Batch  41  loss:  0.0002504696021787822
Batch  51  loss:  0.00022847346554044634
Batch  61  loss:  0.00019009150855708867
Batch  71  loss:  0.0002662678307387978
Batch  81  loss:  0.00012517027789726853
Batch  91  loss:  0.00020067310833837837
Batch  101  loss:  0.0002039894461631775
Batch  111  loss:  0.00023958689416758716
Batch  121  loss:  0.00019996795163024217
Batch  131  loss:  0.00019021573825739324
Batch  141  loss:  0.0002002142573473975
Batch  151  loss:  0.0002040980471065268
Batch  161  loss:  0.0002153225796064362
Batch  171  loss:  0.00021517346613109112
Batch  181  loss:  0.00017316306184511632
Batch  191  loss:  0.0002264331269543618
Validation on real data: 
LOSS supervised-train 0.00020412529764143983, valid 0.00016168122238013893
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00017810369899962097
Batch  11  loss:  0.00019596240599639714
Batch  21  loss:  0.00018752175674308091
Batch  31  loss:  0.00016908827819861472
Batch  41  loss:  0.00022053159773349762
Batch  51  loss:  0.00022382117458619177
Batch  61  loss:  0.00021109034423716366
Batch  71  loss:  0.00023176916874945164
Batch  81  loss:  0.00015086900384631008
Batch  91  loss:  0.00022383153554983437
Batch  101  loss:  0.00019609123410191387
Batch  111  loss:  0.00020900607341900468
Batch  121  loss:  0.00016341570881195366
Batch  131  loss:  0.00017000577645376325
Batch  141  loss:  0.00026362965581938624
Batch  151  loss:  0.0002064147120108828
Batch  161  loss:  0.00027285126270726323
Batch  171  loss:  0.00017644990293774754
Batch  181  loss:  0.00019103636441286653
Batch  191  loss:  0.0002237518783658743
Validation on real data: 
LOSS supervised-train 0.00020074481719348114, valid 0.0001767195644788444
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0001780172751750797
Batch  11  loss:  0.00022139697102829814
Batch  21  loss:  0.00020123914873693138
Batch  31  loss:  0.00013068894622847438
Batch  41  loss:  0.00023401464568451047
Batch  51  loss:  0.00023048593720886856
Batch  61  loss:  0.0001389655953971669
Batch  71  loss:  0.00023619073908776045
Batch  81  loss:  0.00016692343342583627
Batch  91  loss:  0.00017815663886722177
Batch  101  loss:  0.00022581167286261916
Batch  111  loss:  0.00019405942293815315
Batch  121  loss:  0.00019388581858947873
Batch  131  loss:  0.00015867702313698828
Batch  141  loss:  0.00016625045100226998
Batch  151  loss:  0.00018900785653386265
Batch  161  loss:  0.0003545748768374324
Batch  171  loss:  0.00018409962649457157
Batch  181  loss:  0.00017000935622490942
Batch  191  loss:  0.00016635007341392338
Validation on real data: 
LOSS supervised-train 0.00020188051035802346, valid 0.00019553699530661106
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00014336887397803366
Batch  11  loss:  0.0002466632577124983
Batch  21  loss:  0.00023193404194898903
Batch  31  loss:  0.00017107486200984567
Batch  41  loss:  0.00023189095372799784
Batch  51  loss:  0.0002030565228778869
Batch  61  loss:  0.00017256663704756647
Batch  71  loss:  0.00024790922179818153
Batch  81  loss:  0.00017390210996381938
Batch  91  loss:  0.00017971514898817986
Batch  101  loss:  0.0001595368521520868
Batch  111  loss:  0.00017784289957489818
Batch  121  loss:  0.00015141691255848855
Batch  131  loss:  0.00015875950339250267
Batch  141  loss:  0.00016257040260825306
Batch  151  loss:  0.00024259429483208805
Batch  161  loss:  0.00025861486210487783
Batch  171  loss:  0.00015528275980614126
Batch  181  loss:  0.00017475795175414532
Batch  191  loss:  0.00022255546355154365
Validation on real data: 
LOSS supervised-train 0.00020133308360527734, valid 0.00014251771790441126
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.00019308326591271907
Batch  11  loss:  0.00020707387011498213
Batch  21  loss:  0.0002293204888701439
Batch  31  loss:  0.00014676620776299387
Batch  41  loss:  0.00023144467559177428
Batch  51  loss:  0.0002750797139015049
Batch  61  loss:  0.00016888862592168152
Batch  71  loss:  0.00020961396512575448
Batch  81  loss:  0.0001831529225455597
Batch  91  loss:  0.00023774562578182667
Batch  101  loss:  0.00018536548304837197
Batch  111  loss:  0.00019779869762714952
Batch  121  loss:  0.00020828632113989443
Batch  131  loss:  0.0002401133970124647
Batch  141  loss:  0.00022793987591285259
Batch  151  loss:  0.00014203436148818582
Batch  161  loss:  0.00027741160010918975
Batch  171  loss:  0.00021746229322161525
Batch  181  loss:  0.0001385843352181837
Batch  191  loss:  0.0001879756891867146
Validation on real data: 
LOSS supervised-train 0.00020047900874487824, valid 0.0001661446876823902
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00019920106569770724
Batch  11  loss:  0.00020660943118855357
Batch  21  loss:  0.00020040302479173988
Batch  31  loss:  0.00018965666822623461
Batch  41  loss:  0.000237568499869667
Batch  51  loss:  0.00020312696869950742
Batch  61  loss:  0.00016204628627747297
Batch  71  loss:  0.000286899390630424
Batch  81  loss:  0.00017017799837049097
Batch  91  loss:  0.00019413826521486044
Batch  101  loss:  0.00016975565813481808
Batch  111  loss:  0.00019615102792158723
Batch  121  loss:  0.00015358060772996396
Batch  131  loss:  0.00017712832777760923
Batch  141  loss:  0.0001530502486275509
Batch  151  loss:  0.0002220749738626182
Batch  161  loss:  0.00029073396581225097
Batch  171  loss:  0.00019225012511014938
Batch  181  loss:  0.00019874345161952078
Batch  191  loss:  0.0001975171617232263
Validation on real data: 
LOSS supervised-train 0.00019978244952653768, valid 0.00014913271297700703
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00016530616267118603
Batch  11  loss:  0.00015770256868563592
Batch  21  loss:  0.0002154092799173668
Batch  31  loss:  0.00017977668903768063
Batch  41  loss:  0.00021011862554587424
Batch  51  loss:  0.000204353520530276
Batch  61  loss:  0.00015608075773343444
Batch  71  loss:  0.00023637679987587035
Batch  81  loss:  0.00016959744971245527
Batch  91  loss:  0.00019223851268179715
Batch  101  loss:  0.00017461508105043322
Batch  111  loss:  0.00019411786342971027
Batch  121  loss:  0.0001474700402468443
Batch  131  loss:  0.0001774164120433852
Batch  141  loss:  0.0001896580506581813
Batch  151  loss:  0.0002517380635254085
Batch  161  loss:  0.0003218787023797631
Batch  171  loss:  0.0001874373119790107
Batch  181  loss:  0.0001755883131409064
Batch  191  loss:  0.00017512119666207582
Validation on real data: 
LOSS supervised-train 0.0001966704093501903, valid 0.00021461615688167512
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.00021116225980222225
Batch  11  loss:  0.00022642673866357654
Batch  21  loss:  0.0002300220658071339
Batch  31  loss:  0.0001795120188035071
Batch  41  loss:  0.0002686771040316671
Batch  51  loss:  0.00025983035448007286
Batch  61  loss:  0.00016713263175915927
Batch  71  loss:  0.00027284881798550487
Batch  81  loss:  0.00018650639685802162
Batch  91  loss:  0.0001876643655123189
Batch  101  loss:  0.0001929526770254597
Batch  111  loss:  0.00020691972167696804
Batch  121  loss:  0.00011835317127406597
Batch  131  loss:  0.00014000294322613627
Batch  141  loss:  0.00021519872825592756
Batch  151  loss:  0.00018844932492356747
Batch  161  loss:  0.00022271591296885163
Batch  171  loss:  0.0001643877476453781
Batch  181  loss:  0.00015056406846269965
Batch  191  loss:  0.00019232876366004348
Validation on real data: 
LOSS supervised-train 0.0001948425495720585, valid 0.00020389322889968753
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00018153426935896277
Batch  11  loss:  0.00017904577543959022
Batch  21  loss:  0.0001981178647838533
Batch  31  loss:  0.00018286712293047458
Batch  41  loss:  0.00026815177989192307
Batch  51  loss:  0.00022369010548572987
Batch  61  loss:  0.00016422744374722242
Batch  71  loss:  0.00022450776305049658
Batch  81  loss:  0.0001596597139723599
Batch  91  loss:  0.00023278208391275257
Batch  101  loss:  0.00015199976041913033
Batch  111  loss:  0.00020329191465862095
Batch  121  loss:  0.00013577732897829264
Batch  131  loss:  0.0001621712144697085
Batch  141  loss:  0.00019400175369810313
Batch  151  loss:  0.00015153332788031548
Batch  161  loss:  0.0002578322892077267
Batch  171  loss:  0.00017857045168057084
Batch  181  loss:  0.0001697361731203273
Batch  191  loss:  0.00019834232807625085
Validation on real data: 
LOSS supervised-train 0.00019923750543966888, valid 0.00020373094594106078
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00020696262072306126
Batch  11  loss:  0.0001799225137801841
Batch  21  loss:  0.0002136364346370101
Batch  31  loss:  0.00016091964789666235
Batch  41  loss:  0.00018651969730854034
Batch  51  loss:  0.0001635088410694152
Batch  61  loss:  0.00017996298265643418
Batch  71  loss:  0.00026052744942717254
Batch  81  loss:  0.00015488313511013985
Batch  91  loss:  0.0002321680512977764
Batch  101  loss:  0.00018760800594463944
Batch  111  loss:  0.00023245162446983159
Batch  121  loss:  0.00013611502072308213
Batch  131  loss:  0.00017281337932217866
Batch  141  loss:  0.0002313044387847185
Batch  151  loss:  0.00016533811867702752
Batch  161  loss:  0.00021244182426016778
Batch  171  loss:  0.00018704828107729554
Batch  181  loss:  0.0001785959320841357
Batch  191  loss:  0.00021960104641038924
Validation on real data: 
LOSS supervised-train 0.0001972024768110714, valid 0.0001450137933716178
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0001869522238848731
Batch  11  loss:  0.00019060756312683225
Batch  21  loss:  0.0002227873628726229
Batch  31  loss:  0.00019502973009366542
Batch  41  loss:  0.00021537802240345627
Batch  51  loss:  0.0002205057826358825
Batch  61  loss:  0.00016201888502109796
Batch  71  loss:  0.00022972867009229958
Batch  81  loss:  0.00015632048598490655
Batch  91  loss:  0.00018931037629954517
Batch  101  loss:  0.00014222228492144495
Batch  111  loss:  0.0002387273998465389
Batch  121  loss:  0.00013008776295464486
Batch  131  loss:  0.00015834392979741096
Batch  141  loss:  0.00018517966964282095
Batch  151  loss:  0.00019380565208848566
Batch  161  loss:  0.0002753057924564928
Batch  171  loss:  0.00015902132145129144
Batch  181  loss:  0.00019305066962260753
Batch  191  loss:  0.0001643462455831468
Validation on real data: 
LOSS supervised-train 0.0001865840366735938, valid 0.00018547852232586592
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00019054250151384622
Batch  11  loss:  0.0001724468165775761
Batch  21  loss:  0.00018565678328741342
Batch  31  loss:  0.00016144171240739524
Batch  41  loss:  0.0002115447714459151
Batch  51  loss:  0.00026029933360405266
Batch  61  loss:  0.00014811739674769342
Batch  71  loss:  0.0002627527865115553
Batch  81  loss:  0.00012663149391300976
Batch  91  loss:  0.0002376576594542712
Batch  101  loss:  0.00018559324962552637
Batch  111  loss:  0.00022762137814424932
Batch  121  loss:  0.000166014171554707
Batch  131  loss:  0.0002040541876340285
Batch  141  loss:  0.0002019423700403422
Batch  151  loss:  0.0001770772214513272
Batch  161  loss:  0.00022686654119752347
Batch  171  loss:  0.00020593355293385684
Batch  181  loss:  0.00016268483886960894
Batch  191  loss:  0.00015246262773871422
Validation on real data: 
LOSS supervised-train 0.00019365718228073092, valid 0.00018054622341878712
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00019226812582928687
Batch  11  loss:  0.0001940871006809175
Batch  21  loss:  0.00022545385581906885
Batch  31  loss:  0.00013781353482045233
Batch  41  loss:  0.0001977410720428452
Batch  51  loss:  0.00015203029033727944
Batch  61  loss:  0.00015828479081392288
Batch  71  loss:  0.00024168861273210496
Batch  81  loss:  0.0001838937314460054
Batch  91  loss:  0.00023363539366982877
Batch  101  loss:  0.00015014669043011963
Batch  111  loss:  0.00022222170082386583
Batch  121  loss:  0.00014670910604763776
Batch  131  loss:  0.0001806978980312124
Batch  141  loss:  0.00017424598627258092
Batch  151  loss:  0.0001829888642532751
Batch  161  loss:  0.0003049042134080082
Batch  171  loss:  0.00020026876882184297
Batch  181  loss:  0.00020648882491514087
Batch  191  loss:  0.00014460462261922657
Validation on real data: 
LOSS supervised-train 0.00018869505096517968, valid 0.000154890090925619
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00020347426470834762
Batch  11  loss:  0.0001909302081912756
Batch  21  loss:  0.00017923950508702546
Batch  31  loss:  0.00019160259398631752
Batch  41  loss:  0.00019204930867999792
Batch  51  loss:  0.00022255715157371014
Batch  61  loss:  0.00017544942966196686
Batch  71  loss:  0.00019689265172928572
Batch  81  loss:  0.00012661865912377834
Batch  91  loss:  0.00017175254470203072
Batch  101  loss:  0.00018740662198979408
Batch  111  loss:  0.0002232647966593504
Batch  121  loss:  0.00017590615607332438
Batch  131  loss:  0.00017330439004581422
Batch  141  loss:  0.0001900366332847625
Batch  151  loss:  0.00017044070409610868
Batch  161  loss:  0.0002741717908065766
Batch  171  loss:  0.00017555357771925628
Batch  181  loss:  0.00015627733955625445
Batch  191  loss:  0.00016543682431802154
Validation on real data: 
LOSS supervised-train 0.00018389449705864537, valid 0.00016661710105836391
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00016915738524403423
Batch  11  loss:  0.00019088310364168137
Batch  21  loss:  0.00019177276408299804
Batch  31  loss:  0.00016191489703487605
Batch  41  loss:  0.00024943039170466363
Batch  51  loss:  0.00017966917948797345
Batch  61  loss:  0.00015649321721866727
Batch  71  loss:  0.0002182733005611226
Batch  81  loss:  0.00018329489103052765
Batch  91  loss:  0.0001848678948590532
Batch  101  loss:  0.0001694850216154009
Batch  111  loss:  0.00019293806690257043
Batch  121  loss:  0.00017076227231882513
Batch  131  loss:  0.00018706012633629143
Batch  141  loss:  0.00020557775860652328
Batch  151  loss:  0.00019874646386597306
Batch  161  loss:  0.00029577017994597554
Batch  171  loss:  0.00018267858831677586
Batch  181  loss:  0.00019395520212128758
Batch  191  loss:  0.0001795342832338065
Validation on real data: 
LOSS supervised-train 0.00018676506926567526, valid 0.0001462369691580534
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00018794437346514314
Batch  11  loss:  0.00017582958389539272
Batch  21  loss:  0.00021883699810132384
Batch  31  loss:  0.000154561159433797
Batch  41  loss:  0.00022568005078937858
Batch  51  loss:  0.00017157764523290098
Batch  61  loss:  0.00014478768571279943
Batch  71  loss:  0.00019321638683322817
Batch  81  loss:  0.00016227856394834816
Batch  91  loss:  0.0001900387869682163
Batch  101  loss:  0.00019155465997755527
Batch  111  loss:  0.00022260629339143634
Batch  121  loss:  0.00018396801897324622
Batch  131  loss:  0.00018314880435355008
Batch  141  loss:  0.00018249978893436491
Batch  151  loss:  0.00016460384358651936
Batch  161  loss:  0.00026354799047112465
Batch  171  loss:  0.0001754183613229543
Batch  181  loss:  0.00018737393838819116
Batch  191  loss:  0.00016446290828753263
Validation on real data: 
LOSS supervised-train 0.00018232631278806367, valid 0.0001538900687592104
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00019428454106673598
Batch  11  loss:  0.00021771616593468934
Batch  21  loss:  0.00019763472664635628
Batch  31  loss:  0.00017168988415505737
Batch  41  loss:  0.00019846457871608436
Batch  51  loss:  0.0001655472005950287
Batch  61  loss:  0.00016070868878159672
Batch  71  loss:  0.00023170310305431485
Batch  81  loss:  0.0001588143059052527
Batch  91  loss:  0.00021704906248487532
Batch  101  loss:  0.00020591692009475082
Batch  111  loss:  0.0002146379993064329
Batch  121  loss:  0.00016208893794100732
Batch  131  loss:  0.00014142546569928527
Batch  141  loss:  0.00018902710871770978
Batch  151  loss:  0.0001686096948105842
Batch  161  loss:  0.0002541953872423619
Batch  171  loss:  0.00015150615945458412
Batch  181  loss:  0.00016398105071857572
Batch  191  loss:  0.00020061858231201768
Validation on real data: 
LOSS supervised-train 0.00018298186743777477, valid 0.00014086865121498704
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00018345362332183868
Batch  11  loss:  0.00016922259237617254
Batch  21  loss:  0.00021322796237654984
Batch  31  loss:  0.00014847656711935997
Batch  41  loss:  0.00019489620171952993
Batch  51  loss:  0.0001446276728529483
Batch  61  loss:  0.00013384952035266906
Batch  71  loss:  0.00017891528841573745
Batch  81  loss:  0.00014059354725759476
Batch  91  loss:  0.0001817338925320655
Batch  101  loss:  0.00016207832959480584
Batch  111  loss:  0.00022903237550053746
Batch  121  loss:  0.0001615225919522345
Batch  131  loss:  0.00015840552805457264
Batch  141  loss:  0.0001755213743308559
Batch  151  loss:  0.00018978297885041684
Batch  161  loss:  0.000270931632257998
Batch  171  loss:  0.00015412679931614548
Batch  181  loss:  0.00017418645438738167
Batch  191  loss:  0.00016665445582475513
Validation on real data: 
LOSS supervised-train 0.00018154528097511503, valid 0.00014842426753602922
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00016383736510761082
Batch  11  loss:  0.00018402970454189926
Batch  21  loss:  0.00015424903540406376
Batch  31  loss:  0.00014015616034157574
Batch  41  loss:  0.00022230706235859543
Batch  51  loss:  0.00023309540119953454
Batch  61  loss:  0.00020958026289008558
Batch  71  loss:  0.00022830921807326376
Batch  81  loss:  0.00013544815010391176
Batch  91  loss:  0.00015556188009213656
Batch  101  loss:  0.0001499896461609751
Batch  111  loss:  0.00021924072643741965
Batch  121  loss:  0.0001301542652072385
Batch  131  loss:  0.0001513141905888915
Batch  141  loss:  0.00024827918969094753
Batch  151  loss:  0.00015773491759318858
Batch  161  loss:  0.00028606687556020916
Batch  171  loss:  0.00016505071835126728
Batch  181  loss:  0.00019692190107889473
Batch  191  loss:  0.0001497649063821882
Validation on real data: 
LOSS supervised-train 0.00018028301841695792, valid 0.00016659070388413966
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00017888072761707008
Batch  11  loss:  0.00017648252833168954
Batch  21  loss:  0.0001693920057732612
Batch  31  loss:  0.00018673023441806436
Batch  41  loss:  0.00023100216640159488
Batch  51  loss:  0.0001449184346711263
Batch  61  loss:  0.0001316511770710349
Batch  71  loss:  0.00020803055667784065
Batch  81  loss:  0.00016240811964962631
Batch  91  loss:  0.00016520415374543518
Batch  101  loss:  0.00016635883366689086
Batch  111  loss:  0.00022354185057338327
Batch  121  loss:  0.00020790798589587212
Batch  131  loss:  0.00016154501645360142
Batch  141  loss:  0.0001852236018748954
Batch  151  loss:  0.0001529190776636824
Batch  161  loss:  0.00021542544709518552
Batch  171  loss:  0.00017794762970879674
Batch  181  loss:  0.00013995465997140855
Batch  191  loss:  0.00018573619308881462
Validation on real data: 
LOSS supervised-train 0.00017851813812740146, valid 0.00017823834787122905
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00021465649479068816
Batch  11  loss:  0.00014603738964069635
Batch  21  loss:  0.00019242099369876087
Batch  31  loss:  0.00017860544903669506
Batch  41  loss:  0.0002628703077789396
Batch  51  loss:  0.00018289241415914148
Batch  61  loss:  0.00015059504949022084
Batch  71  loss:  0.0002146052138414234
Batch  81  loss:  0.00013404399214778095
Batch  91  loss:  0.00016965005488600582
Batch  101  loss:  0.0001568817242514342
Batch  111  loss:  0.00022114570310804993
Batch  121  loss:  0.00016441331536043435
Batch  131  loss:  0.00017298950115218759
Batch  141  loss:  0.00018583863857202232
Batch  151  loss:  0.00016747531481087208
Batch  161  loss:  0.0002720564662013203
Batch  171  loss:  0.00014604305033572018
Batch  181  loss:  0.00019914719450753182
Batch  191  loss:  0.0001466858375351876
Validation on real data: 
LOSS supervised-train 0.0001798919343855232, valid 0.00015767684089951217
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0001504807296441868
Batch  11  loss:  0.0001868999097496271
Batch  21  loss:  0.00016916278400458395
Batch  31  loss:  0.00019208682351745665
Batch  41  loss:  0.00017930137983057648
Batch  51  loss:  0.00017637533892411739
Batch  61  loss:  0.00013112295710016042
Batch  71  loss:  0.0002168252831324935
Batch  81  loss:  0.00014230252418201417
Batch  91  loss:  0.00016621820395812392
Batch  101  loss:  0.00015194584557320923
Batch  111  loss:  0.00017108212341554463
Batch  121  loss:  0.00015116084250621498
Batch  131  loss:  0.0001264134916709736
Batch  141  loss:  0.00019503606017678976
Batch  151  loss:  0.00018950946105178446
Batch  161  loss:  0.0002256098377984017
Batch  171  loss:  0.00017266307258978486
Batch  181  loss:  0.00015570860705338418
Batch  191  loss:  0.00012736702046822757
Validation on real data: 
LOSS supervised-train 0.00017147475118690637, valid 0.00013553464668802917
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0001684815069893375
Batch  11  loss:  0.00018166846712119877
Batch  21  loss:  0.00020632628002204
Batch  31  loss:  0.00014208290667738765
Batch  41  loss:  0.0001861188211478293
Batch  51  loss:  0.00015951473324093968
Batch  61  loss:  0.00014508153253700584
Batch  71  loss:  0.0001799464807845652
Batch  81  loss:  0.00019320081628393382
Batch  91  loss:  0.00020337311434559524
Batch  101  loss:  0.00018574266869109124
Batch  111  loss:  0.00022906080994289368
Batch  121  loss:  0.00012879236601293087
Batch  131  loss:  0.00013651228800881654
Batch  141  loss:  0.00018229444685857743
Batch  151  loss:  0.00019521154172252864
Batch  161  loss:  0.0002895190555136651
Batch  171  loss:  0.00017487751028966159
Batch  181  loss:  0.00013253778161015362
Batch  191  loss:  0.00016052648425102234
Validation on real data: 
LOSS supervised-train 0.00017618702509935246, valid 0.00015910118236206472
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00017976720118895173
Batch  11  loss:  0.0002243204362457618
Batch  21  loss:  0.00017490476602688432
Batch  31  loss:  0.00015540180902462453
Batch  41  loss:  0.00018605939112603664
Batch  51  loss:  0.0001608190214028582
Batch  61  loss:  0.0001495276956120506
Batch  71  loss:  0.0001901357463793829
Batch  81  loss:  0.0001636053348192945
Batch  91  loss:  0.00017083564307540655
Batch  101  loss:  0.000130567786982283
Batch  111  loss:  0.00020063297415617853
Batch  121  loss:  0.00016206711006816477
Batch  131  loss:  0.00017332567949779332
Batch  141  loss:  0.00017493280756752938
Batch  151  loss:  0.00014485136489383876
Batch  161  loss:  0.00020799346384592354
Batch  171  loss:  0.00014471690519712865
Batch  181  loss:  0.00019318432896398008
Batch  191  loss:  0.00014268240192905068
Validation on real data: 
LOSS supervised-train 0.00017072816346626495, valid 0.00010932904842775315
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00017105569713748991
Batch  11  loss:  0.00015686159895267338
Batch  21  loss:  0.00015728920698165894
Batch  31  loss:  0.00015749655722174793
Batch  41  loss:  0.0002491743362043053
Batch  51  loss:  0.00020397284242790192
Batch  61  loss:  0.00013852289703208953
Batch  71  loss:  0.00018881703726947308
Batch  81  loss:  0.00016898452304303646
Batch  91  loss:  0.0001865222438937053
Batch  101  loss:  0.00012495990085881203
Batch  111  loss:  0.000189263402717188
Batch  121  loss:  0.0002146215847460553
Batch  131  loss:  0.0001871639396995306
Batch  141  loss:  0.00016256343224085867
Batch  151  loss:  0.00019539627828635275
Batch  161  loss:  0.00024050656065810472
Batch  171  loss:  0.0001514136529294774
Batch  181  loss:  0.00017300396575592458
Batch  191  loss:  0.00020908786973450333
Validation on real data: 
LOSS supervised-train 0.00017319541148026475, valid 0.00014782726066187024
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00017478031804785132
Batch  11  loss:  0.0001784686028258875
Batch  21  loss:  0.00014695031859446317
Batch  31  loss:  0.0001340618182439357
Batch  41  loss:  0.00021320530504453927
Batch  51  loss:  0.00019210937898606062
Batch  61  loss:  0.00015471226652152836
Batch  71  loss:  0.0001804299681680277
Batch  81  loss:  0.00015547567454632372
Batch  91  loss:  0.00015776148939039558
Batch  101  loss:  0.00014676671708002687
Batch  111  loss:  0.00022158103820402175
Batch  121  loss:  0.00012422894360497594
Batch  131  loss:  0.00012865055759903044
Batch  141  loss:  0.0001672012294875458
Batch  151  loss:  0.00019755221728701144
Batch  161  loss:  0.00021537256543524563
Batch  171  loss:  0.00015637172327842563
Batch  181  loss:  0.00012579213944263756
Batch  191  loss:  0.0001830822875490412
Validation on real data: 
LOSS supervised-train 0.00016918981436901958, valid 0.00014964069123379886
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0001768586371326819
Batch  11  loss:  0.00015288269787561148
Batch  21  loss:  0.00014558802649844438
Batch  31  loss:  0.00014004293188918382
Batch  41  loss:  0.0002126011240761727
Batch  51  loss:  0.00015319797967094928
Batch  61  loss:  0.00017705935169942677
Batch  71  loss:  0.00024394206411670893
Batch  81  loss:  0.00014376541366800666
Batch  91  loss:  0.00016889165272004902
Batch  101  loss:  0.00015299255028367043
Batch  111  loss:  0.0001753892283886671
Batch  121  loss:  0.00012432692165020853
Batch  131  loss:  0.00014479350647889078
Batch  141  loss:  0.00015049146895762533
Batch  151  loss:  0.00014114065561443567
Batch  161  loss:  0.0002524202864151448
Batch  171  loss:  0.0001661970018176362
Batch  181  loss:  0.00014911028847564012
Batch  191  loss:  0.0001422202039975673
Validation on real data: 
LOSS supervised-train 0.0001698253424547147, valid 0.00014498771633952856
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00016532700101379305
Batch  11  loss:  0.00018559867748990655
Batch  21  loss:  0.0001782184117473662
Batch  31  loss:  0.0001449336123187095
Batch  41  loss:  0.00021307452698238194
Batch  51  loss:  0.0001617025991436094
Batch  61  loss:  0.00013908215623814613
Batch  71  loss:  0.00018303382967133075
Batch  81  loss:  0.0001736946142045781
Batch  91  loss:  0.00017629128706175834
Batch  101  loss:  0.00016289626364596188
Batch  111  loss:  0.0002082510181935504
Batch  121  loss:  0.00018407616880722344
Batch  131  loss:  0.0001769045920809731
Batch  141  loss:  0.0001593440683791414
Batch  151  loss:  0.00020217362907715142
Batch  161  loss:  0.0002835362101905048
Batch  171  loss:  0.00017782462236937135
Batch  181  loss:  0.0001387957454426214
Batch  191  loss:  0.00016720649728085846
Validation on real data: 
LOSS supervised-train 0.00017361621878080768, valid 0.00014535807713400573
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0002074719377560541
Batch  11  loss:  0.0001802495971787721
Batch  21  loss:  0.00016330763173755258
Batch  31  loss:  0.00016296457033604383
Batch  41  loss:  0.00021665745589416474
Batch  51  loss:  0.00016679070540703833
Batch  61  loss:  0.00012863274605479091
Batch  71  loss:  0.00022019458992872387
Batch  81  loss:  0.00011805127724073827
Batch  91  loss:  0.0001717747945804149
Batch  101  loss:  0.00014199219003785402
Batch  111  loss:  0.0002185481134802103
Batch  121  loss:  0.00012706800771411508
Batch  131  loss:  0.00015291683666873723
Batch  141  loss:  0.0001425630907760933
Batch  151  loss:  0.0001369098317809403
Batch  161  loss:  0.00023003721435088664
Batch  171  loss:  0.0001459843770135194
Batch  181  loss:  0.00015007505135145038
Batch  191  loss:  0.00017014873446896672
Validation on real data: 
LOSS supervised-train 0.00016960450615442822, valid 0.0001618188398424536
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.000159867326146923
Batch  11  loss:  0.00020646922348532826
Batch  21  loss:  0.00017741521878633648
Batch  31  loss:  0.00019373654504306614
Batch  41  loss:  0.0002092416543746367
Batch  51  loss:  0.0001705564500298351
Batch  61  loss:  0.00013731543731410056
Batch  71  loss:  0.00014958732936065644
Batch  81  loss:  0.00015946084749884903
Batch  91  loss:  0.00011852796887978911
Batch  101  loss:  0.00016857802984304726
Batch  111  loss:  0.00019606569549068809
Batch  121  loss:  0.00015841113054193556
Batch  131  loss:  0.00017361625214107335
Batch  141  loss:  0.00017268871306441724
Batch  151  loss:  0.00013240856060292572
Batch  161  loss:  0.0002445173158776015
Batch  171  loss:  0.0001398653257638216
Batch  181  loss:  0.0001748533104546368
Batch  191  loss:  0.0001442387729184702
Validation on real data: 
LOSS supervised-train 0.00016540318785700946, valid 0.0001229370100190863
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00020208250498399138
Batch  11  loss:  0.00017579153063707054
Batch  21  loss:  0.00021603747154586017
Batch  31  loss:  0.00015906374028418213
Batch  41  loss:  0.00021391158225014806
Batch  51  loss:  0.00015481541049666703
Batch  61  loss:  0.00013842101907357574
Batch  71  loss:  0.0001842065976234153
Batch  81  loss:  0.00015676894690841436
Batch  91  loss:  0.00014168712368700653
Batch  101  loss:  0.00018134299898520112
Batch  111  loss:  0.00020209587819408625
Batch  121  loss:  0.0001325483899563551
Batch  131  loss:  0.00012883356248494238
Batch  141  loss:  0.00017798951012082398
Batch  151  loss:  0.00016869827231857926
Batch  161  loss:  0.0002330811257706955
Batch  171  loss:  0.00017796950123738497
Batch  181  loss:  0.000153865956235677
Batch  191  loss:  0.00018694851314648986
Validation on real data: 
LOSS supervised-train 0.0001685585243831156, valid 0.00015033662202768028
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0001333858526777476
Batch  11  loss:  0.00019982390222139657
Batch  21  loss:  0.00016179414524231106
Batch  31  loss:  0.00016519427299499512
Batch  41  loss:  0.00022228824673220515
Batch  51  loss:  0.00015330647875089198
Batch  61  loss:  0.00016234649228863418
Batch  71  loss:  0.00020683032926172018
Batch  81  loss:  0.0001910906285047531
Batch  91  loss:  0.00017480194219388068
Batch  101  loss:  0.00014438240032177418
Batch  111  loss:  0.00017755535372998565
Batch  121  loss:  0.00013438399764709175
Batch  131  loss:  0.00013066627434454858
Batch  141  loss:  0.00014830433065071702
Batch  151  loss:  0.00017033566837199032
Batch  161  loss:  0.0002850785676855594
Batch  171  loss:  0.00020761370251420885
Batch  181  loss:  0.00016946019604802132
Batch  191  loss:  0.00014156113320495933
Validation on real data: 
LOSS supervised-train 0.0001660694721067557, valid 0.00012605241499841213
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0001471332652727142
Batch  11  loss:  0.00017863026005215943
Batch  21  loss:  0.0001638701360207051
Batch  31  loss:  0.00017489765014033765
Batch  41  loss:  0.0001669674675213173
Batch  51  loss:  0.00017823879898060113
Batch  61  loss:  0.00013895919255446643
Batch  71  loss:  0.00015198910841718316
Batch  81  loss:  0.00013125517580192536
Batch  91  loss:  0.0001793767441995442
Batch  101  loss:  0.0001326008205069229
Batch  111  loss:  0.00020637390844058245
Batch  121  loss:  0.00015144323697313666
Batch  131  loss:  0.00019923724175896496
Batch  141  loss:  0.00019597815116867423
Batch  151  loss:  0.00018767148139886558
Batch  161  loss:  0.0002445750287733972
Batch  171  loss:  0.0001252695801667869
Batch  181  loss:  0.00012525611964520067
Batch  191  loss:  0.0001345325290458277
Validation on real data: 
LOSS supervised-train 0.00016555064357817172, valid 0.0001406151568517089
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00013878742174711078
Batch  11  loss:  0.0001907978003146127
Batch  21  loss:  0.00019046466331928968
Batch  31  loss:  0.00015586803783662617
Batch  41  loss:  0.00016797988791950047
Batch  51  loss:  0.00020480855891946703
Batch  61  loss:  0.00014964128786232322
Batch  71  loss:  0.00019991157751064748
Batch  81  loss:  0.000126410202938132
Batch  91  loss:  0.00015607665409334004
Batch  101  loss:  0.00013839919120073318
Batch  111  loss:  0.00017376916366629303
Batch  121  loss:  0.00017235954874195158
Batch  131  loss:  0.00011296979937469587
Batch  141  loss:  0.00017843651585280895
Batch  151  loss:  0.0001907850819407031
Batch  161  loss:  0.0003109521057922393
Batch  171  loss:  0.00017262286564800888
Batch  181  loss:  0.0001457914331695065
Batch  191  loss:  0.0001447211398044601
Validation on real data: 
LOSS supervised-train 0.00015998095677787204, valid 0.00015018177509773523
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00018259245553053916
Batch  11  loss:  0.00014406016271095723
Batch  21  loss:  0.0001228524197358638
Batch  31  loss:  0.00015341507969424129
Batch  41  loss:  0.00018921055016107857
Batch  51  loss:  0.0001605519646545872
Batch  61  loss:  0.00014612938684877008
Batch  71  loss:  0.00015939309378154576
Batch  81  loss:  0.00013190237223170698
Batch  91  loss:  0.00010772021050797775
Batch  101  loss:  0.000178936606971547
Batch  111  loss:  0.00020557157404255122
Batch  121  loss:  0.00015901915321592242
Batch  131  loss:  0.0001561402896186337
Batch  141  loss:  0.00015821248234715313
Batch  151  loss:  0.00015986729704309255
Batch  161  loss:  0.00020917734946124256
Batch  171  loss:  0.0001504248648416251
Batch  181  loss:  0.00015368280583061278
Batch  191  loss:  0.00011345741950208321
Validation on real data: 
LOSS supervised-train 0.00015875634497206193, valid 0.0001334730623057112
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0001847617531893775
Batch  11  loss:  0.00015838953549973667
Batch  21  loss:  0.0001577734510647133
Batch  31  loss:  0.00011715834989445284
Batch  41  loss:  0.00021280389046296477
Batch  51  loss:  0.0001344229676760733
Batch  61  loss:  0.00014089148316998035
Batch  71  loss:  0.00019178727234248072
Batch  81  loss:  0.0001245220919372514
Batch  91  loss:  0.00010121527157025412
Batch  101  loss:  0.00016052208957262337
Batch  111  loss:  0.00018372881459072232
Batch  121  loss:  0.00013810013479087502
Batch  131  loss:  0.00015704476390965283
Batch  141  loss:  0.00015408107719849795
Batch  151  loss:  0.0001390909164911136
Batch  161  loss:  0.0002076090022455901
Batch  171  loss:  0.00015478799468837678
Batch  181  loss:  0.00012583713396452367
Batch  191  loss:  0.00015648228873033077
Validation on real data: 
LOSS supervised-train 0.0001564193864032859, valid 0.00012533986591733992
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0001543836697237566
Batch  11  loss:  0.00015281568630598485
Batch  21  loss:  0.0001863655779743567
Batch  31  loss:  0.0001516336196800694
Batch  41  loss:  0.00018897854897659272
Batch  51  loss:  0.0001643556752242148
Batch  61  loss:  0.0001550578890601173
Batch  71  loss:  0.00017969458713196218
Batch  81  loss:  0.0001433797151548788
Batch  91  loss:  0.00020550105546135455
Batch  101  loss:  0.00012032675294904038
Batch  111  loss:  0.00018422426364850253
Batch  121  loss:  0.00016677893290761858
Batch  131  loss:  0.00014074625505600125
Batch  141  loss:  0.00015398200775962323
Batch  151  loss:  0.00020433365716598928
Batch  161  loss:  0.00017960670811589807
Batch  171  loss:  0.00013445560762193054
Batch  181  loss:  9.943680925061926e-05
Batch  191  loss:  0.00014445636770687997
Validation on real data: 
LOSS supervised-train 0.0001565961618689471, valid 0.00015303946565836668
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  table ; Model ID: 3f5daa8fe93b68fa87e2d08958d6900c
--------------------
Training baseline regression model:  2022-03-30 10:36:21.468576
Detector:  point_transformer
Object:  table
--------------------
device is  cuda
--------------------
Number of trainable parameters:  891544
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.26840662956237793
Batch  11  loss:  0.25012126564979553
Batch  21  loss:  0.23438586294651031
Batch  31  loss:  0.19696563482284546
Batch  41  loss:  0.1969936192035675
Batch  51  loss:  0.21514861285686493
Batch  61  loss:  0.1860734522342682
Batch  71  loss:  0.20523719489574432
Batch  81  loss:  0.17232105135917664
Batch  91  loss:  0.2020132839679718
Batch  101  loss:  0.2078719288110733
Batch  111  loss:  0.2051590532064438
Batch  121  loss:  0.17481565475463867
Batch  131  loss:  0.1773303896188736
Batch  141  loss:  0.19982893764972687
Batch  151  loss:  0.17983821034431458
Batch  161  loss:  0.198464497923851
Batch  171  loss:  0.2004070281982422
Batch  181  loss:  0.18069609999656677
Batch  191  loss:  0.17627330124378204
Validation on real data: 
LOSS supervised-train 0.2066353326663375, valid 0.18779803812503815
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.15630978345870972
Batch  11  loss:  0.17357972264289856
Batch  21  loss:  0.24227389693260193
Batch  31  loss:  0.1831093579530716
Batch  41  loss:  0.1275692880153656
Batch  51  loss:  0.23538963496685028
Batch  61  loss:  0.12261533737182617
Batch  71  loss:  0.22866787016391754
Batch  81  loss:  0.1563846617937088
Batch  91  loss:  0.17564135789871216
Batch  101  loss:  0.1695018857717514
Batch  111  loss:  0.1505803018808365
Batch  121  loss:  0.15827800333499908
Batch  131  loss:  0.16150245070457458
Batch  141  loss:  0.18545424938201904
Batch  151  loss:  0.17064233124256134
Batch  161  loss:  0.16558192670345306
Batch  171  loss:  0.1863373965024948
Batch  181  loss:  0.1540551483631134
Batch  191  loss:  0.14133323729038239
Validation on real data: 
LOSS supervised-train 0.17563768934458493, valid 0.17402896285057068
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.1598302572965622
Batch  11  loss:  0.1723686307668686
Batch  21  loss:  0.2305939793586731
Batch  31  loss:  0.16775044798851013
Batch  41  loss:  0.10397864878177643
Batch  51  loss:  0.2255379855632782
Batch  61  loss:  0.13272526860237122
Batch  71  loss:  0.1963990181684494
Batch  81  loss:  0.1377885341644287
Batch  91  loss:  0.18248558044433594
Batch  101  loss:  0.12667079269886017
Batch  111  loss:  0.1481385976076126
Batch  121  loss:  0.14157918095588684
Batch  131  loss:  0.14242368936538696
Batch  141  loss:  0.19297173619270325
Batch  151  loss:  0.12474647164344788
Batch  161  loss:  0.16005370020866394
Batch  171  loss:  0.17376406490802765
Batch  181  loss:  0.1407501995563507
Batch  191  loss:  0.12917405366897583
Validation on real data: 
LOSS supervised-train 0.15977997649461032, valid 0.14359372854232788
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.17084522545337677
Batch  11  loss:  0.14002379775047302
Batch  21  loss:  0.17046867311000824
Batch  31  loss:  0.1367601603269577
Batch  41  loss:  0.10645541548728943
Batch  51  loss:  0.16704906523227692
Batch  61  loss:  0.11909779906272888
Batch  71  loss:  0.18720683455467224
Batch  81  loss:  0.12113511562347412
Batch  91  loss:  0.1423386037349701
Batch  101  loss:  0.14493955671787262
Batch  111  loss:  0.16057269275188446
Batch  121  loss:  0.11747670918703079
Batch  131  loss:  0.14449535310268402
Batch  141  loss:  0.16011525690555573
Batch  151  loss:  0.14444924890995026
Batch  161  loss:  0.1508055031299591
Batch  171  loss:  0.18232937157154083
Batch  181  loss:  0.12416615337133408
Batch  191  loss:  0.1198064386844635
Validation on real data: 
LOSS supervised-train 0.14549124352633952, valid 0.14752596616744995
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.1630045622587204
Batch  11  loss:  0.12637655436992645
Batch  21  loss:  0.20261363685131073
Batch  31  loss:  0.13068345189094543
Batch  41  loss:  0.13490016758441925
Batch  51  loss:  0.1824295073747635
Batch  61  loss:  0.08597639203071594
Batch  71  loss:  0.18055707216262817
Batch  81  loss:  0.11411933600902557
Batch  91  loss:  0.13802552223205566
Batch  101  loss:  0.12178575247526169
Batch  111  loss:  0.1298188716173172
Batch  121  loss:  0.08995574712753296
Batch  131  loss:  0.10993706434965134
Batch  141  loss:  0.1293252557516098
Batch  151  loss:  0.15037751197814941
Batch  161  loss:  0.15326854586601257
Batch  171  loss:  0.13371214270591736
Batch  181  loss:  0.12017744779586792
Batch  191  loss:  0.12183710187673569
Validation on real data: 
LOSS supervised-train 0.13477700037881732, valid 0.16001802682876587
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.14099931716918945
Batch  11  loss:  0.11458993703126907
Batch  21  loss:  0.13666455447673798
Batch  31  loss:  0.10528425127267838
Batch  41  loss:  0.11669326573610306
Batch  51  loss:  0.11580891162157059
Batch  61  loss:  0.08779039978981018
Batch  71  loss:  0.1566622257232666
Batch  81  loss:  0.10749512165784836
Batch  91  loss:  0.15990020334720612
Batch  101  loss:  0.13238076865673065
Batch  111  loss:  0.1214604526758194
Batch  121  loss:  0.11522996425628662
Batch  131  loss:  0.12733162939548492
Batch  141  loss:  0.13801364600658417
Batch  151  loss:  0.12803420424461365
Batch  161  loss:  0.11841260641813278
Batch  171  loss:  0.10164225101470947
Batch  181  loss:  0.13932807743549347
Batch  191  loss:  0.08337853848934174
Validation on real data: 
LOSS supervised-train 0.11933660555630922, valid 0.22572001814842224
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.1126297190785408
Batch  11  loss:  0.1089160144329071
Batch  21  loss:  0.11912070214748383
Batch  31  loss:  0.05987625941634178
Batch  41  loss:  0.10585776716470718
Batch  51  loss:  0.11315131932497025
Batch  61  loss:  0.05645820498466492
Batch  71  loss:  0.15385876595973969
Batch  81  loss:  0.09057054668664932
Batch  91  loss:  0.10806085169315338
Batch  101  loss:  0.11358907073736191
Batch  111  loss:  0.09686988592147827
Batch  121  loss:  0.065686896443367
Batch  131  loss:  0.06670186668634415
Batch  141  loss:  0.11147495359182358
Batch  151  loss:  0.11521156132221222
Batch  161  loss:  0.13776002824306488
Batch  171  loss:  0.09665986150503159
Batch  181  loss:  0.11671867221593857
Batch  191  loss:  0.09662407636642456
Validation on real data: 
LOSS supervised-train 0.10952436437830329, valid 0.20403537154197693
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.07470465451478958
Batch  11  loss:  0.11735556274652481
Batch  21  loss:  0.1450529396533966
Batch  31  loss:  0.07384642213582993
Batch  41  loss:  0.10810805857181549
Batch  51  loss:  0.10460929572582245
Batch  61  loss:  0.056049030274152756
Batch  71  loss:  0.12106893956661224
Batch  81  loss:  0.09183555096387863
Batch  91  loss:  0.1400683969259262
Batch  101  loss:  0.10486578941345215
Batch  111  loss:  0.11202863603830338
Batch  121  loss:  0.06112879514694214
Batch  131  loss:  0.06880585849285126
Batch  141  loss:  0.10376372188329697
Batch  151  loss:  0.09651584178209305
Batch  161  loss:  0.10023685544729233
Batch  171  loss:  0.08519197255373001
Batch  181  loss:  0.08695545047521591
Batch  191  loss:  0.06416630744934082
Validation on real data: 
LOSS supervised-train 0.10187622308731079, valid 0.09093032777309418
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.06804751604795456
Batch  11  loss:  0.06482169777154922
Batch  21  loss:  0.1281711310148239
Batch  31  loss:  0.0599835179746151
Batch  41  loss:  0.07854828983545303
Batch  51  loss:  0.07727876305580139
Batch  61  loss:  0.048846535384655
Batch  71  loss:  0.05608712136745453
Batch  81  loss:  0.05079079419374466
Batch  91  loss:  0.07264529913663864
Batch  101  loss:  0.053976476192474365
Batch  111  loss:  0.06017186865210533
Batch  121  loss:  0.050969019532203674
Batch  131  loss:  0.039305705577135086
Batch  141  loss:  0.08150926232337952
Batch  151  loss:  0.08748047798871994
Batch  161  loss:  0.0723474845290184
Batch  171  loss:  0.11190582066774368
Batch  181  loss:  0.052601709961891174
Batch  191  loss:  0.048802655190229416
Validation on real data: 
LOSS supervised-train 0.07886915578506887, valid 0.09885478019714355
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.05262111499905586
Batch  11  loss:  0.03780147805809975
Batch  21  loss:  0.0787237137556076
Batch  31  loss:  0.07209224998950958
Batch  41  loss:  0.08264318108558655
Batch  51  loss:  0.0847364217042923
Batch  61  loss:  0.09933767467737198
Batch  71  loss:  0.10742447525262833
Batch  81  loss:  0.0661049410700798
Batch  91  loss:  0.08837325125932693
Batch  101  loss:  0.06873946636915207
Batch  111  loss:  0.051481761038303375
Batch  121  loss:  0.040520939975976944
Batch  131  loss:  0.04111699387431145
Batch  141  loss:  0.069687619805336
Batch  151  loss:  0.040465664118528366
Batch  161  loss:  0.05024639889597893
Batch  171  loss:  0.07126114517450333
Batch  181  loss:  0.03998276963829994
Batch  191  loss:  0.05551999434828758
Validation on real data: 
LOSS supervised-train 0.07331624194979668, valid 0.075026735663414
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.03842052072286606
Batch  11  loss:  0.061672791838645935
Batch  21  loss:  0.06969110667705536
Batch  31  loss:  0.043234970420598984
Batch  41  loss:  0.06653156131505966
Batch  51  loss:  0.04925797879695892
Batch  61  loss:  0.0602131262421608
Batch  71  loss:  0.05662790313363075
Batch  81  loss:  0.06979458034038544
Batch  91  loss:  0.06645546853542328
Batch  101  loss:  0.056006643921136856
Batch  111  loss:  0.04034320265054703
Batch  121  loss:  0.053687747567892075
Batch  131  loss:  0.04060566797852516
Batch  141  loss:  0.09062395244836807
Batch  151  loss:  0.05632815137505531
Batch  161  loss:  0.04290299862623215
Batch  171  loss:  0.1152997687458992
Batch  181  loss:  0.03717328980565071
Batch  191  loss:  0.03230246156454086
Validation on real data: 
LOSS supervised-train 0.06169301151763648, valid 0.1641014665365219
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.04458893835544586
Batch  11  loss:  0.03999068960547447
Batch  21  loss:  0.09646818786859512
Batch  31  loss:  0.05769067257642746
Batch  41  loss:  0.048990629613399506
Batch  51  loss:  0.05866880342364311
Batch  61  loss:  0.08409425616264343
Batch  71  loss:  0.07172009348869324
Batch  81  loss:  0.08105914294719696
Batch  91  loss:  0.07507836818695068
Batch  101  loss:  0.052929528057575226
Batch  111  loss:  0.04608524218201637
Batch  121  loss:  0.0431026816368103
Batch  131  loss:  0.02671435847878456
Batch  141  loss:  0.06518007814884186
Batch  151  loss:  0.04305976629257202
Batch  161  loss:  0.05608341097831726
Batch  171  loss:  0.06533792614936829
Batch  181  loss:  0.024302629753947258
Batch  191  loss:  0.017504163086414337
Validation on real data: 
LOSS supervised-train 0.05754219957627356, valid 0.29393714666366577
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.03867305815219879
Batch  11  loss:  0.0336507223546505
Batch  21  loss:  0.0435728095471859
Batch  31  loss:  0.04054633900523186
Batch  41  loss:  0.0631985068321228
Batch  51  loss:  0.031037693843245506
Batch  61  loss:  0.06743042916059494
Batch  71  loss:  0.07194355130195618
Batch  81  loss:  0.033729176968336105
Batch  91  loss:  0.06876808404922485
Batch  101  loss:  0.05340457707643509
Batch  111  loss:  0.049521464854478836
Batch  121  loss:  0.045996394008398056
Batch  131  loss:  0.06706859916448593
Batch  141  loss:  0.1493358463048935
Batch  151  loss:  0.06402523070573807
Batch  161  loss:  0.045649752020835876
Batch  171  loss:  0.06817779690027237
Batch  181  loss:  0.03840845450758934
Batch  191  loss:  0.03799993544816971
Validation on real data: 
LOSS supervised-train 0.05709681347943842, valid 0.13609135150909424
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.04131633788347244
Batch  11  loss:  0.027683109045028687
Batch  21  loss:  0.06498657166957855
Batch  31  loss:  0.02414712868630886
Batch  41  loss:  0.03516766428947449
Batch  51  loss:  0.032216936349868774
Batch  61  loss:  0.07015793025493622
Batch  71  loss:  0.06294086575508118
Batch  81  loss:  0.026194781064987183
Batch  91  loss:  0.0582972876727581
Batch  101  loss:  0.03826654329895973
Batch  111  loss:  0.01220390573143959
Batch  121  loss:  0.046591732650995255
Batch  131  loss:  0.06588737666606903
Batch  141  loss:  0.0749863013625145
Batch  151  loss:  0.05743216350674629
Batch  161  loss:  0.03622276335954666
Batch  171  loss:  0.04538832604885101
Batch  181  loss:  0.025990208610892296
Batch  191  loss:  0.02023157849907875
Validation on real data: 
LOSS supervised-train 0.04526867968961597, valid 0.08390837907791138
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.021253418177366257
Batch  11  loss:  0.0325547456741333
Batch  21  loss:  0.06131355091929436
Batch  31  loss:  0.021294772624969482
Batch  41  loss:  0.03800322115421295
Batch  51  loss:  0.055590417236089706
Batch  61  loss:  0.0630122721195221
Batch  71  loss:  0.041498079895973206
Batch  81  loss:  0.03175071254372597
Batch  91  loss:  0.027916578575968742
Batch  101  loss:  0.025531940162181854
Batch  111  loss:  0.02678486332297325
Batch  121  loss:  0.029167495667934418
Batch  131  loss:  0.042639121413230896
Batch  141  loss:  0.10961054265499115
Batch  151  loss:  0.040625859051942825
Batch  161  loss:  0.05193429812788963
Batch  171  loss:  0.05344300717115402
Batch  181  loss:  0.02819804660975933
Batch  191  loss:  0.01998857408761978
Validation on real data: 
LOSS supervised-train 0.04046119457576424, valid 0.1134706363081932
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.06436106562614441
Batch  11  loss:  0.032050106674432755
Batch  21  loss:  0.02621868997812271
Batch  31  loss:  0.016341710463166237
Batch  41  loss:  0.05665988475084305
Batch  51  loss:  0.02864515408873558
Batch  61  loss:  0.06856148689985275
Batch  71  loss:  0.026160068809986115
Batch  81  loss:  0.07128723710775375
Batch  91  loss:  0.0648995041847229
Batch  101  loss:  0.03426824137568474
Batch  111  loss:  0.01831022836267948
Batch  121  loss:  0.02837636135518551
Batch  131  loss:  0.043842848390340805
Batch  141  loss:  0.04928071051836014
Batch  151  loss:  0.027206620201468468
Batch  161  loss:  0.025827273726463318
Batch  171  loss:  0.03028992749750614
Batch  181  loss:  0.03160182759165764
Batch  191  loss:  0.05498646944761276
Validation on real data: 
LOSS supervised-train 0.04065860428381711, valid 0.06346283853054047
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.044551584869623184
Batch  11  loss:  0.02597281150519848
Batch  21  loss:  0.044544514268636703
Batch  31  loss:  0.036991480737924576
Batch  41  loss:  0.029242152348160744
Batch  51  loss:  0.018578041344881058
Batch  61  loss:  0.053868092596530914
Batch  71  loss:  0.023769544437527657
Batch  81  loss:  0.025625700131058693
Batch  91  loss:  0.04247407242655754
Batch  101  loss:  0.018208200111985207
Batch  111  loss:  0.01655643992125988
Batch  121  loss:  0.04791894927620888
Batch  131  loss:  0.034699153155088425
Batch  141  loss:  0.052209947258234024
Batch  151  loss:  0.033289115875959396
Batch  161  loss:  0.017270734533667564
Batch  171  loss:  0.031966760754585266
Batch  181  loss:  0.029805269092321396
Batch  191  loss:  0.04554714262485504
Validation on real data: 
LOSS supervised-train 0.03473845089087263, valid 0.0551527738571167
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.04506247118115425
Batch  11  loss:  0.023917904123663902
Batch  21  loss:  0.050193317234516144
Batch  31  loss:  0.02281964011490345
Batch  41  loss:  0.03531055524945259
Batch  51  loss:  0.06807326525449753
Batch  61  loss:  0.06287562102079391
Batch  71  loss:  0.03595636412501335
Batch  81  loss:  0.012597181834280491
Batch  91  loss:  0.018842196092009544
Batch  101  loss:  0.014318229630589485
Batch  111  loss:  0.019570477306842804
Batch  121  loss:  0.018949050456285477
Batch  131  loss:  0.03292027488350868
Batch  141  loss:  0.07317792624235153
Batch  151  loss:  0.03414038568735123
Batch  161  loss:  0.024081595242023468
Batch  171  loss:  0.03371268883347511
Batch  181  loss:  0.021447066217660904
Batch  191  loss:  0.0603177435696125
Validation on real data: 
LOSS supervised-train 0.03370472345734015, valid 0.1752670407295227
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.024100417271256447
Batch  11  loss:  0.01877215877175331
Batch  21  loss:  0.031865570694208145
Batch  31  loss:  0.013910272158682346
Batch  41  loss:  0.04880571365356445
Batch  51  loss:  0.03132246062159538
Batch  61  loss:  0.04981239512562752
Batch  71  loss:  0.009323623031377792
Batch  81  loss:  0.012320703826844692
Batch  91  loss:  0.02954268641769886
Batch  101  loss:  0.02702404372394085
Batch  111  loss:  0.011623702943325043
Batch  121  loss:  0.04549306631088257
Batch  131  loss:  0.029585126787424088
Batch  141  loss:  0.025460967794060707
Batch  151  loss:  0.014571059495210648
Batch  161  loss:  0.025104114785790443
Batch  171  loss:  0.017367064952850342
Batch  181  loss:  0.01830376125872135
Batch  191  loss:  0.03997133672237396
Validation on real data: 
LOSS supervised-train 0.030667134998366236, valid 0.26856595277786255
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.042745303362607956
Batch  11  loss:  0.014215691015124321
Batch  21  loss:  0.010974938981235027
Batch  31  loss:  0.02419545128941536
Batch  41  loss:  0.017262892797589302
Batch  51  loss:  0.03609029948711395
Batch  61  loss:  0.0667642280459404
Batch  71  loss:  0.019307497888803482
Batch  81  loss:  0.030067065730690956
Batch  91  loss:  0.025688566267490387
Batch  101  loss:  0.02800234779715538
Batch  111  loss:  0.018305189907550812
Batch  121  loss:  0.016208471730351448
Batch  131  loss:  0.020824529230594635
Batch  141  loss:  0.029679039493203163
Batch  151  loss:  0.03216053918004036
Batch  161  loss:  0.018660813570022583
Batch  171  loss:  0.03246450424194336
Batch  181  loss:  0.038478706032037735
Batch  191  loss:  0.04767558351159096
Validation on real data: 
LOSS supervised-train 0.028846242232248187, valid 0.1736103594303131
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.013774077408015728
Batch  11  loss:  0.030239855870604515
Batch  21  loss:  0.04117371886968613
Batch  31  loss:  0.02350418083369732
Batch  41  loss:  0.018426958471536636
Batch  51  loss:  0.06703044474124908
Batch  61  loss:  0.07138900458812714
Batch  71  loss:  0.028502967208623886
Batch  81  loss:  0.0294870063662529
Batch  91  loss:  0.028607802465558052
Batch  101  loss:  0.012901682406663895
Batch  111  loss:  0.037915442138910294
Batch  121  loss:  0.03618038445711136
Batch  131  loss:  0.021253308281302452
Batch  141  loss:  0.04063638672232628
Batch  151  loss:  0.03866378962993622
Batch  161  loss:  0.010783908888697624
Batch  171  loss:  0.03547660633921623
Batch  181  loss:  0.018056323751807213
Batch  191  loss:  0.034275177866220474
Validation on real data: 
LOSS supervised-train 0.029251440120860936, valid 0.1559600681066513
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.022872457280755043
Batch  11  loss:  0.034802552312612534
Batch  21  loss:  0.016146749258041382
Batch  31  loss:  0.04407095909118652
Batch  41  loss:  0.017710179090499878
Batch  51  loss:  0.039151936769485474
Batch  61  loss:  0.06675029546022415
Batch  71  loss:  0.028936542570590973
Batch  81  loss:  0.011656464077532291
Batch  91  loss:  0.007794548291712999
Batch  101  loss:  0.01880647800862789
Batch  111  loss:  0.009243281558156013
Batch  121  loss:  0.029742693528532982
Batch  131  loss:  0.021092528477311134
Batch  141  loss:  0.019687136635184288
Batch  151  loss:  0.056751642376184464
Batch  161  loss:  0.028587406501173973
Batch  171  loss:  0.008316983468830585
Batch  181  loss:  0.019589567556977272
Batch  191  loss:  0.0423525907099247
Validation on real data: 
LOSS supervised-train 0.024928508060984313, valid 0.2595580816268921
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.017177224159240723
Batch  11  loss:  0.025636371225118637
Batch  21  loss:  0.011874433606863022
Batch  31  loss:  0.02920977585017681
Batch  41  loss:  0.027514729648828506
Batch  51  loss:  0.03691304102540016
Batch  61  loss:  0.030814848840236664
Batch  71  loss:  0.03604912757873535
Batch  81  loss:  0.012143648229539394
Batch  91  loss:  0.02326708287000656
Batch  101  loss:  0.03528992086648941
Batch  111  loss:  0.008014615625143051
Batch  121  loss:  0.013022997416555882
Batch  131  loss:  0.05038587376475334
Batch  141  loss:  0.01638418808579445
Batch  151  loss:  0.02051403932273388
Batch  161  loss:  0.006986705586314201
Batch  171  loss:  0.009447220712900162
Batch  181  loss:  0.0066811153665184975
Batch  191  loss:  0.04193689301609993
Validation on real data: 
LOSS supervised-train 0.02008536032633856, valid 0.15065519511699677
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.012823333032429218
Batch  11  loss:  0.020202288404107094
Batch  21  loss:  0.011248156428337097
Batch  31  loss:  0.026338577270507812
Batch  41  loss:  0.018912987783551216
Batch  51  loss:  0.04522906616330147
Batch  61  loss:  0.04758564010262489
Batch  71  loss:  0.013113877736032009
Batch  81  loss:  0.007960623130202293
Batch  91  loss:  0.01661004312336445
Batch  101  loss:  0.020755264908075333
Batch  111  loss:  0.009428377263247967
Batch  121  loss:  0.009488743729889393
Batch  131  loss:  0.015020325779914856
Batch  141  loss:  0.031018147245049477
Batch  151  loss:  0.01475191954523325
Batch  161  loss:  0.02004343457520008
Batch  171  loss:  0.022072190418839455
Batch  181  loss:  0.032681480050086975
Batch  191  loss:  0.0364580899477005
Validation on real data: 
LOSS supervised-train 0.024977280895691366, valid 0.08910265564918518
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.03884272649884224
Batch  11  loss:  0.015219084918498993
Batch  21  loss:  0.01314385887235403
Batch  31  loss:  0.020767852663993835
Batch  41  loss:  0.0137910395860672
Batch  51  loss:  0.016139090061187744
Batch  61  loss:  0.05752703174948692
Batch  71  loss:  0.0363161638379097
Batch  81  loss:  0.010806901380419731
Batch  91  loss:  0.02134854346513748
Batch  101  loss:  0.007689844351261854
Batch  111  loss:  0.006320235785096884
Batch  121  loss:  0.03353240713477135
Batch  131  loss:  0.042281150817871094
Batch  141  loss:  0.018674343824386597
Batch  151  loss:  0.03610946610569954
Batch  161  loss:  0.013648022897541523
Batch  171  loss:  0.010990552604198456
Batch  181  loss:  0.010256861336529255
Batch  191  loss:  0.05125200003385544
Validation on real data: 
LOSS supervised-train 0.022507161799585448, valid 0.02901517227292061
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.005815231706947088
Batch  11  loss:  0.020638542249798775
Batch  21  loss:  0.011479250155389309
Batch  31  loss:  0.03254014998674393
Batch  41  loss:  0.014835360459983349
Batch  51  loss:  0.018746374174952507
Batch  61  loss:  0.05083324387669563
Batch  71  loss:  0.040674641728401184
Batch  81  loss:  0.008213908411562443
Batch  91  loss:  0.019277742132544518
Batch  101  loss:  0.00827170442789793
Batch  111  loss:  0.005837366916239262
Batch  121  loss:  0.0414239726960659
Batch  131  loss:  0.034003354609012604
Batch  141  loss:  0.02127554640173912
Batch  151  loss:  0.013144459575414658
Batch  161  loss:  0.006335778161883354
Batch  171  loss:  0.007982277311384678
Batch  181  loss:  0.01765335351228714
Batch  191  loss:  0.04171350970864296
Validation on real data: 
LOSS supervised-train 0.0195265682565514, valid 0.09772129356861115
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.027321673929691315
Batch  11  loss:  0.03347734734416008
Batch  21  loss:  0.013636650517582893
Batch  31  loss:  0.014873949810862541
Batch  41  loss:  0.0077249715104699135
Batch  51  loss:  0.019081968814134598
Batch  61  loss:  0.037583041936159134
Batch  71  loss:  0.017586348578333855
Batch  81  loss:  0.020454373210668564
Batch  91  loss:  0.010190772823989391
Batch  101  loss:  0.026545634493231773
Batch  111  loss:  0.009320947341620922
Batch  121  loss:  0.02887558564543724
Batch  131  loss:  0.03018321469426155
Batch  141  loss:  0.014863936230540276
Batch  151  loss:  0.022442741319537163
Batch  161  loss:  0.02091301418840885
Batch  171  loss:  0.028115522116422653
Batch  181  loss:  0.005890389438718557
Batch  191  loss:  0.01401299424469471
Validation on real data: 
LOSS supervised-train 0.019430225339019672, valid 0.09645073115825653
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.01329236850142479
Batch  11  loss:  0.03842604160308838
Batch  21  loss:  0.00804997980594635
Batch  31  loss:  0.022936871275305748
Batch  41  loss:  0.017457038164138794
Batch  51  loss:  0.022196106612682343
Batch  61  loss:  0.05315176770091057
Batch  71  loss:  0.05223015695810318
Batch  81  loss:  0.01310482807457447
Batch  91  loss:  0.027086390182375908
Batch  101  loss:  0.014738820493221283
Batch  111  loss:  0.011437048204243183
Batch  121  loss:  0.016482705250382423
Batch  131  loss:  0.01213283184915781
Batch  141  loss:  0.03036445565521717
Batch  151  loss:  0.04148682951927185
Batch  161  loss:  0.01852574571967125
Batch  171  loss:  0.02848849631845951
Batch  181  loss:  0.02852201648056507
Batch  191  loss:  0.020682377740740776
Validation on real data: 
LOSS supervised-train 0.019511065774131565, valid 0.0183560773730278
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.05419088155031204
Batch  11  loss:  0.031451452523469925
Batch  21  loss:  0.011150555685162544
Batch  31  loss:  0.02131919376552105
Batch  41  loss:  0.010571660473942757
Batch  51  loss:  0.01807204820215702
Batch  61  loss:  0.050234124064445496
Batch  71  loss:  0.025760771706700325
Batch  81  loss:  0.012191186659038067
Batch  91  loss:  0.034045204520225525
Batch  101  loss:  0.015041470527648926
Batch  111  loss:  0.00499276677146554
Batch  121  loss:  0.01995503343641758
Batch  131  loss:  0.039633858948946
Batch  141  loss:  0.007577469106763601
Batch  151  loss:  0.00754950987175107
Batch  161  loss:  0.007903854362666607
Batch  171  loss:  0.023936837911605835
Batch  181  loss:  0.006846771575510502
Batch  191  loss:  0.010443944483995438
Validation on real data: 
LOSS supervised-train 0.017584160681581126, valid 0.037429891526699066
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.013003344647586346
Batch  11  loss:  0.03163351118564606
Batch  21  loss:  0.007696392480283976
Batch  31  loss:  0.010980314575135708
Batch  41  loss:  0.016514083370566368
Batch  51  loss:  0.026707250624895096
Batch  61  loss:  0.019515126943588257
Batch  71  loss:  0.03390232473611832
Batch  81  loss:  0.0051860325038433075
Batch  91  loss:  0.014853637665510178
Batch  101  loss:  0.009549283422529697
Batch  111  loss:  0.012559437192976475
Batch  121  loss:  0.02368760295212269
Batch  131  loss:  0.041883647441864014
Batch  141  loss:  0.005528459325432777
Batch  151  loss:  0.01872830092906952
Batch  161  loss:  0.012532446533441544
Batch  171  loss:  0.004069003742188215
Batch  181  loss:  0.012968059629201889
Batch  191  loss:  0.005801607389003038
Validation on real data: 
LOSS supervised-train 0.01966094339150004, valid 0.050469763576984406
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.01365096028894186
Batch  11  loss:  0.014432989992201328
Batch  21  loss:  0.015911413356661797
Batch  31  loss:  0.012291875667870045
Batch  41  loss:  0.014977631159126759
Batch  51  loss:  0.02211260050535202
Batch  61  loss:  0.06868813186883926
Batch  71  loss:  0.025838647037744522
Batch  81  loss:  0.022602055221796036
Batch  91  loss:  0.011630791239440441
Batch  101  loss:  0.011108893901109695
Batch  111  loss:  0.010615111328661442
Batch  121  loss:  0.011890838854014874
Batch  131  loss:  0.027323732152581215
Batch  141  loss:  0.01467063371092081
Batch  151  loss:  0.042845286428928375
Batch  161  loss:  0.011888840235769749
Batch  171  loss:  0.02309134043753147
Batch  181  loss:  0.031173018738627434
Batch  191  loss:  0.025800608098506927
Validation on real data: 
LOSS supervised-train 0.01913246395182796, valid 0.057425159960985184
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.014223555102944374
Batch  11  loss:  0.04025440663099289
Batch  21  loss:  0.007599492557346821
Batch  31  loss:  0.017436625435948372
Batch  41  loss:  0.019458778202533722
Batch  51  loss:  0.02106768824160099
Batch  61  loss:  0.048414964228868484
Batch  71  loss:  0.03224864602088928
Batch  81  loss:  0.00926598347723484
Batch  91  loss:  0.01314607635140419
Batch  101  loss:  0.005438171327114105
Batch  111  loss:  0.014803935773670673
Batch  121  loss:  0.0197747889906168
Batch  131  loss:  0.04226578772068024
Batch  141  loss:  0.006147710140794516
Batch  151  loss:  0.0045501175336539745
Batch  161  loss:  0.007593111600726843
Batch  171  loss:  0.014147158712148666
Batch  181  loss:  0.008476834744215012
Batch  191  loss:  0.02115871198475361
Validation on real data: 
LOSS supervised-train 0.01645211815368384, valid 0.030086666345596313
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.008275497704744339
Batch  11  loss:  0.029326336458325386
Batch  21  loss:  0.015953388065099716
Batch  31  loss:  0.005109368357807398
Batch  41  loss:  0.006294816732406616
Batch  51  loss:  0.012105287984013557
Batch  61  loss:  0.024707386270165443
Batch  71  loss:  0.031579162925481796
Batch  81  loss:  0.029751313850283623
Batch  91  loss:  0.017378496006131172
Batch  101  loss:  0.0068687270395457745
Batch  111  loss:  0.005024597514420748
Batch  121  loss:  0.006591916084289551
Batch  131  loss:  0.029468607157468796
Batch  141  loss:  0.02238510549068451
Batch  151  loss:  0.00856846570968628
Batch  161  loss:  0.010228452272713184
Batch  171  loss:  0.005556238815188408
Batch  181  loss:  0.01097339577972889
Batch  191  loss:  0.026093589141964912
Validation on real data: 
LOSS supervised-train 0.013416310143657028, valid 0.021308256313204765
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.011184457689523697
Batch  11  loss:  0.02287447452545166
Batch  21  loss:  0.005103754810988903
Batch  31  loss:  0.032607413828372955
Batch  41  loss:  0.013533967547118664
Batch  51  loss:  0.00786642637103796
Batch  61  loss:  0.024524522945284843
Batch  71  loss:  0.04088540002703667
Batch  81  loss:  0.007094448432326317
Batch  91  loss:  0.007152124308049679
Batch  101  loss:  0.004089674912393093
Batch  111  loss:  0.027313537895679474
Batch  121  loss:  0.015175838023424149
Batch  131  loss:  0.00890575721859932
Batch  141  loss:  0.006316660437732935
Batch  151  loss:  0.02307807467877865
Batch  161  loss:  0.009026486426591873
Batch  171  loss:  0.01808226853609085
Batch  181  loss:  0.016176162287592888
Batch  191  loss:  0.025896284729242325
Validation on real data: 
LOSS supervised-train 0.015448949912097306, valid 0.3306174576282501
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.013082721270620823
Batch  11  loss:  0.012326796539127827
Batch  21  loss:  0.008420092053711414
Batch  31  loss:  0.008113534189760685
Batch  41  loss:  0.030500933527946472
Batch  51  loss:  0.03326593339443207
Batch  61  loss:  0.04962807893753052
Batch  71  loss:  0.030156191438436508
Batch  81  loss:  0.004673359915614128
Batch  91  loss:  0.031996384263038635
Batch  101  loss:  0.005666767247021198
Batch  111  loss:  0.007811467628926039
Batch  121  loss:  0.010037096217274666
Batch  131  loss:  0.005840889643877745
Batch  141  loss:  0.005658234003931284
Batch  151  loss:  0.014375248923897743
Batch  161  loss:  0.017345154657959938
Batch  171  loss:  0.004641754552721977
Batch  181  loss:  0.01710345223546028
Batch  191  loss:  0.014116868376731873
Validation on real data: 
LOSS supervised-train 0.014094274016097188, valid 0.03756629675626755
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.009210122749209404
Batch  11  loss:  0.01560167409479618
Batch  21  loss:  0.009325305931270123
Batch  31  loss:  0.004044987261295319
Batch  41  loss:  0.010607129894196987
Batch  51  loss:  0.02101168781518936
Batch  61  loss:  0.05101339519023895
Batch  71  loss:  0.02437570132315159
Batch  81  loss:  0.011602823622524738
Batch  91  loss:  0.0073201037012040615
Batch  101  loss:  0.039086878299713135
Batch  111  loss:  0.007425094023346901
Batch  121  loss:  0.023310337215662003
Batch  131  loss:  0.009892544709146023
Batch  141  loss:  0.0065306113101542
Batch  151  loss:  0.005245808977633715
Batch  161  loss:  0.012645232491195202
Batch  171  loss:  0.022255437448620796
Batch  181  loss:  0.011751916259527206
Batch  191  loss:  0.022799892351031303
Validation on real data: 
LOSS supervised-train 0.015600765958661214, valid 0.03870456665754318
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.026889052242040634
Batch  11  loss:  0.060354579240083694
Batch  21  loss:  0.008009877987205982
Batch  31  loss:  0.024259980767965317
Batch  41  loss:  0.029798397794365883
Batch  51  loss:  0.012460615485906601
Batch  61  loss:  0.039336781948804855
Batch  71  loss:  0.043458741158246994
Batch  81  loss:  0.01443410012871027
Batch  91  loss:  0.01478781271725893
Batch  101  loss:  0.00813526101410389
Batch  111  loss:  0.01978173293173313
Batch  121  loss:  0.00620778976008296
Batch  131  loss:  0.015311224386096
Batch  141  loss:  0.00951340701431036
Batch  151  loss:  0.018881957978010178
Batch  161  loss:  0.005733848549425602
Batch  171  loss:  0.01226540096104145
Batch  181  loss:  0.006527516059577465
Batch  191  loss:  0.011622540652751923
Validation on real data: 
LOSS supervised-train 0.016646610014140605, valid 0.13534073531627655
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.013094396330416203
Batch  11  loss:  0.02976638451218605
Batch  21  loss:  0.00630046334117651
Batch  31  loss:  0.005434500984847546
Batch  41  loss:  0.019042856991291046
Batch  51  loss:  0.003807203145697713
Batch  61  loss:  0.0374935120344162
Batch  71  loss:  0.04121334105730057
Batch  81  loss:  0.005132700782269239
Batch  91  loss:  0.02757316082715988
Batch  101  loss:  0.011166281066834927
Batch  111  loss:  0.004430174361914396
Batch  121  loss:  0.00944090262055397
Batch  131  loss:  0.0065829032100737095
Batch  141  loss:  0.0030353697948157787
Batch  151  loss:  0.0033684216905385256
Batch  161  loss:  0.005555788055062294
Batch  171  loss:  0.004449657630175352
Batch  181  loss:  0.007105051539838314
Batch  191  loss:  0.003879793221130967
Validation on real data: 
LOSS supervised-train 0.012198815097799525, valid 0.0486215315759182
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.006954743526875973
Batch  11  loss:  0.01793062686920166
Batch  21  loss:  0.010299588553607464
Batch  31  loss:  0.005460563115775585
Batch  41  loss:  0.03611544519662857
Batch  51  loss:  0.011998903937637806
Batch  61  loss:  0.03661707043647766
Batch  71  loss:  0.06201903149485588
Batch  81  loss:  0.01217868085950613
Batch  91  loss:  0.008211183361709118
Batch  101  loss:  0.007900661788880825
Batch  111  loss:  0.005841184873133898
Batch  121  loss:  0.004724695347249508
Batch  131  loss:  0.009119290858507156
Batch  141  loss:  0.006823655217885971
Batch  151  loss:  0.004292597528547049
Batch  161  loss:  0.005509344860911369
Batch  171  loss:  0.00462229922413826
Batch  181  loss:  0.005599142052233219
Batch  191  loss:  0.007382917683571577
Validation on real data: 
LOSS supervised-train 0.012690087772207335, valid 0.02786465361714363
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.007646081503480673
Batch  11  loss:  0.016750406473875046
Batch  21  loss:  0.006086087319999933
Batch  31  loss:  0.014147170819342136
Batch  41  loss:  0.0036671124398708344
Batch  51  loss:  0.03502117469906807
Batch  61  loss:  0.006920617073774338
Batch  71  loss:  0.020291384309530258
Batch  81  loss:  0.006469280458986759
Batch  91  loss:  0.007541759870946407
Batch  101  loss:  0.003336000954732299
Batch  111  loss:  0.0042565190233290195
Batch  121  loss:  0.00886096153408289
Batch  131  loss:  0.02480802685022354
Batch  141  loss:  0.00451273750513792
Batch  151  loss:  0.007696906104683876
Batch  161  loss:  0.007512775715440512
Batch  171  loss:  0.004351947456598282
Batch  181  loss:  0.0038401891943067312
Batch  191  loss:  0.002399247605353594
Validation on real data: 
LOSS supervised-train 0.010614849961129949, valid 0.030905764549970627
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.008625185117125511
Batch  11  loss:  0.01270220149308443
Batch  21  loss:  0.015421979129314423
Batch  31  loss:  0.03178515285253525
Batch  41  loss:  0.029602717608213425
Batch  51  loss:  0.004745442885905504
Batch  61  loss:  0.020149797201156616
Batch  71  loss:  0.019549204036593437
Batch  81  loss:  0.00622957618907094
Batch  91  loss:  0.0034428483340889215
Batch  101  loss:  0.007721565198153257
Batch  111  loss:  0.00822980422526598
Batch  121  loss:  0.010017882101237774
Batch  131  loss:  0.018937788903713226
Batch  141  loss:  0.0067802248522639275
Batch  151  loss:  0.028999051079154015
Batch  161  loss:  0.008798645809292793
Batch  171  loss:  0.004224088508635759
Batch  181  loss:  0.007805199362337589
Batch  191  loss:  0.02286544069647789
Validation on real data: 
LOSS supervised-train 0.01133363863104023, valid 0.020007245242595673
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.01153584010899067
Batch  11  loss:  0.005596961826086044
Batch  21  loss:  0.009847745299339294
Batch  31  loss:  0.03311574086546898
Batch  41  loss:  0.003695682156831026
Batch  51  loss:  0.003146826522424817
Batch  61  loss:  0.0073473090305924416
Batch  71  loss:  0.01551111415028572
Batch  81  loss:  0.004995677620172501
Batch  91  loss:  0.005960328038781881
Batch  101  loss:  0.017275530844926834
Batch  111  loss:  0.006212256848812103
Batch  121  loss:  0.014524373225867748
Batch  131  loss:  0.01713087223470211
Batch  141  loss:  0.007710847072303295
Batch  151  loss:  0.0034895911812782288
Batch  161  loss:  0.006538728252053261
Batch  171  loss:  0.007625672034919262
Batch  181  loss:  0.003098677843809128
Batch  191  loss:  0.006610851734876633
Validation on real data: 
LOSS supervised-train 0.00791832503862679, valid 0.060608088970184326
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.003829558379948139
Batch  11  loss:  0.005598415154963732
Batch  21  loss:  0.020438965409994125
Batch  31  loss:  0.011359906755387783
Batch  41  loss:  0.016522247344255447
Batch  51  loss:  0.00481113325804472
Batch  61  loss:  0.006840637885034084
Batch  71  loss:  0.013654702343046665
Batch  81  loss:  0.027349261566996574
Batch  91  loss:  0.011093264445662498
Batch  101  loss:  0.005898024886846542
Batch  111  loss:  0.003288579871878028
Batch  121  loss:  0.005459416192024946
Batch  131  loss:  0.02540392428636551
Batch  141  loss:  0.0044935885816812515
Batch  151  loss:  0.003224135609343648
Batch  161  loss:  0.015050753019750118
Batch  171  loss:  0.0030250768177211285
Batch  181  loss:  0.00493015768006444
Batch  191  loss:  0.017290517687797546
Validation on real data: 
LOSS supervised-train 0.009213620070368052, valid 0.03900756686925888
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.00498035317286849
Batch  11  loss:  0.00501459464430809
Batch  21  loss:  0.0034525268711149693
Batch  31  loss:  0.018150972202420235
Batch  41  loss:  0.0036301922518759966
Batch  51  loss:  0.014108389616012573
Batch  61  loss:  0.029791347682476044
Batch  71  loss:  0.008719692006707191
Batch  81  loss:  0.0035544156562536955
Batch  91  loss:  0.00535161467269063
Batch  101  loss:  0.002757605165243149
Batch  111  loss:  0.002875890117138624
Batch  121  loss:  0.007044899743050337
Batch  131  loss:  0.036131829023361206
Batch  141  loss:  0.0032123709097504616
Batch  151  loss:  0.0037563724908977747
Batch  161  loss:  0.012910209596157074
Batch  171  loss:  0.012474124319851398
Batch  181  loss:  0.0036954544484615326
Batch  191  loss:  0.0059674582444131374
Validation on real data: 
LOSS supervised-train 0.009009756090235896, valid 0.0206441693007946
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.01940002106130123
Batch  11  loss:  0.010684577748179436
Batch  21  loss:  0.0037057073786854744
Batch  31  loss:  0.0026687495410442352
Batch  41  loss:  0.002407577820122242
Batch  51  loss:  0.0033282779622823
Batch  61  loss:  0.024952013045549393
Batch  71  loss:  0.009211431257426739
Batch  81  loss:  0.0038919143844395876
Batch  91  loss:  0.002751024905592203
Batch  101  loss:  0.007139791268855333
Batch  111  loss:  0.003207026980817318
Batch  121  loss:  0.003355203429237008
Batch  131  loss:  0.02376355044543743
Batch  141  loss:  0.002634317148476839
Batch  151  loss:  0.009782599285244942
Batch  161  loss:  0.006804083473980427
Batch  171  loss:  0.019078655168414116
Batch  181  loss:  0.004275992978364229
Batch  191  loss:  0.004378567915409803
Validation on real data: 
LOSS supervised-train 0.0069236952153733, valid 0.03001076728105545
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.003475925186648965
Batch  11  loss:  0.004360469989478588
Batch  21  loss:  0.006118199322372675
Batch  31  loss:  0.004948231857270002
Batch  41  loss:  0.03136260434985161
Batch  51  loss:  0.004450661595910788
Batch  61  loss:  0.016422687098383904
Batch  71  loss:  0.004053752403706312
Batch  81  loss:  0.0051970891654491425
Batch  91  loss:  0.004020017571747303
Batch  101  loss:  0.005455228965729475
Batch  111  loss:  0.004587751347571611
Batch  121  loss:  0.004482746589928865
Batch  131  loss:  0.023693498224020004
Batch  141  loss:  0.0230401661247015
Batch  151  loss:  0.004595976788550615
Batch  161  loss:  0.002953949151560664
Batch  171  loss:  0.0037746259476989508
Batch  181  loss:  0.005259667988866568
Batch  191  loss:  0.002678113291040063
Validation on real data: 
LOSS supervised-train 0.008496993766748347, valid 0.03448157384991646
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.00503113679587841
Batch  11  loss:  0.007620058488100767
Batch  21  loss:  0.017354419454932213
Batch  31  loss:  0.0023402071092277765
Batch  41  loss:  0.003850906388834119
Batch  51  loss:  0.0030179740861058235
Batch  61  loss:  0.00397921958938241
Batch  71  loss:  0.009267391636967659
Batch  81  loss:  0.003421376459300518
Batch  91  loss:  0.003034365363419056
Batch  101  loss:  0.0031907421071082354
Batch  111  loss:  0.0026607620529830456
Batch  121  loss:  0.010272121988236904
Batch  131  loss:  0.004131196532398462
Batch  141  loss:  0.0035939228255301714
Batch  151  loss:  0.003995500039309263
Batch  161  loss:  0.0034794204402714968
Batch  171  loss:  0.004053284414112568
Batch  181  loss:  0.0025203912518918514
Batch  191  loss:  0.005165087524801493
Validation on real data: 
LOSS supervised-train 0.006398416092270054, valid 0.005987478420138359
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.007786915171891451
Batch  11  loss:  0.028768833726644516
Batch  21  loss:  0.010129882022738457
Batch  31  loss:  0.011357218027114868
Batch  41  loss:  0.01572117581963539
Batch  51  loss:  0.0028814589604735374
Batch  61  loss:  0.012779484502971172
Batch  71  loss:  0.023983009159564972
Batch  81  loss:  0.008877282962203026
Batch  91  loss:  0.012942373752593994
Batch  101  loss:  0.0053524780087172985
Batch  111  loss:  0.016783734783530235
Batch  121  loss:  0.00401807576417923
Batch  131  loss:  0.007292758207768202
Batch  141  loss:  0.002521290211006999
Batch  151  loss:  0.003909723833203316
Batch  161  loss:  0.0040781814604997635
Batch  171  loss:  0.00882005039602518
Batch  181  loss:  0.022576309740543365
Batch  191  loss:  0.012903128750622272
Validation on real data: 
LOSS supervised-train 0.009291783217922784, valid 0.016646726056933403
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.004539625719189644
Batch  11  loss:  0.010240397416055202
Batch  21  loss:  0.003010563086718321
Batch  31  loss:  0.011521897278726101
Batch  41  loss:  0.004992621950805187
Batch  51  loss:  0.003895525587722659
Batch  61  loss:  0.02267211303114891
Batch  71  loss:  0.012186267413198948
Batch  81  loss:  0.005295903421938419
Batch  91  loss:  0.009164733812212944
Batch  101  loss:  0.009070826694369316
Batch  111  loss:  0.00953453779220581
Batch  121  loss:  0.0051281992346048355
Batch  131  loss:  0.00694571016356349
Batch  141  loss:  0.003789963899180293
Batch  151  loss:  0.0031358697451651096
Batch  161  loss:  0.004979421850293875
Batch  171  loss:  0.010119861923158169
Batch  181  loss:  0.0025572588201612234
Batch  191  loss:  0.0026904470287263393
Validation on real data: 
LOSS supervised-train 0.007600792410084978, valid 0.009767172858119011
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0028638311196118593
Batch  11  loss:  0.007846266962587833
Batch  21  loss:  0.0025038181338459253
Batch  31  loss:  0.0035131536424160004
Batch  41  loss:  0.009824426844716072
Batch  51  loss:  0.0018450709758326411
Batch  61  loss:  0.022324956953525543
Batch  71  loss:  0.017097575590014458
Batch  81  loss:  0.00723816966637969
Batch  91  loss:  0.003851388581097126
Batch  101  loss:  0.014432141557335854
Batch  111  loss:  0.008699405007064342
Batch  121  loss:  0.0076297312043607235
Batch  131  loss:  0.004434417933225632
Batch  141  loss:  0.0032999454997479916
Batch  151  loss:  0.015161532908678055
Batch  161  loss:  0.010375052690505981
Batch  171  loss:  0.004055439028888941
Batch  181  loss:  0.005245818756520748
Batch  191  loss:  0.0021763371769338846
Validation on real data: 
LOSS supervised-train 0.006799219125532545, valid 0.04695650935173035
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.003737804712727666
Batch  11  loss:  0.009222051128745079
Batch  21  loss:  0.0028365999460220337
Batch  31  loss:  0.024835774675011635
Batch  41  loss:  0.0037490827962756157
Batch  51  loss:  0.0019385481718927622
Batch  61  loss:  0.0026122387498617172
Batch  71  loss:  0.005757189355790615
Batch  81  loss:  0.003146996023133397
Batch  91  loss:  0.00719907321035862
Batch  101  loss:  0.008790041320025921
Batch  111  loss:  0.004805433098226786
Batch  121  loss:  0.0050275432877242565
Batch  131  loss:  0.00775505043566227
Batch  141  loss:  0.002606288529932499
Batch  151  loss:  0.0036964677274227142
Batch  161  loss:  0.0028474736027419567
Batch  171  loss:  0.0021044982131570578
Batch  181  loss:  0.005180797073990107
Batch  191  loss:  0.0026810369454324245
Validation on real data: 
LOSS supervised-train 0.007863720682798885, valid 0.03614768385887146
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.04122311994433403
Batch  11  loss:  0.006912825163453817
Batch  21  loss:  0.021700257435441017
Batch  31  loss:  0.004882389679551125
Batch  41  loss:  0.005584245081990957
Batch  51  loss:  0.0030991958919912577
Batch  61  loss:  0.021902266889810562
Batch  71  loss:  0.010687436908483505
Batch  81  loss:  0.005886641796678305
Batch  91  loss:  0.0029553137719631195
Batch  101  loss:  0.004298812709748745
Batch  111  loss:  0.0025161593221127987
Batch  121  loss:  0.0040932511910796165
Batch  131  loss:  0.005084805190563202
Batch  141  loss:  0.00427718460559845
Batch  151  loss:  0.004832811653614044
Batch  161  loss:  0.0033423625864088535
Batch  171  loss:  0.029259702190756798
Batch  181  loss:  0.00427214102819562
Batch  191  loss:  0.00288574886508286
Validation on real data: 
LOSS supervised-train 0.0067196201236220075, valid 0.042328912764787674
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.004367058631032705
Batch  11  loss:  0.004210924729704857
Batch  21  loss:  0.0038158963434398174
Batch  31  loss:  0.022025730460882187
Batch  41  loss:  0.014608073979616165
Batch  51  loss:  0.0019701977726072073
Batch  61  loss:  0.004414644557982683
Batch  71  loss:  0.005196092184633017
Batch  81  loss:  0.009101435542106628
Batch  91  loss:  0.0037461065221577883
Batch  101  loss:  0.00813518837094307
Batch  111  loss:  0.002937300130724907
Batch  121  loss:  0.007434655446559191
Batch  131  loss:  0.006126169580966234
Batch  141  loss:  0.00568460812792182
Batch  151  loss:  0.008290893398225307
Batch  161  loss:  0.003686476033180952
Batch  171  loss:  0.0022951001301407814
Batch  181  loss:  0.0022916237358003855
Batch  191  loss:  0.002964908489957452
Validation on real data: 
LOSS supervised-train 0.006075491641531698, valid 0.19497781991958618
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.011849732138216496
Batch  11  loss:  0.011940497905015945
Batch  21  loss:  0.0056377071887254715
Batch  31  loss:  0.004112713038921356
Batch  41  loss:  0.020684581249952316
Batch  51  loss:  0.003608760191127658
Batch  61  loss:  0.005390218459069729
Batch  71  loss:  0.003516875207424164
Batch  81  loss:  0.007233409211039543
Batch  91  loss:  0.0020441182423382998
Batch  101  loss:  0.0033752231393009424
Batch  111  loss:  0.006628596689552069
Batch  121  loss:  0.009416542947292328
Batch  131  loss:  0.0034585213288664818
Batch  141  loss:  0.01790800131857395
Batch  151  loss:  0.0036211747210472822
Batch  161  loss:  0.01724347658455372
Batch  171  loss:  0.004251014906913042
Batch  181  loss:  0.01935727894306183
Batch  191  loss:  0.017735641449689865
Validation on real data: 
LOSS supervised-train 0.008022667281911708, valid 0.053244225680828094
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.014092097990214825
Batch  11  loss:  0.006102582905441523
Batch  21  loss:  0.004134428221732378
Batch  31  loss:  0.002957563381642103
Batch  41  loss:  0.006305313669145107
Batch  51  loss:  0.003936721943318844
Batch  61  loss:  0.006373421289026737
Batch  71  loss:  0.010617248713970184
Batch  81  loss:  0.011046571657061577
Batch  91  loss:  0.004137312527745962
Batch  101  loss:  0.002829077187925577
Batch  111  loss:  0.008141394704580307
Batch  121  loss:  0.003306252183392644
Batch  131  loss:  0.005476124119013548
Batch  141  loss:  0.0238000750541687
Batch  151  loss:  0.002143508056178689
Batch  161  loss:  0.02850288338959217
Batch  171  loss:  0.03726756572723389
Batch  181  loss:  0.0031959586776793003
Batch  191  loss:  0.0027209343388676643
Validation on real data: 
LOSS supervised-train 0.008475892617134377, valid 0.10216349363327026
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.009694894775748253
Batch  11  loss:  0.005555693991482258
Batch  21  loss:  0.004636495374143124
Batch  31  loss:  0.017365457490086555
Batch  41  loss:  0.0068039498291909695
Batch  51  loss:  0.0034125600941479206
Batch  61  loss:  0.004702858626842499
Batch  71  loss:  0.02022218145430088
Batch  81  loss:  0.0055164676159620285
Batch  91  loss:  0.0045573292300105095
Batch  101  loss:  0.0040613929741084576
Batch  111  loss:  0.0040108091197907925
Batch  121  loss:  0.0027358755469322205
Batch  131  loss:  0.0071371374651789665
Batch  141  loss:  0.002581103006377816
Batch  151  loss:  0.026208922266960144
Batch  161  loss:  0.007743022404611111
Batch  171  loss:  0.002237874548882246
Batch  181  loss:  0.006111370399594307
Batch  191  loss:  0.003155226819217205
Validation on real data: 
LOSS supervised-train 0.006497138132108376, valid 0.0043118782341480255
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0076306527480483055
Batch  11  loss:  0.004715011455118656
Batch  21  loss:  0.002534198807552457
Batch  31  loss:  0.004428842104971409
Batch  41  loss:  0.006873710080981255
Batch  51  loss:  0.001997760497033596
Batch  61  loss:  0.0024436928797513247
Batch  71  loss:  0.0030302368104457855
Batch  81  loss:  0.0049140271730721
Batch  91  loss:  0.00405082805082202
Batch  101  loss:  0.0043559325858950615
Batch  111  loss:  0.00247899885289371
Batch  121  loss:  0.008114581927657127
Batch  131  loss:  0.0067828260362148285
Batch  141  loss:  0.005744666792452335
Batch  151  loss:  0.001654534600675106
Batch  161  loss:  0.002275726292282343
Batch  171  loss:  0.005157485604286194
Batch  181  loss:  0.0031528868712484837
Batch  191  loss:  0.0032680484000593424
Validation on real data: 
LOSS supervised-train 0.0050387396709993485, valid 0.03383380174636841
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0029764571227133274
Batch  11  loss:  0.004271804820746183
Batch  21  loss:  0.0026934051420539618
Batch  31  loss:  0.018096743151545525
Batch  41  loss:  0.0029999481048434973
Batch  51  loss:  0.001784400548785925
Batch  61  loss:  0.010695014148950577
Batch  71  loss:  0.004297562874853611
Batch  81  loss:  0.010851874016225338
Batch  91  loss:  0.0030708208214491606
Batch  101  loss:  0.005646174307912588
Batch  111  loss:  0.0024148141965270042
Batch  121  loss:  0.0038780909962952137
Batch  131  loss:  0.003467740025371313
Batch  141  loss:  0.004313006531447172
Batch  151  loss:  0.0022018018644303083
Batch  161  loss:  0.00250196922570467
Batch  171  loss:  0.002284766873344779
Batch  181  loss:  0.002425657818093896
Batch  191  loss:  0.0016115218168124557
Validation on real data: 
LOSS supervised-train 0.00445178784080781, valid 0.015224924311041832
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0036532911472022533
Batch  11  loss:  0.008881870657205582
Batch  21  loss:  0.003023117082193494
Batch  31  loss:  0.0015256264014169574
Batch  41  loss:  0.013666192069649696
Batch  51  loss:  0.00169066630769521
Batch  61  loss:  0.0338200144469738
Batch  71  loss:  0.011909835040569305
Batch  81  loss:  0.003134259022772312
Batch  91  loss:  0.003865155391395092
Batch  101  loss:  0.0016075533349066973
Batch  111  loss:  0.003757462138310075
Batch  121  loss:  0.0028313221409916878
Batch  131  loss:  0.00237356498837471
Batch  141  loss:  0.0021849125623703003
Batch  151  loss:  0.002705623162910342
Batch  161  loss:  0.0030254509765654802
Batch  171  loss:  0.002610304392874241
Batch  181  loss:  0.005911868531256914
Batch  191  loss:  0.001778359990566969
Validation on real data: 
LOSS supervised-train 0.003959701244602911, valid 0.024016734212636948
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.011188117787241936
Batch  11  loss:  0.003153243102133274
Batch  21  loss:  0.0020474682096391916
Batch  31  loss:  0.004655348137021065
Batch  41  loss:  0.0030403791461139917
Batch  51  loss:  0.002411683788523078
Batch  61  loss:  0.021360237151384354
Batch  71  loss:  0.0016264679143205285
Batch  81  loss:  0.0038383814971894026
Batch  91  loss:  0.0028578925412148237
Batch  101  loss:  0.006875301245599985
Batch  111  loss:  0.0018867251928895712
Batch  121  loss:  0.004001603461802006
Batch  131  loss:  0.004405474755913019
Batch  141  loss:  0.004525576252490282
Batch  151  loss:  0.0017712985863909125
Batch  161  loss:  0.0022478944156318903
Batch  171  loss:  0.0027619386091828346
Batch  181  loss:  0.003692412283271551
Batch  191  loss:  0.0012866717297583818
Validation on real data: 
LOSS supervised-train 0.003116375117388088, valid 0.024135584011673927
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.00199730577878654
Batch  11  loss:  0.0030582183972001076
Batch  21  loss:  0.0022082687355577946
Batch  31  loss:  0.0021117806900292635
Batch  41  loss:  0.0012685854453593493
Batch  51  loss:  0.003382517956197262
Batch  61  loss:  0.02082100510597229
Batch  71  loss:  0.012415248900651932
Batch  81  loss:  0.002791536273434758
Batch  91  loss:  0.015295024961233139
Batch  101  loss:  0.0018348967423662543
Batch  111  loss:  0.002512669423595071
Batch  121  loss:  0.0034710224717855453
Batch  131  loss:  0.01608254387974739
Batch  141  loss:  0.0014117152895778418
Batch  151  loss:  0.0018769013695418835
Batch  161  loss:  0.004745210986584425
Batch  171  loss:  0.0018102164613083005
Batch  181  loss:  0.0028130793944001198
Batch  191  loss:  0.0016381513560190797
Validation on real data: 
LOSS supervised-train 0.0035959764081053436, valid 0.05783182755112648
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.002260558307170868
Batch  11  loss:  0.0029838872142136097
Batch  21  loss:  0.002973016817122698
Batch  31  loss:  0.0022316952235996723
Batch  41  loss:  0.01515173725783825
Batch  51  loss:  0.0018291713204234838
Batch  61  loss:  0.007658348418772221
Batch  71  loss:  0.0022455023135989904
Batch  81  loss:  0.00943470187485218
Batch  91  loss:  0.00885381642729044
Batch  101  loss:  0.015113543719053268
Batch  111  loss:  0.002143255667760968
Batch  121  loss:  0.0048080566339194775
Batch  131  loss:  0.0027275055181235075
Batch  141  loss:  0.005081241484731436
Batch  151  loss:  0.0023116872180253267
Batch  161  loss:  0.002252445090562105
Batch  171  loss:  0.0031215946655720472
Batch  181  loss:  0.006662718951702118
Batch  191  loss:  0.001985906856134534
Validation on real data: 
LOSS supervised-train 0.003911467228317633, valid 0.024392200633883476
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0018320097588002682
Batch  11  loss:  0.00367148220539093
Batch  21  loss:  0.0013998630456626415
Batch  31  loss:  0.0016665880102664232
Batch  41  loss:  0.00181630440056324
Batch  51  loss:  0.0013264142908155918
Batch  61  loss:  0.001850987784564495
Batch  71  loss:  0.0024191667325794697
Batch  81  loss:  0.003368145087733865
Batch  91  loss:  0.0027280806098133326
Batch  101  loss:  0.0016264026053249836
Batch  111  loss:  0.0014194920659065247
Batch  121  loss:  0.004109631292521954
Batch  131  loss:  0.001674341969192028
Batch  141  loss:  0.0024714574683457613
Batch  151  loss:  0.001926966360770166
Batch  161  loss:  0.002705672523006797
Batch  171  loss:  0.002368299523368478
Batch  181  loss:  0.0012729923473671079
Batch  191  loss:  0.0019203094998374581
Validation on real data: 
LOSS supervised-train 0.002965359466325026, valid 0.010791662149131298
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.007859423756599426
Batch  11  loss:  0.0024263469967991114
Batch  21  loss:  0.0012158378958702087
Batch  31  loss:  0.0027680981438606977
Batch  41  loss:  0.0013849303359165788
Batch  51  loss:  0.002193297492340207
Batch  61  loss:  0.002126759849488735
Batch  71  loss:  0.00407159049063921
Batch  81  loss:  0.0033337408676743507
Batch  91  loss:  0.0017901191022247076
Batch  101  loss:  0.0024303339887410402
Batch  111  loss:  0.0015476252883672714
Batch  121  loss:  0.002578654559329152
Batch  131  loss:  0.005637385882437229
Batch  141  loss:  0.0015263613313436508
Batch  151  loss:  0.0014565631281584501
Batch  161  loss:  0.002472387859597802
Batch  171  loss:  0.0020140819251537323
Batch  181  loss:  0.0010760045843198895
Batch  191  loss:  0.004766996018588543
Validation on real data: 
LOSS supervised-train 0.0027845078462269157, valid 0.009818759746849537
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0018818241078406572
Batch  11  loss:  0.005770594347268343
Batch  21  loss:  0.0016645879950374365
Batch  31  loss:  0.002408130094408989
Batch  41  loss:  0.0011829922441393137
Batch  51  loss:  0.0015672048320993781
Batch  61  loss:  0.003669871250167489
Batch  71  loss:  0.008268038742244244
Batch  81  loss:  0.0020454414188861847
Batch  91  loss:  0.0014108334435150027
Batch  101  loss:  0.0011311974376440048
Batch  111  loss:  0.0011950735934078693
Batch  121  loss:  0.002383618615567684
Batch  131  loss:  0.0020551809575408697
Batch  141  loss:  0.0021784028504043818
Batch  151  loss:  0.0013645756989717484
Batch  161  loss:  0.003680191468447447
Batch  171  loss:  0.001466238871216774
Batch  181  loss:  0.003932098858058453
Batch  191  loss:  0.014497573487460613
Validation on real data: 
LOSS supervised-train 0.003025383109343238, valid 0.028044935315847397
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0037807903718203306
Batch  11  loss:  0.020379748195409775
Batch  21  loss:  0.0025596446357667446
Batch  31  loss:  0.0017936945660039783
Batch  41  loss:  0.0019775712862610817
Batch  51  loss:  0.0016698938561603427
Batch  61  loss:  0.020161714404821396
Batch  71  loss:  0.0034393444657325745
Batch  81  loss:  0.015095631591975689
Batch  91  loss:  0.0026531931944191456
Batch  101  loss:  0.001150882919318974
Batch  111  loss:  0.0028529916889965534
Batch  121  loss:  0.009563426487147808
Batch  131  loss:  0.007175061386078596
Batch  141  loss:  0.014731379225850105
Batch  151  loss:  0.002118326723575592
Batch  161  loss:  0.014281181618571281
Batch  171  loss:  0.005891234613955021
Batch  181  loss:  0.0015229322016239166
Batch  191  loss:  0.003378397785127163
Validation on real data: 
LOSS supervised-train 0.004805895567988046, valid 0.002438860945403576
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.002313492586836219
Batch  11  loss:  0.0035565884318202734
Batch  21  loss:  0.006583844777196646
Batch  31  loss:  0.003403489710763097
Batch  41  loss:  0.0016045077936723828
Batch  51  loss:  0.0016485400265082717
Batch  61  loss:  0.002966112457215786
Batch  71  loss:  0.009730994701385498
Batch  81  loss:  0.004924273584038019
Batch  91  loss:  0.002425109501928091
Batch  101  loss:  0.0018315860070288181
Batch  111  loss:  0.0022094242740422487
Batch  121  loss:  0.003883664496243
Batch  131  loss:  0.0016026743687689304
Batch  141  loss:  0.006383467931300402
Batch  151  loss:  0.0019456094596534967
Batch  161  loss:  0.0022611061576753855
Batch  171  loss:  0.002000565407797694
Batch  181  loss:  0.0014103756984695792
Batch  191  loss:  0.0017352967988699675
Validation on real data: 
LOSS supervised-train 0.0035308935056673364, valid 0.009738712571561337
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.001839210744947195
Batch  11  loss:  0.0030159871093928814
Batch  21  loss:  0.0016773202223703265
Batch  31  loss:  0.001029554521664977
Batch  41  loss:  0.0016348178032785654
Batch  51  loss:  0.003695578081533313
Batch  61  loss:  0.023149380460381508
Batch  71  loss:  0.001975473714992404
Batch  81  loss:  0.0021575288847088814
Batch  91  loss:  0.007367149461060762
Batch  101  loss:  0.0022665555588901043
Batch  111  loss:  0.0018971537938341498
Batch  121  loss:  0.01478280033916235
Batch  131  loss:  0.04646504670381546
Batch  141  loss:  0.006411164999008179
Batch  151  loss:  0.0029868304263800383
Batch  161  loss:  0.0026813328731805086
Batch  171  loss:  0.00881682988256216
Batch  181  loss:  0.010937136597931385
Batch  191  loss:  0.00994621217250824
Validation on real data: 
LOSS supervised-train 0.004985251080070157, valid 0.07049373537302017
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.004179256968200207
Batch  11  loss:  0.005653806962072849
Batch  21  loss:  0.004452970344573259
Batch  31  loss:  0.003990569151937962
Batch  41  loss:  0.00260405451990664
Batch  51  loss:  0.0024332860484719276
Batch  61  loss:  0.0017168563790619373
Batch  71  loss:  0.0064039635471999645
Batch  81  loss:  0.0024611938279122114
Batch  91  loss:  0.0038545499555766582
Batch  101  loss:  0.0076254080049693584
Batch  111  loss:  0.002121474128216505
Batch  121  loss:  0.002876207698136568
Batch  131  loss:  0.0020946082659065723
Batch  141  loss:  0.0023808605037629604
Batch  151  loss:  0.0018007587641477585
Batch  161  loss:  0.0018765252316370606
Batch  171  loss:  0.0014721336774528027
Batch  181  loss:  0.018632983788847923
Batch  191  loss:  0.0041342382319271564
Validation on real data: 
LOSS supervised-train 0.004090599720948376, valid 0.034629642963409424
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0023288943339139223
Batch  11  loss:  0.002768908627331257
Batch  21  loss:  0.0022616907954216003
Batch  31  loss:  0.0021131436806172132
Batch  41  loss:  0.0020440146327018738
Batch  51  loss:  0.0016086379764601588
Batch  61  loss:  0.002948695095255971
Batch  71  loss:  0.012483654543757439
Batch  81  loss:  0.0023701295722275972
Batch  91  loss:  0.0020146395545452833
Batch  101  loss:  0.0015652031870558858
Batch  111  loss:  0.0020432421006262302
Batch  121  loss:  0.0031559544149786234
Batch  131  loss:  0.0015281151281669736
Batch  141  loss:  0.003543214639648795
Batch  151  loss:  0.0016664129216223955
Batch  161  loss:  0.008817707188427448
Batch  171  loss:  0.002246496733278036
Batch  181  loss:  0.0016979273641481996
Batch  191  loss:  0.0015994327841326594
Validation on real data: 
LOSS supervised-train 0.0034725289116613565, valid 0.007295391522347927
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0015104793710634112
Batch  11  loss:  0.0028476407751441
Batch  21  loss:  0.0012522857869043946
Batch  31  loss:  0.0012867179466411471
Batch  41  loss:  0.0022683932911604643
Batch  51  loss:  0.0013875315198674798
Batch  61  loss:  0.021303856745362282
Batch  71  loss:  0.01818493753671646
Batch  81  loss:  0.002347197849303484
Batch  91  loss:  0.0013589438749477267
Batch  101  loss:  0.0029822001233696938
Batch  111  loss:  0.0011485159629955888
Batch  121  loss:  0.0027188307140022516
Batch  131  loss:  0.0014787156833335757
Batch  141  loss:  0.0014522752026095986
Batch  151  loss:  0.002529498189687729
Batch  161  loss:  0.006035579834133387
Batch  171  loss:  0.001367575372569263
Batch  181  loss:  0.0007978797657415271
Batch  191  loss:  0.0017174551030620933
Validation on real data: 
LOSS supervised-train 0.002591303047956899, valid 0.017595767974853516
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.005376453977078199
Batch  11  loss:  0.006241935305297375
Batch  21  loss:  0.0018515682313591242
Batch  31  loss:  0.0017007815185934305
Batch  41  loss:  0.0014874469488859177
Batch  51  loss:  0.002629517810419202
Batch  61  loss:  0.001432331744581461
Batch  71  loss:  0.0021081503946334124
Batch  81  loss:  0.003441400360316038
Batch  91  loss:  0.0016330027719959617
Batch  101  loss:  0.0013138436479493976
Batch  111  loss:  0.0015454415697604418
Batch  121  loss:  0.0021571617107838392
Batch  131  loss:  0.0018098850268870592
Batch  141  loss:  0.0054625761695206165
Batch  151  loss:  0.0014756089076399803
Batch  161  loss:  0.0031011702958494425
Batch  171  loss:  0.002030098345130682
Batch  181  loss:  0.003591697197407484
Batch  191  loss:  0.0018728194991126657
Validation on real data: 
LOSS supervised-train 0.0032626669463934376, valid 0.007551997900009155
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.012302247807383537
Batch  11  loss:  0.003563919570297003
Batch  21  loss:  0.002056606812402606
Batch  31  loss:  0.002843541791662574
Batch  41  loss:  0.0032579461112618446
Batch  51  loss:  0.0015366998268291354
Batch  61  loss:  0.0020716427825391293
Batch  71  loss:  0.0022965159732848406
Batch  81  loss:  0.003791829338297248
Batch  91  loss:  0.002925923792645335
Batch  101  loss:  0.0021495986729860306
Batch  111  loss:  0.0028137986082583666
Batch  121  loss:  0.0015288591384887695
Batch  131  loss:  0.002226656535640359
Batch  141  loss:  0.001688977936282754
Batch  151  loss:  0.00218777172267437
Batch  161  loss:  0.001496303011663258
Batch  171  loss:  0.002405143342912197
Batch  181  loss:  0.0011959836119785905
Batch  191  loss:  0.0018433131044730544
Validation on real data: 
LOSS supervised-train 0.0033299389595049435, valid 0.0019929620902985334
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.001830436522141099
Batch  11  loss:  0.006229662336409092
Batch  21  loss:  0.0014564284356310964
Batch  31  loss:  0.00323374941945076
Batch  41  loss:  0.001457334030419588
Batch  51  loss:  0.0015465633478015661
Batch  61  loss:  0.003786838846281171
Batch  71  loss:  0.003388888668268919
Batch  81  loss:  0.0026968042366206646
Batch  91  loss:  0.0034777738619595766
Batch  101  loss:  0.0011701103067025542
Batch  111  loss:  0.0018434904050081968
Batch  121  loss:  0.0014859751099720597
Batch  131  loss:  0.0014966087182983756
Batch  141  loss:  0.00502025755122304
Batch  151  loss:  0.0012456434778869152
Batch  161  loss:  0.0010115032782778144
Batch  171  loss:  0.0013315549585968256
Batch  181  loss:  0.001215980271808803
Batch  191  loss:  0.0011337685864418745
Validation on real data: 
LOSS supervised-train 0.002441134703112766, valid 0.01715651899576187
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.002056664554402232
Batch  11  loss:  0.002725328551605344
Batch  21  loss:  0.001492275740019977
Batch  31  loss:  0.0012704136315733194
Batch  41  loss:  0.00801857840269804
Batch  51  loss:  0.0013678085524588823
Batch  61  loss:  0.0012691555311903358
Batch  71  loss:  0.007586765568703413
Batch  81  loss:  0.0019581930246204138
Batch  91  loss:  0.0010051666758954525
Batch  101  loss:  0.0014413127209991217
Batch  111  loss:  0.0014964526053518057
Batch  121  loss:  0.004715156275779009
Batch  131  loss:  0.0016548363491892815
Batch  141  loss:  0.0009926093043759465
Batch  151  loss:  0.001433827681466937
Batch  161  loss:  0.001270615728572011
Batch  171  loss:  0.0033268730621784925
Batch  181  loss:  0.0011834630277007818
Batch  191  loss:  0.0017961508128792048
Validation on real data: 
LOSS supervised-train 0.0018062590045155957, valid 0.057542793452739716
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.001499114208854735
Batch  11  loss:  0.002529102610424161
Batch  21  loss:  0.0012418903643265367
Batch  31  loss:  0.005516356788575649
Batch  41  loss:  0.0010462980717420578
Batch  51  loss:  0.002626481233164668
Batch  61  loss:  0.0020461478270590305
Batch  71  loss:  0.001705178408883512
Batch  81  loss:  0.001683859620243311
Batch  91  loss:  0.0012124675558879972
Batch  101  loss:  0.002332968870177865
Batch  111  loss:  0.0009420060669071972
Batch  121  loss:  0.0015950289089232683
Batch  131  loss:  0.0010328160133212805
Batch  141  loss:  0.0007890243432484567
Batch  151  loss:  0.0017725330544635653
Batch  161  loss:  0.0015957048162817955
Batch  171  loss:  0.0013262864667922258
Batch  181  loss:  0.0010132481111213565
Batch  191  loss:  0.0016505139647051692
Validation on real data: 
LOSS supervised-train 0.0016857792666996828, valid 0.002601298503577709
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0016627084696665406
Batch  11  loss:  0.0018121025059372187
Batch  21  loss:  0.0013905043015256524
Batch  31  loss:  0.0010452937567606568
Batch  41  loss:  0.0009529222152195871
Batch  51  loss:  0.001091589918360114
Batch  61  loss:  0.0016797546995803714
Batch  71  loss:  0.0013631302863359451
Batch  81  loss:  0.0019226353615522385
Batch  91  loss:  0.0014076901134103537
Batch  101  loss:  0.0010323015740141273
Batch  111  loss:  0.0011063753627240658
Batch  121  loss:  0.0024540398735553026
Batch  131  loss:  0.0011378494091331959
Batch  141  loss:  0.0008144741295836866
Batch  151  loss:  0.0016635318752378225
Batch  161  loss:  0.002068408764898777
Batch  171  loss:  0.0013472033897414804
Batch  181  loss:  0.0014539267867803574
Batch  191  loss:  0.003162282519042492
Validation on real data: 
LOSS supervised-train 0.0016120245709316805, valid 0.01093154028058052
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0012332501355558634
Batch  11  loss:  0.0026893713511526585
Batch  21  loss:  0.002289176220074296
Batch  31  loss:  0.0012285792035982013
Batch  41  loss:  0.0019349638605490327
Batch  51  loss:  0.001678640372119844
Batch  61  loss:  0.0032049219589680433
Batch  71  loss:  0.0011762622743844986
Batch  81  loss:  0.0013289080234244466
Batch  91  loss:  0.0011238743318244815
Batch  101  loss:  0.0017486120341345668
Batch  111  loss:  0.001173469820059836
Batch  121  loss:  0.0018099024891853333
Batch  131  loss:  0.0012170405825600028
Batch  141  loss:  0.0011880025267601013
Batch  151  loss:  0.0013583480613306165
Batch  161  loss:  0.0039009067695587873
Batch  171  loss:  0.0016362109454348683
Batch  181  loss:  0.0027940827421844006
Batch  191  loss:  0.0010786185739561915
Validation on real data: 
LOSS supervised-train 0.0017603018347290345, valid 0.008548981510102749
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.001147312461398542
Batch  11  loss:  0.003709036624059081
Batch  21  loss:  0.0014603862073272467
Batch  31  loss:  0.0014821630902588367
Batch  41  loss:  0.0014259135350584984
Batch  51  loss:  0.0014635712141171098
Batch  61  loss:  0.0010530397994443774
Batch  71  loss:  0.0014408391434699297
Batch  81  loss:  0.001435073558241129
Batch  91  loss:  0.0013137876521795988
Batch  101  loss:  0.0010730396024882793
Batch  111  loss:  0.0009234757744707167
Batch  121  loss:  0.0012716364581137896
Batch  131  loss:  0.0009017755510285497
Batch  141  loss:  0.0010160753736272454
Batch  151  loss:  0.0016346505144611
Batch  161  loss:  0.0013573435135185719
Batch  171  loss:  0.0015929820947349072
Batch  181  loss:  0.0008438747026957572
Batch  191  loss:  0.0012322611873969436
Validation on real data: 
LOSS supervised-train 0.0014504774523084053, valid 0.014827502891421318
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0012679648352786899
Batch  11  loss:  0.001812764909118414
Batch  21  loss:  0.0020529376342892647
Batch  31  loss:  0.0012337967054918408
Batch  41  loss:  0.0008533494547009468
Batch  51  loss:  0.002211575163528323
Batch  61  loss:  0.0025287442840635777
Batch  71  loss:  0.014348003081977367
Batch  81  loss:  0.0014563890872523189
Batch  91  loss:  0.002649707719683647
Batch  101  loss:  0.004204781726002693
Batch  111  loss:  0.0010803448967635632
Batch  121  loss:  0.002906251233071089
Batch  131  loss:  0.0014929847093299031
Batch  141  loss:  0.0013056197203695774
Batch  151  loss:  0.0017346766544505954
Batch  161  loss:  0.0017359000630676746
Batch  171  loss:  0.0016611450118944049
Batch  181  loss:  0.000779413734562695
Batch  191  loss:  0.001073871273547411
Validation on real data: 
LOSS supervised-train 0.0018810520711122082, valid 0.04000025987625122
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0015628141118213534
Batch  11  loss:  0.0013027953682467341
Batch  21  loss:  0.0019147979328408837
Batch  31  loss:  0.0007179851527325809
Batch  41  loss:  0.001421458669938147
Batch  51  loss:  0.0014725830405950546
Batch  61  loss:  0.00141190888825804
Batch  71  loss:  0.0009555343422107399
Batch  81  loss:  0.0013846491929143667
Batch  91  loss:  0.0012448407942429185
Batch  101  loss:  0.0013563405955210328
Batch  111  loss:  0.001204988220706582
Batch  121  loss:  0.001383428112603724
Batch  131  loss:  0.001053492771461606
Batch  141  loss:  0.001534175593405962
Batch  151  loss:  0.002291891723871231
Batch  161  loss:  0.002985268132761121
Batch  171  loss:  0.0017929439200088382
Batch  181  loss:  0.001034802757203579
Batch  191  loss:  0.0010860157199203968
Validation on real data: 
LOSS supervised-train 0.0013422273375908844, valid 0.0017313652206212282
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0008598196436651051
Batch  11  loss:  0.001409114571288228
Batch  21  loss:  0.0017519044922664762
Batch  31  loss:  0.0009592966525815427
Batch  41  loss:  0.0014126362511888146
Batch  51  loss:  0.00141068734228611
Batch  61  loss:  0.0011969312326982617
Batch  71  loss:  0.0010720291174948215
Batch  81  loss:  0.0017141057178378105
Batch  91  loss:  0.0020708951633423567
Batch  101  loss:  0.0011616587871685624
Batch  111  loss:  0.0009906478226184845
Batch  121  loss:  0.001682833069935441
Batch  131  loss:  0.0013041043421253562
Batch  141  loss:  0.005840423051267862
Batch  151  loss:  0.0012962473556399345
Batch  161  loss:  0.0017075231298804283
Batch  171  loss:  0.0015082936733961105
Batch  181  loss:  0.0007377874571830034
Batch  191  loss:  0.000879825500305742
Validation on real data: 
LOSS supervised-train 0.0013683417835272849, valid 0.0017874226905405521
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0009974760469049215
Batch  11  loss:  0.0019193142652511597
Batch  21  loss:  0.0014287868980318308
Batch  31  loss:  0.001076334505341947
Batch  41  loss:  0.0010767095955088735
Batch  51  loss:  0.0018712144810706377
Batch  61  loss:  0.001536907278932631
Batch  71  loss:  0.0027252037543803453
Batch  81  loss:  0.0019360011210665107
Batch  91  loss:  0.0014094162033870816
Batch  101  loss:  0.0011641960591077805
Batch  111  loss:  0.001398822059854865
Batch  121  loss:  0.0013794845435768366
Batch  131  loss:  0.0030190199613571167
Batch  141  loss:  0.0009254877222701907
Batch  151  loss:  0.0011892832117155194
Batch  161  loss:  0.0009788267780095339
Batch  171  loss:  0.0009781973203644156
Batch  181  loss:  0.0009323490667156875
Batch  191  loss:  0.0010596817592158914
Validation on real data: 
LOSS supervised-train 0.0013420624076388777, valid 0.015552138909697533
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0008690674440003932
Batch  11  loss:  0.0016091731376945972
Batch  21  loss:  0.0018865945748984814
Batch  31  loss:  0.0010311439400538802
Batch  41  loss:  0.0015278825303539634
Batch  51  loss:  0.00205914955586195
Batch  61  loss:  0.001512413495220244
Batch  71  loss:  0.0013247824972495437
Batch  81  loss:  0.0017703428165987134
Batch  91  loss:  0.0015099506126716733
Batch  101  loss:  0.011095664463937283
Batch  111  loss:  0.001547242165543139
Batch  121  loss:  0.0023640121798962355
Batch  131  loss:  0.0013789321528747678
Batch  141  loss:  0.0011217748979106545
Batch  151  loss:  0.0011804199311882257
Batch  161  loss:  0.002499095629900694
Batch  171  loss:  0.0013875806471332908
Batch  181  loss:  0.0006722898688167334
Batch  191  loss:  0.0010394442360848188
Validation on real data: 
LOSS supervised-train 0.0016094298585085198, valid 0.025004751980304718
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0015303445979952812
Batch  11  loss:  0.0017915203934535384
Batch  21  loss:  0.0020865490660071373
Batch  31  loss:  0.0011585336178541183
Batch  41  loss:  0.0010253750951960683
Batch  51  loss:  0.0014670838136225939
Batch  61  loss:  0.0017271931283175945
Batch  71  loss:  0.002327000955119729
Batch  81  loss:  0.0017674016999080777
Batch  91  loss:  0.0008679903694428504
Batch  101  loss:  0.0011710675898939371
Batch  111  loss:  0.0011116854147985578
Batch  121  loss:  0.0015511608216911554
Batch  131  loss:  0.0012972429394721985
Batch  141  loss:  0.0008030897006392479
Batch  151  loss:  0.001399112050421536
Batch  161  loss:  0.0017813178710639477
Batch  171  loss:  0.0007991851889528334
Batch  181  loss:  0.00083922129124403
Batch  191  loss:  0.0008316155872307718
Validation on real data: 
LOSS supervised-train 0.001444176372315269, valid 0.01059368159621954
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0011959195835515857
Batch  11  loss:  0.0013811197131872177
Batch  21  loss:  0.0010238843970000744
Batch  31  loss:  0.001493163057602942
Batch  41  loss:  0.0009293529437854886
Batch  51  loss:  0.0011283234925940633
Batch  61  loss:  0.001261125784367323
Batch  71  loss:  0.0012828062754124403
Batch  81  loss:  0.0014924367424100637
Batch  91  loss:  0.001276114722713828
Batch  101  loss:  0.0013345610350370407
Batch  111  loss:  0.0009953374974429607
Batch  121  loss:  0.0014381340006366372
Batch  131  loss:  0.001197294914163649
Batch  141  loss:  0.000917717523407191
Batch  151  loss:  0.0013217288069427013
Batch  161  loss:  0.00132016243878752
Batch  171  loss:  0.001017673872411251
Batch  181  loss:  0.0009379031835123897
Batch  191  loss:  0.0008810852305032313
Validation on real data: 
LOSS supervised-train 0.001200132964295335, valid 0.007131445221602917
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0008336059399880469
Batch  11  loss:  0.00129554548766464
Batch  21  loss:  0.0014765880769118667
Batch  31  loss:  0.0006495479610748589
Batch  41  loss:  0.0006791751948185265
Batch  51  loss:  0.0007669480983167887
Batch  61  loss:  0.0009607912506908178
Batch  71  loss:  0.0008649991359561682
Batch  81  loss:  0.0007607779698446393
Batch  91  loss:  0.0011031992034986615
Batch  101  loss:  0.00115571403875947
Batch  111  loss:  0.0014614094980061054
Batch  121  loss:  0.0013708145124837756
Batch  131  loss:  0.0011955902446061373
Batch  141  loss:  0.001098739798180759
Batch  151  loss:  0.007034345529973507
Batch  161  loss:  0.0012073147809132934
Batch  171  loss:  0.0010349452495574951
Batch  181  loss:  0.0008843771065585315
Batch  191  loss:  0.0011412346502766013
Validation on real data: 
LOSS supervised-train 0.0012514292224659585, valid 0.004699273034930229
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0017668047221377492
Batch  11  loss:  0.0011928409803658724
Batch  21  loss:  0.0011676999274641275
Batch  31  loss:  0.0038282587192952633
Batch  41  loss:  0.002312231808900833
Batch  51  loss:  0.0012930601369589567
Batch  61  loss:  0.0012636196333914995
Batch  71  loss:  0.0011293410789221525
Batch  81  loss:  0.0011652439134195447
Batch  91  loss:  0.002581424079835415
Batch  101  loss:  0.0008670539828017354
Batch  111  loss:  0.0009072231478057802
Batch  121  loss:  0.0017289298120886087
Batch  131  loss:  0.0011843177489936352
Batch  141  loss:  0.0008809351711533964
Batch  151  loss:  0.001903165248222649
Batch  161  loss:  0.0013038137694820762
Batch  171  loss:  0.001296824892051518
Batch  181  loss:  0.0008932781056500971
Batch  191  loss:  0.000908997084479779
Validation on real data: 
LOSS supervised-train 0.0014946536398201715, valid 0.010658727958798409
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0010883337818086147
Batch  11  loss:  0.0008485226426273584
Batch  21  loss:  0.0009342966950498521
Batch  31  loss:  0.0008579540881328285
Batch  41  loss:  0.0007754586404189467
Batch  51  loss:  0.0011874071788042784
Batch  61  loss:  0.002101378981024027
Batch  71  loss:  0.0011893088230863214
Batch  81  loss:  0.0009537574369460344
Batch  91  loss:  0.0021602825727313757
Batch  101  loss:  0.0008856036583893001
Batch  111  loss:  0.0011681331088766456
Batch  121  loss:  0.0012214769376441836
Batch  131  loss:  0.001127874944359064
Batch  141  loss:  0.0018932479433715343
Batch  151  loss:  0.0013338869903236628
Batch  161  loss:  0.0013163493713364005
Batch  171  loss:  0.0011028447188436985
Batch  181  loss:  0.0009444589959457517
Batch  191  loss:  0.0009183809743262827
Validation on real data: 
LOSS supervised-train 0.0011596285199630073, valid 0.013710112310945988
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.000963234284427017
Batch  11  loss:  0.0013203666312620044
Batch  21  loss:  0.0010055936872959137
Batch  31  loss:  0.000828775460831821
Batch  41  loss:  0.0006720966775901616
Batch  51  loss:  0.001116006402298808
Batch  61  loss:  0.0012399237602949142
Batch  71  loss:  0.000995033304207027
Batch  81  loss:  0.0011034330818802118
Batch  91  loss:  0.000727448146790266
Batch  101  loss:  0.0018386873416602612
Batch  111  loss:  0.001395459403283894
Batch  121  loss:  0.0011674938723444939
Batch  131  loss:  0.0012707143323495984
Batch  141  loss:  0.0009513429831713438
Batch  151  loss:  0.001324069919064641
Batch  161  loss:  0.0013692002976313233
Batch  171  loss:  0.001152140204794705
Batch  181  loss:  0.0009618565090931952
Batch  191  loss:  0.0009317141375504434
Validation on real data: 
LOSS supervised-train 0.0011168183109839446, valid 0.0030190511606633663
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0016114322934299707
Batch  11  loss:  0.001062982133589685
Batch  21  loss:  0.0010834290878847241
Batch  31  loss:  0.0006452931556850672
Batch  41  loss:  0.0006545779760926962
Batch  51  loss:  0.0009787526214495301
Batch  61  loss:  0.001089015742763877
Batch  71  loss:  0.0011919515673071146
Batch  81  loss:  0.0009172835270874202
Batch  91  loss:  0.000982146942988038
Batch  101  loss:  0.0010214686626568437
Batch  111  loss:  0.0009869362693279982
Batch  121  loss:  0.0012945208000019193
Batch  131  loss:  0.0015274040633812547
Batch  141  loss:  0.0010445776861160994
Batch  151  loss:  0.001372362021356821
Batch  161  loss:  0.0013040065532550216
Batch  171  loss:  0.0010134328622370958
Batch  181  loss:  0.0007800354505889118
Batch  191  loss:  0.0008019247325137258
Validation on real data: 
LOSS supervised-train 0.0010593624392640777, valid 0.04436730593442917
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.001081466325558722
Batch  11  loss:  0.0010715253883972764
Batch  21  loss:  0.0012900858419016004
Batch  31  loss:  0.000537826563231647
Batch  41  loss:  0.000842622946947813
Batch  51  loss:  0.0009975002612918615
Batch  61  loss:  0.0008057450177147985
Batch  71  loss:  0.0010918447514995933
Batch  81  loss:  0.0006803087890148163
Batch  91  loss:  0.0013208683812990785
Batch  101  loss:  0.0011690251994878054
Batch  111  loss:  0.0012473277747631073
Batch  121  loss:  0.001326134311966598
Batch  131  loss:  0.001095545245334506
Batch  141  loss:  0.0010067772818729281
Batch  151  loss:  0.0011372071458026767
Batch  161  loss:  0.001047800062224269
Batch  171  loss:  0.0009233547607436776
Batch  181  loss:  0.0007904723170213401
Batch  191  loss:  0.000993735739029944
Validation on real data: 
LOSS supervised-train 0.0009892316811601631, valid 0.0023182916920632124
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0017322408966720104
Batch  11  loss:  0.0009342526318505406
Batch  21  loss:  0.0008606623159721494
Batch  31  loss:  0.0007312308880500495
Batch  41  loss:  0.0007662430289201438
Batch  51  loss:  0.001044773613102734
Batch  61  loss:  0.0007543519022874534
Batch  71  loss:  0.000682193145621568
Batch  81  loss:  0.0008126848842948675
Batch  91  loss:  0.0013629066525027156
Batch  101  loss:  0.0007897400646470487
Batch  111  loss:  0.0008836091728881001
Batch  121  loss:  0.0010800294112414122
Batch  131  loss:  0.0010913298465311527
Batch  141  loss:  0.0008017377112992108
Batch  151  loss:  0.0010832332773134112
Batch  161  loss:  0.0008496115333400667
Batch  171  loss:  0.0009410491911694407
Batch  181  loss:  0.0006481963791884482
Batch  191  loss:  0.0006832702201791108
Validation on real data: 
LOSS supervised-train 0.000950527949462412, valid 0.008650343865156174
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0009629051783122122
Batch  11  loss:  0.0009103855118155479
Batch  21  loss:  0.0008320934139192104
Batch  31  loss:  0.0006081898463889956
Batch  41  loss:  0.0008134573581628501
Batch  51  loss:  0.0007263175211846828
Batch  61  loss:  0.000679201097227633
Batch  71  loss:  0.0007754834368824959
Batch  81  loss:  0.0007784697227180004
Batch  91  loss:  0.0013579457299783826
Batch  101  loss:  0.0010585346026346087
Batch  111  loss:  0.0010450148256495595
Batch  121  loss:  0.0009970878018066287
Batch  131  loss:  0.001201904146000743
Batch  141  loss:  0.000860037631355226
Batch  151  loss:  0.000941468751989305
Batch  161  loss:  0.005807952955365181
Batch  171  loss:  0.0010855327127501369
Batch  181  loss:  0.000847162795253098
Batch  191  loss:  0.0012873188825324178
Validation on real data: 
LOSS supervised-train 0.0010039621530449948, valid 0.003932582214474678
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0007921537035144866
Batch  11  loss:  0.0029628353659063578
Batch  21  loss:  0.0010814097477123141
Batch  31  loss:  0.0006312975892797112
Batch  41  loss:  0.0004757320275530219
Batch  51  loss:  0.0007498065242543817
Batch  61  loss:  0.000835997867397964
Batch  71  loss:  0.0008468970190733671
Batch  81  loss:  0.0006340749678201973
Batch  91  loss:  0.0011855786433443427
Batch  101  loss:  0.0009019556455314159
Batch  111  loss:  0.0008739284821785986
Batch  121  loss:  0.001192458556033671
Batch  131  loss:  0.0009566838853061199
Batch  141  loss:  0.0008548184996470809
Batch  151  loss:  0.001276594353839755
Batch  161  loss:  0.0016287597827613354
Batch  171  loss:  0.0009652183507569134
Batch  181  loss:  0.0010042930953204632
Batch  191  loss:  0.0010000001639127731
Validation on real data: 
LOSS supervised-train 0.0009067187768232543, valid 0.020613299682736397
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0007953521562740207
Batch  11  loss:  0.0009817845420911908
Batch  21  loss:  0.0010868737008422613
Batch  31  loss:  0.0005934933433309197
Batch  41  loss:  0.0007529729628004134
Batch  51  loss:  0.0007414546562358737
Batch  61  loss:  0.0007987885037437081
Batch  71  loss:  0.0007522751111537218
Batch  81  loss:  0.0006549750687554479
Batch  91  loss:  0.000998396542854607
Batch  101  loss:  0.0008979345439001918
Batch  111  loss:  0.0010439105099067092
Batch  121  loss:  0.0010636950610205531
Batch  131  loss:  0.000876108999364078
Batch  141  loss:  0.0010290659265592694
Batch  151  loss:  0.0012442832812666893
Batch  161  loss:  0.001533974427729845
Batch  171  loss:  0.0006269190344028175
Batch  181  loss:  0.0012349929893389344
Batch  191  loss:  0.0010098912753164768
Validation on real data: 
LOSS supervised-train 0.0009314801236905623, valid 0.000751183251850307
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0006786207668483257
Batch  11  loss:  0.0013316009426489472
Batch  21  loss:  0.0009473668760620058
Batch  31  loss:  0.0007581615354865789
Batch  41  loss:  0.0008403538959100842
Batch  51  loss:  0.0008018809603527188
Batch  61  loss:  0.0009163579088635743
Batch  71  loss:  0.001121294335462153
Batch  81  loss:  0.0012654998572543263
Batch  91  loss:  0.0014094231883063912
Batch  101  loss:  0.0006270004087127745
Batch  111  loss:  0.001178708509542048
Batch  121  loss:  0.0008147814660333097
Batch  131  loss:  0.0008017614018172026
Batch  141  loss:  0.0006683274987153709
Batch  151  loss:  0.0010763342725113034
Batch  161  loss:  0.0009321707184426486
Batch  171  loss:  0.0009840428829193115
Batch  181  loss:  0.000976758310571313
Batch  191  loss:  0.0011721824994310737
Validation on real data: 
LOSS supervised-train 0.000901804898167029, valid 0.01621583290398121
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0009382772259414196
Batch  11  loss:  0.0007187580922618508
Batch  21  loss:  0.0034424045588821173
Batch  31  loss:  0.0004806791839655489
Batch  41  loss:  0.0007178718224167824
Batch  51  loss:  0.0016223345883190632
Batch  61  loss:  0.0005941297858953476
Batch  71  loss:  0.0007255865493789315
Batch  81  loss:  0.0006244066753424704
Batch  91  loss:  0.0012316961074247956
Batch  101  loss:  0.0008183312020264566
Batch  111  loss:  0.000736502930521965
Batch  121  loss:  0.0008845676202327013
Batch  131  loss:  0.0008124445448629558
Batch  141  loss:  0.0006298844236880541
Batch  151  loss:  0.0007697650580666959
Batch  161  loss:  0.0009007641347125173
Batch  171  loss:  0.001210588961839676
Batch  181  loss:  0.0009737337823025882
Batch  191  loss:  0.0016142514068633318
Validation on real data: 
LOSS supervised-train 0.0008591240167152137, valid 0.0018031077925115824
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0010779611766338348
Batch  11  loss:  0.0007858597091399133
Batch  21  loss:  0.0007959966314956546
Batch  31  loss:  0.0005894529167562723
Batch  41  loss:  0.0007375141140073538
Batch  51  loss:  0.000740491203032434
Batch  61  loss:  0.0009703683899715543
Batch  71  loss:  0.0008225470664910972
Batch  81  loss:  0.0005321330390870571
Batch  91  loss:  0.001243862323462963
Batch  101  loss:  0.000809656223282218
Batch  111  loss:  0.001406549010425806
Batch  121  loss:  0.0007492706063203514
Batch  131  loss:  0.001015112386085093
Batch  141  loss:  0.0007873236900195479
Batch  151  loss:  0.0012687664711847901
Batch  161  loss:  0.0008129538618959486
Batch  171  loss:  0.0007826933288015425
Batch  181  loss:  0.0009950025705620646
Batch  191  loss:  0.0008915703510865569
Validation on real data: 
LOSS supervised-train 0.0008834477874916047, valid 0.016671039164066315
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0009663739474490285
Batch  11  loss:  0.001246133353561163
Batch  21  loss:  0.0006781083066016436
Batch  31  loss:  0.0010369677329435945
Batch  41  loss:  0.0007020466728135943
Batch  51  loss:  0.0012489741202443838
Batch  61  loss:  0.0011286078952252865
Batch  71  loss:  0.0006681857048533857
Batch  81  loss:  0.0007716476102359593
Batch  91  loss:  0.0011506953742355108
Batch  101  loss:  0.0009059919975697994
Batch  111  loss:  0.0007068920531310141
Batch  121  loss:  0.0011759221088141203
Batch  131  loss:  0.0008908345480449498
Batch  141  loss:  0.0006280241650529206
Batch  151  loss:  0.0011193249374628067
Batch  161  loss:  0.0009274661424569786
Batch  171  loss:  0.0008253823616541922
Batch  181  loss:  0.0009242274682037532
Batch  191  loss:  0.0009295085328631103
Validation on real data: 
LOSS supervised-train 0.0009509308382985182, valid 0.0015167302917689085
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  vessel ; Model ID: 5c54100c798dd681bfeb646a8eadb57
--------------------
Training baseline regression model:  2022-03-30 11:41:57.422069
Detector:  point_transformer
Object:  vessel
--------------------
device is  cuda
--------------------
Number of trainable parameters:  903856
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.1333465725183487
Batch  11  loss:  0.09561043232679367
Batch  21  loss:  0.06916213035583496
Batch  31  loss:  0.03237761929631233
Batch  41  loss:  0.03134652599692345
Batch  51  loss:  0.014809533022344112
Batch  61  loss:  0.01776946522295475
Batch  71  loss:  0.019604090601205826
Batch  81  loss:  0.013092783279716969
Batch  91  loss:  0.006585977040231228
Batch  101  loss:  0.009160567075014114
Batch  111  loss:  0.010933061130344868
Batch  121  loss:  0.009095951914787292
Batch  131  loss:  0.010899608954787254
Batch  141  loss:  0.008565655909478664
Batch  151  loss:  0.0045063006691634655
Batch  161  loss:  0.003577984170988202
Batch  171  loss:  0.004990488290786743
Batch  181  loss:  0.006189445499330759
Batch  191  loss:  0.004108216147869825
Validation on real data: 
LOSS supervised-train 0.022625033953227104, valid 0.0025939377956092358
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.004121649544686079
Batch  11  loss:  0.007580512203276157
Batch  21  loss:  0.0033702226355671883
Batch  31  loss:  0.00415740255266428
Batch  41  loss:  0.0033866215962916613
Batch  51  loss:  0.0030536428093910217
Batch  61  loss:  0.004306570161134005
Batch  71  loss:  0.003938635345548391
Batch  81  loss:  0.004518814850598574
Batch  91  loss:  0.002105294493958354
Batch  101  loss:  0.001473438460379839
Batch  111  loss:  0.0026883285026997328
Batch  121  loss:  0.00222270330414176
Batch  131  loss:  0.0027669365517795086
Batch  141  loss:  0.0027724921237677336
Batch  151  loss:  0.0027424257714301348
Batch  161  loss:  0.003469757968559861
Batch  171  loss:  0.006724370177835226
Batch  181  loss:  0.00344820786267519
Batch  191  loss:  0.0022539361380040646
Validation on real data: 
LOSS supervised-train 0.003825791828567162, valid 0.0026237606070935726
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.003554253140464425
Batch  11  loss:  0.004734422545880079
Batch  21  loss:  0.0019821757450699806
Batch  31  loss:  0.002427326515316963
Batch  41  loss:  0.0033840197138488293
Batch  51  loss:  0.002685305429622531
Batch  61  loss:  0.003400434274226427
Batch  71  loss:  0.0026675050612539053
Batch  81  loss:  0.001879043411463499
Batch  91  loss:  0.00162506103515625
Batch  101  loss:  0.001571953995153308
Batch  111  loss:  0.0026270272210240364
Batch  121  loss:  0.002880565356463194
Batch  131  loss:  0.0024981300812214613
Batch  141  loss:  0.0018382301786914468
Batch  151  loss:  0.0020213413517922163
Batch  161  loss:  0.0020626785699278116
Batch  171  loss:  0.0029301855247467756
Batch  181  loss:  0.002194594591856003
Batch  191  loss:  0.0024994148407131433
Validation on real data: 
LOSS supervised-train 0.002564204180089291, valid 0.002043615560978651
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0030071749351918697
Batch  11  loss:  0.0035842007491737604
Batch  21  loss:  0.0014384048990905285
Batch  31  loss:  0.0026156699750572443
Batch  41  loss:  0.0029674952384084463
Batch  51  loss:  0.0012272556778043509
Batch  61  loss:  0.0024325808044523
Batch  71  loss:  0.0021145122591406107
Batch  81  loss:  0.000921514758374542
Batch  91  loss:  0.0013761880109086633
Batch  101  loss:  0.0017292214324697852
Batch  111  loss:  0.0020702641922980547
Batch  121  loss:  0.0022531922440975904
Batch  131  loss:  0.0017817227635532618
Batch  141  loss:  0.0013963120291009545
Batch  151  loss:  0.0011986703611910343
Batch  161  loss:  0.0017429465660825372
Batch  171  loss:  0.0023164558224380016
Batch  181  loss:  0.0017032891046255827
Batch  191  loss:  0.002043353859335184
Validation on real data: 
LOSS supervised-train 0.001974801997421309, valid 0.0014092077035456896
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0031997680198401213
Batch  11  loss:  0.0032231330405920744
Batch  21  loss:  0.0013981794472783804
Batch  31  loss:  0.0021975815761834383
Batch  41  loss:  0.0024377910885959864
Batch  51  loss:  0.0015608408721163869
Batch  61  loss:  0.0024305987171828747
Batch  71  loss:  0.00210691150277853
Batch  81  loss:  0.0010817068396136165
Batch  91  loss:  0.0012803941499441862
Batch  101  loss:  0.0014472391922026873
Batch  111  loss:  0.0018202090868726373
Batch  121  loss:  0.0015296816127374768
Batch  131  loss:  0.0013332071248441935
Batch  141  loss:  0.0010639788815751672
Batch  151  loss:  0.0012036747066304088
Batch  161  loss:  0.0009493445977568626
Batch  171  loss:  0.0020714958664029837
Batch  181  loss:  0.0021203244104981422
Batch  191  loss:  0.0019325711764395237
Validation on real data: 
LOSS supervised-train 0.0016744440476759337, valid 0.0009526858339086175
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.003382495604455471
Batch  11  loss:  0.0028144402895122766
Batch  21  loss:  0.000985326012596488
Batch  31  loss:  0.001825938350521028
Batch  41  loss:  0.0024969454389065504
Batch  51  loss:  0.00123233487829566
Batch  61  loss:  0.0020052397157996893
Batch  71  loss:  0.0014548735925927758
Batch  81  loss:  0.0009708178113214672
Batch  91  loss:  0.0011002804385498166
Batch  101  loss:  0.0012707741698250175
Batch  111  loss:  0.0016949456185102463
Batch  121  loss:  0.0013478518230840564
Batch  131  loss:  0.001126619870774448
Batch  141  loss:  0.001071607111953199
Batch  151  loss:  0.0010823888005688787
Batch  161  loss:  0.000784610107075423
Batch  171  loss:  0.0016210933681577444
Batch  181  loss:  0.0015530745731666684
Batch  191  loss:  0.0022074428852647543
Validation on real data: 
LOSS supervised-train 0.0014297835933393798, valid 0.0010890304110944271
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0028811891097575426
Batch  11  loss:  0.0022483894135802984
Batch  21  loss:  0.0008900777320377529
Batch  31  loss:  0.0019945157691836357
Batch  41  loss:  0.0017023421823978424
Batch  51  loss:  0.0012231026776134968
Batch  61  loss:  0.0014789887936785817
Batch  71  loss:  0.0014136434765532613
Batch  81  loss:  0.0008618225692771375
Batch  91  loss:  0.0009582341881468892
Batch  101  loss:  0.0007371649262495339
Batch  111  loss:  0.001087299664504826
Batch  121  loss:  0.0014780506025999784
Batch  131  loss:  0.0010442112106829882
Batch  141  loss:  0.0005583149613812566
Batch  151  loss:  0.0008369621355086565
Batch  161  loss:  0.0008883062400855124
Batch  171  loss:  0.0012562279589474201
Batch  181  loss:  0.0011691991239786148
Batch  191  loss:  0.001588301151059568
Validation on real data: 
LOSS supervised-train 0.0012202701295609586, valid 0.0011275972938165069
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.002240884816274047
Batch  11  loss:  0.0019174100598320365
Batch  21  loss:  0.0008451931062154472
Batch  31  loss:  0.001810747548006475
Batch  41  loss:  0.0019661420956254005
Batch  51  loss:  0.0010366583010181785
Batch  61  loss:  0.0017136683454737067
Batch  71  loss:  0.0018414458027109504
Batch  81  loss:  0.0006046691560186446
Batch  91  loss:  0.0006504784105345607
Batch  101  loss:  0.001094239647500217
Batch  111  loss:  0.0012091362150385976
Batch  121  loss:  0.0011876437347382307
Batch  131  loss:  0.0009934718254953623
Batch  141  loss:  0.0007250274065881968
Batch  151  loss:  0.0008817390771582723
Batch  161  loss:  0.0010727400658652186
Batch  171  loss:  0.001029334613122046
Batch  181  loss:  0.000903331907466054
Batch  191  loss:  0.0016190465539693832
Validation on real data: 
LOSS supervised-train 0.0011470207550155465, valid 0.000847482355311513
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.001981128240004182
Batch  11  loss:  0.0018454700475558639
Batch  21  loss:  0.0005985053721815348
Batch  31  loss:  0.0015629790723323822
Batch  41  loss:  0.0013491016579791903
Batch  51  loss:  0.0008357707993127406
Batch  61  loss:  0.001577557297423482
Batch  71  loss:  0.0013885268708691
Batch  81  loss:  0.0006318169762380421
Batch  91  loss:  0.0006377961835823953
Batch  101  loss:  0.0008360704523511231
Batch  111  loss:  0.0011014373740181327
Batch  121  loss:  0.001460945000872016
Batch  131  loss:  0.0009070364758372307
Batch  141  loss:  0.0006531538092531264
Batch  151  loss:  0.0008015419589355588
Batch  161  loss:  0.0006244581891223788
Batch  171  loss:  0.0007083032396622002
Batch  181  loss:  0.001101034227758646
Batch  191  loss:  0.001265672966837883
Validation on real data: 
LOSS supervised-train 0.001010774182359455, valid 0.0008188997162505984
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0016988066490739584
Batch  11  loss:  0.0016432265983894467
Batch  21  loss:  0.0005149661446921527
Batch  31  loss:  0.0012791827321052551
Batch  41  loss:  0.0013840594328939915
Batch  51  loss:  0.0006585448863916099
Batch  61  loss:  0.0014069548342376947
Batch  71  loss:  0.00126015511341393
Batch  81  loss:  0.0006274568731896579
Batch  91  loss:  0.000856969621963799
Batch  101  loss:  0.0007836763979867101
Batch  111  loss:  0.0009077867725864053
Batch  121  loss:  0.0009062757017090917
Batch  131  loss:  0.0007744397153146565
Batch  141  loss:  0.0006335455691441894
Batch  151  loss:  0.0007441831403411925
Batch  161  loss:  0.0006809414480812848
Batch  171  loss:  0.0010465309023857117
Batch  181  loss:  0.0013172682374715805
Batch  191  loss:  0.0014293816639110446
Validation on real data: 
LOSS supervised-train 0.0008950537870987318, valid 0.0006298180087469518
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.001663577277213335
Batch  11  loss:  0.0014311252161860466
Batch  21  loss:  0.0005797739722765982
Batch  31  loss:  0.001137524377554655
Batch  41  loss:  0.0009712617611512542
Batch  51  loss:  0.000592832628171891
Batch  61  loss:  0.0014916093787178397
Batch  71  loss:  0.0010174426715821028
Batch  81  loss:  0.0006578738102689385
Batch  91  loss:  0.0006996773299761117
Batch  101  loss:  0.0005973777733743191
Batch  111  loss:  0.0008778569172136486
Batch  121  loss:  0.0007290481589734554
Batch  131  loss:  0.0008623746689409018
Batch  141  loss:  0.0004908288829028606
Batch  151  loss:  0.0006037205457687378
Batch  161  loss:  0.0006013790261931717
Batch  171  loss:  0.0007706136675551534
Batch  181  loss:  0.0006926677306182683
Batch  191  loss:  0.001293339068070054
Validation on real data: 
LOSS supervised-train 0.0008298416188335978, valid 0.0006598642212338746
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0013783425092697144
Batch  11  loss:  0.0012741296086460352
Batch  21  loss:  0.000683592923451215
Batch  31  loss:  0.0011925507569685578
Batch  41  loss:  0.000993863446637988
Batch  51  loss:  0.0004724286845885217
Batch  61  loss:  0.0012437248369678855
Batch  71  loss:  0.0008668520604260266
Batch  81  loss:  0.0005458646919578314
Batch  91  loss:  0.0006892801611684263
Batch  101  loss:  0.0005936525412835181
Batch  111  loss:  0.0009723185794427991
Batch  121  loss:  0.0007036722963675857
Batch  131  loss:  0.0005313266883604228
Batch  141  loss:  0.000619778991676867
Batch  151  loss:  0.0008453791961073875
Batch  161  loss:  0.000415731017710641
Batch  171  loss:  0.0005140145658515394
Batch  181  loss:  0.0007349785882979631
Batch  191  loss:  0.0010835514403879642
Validation on real data: 
LOSS supervised-train 0.0007528443752380554, valid 0.0005138817941769958
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.001374520012177527
Batch  11  loss:  0.001079489360563457
Batch  21  loss:  0.00039468694012612104
Batch  31  loss:  0.0012255392502993345
Batch  41  loss:  0.0007590503664687276
Batch  51  loss:  0.0004225336597301066
Batch  61  loss:  0.0010440460173413157
Batch  71  loss:  0.0011713296407833695
Batch  81  loss:  0.0004643352294806391
Batch  91  loss:  0.0005660841707140207
Batch  101  loss:  0.0009714456391520798
Batch  111  loss:  0.0007389584789052606
Batch  121  loss:  0.0009829218033701181
Batch  131  loss:  0.0007129652658477426
Batch  141  loss:  0.0005097392131574452
Batch  151  loss:  0.0007138449000194669
Batch  161  loss:  0.00042936395038850605
Batch  171  loss:  0.000746284902561456
Batch  181  loss:  0.0009196882019750774
Batch  191  loss:  0.0011270369868725538
Validation on real data: 
LOSS supervised-train 0.0007183950269245543, valid 0.0005524031585082412
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.001093617407605052
Batch  11  loss:  0.0011522687273100019
Batch  21  loss:  0.0004976828349754214
Batch  31  loss:  0.0012003598967567086
Batch  41  loss:  0.0007816031575202942
Batch  51  loss:  0.00045408130972646177
Batch  61  loss:  0.001478658290579915
Batch  71  loss:  0.0011555146193131804
Batch  81  loss:  0.00046933951671235263
Batch  91  loss:  0.0005886002327315509
Batch  101  loss:  0.000891184201464057
Batch  111  loss:  0.0007014346774667501
Batch  121  loss:  0.0006319559179246426
Batch  131  loss:  0.0006817585090175271
Batch  141  loss:  0.0005327029502950609
Batch  151  loss:  0.0006466920021921396
Batch  161  loss:  0.0004697820695582777
Batch  171  loss:  0.00048141120350919664
Batch  181  loss:  0.0006291528115980327
Batch  191  loss:  0.001139795291237533
Validation on real data: 
LOSS supervised-train 0.0006935828999849036, valid 0.0006321766413748264
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0009851723443716764
Batch  11  loss:  0.0010647904127836227
Batch  21  loss:  0.0004068008856847882
Batch  31  loss:  0.0009247054113075137
Batch  41  loss:  0.000889499147888273
Batch  51  loss:  0.00036420015385374427
Batch  61  loss:  0.0013628581073135138
Batch  71  loss:  0.0010318420827388763
Batch  81  loss:  0.00046650084550492465
Batch  91  loss:  0.0005415063351392746
Batch  101  loss:  0.000537655723746866
Batch  111  loss:  0.0006923016626387835
Batch  121  loss:  0.0005410016165114939
Batch  131  loss:  0.000541732762940228
Batch  141  loss:  0.0004383181221783161
Batch  151  loss:  0.0005465901922434568
Batch  161  loss:  0.0004005649534519762
Batch  171  loss:  0.0004305456532165408
Batch  181  loss:  0.0005764834932051599
Batch  191  loss:  0.0009715122287161648
Validation on real data: 
LOSS supervised-train 0.0006285206579195801, valid 0.0004851216508541256
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0010203326819464564
Batch  11  loss:  0.0009007998160086572
Batch  21  loss:  0.00036101191653870046
Batch  31  loss:  0.001095570856705308
Batch  41  loss:  0.0007454261649399996
Batch  51  loss:  0.00036943162558600307
Batch  61  loss:  0.001117110252380371
Batch  71  loss:  0.0007795295678079128
Batch  81  loss:  0.00044224390876479447
Batch  91  loss:  0.00053322350140661
Batch  101  loss:  0.0006211654399521649
Batch  111  loss:  0.0006512303370982409
Batch  121  loss:  0.0006351466290652752
Batch  131  loss:  0.0005041754338890314
Batch  141  loss:  0.0005345430108718574
Batch  151  loss:  0.0005380288930609822
Batch  161  loss:  0.0004545592819340527
Batch  171  loss:  0.00046124844811856747
Batch  181  loss:  0.0004554859478957951
Batch  191  loss:  0.0008328623371198773
Validation on real data: 
LOSS supervised-train 0.000576368992915377, valid 0.0005049939500167966
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0010432701092213392
Batch  11  loss:  0.0009971213294193149
Batch  21  loss:  0.000282473920378834
Batch  31  loss:  0.0007635853835381567
Batch  41  loss:  0.0008473091293126345
Batch  51  loss:  0.00047363690100610256
Batch  61  loss:  0.0009969568345695734
Batch  71  loss:  0.000744176039006561
Batch  81  loss:  0.0003807011526077986
Batch  91  loss:  0.000613361073192209
Batch  101  loss:  0.0005908773164264858
Batch  111  loss:  0.0005342542426660657
Batch  121  loss:  0.0005342197837308049
Batch  131  loss:  0.0005795573233626783
Batch  141  loss:  0.0005386101547628641
Batch  151  loss:  0.0005800530198030174
Batch  161  loss:  0.00039659522008150816
Batch  171  loss:  0.0004464821540750563
Batch  181  loss:  0.0005686123622581363
Batch  191  loss:  0.0008006511488929391
Validation on real data: 
LOSS supervised-train 0.0005637167905661045, valid 0.0004957188502885401
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0008092375937849283
Batch  11  loss:  0.0009151517879217863
Batch  21  loss:  0.0002544004819355905
Batch  31  loss:  0.0008668774389661849
Batch  41  loss:  0.0006927478825673461
Batch  51  loss:  0.00033163881744258106
Batch  61  loss:  0.0009567297529429197
Batch  71  loss:  0.0005916829686611891
Batch  81  loss:  0.00035296438727527857
Batch  91  loss:  0.0005061682313680649
Batch  101  loss:  0.0005515112425200641
Batch  111  loss:  0.0005572069203481078
Batch  121  loss:  0.00046148954425007105
Batch  131  loss:  0.0004822065820917487
Batch  141  loss:  0.0004171175532974303
Batch  151  loss:  0.00041910968138836324
Batch  161  loss:  0.00034578298800624907
Batch  171  loss:  0.00046459745499305427
Batch  181  loss:  0.00045836871140636504
Batch  191  loss:  0.0007437878521159291
Validation on real data: 
LOSS supervised-train 0.0005180596934224013, valid 0.0004462313081603497
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0007425645017065108
Batch  11  loss:  0.0007444213842973113
Batch  21  loss:  0.0003508035733830184
Batch  31  loss:  0.0006702013197354972
Batch  41  loss:  0.0007271096110343933
Batch  51  loss:  0.000396907766116783
Batch  61  loss:  0.0008609605138190091
Batch  71  loss:  0.0007857920718379319
Batch  81  loss:  0.0004047254042234272
Batch  91  loss:  0.0005049958126619458
Batch  101  loss:  0.00038583483546972275
Batch  111  loss:  0.0006072999094612896
Batch  121  loss:  0.0004135096678510308
Batch  131  loss:  0.00061061792075634
Batch  141  loss:  0.0005593905225396156
Batch  151  loss:  0.00046815877431072295
Batch  161  loss:  0.00034513429272919893
Batch  171  loss:  0.00037840468576177955
Batch  181  loss:  0.0004464220255613327
Batch  191  loss:  0.0007493057637475431
Validation on real data: 
LOSS supervised-train 0.0005113789283495862, valid 0.0004017110913991928
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0007623641868121922
Batch  11  loss:  0.0006286461721174419
Batch  21  loss:  0.0003008116618730128
Batch  31  loss:  0.0006880205473862588
Batch  41  loss:  0.0005666980869136751
Batch  51  loss:  0.00030466169118881226
Batch  61  loss:  0.0010843898635357618
Batch  71  loss:  0.0007394388085231185
Batch  81  loss:  0.00027147959917783737
Batch  91  loss:  0.0004968330613337457
Batch  101  loss:  0.00039858033414930105
Batch  111  loss:  0.0005942836287431419
Batch  121  loss:  0.0003886691411025822
Batch  131  loss:  0.0004911536234430969
Batch  141  loss:  0.00037935448926873505
Batch  151  loss:  0.0004735575639642775
Batch  161  loss:  0.0003548134700395167
Batch  171  loss:  0.0003709112643264234
Batch  181  loss:  0.0004627176676876843
Batch  191  loss:  0.0007018471951596439
Validation on real data: 
LOSS supervised-train 0.00047462616363191045, valid 0.00036791182355955243
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0006118682795204222
Batch  11  loss:  0.000744383898563683
Batch  21  loss:  0.00033443851862102747
Batch  31  loss:  0.0006439585704356432
Batch  41  loss:  0.0005955739761702716
Batch  51  loss:  0.00026761143817566335
Batch  61  loss:  0.0008439063676632941
Batch  71  loss:  0.0006036016857251525
Batch  81  loss:  0.0003154978039674461
Batch  91  loss:  0.000533581362105906
Batch  101  loss:  0.00029378829640336335
Batch  111  loss:  0.00048556007095612586
Batch  121  loss:  0.0006314615020528436
Batch  131  loss:  0.0005134458770044148
Batch  141  loss:  0.0003885048208758235
Batch  151  loss:  0.0005257493467070162
Batch  161  loss:  0.0003800202684942633
Batch  171  loss:  0.0003501423343550414
Batch  181  loss:  0.0005514576332643628
Batch  191  loss:  0.0005605690530501306
Validation on real data: 
LOSS supervised-train 0.00045636435927008277, valid 0.0003392005746718496
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0005865053390152752
Batch  11  loss:  0.0005464198184199631
Batch  21  loss:  0.00027609014068730175
Batch  31  loss:  0.000665202853269875
Batch  41  loss:  0.0005858722142875195
Batch  51  loss:  0.00035803430364467204
Batch  61  loss:  0.000796763866674155
Batch  71  loss:  0.0005185913760215044
Batch  81  loss:  0.0003885095356963575
Batch  91  loss:  0.00046464777551591396
Batch  101  loss:  0.0004766220226883888
Batch  111  loss:  0.0005074914661236107
Batch  121  loss:  0.0003797627578023821
Batch  131  loss:  0.0003886608756147325
Batch  141  loss:  0.00033074687235057354
Batch  151  loss:  0.000373647955711931
Batch  161  loss:  0.0004122453974559903
Batch  171  loss:  0.000359164405381307
Batch  181  loss:  0.0004821026523131877
Batch  191  loss:  0.0005378109635785222
Validation on real data: 
LOSS supervised-train 0.0004385843095951714, valid 0.00039687901153229177
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0007789910305291414
Batch  11  loss:  0.0005193780525587499
Batch  21  loss:  0.00039877547533251345
Batch  31  loss:  0.0006701926467940211
Batch  41  loss:  0.0004751903179567307
Batch  51  loss:  0.00028292075148783624
Batch  61  loss:  0.000812376441899687
Batch  71  loss:  0.000539398577529937
Batch  81  loss:  0.00029588257893919945
Batch  91  loss:  0.0004632788768503815
Batch  101  loss:  0.0003792542847804725
Batch  111  loss:  0.00037609864375554025
Batch  121  loss:  0.00044166072621010244
Batch  131  loss:  0.00038185250014066696
Batch  141  loss:  0.0003434676618780941
Batch  151  loss:  0.0003565421502571553
Batch  161  loss:  0.00035715423291549087
Batch  171  loss:  0.00032498492510057986
Batch  181  loss:  0.0003980811161454767
Batch  191  loss:  0.0007271325448527932
Validation on real data: 
LOSS supervised-train 0.00043727358846808786, valid 0.00028248102171346545
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0005390412406995893
Batch  11  loss:  0.000524803763255477
Batch  21  loss:  0.00024969223886728287
Batch  31  loss:  0.000654820934869349
Batch  41  loss:  0.00047548016300424933
Batch  51  loss:  0.0002646115026436746
Batch  61  loss:  0.0006379534024745226
Batch  71  loss:  0.0005778941558673978
Batch  81  loss:  0.0002897877129726112
Batch  91  loss:  0.0004134822520427406
Batch  101  loss:  0.000243188624153845
Batch  111  loss:  0.00035796663723886013
Batch  121  loss:  0.0004056759353261441
Batch  131  loss:  0.00040744777652435005
Batch  141  loss:  0.0004187867743894458
Batch  151  loss:  0.0002663868654053658
Batch  161  loss:  0.0002502381394151598
Batch  171  loss:  0.0002809943107422441
Batch  181  loss:  0.00042625507921911776
Batch  191  loss:  0.0006676454795524478
Validation on real data: 
LOSS supervised-train 0.0003922748257900821, valid 0.0003131222620140761
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0005753569421358407
Batch  11  loss:  0.0005567728076130152
Batch  21  loss:  0.0002545164024922997
Batch  31  loss:  0.0006611986318603158
Batch  41  loss:  0.0004252102808095515
Batch  51  loss:  0.0002559682761784643
Batch  61  loss:  0.0004879070329479873
Batch  71  loss:  0.0005654071574099362
Batch  81  loss:  0.0003434988029766828
Batch  91  loss:  0.00038129050517454743
Batch  101  loss:  0.00045389615115709603
Batch  111  loss:  0.0004095272743143141
Batch  121  loss:  0.0003789903421420604
Batch  131  loss:  0.0003536672447808087
Batch  141  loss:  0.0003651210281532258
Batch  151  loss:  0.0003435090184211731
Batch  161  loss:  0.000280033505987376
Batch  171  loss:  0.0002778719936031848
Batch  181  loss:  0.0003903213073499501
Batch  191  loss:  0.000536597624886781
Validation on real data: 
LOSS supervised-train 0.0003855023652431555, valid 0.0003077785950154066
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0005206958157941699
Batch  11  loss:  0.0005353515734896064
Batch  21  loss:  0.0003118423046544194
Batch  31  loss:  0.0005250334506854415
Batch  41  loss:  0.00038409492117352784
Batch  51  loss:  0.00022130632714834064
Batch  61  loss:  0.0008058391395024955
Batch  71  loss:  0.0005377433844842017
Batch  81  loss:  0.0003690973680932075
Batch  91  loss:  0.0003883540630340576
Batch  101  loss:  0.00026279804296791553
Batch  111  loss:  0.0004195640794932842
Batch  121  loss:  0.00034750610939227045
Batch  131  loss:  0.00035765839857049286
Batch  141  loss:  0.00032559825922362506
Batch  151  loss:  0.00034695398062467575
Batch  161  loss:  0.0002648000663612038
Batch  171  loss:  0.00035098797525279224
Batch  181  loss:  0.0003532402915880084
Batch  191  loss:  0.00041944519034586847
Validation on real data: 
LOSS supervised-train 0.0003812912201101426, valid 0.00027319169021211565
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0004804505442734808
Batch  11  loss:  0.0004844720533583313
Batch  21  loss:  0.00026929579325951636
Batch  31  loss:  0.000628663576208055
Batch  41  loss:  0.0003331576590426266
Batch  51  loss:  0.0002493479405529797
Batch  61  loss:  0.0007823399500921369
Batch  71  loss:  0.0005598118295893073
Batch  81  loss:  0.00021268297859933227
Batch  91  loss:  0.00037488172529265285
Batch  101  loss:  0.00025231685140170157
Batch  111  loss:  0.00033187816734425724
Batch  121  loss:  0.0003627775586210191
Batch  131  loss:  0.0003284670237917453
Batch  141  loss:  0.00029832631116732955
Batch  151  loss:  0.00033914216328412294
Batch  161  loss:  0.00022689014440402389
Batch  171  loss:  0.00024109959485940635
Batch  181  loss:  0.0003105666837655008
Batch  191  loss:  0.0004220855771563947
Validation on real data: 
LOSS supervised-train 0.000356954038652475, valid 0.00023503601551055908
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.00042609075899235904
Batch  11  loss:  0.0003950786485802382
Batch  21  loss:  0.00024509220384061337
Batch  31  loss:  0.0005388969439081848
Batch  41  loss:  0.00043124775402247906
Batch  51  loss:  0.000261816312558949
Batch  61  loss:  0.0007041451171971858
Batch  71  loss:  0.0005606142221949995
Batch  81  loss:  0.0002959399134851992
Batch  91  loss:  0.0004913314478471875
Batch  101  loss:  0.00029884427203796804
Batch  111  loss:  0.00039164922782219946
Batch  121  loss:  0.00038114673225209117
Batch  131  loss:  0.00033517592237330973
Batch  141  loss:  0.00031251658219844103
Batch  151  loss:  0.0002757562615443021
Batch  161  loss:  0.00032144851866178215
Batch  171  loss:  0.0002545975148677826
Batch  181  loss:  0.00030038991826586425
Batch  191  loss:  0.0005178567371331155
Validation on real data: 
LOSS supervised-train 0.0003566382020653691, valid 0.00041837652679532766
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0006216302863322198
Batch  11  loss:  0.0004801750765182078
Batch  21  loss:  0.00020025091362185776
Batch  31  loss:  0.0006324590067379177
Batch  41  loss:  0.00035288563231006265
Batch  51  loss:  0.000249125441769138
Batch  61  loss:  0.0005012228502891958
Batch  71  loss:  0.00044523910037241876
Batch  81  loss:  0.00029335543513298035
Batch  91  loss:  0.0002840687520802021
Batch  101  loss:  0.00022602707031182945
Batch  111  loss:  0.0002869307645596564
Batch  121  loss:  0.0003408541379030794
Batch  131  loss:  0.0002976692339871079
Batch  141  loss:  0.0003287099243607372
Batch  151  loss:  0.0003073532716371119
Batch  161  loss:  0.00026097349473275244
Batch  171  loss:  0.0002972628572024405
Batch  181  loss:  0.0003156609309371561
Batch  191  loss:  0.0005271098343655467
Validation on real data: 
LOSS supervised-train 0.00034102441160939635, valid 0.0002814194594975561
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0004492748121265322
Batch  11  loss:  0.00042786874109879136
Batch  21  loss:  0.0002289896219735965
Batch  31  loss:  0.0004811461840290576
Batch  41  loss:  0.0003992959682364017
Batch  51  loss:  0.0002837689535226673
Batch  61  loss:  0.0006524947239086032
Batch  71  loss:  0.0004064052482135594
Batch  81  loss:  0.00027537342975847423
Batch  91  loss:  0.00035398255567997694
Batch  101  loss:  0.00022287607134785503
Batch  111  loss:  0.00031005023629404604
Batch  121  loss:  0.00032519627711735666
Batch  131  loss:  0.00044793655979447067
Batch  141  loss:  0.00033009875915013254
Batch  151  loss:  0.0002943518047686666
Batch  161  loss:  0.0003423533635213971
Batch  171  loss:  0.0002183356846217066
Batch  181  loss:  0.0002990752400364727
Batch  191  loss:  0.00037595885805785656
Validation on real data: 
LOSS supervised-train 0.00033737108737113885, valid 0.00031139340717345476
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00035677789128385484
Batch  11  loss:  0.0004483968950808048
Batch  21  loss:  0.0002688799286261201
Batch  31  loss:  0.0005619183648377657
Batch  41  loss:  0.00031851272797212005
Batch  51  loss:  0.00019777583656832576
Batch  61  loss:  0.00041241510189138353
Batch  71  loss:  0.00040641461964696646
Batch  81  loss:  0.00026972556952387094
Batch  91  loss:  0.0003637672052718699
Batch  101  loss:  0.0002616647689137608
Batch  111  loss:  0.0003098584129475057
Batch  121  loss:  0.00033022044226527214
Batch  131  loss:  0.0003518186858855188
Batch  141  loss:  0.0003618045011535287
Batch  151  loss:  0.0002692437847144902
Batch  161  loss:  0.00026053644251078367
Batch  171  loss:  0.0002548151824157685
Batch  181  loss:  0.0002914663346018642
Batch  191  loss:  0.0003551440895535052
Validation on real data: 
LOSS supervised-train 0.0003403181315661641, valid 0.0002905825967900455
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0003931082028429955
Batch  11  loss:  0.0003711289318744093
Batch  21  loss:  0.0001963921240530908
Batch  31  loss:  0.0004159257805440575
Batch  41  loss:  0.00030892534414306283
Batch  51  loss:  0.00024076300906017423
Batch  61  loss:  0.0004723049933090806
Batch  71  loss:  0.0005027909646742046
Batch  81  loss:  0.00023868981224950403
Batch  91  loss:  0.00033625727519392967
Batch  101  loss:  0.0002777558984234929
Batch  111  loss:  0.000280404114164412
Batch  121  loss:  0.00029555271612480283
Batch  131  loss:  0.00034114959998987615
Batch  141  loss:  0.0002523436851333827
Batch  151  loss:  0.0003196151228621602
Batch  161  loss:  0.0002303888468304649
Batch  171  loss:  0.00022166645794641227
Batch  181  loss:  0.00028100222698412836
Batch  191  loss:  0.00041730012162588537
Validation on real data: 
LOSS supervised-train 0.0003135581473907223, valid 0.0002981321304105222
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00037816393887624145
Batch  11  loss:  0.00035844510421156883
Batch  21  loss:  0.0002093140792567283
Batch  31  loss:  0.00034135347232222557
Batch  41  loss:  0.0002658234734553844
Batch  51  loss:  0.00019588734721764922
Batch  61  loss:  0.0004502365191001445
Batch  71  loss:  0.00047418681788258255
Batch  81  loss:  0.00021106454369146377
Batch  91  loss:  0.0003300172393210232
Batch  101  loss:  0.0002700799668673426
Batch  111  loss:  0.00030693583539687097
Batch  121  loss:  0.0003315413778182119
Batch  131  loss:  0.00027351657627150416
Batch  141  loss:  0.0003200983046554029
Batch  151  loss:  0.00027480110293254256
Batch  161  loss:  0.0002657193981576711
Batch  171  loss:  0.0002821383823174983
Batch  181  loss:  0.00029479264048859477
Batch  191  loss:  0.0003630217397585511
Validation on real data: 
LOSS supervised-train 0.0003051896509714425, valid 0.0003453430254012346
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.000440832634922117
Batch  11  loss:  0.0003327965678181499
Batch  21  loss:  0.00019684094877447933
Batch  31  loss:  0.00045987358316779137
Batch  41  loss:  0.0003319692623335868
Batch  51  loss:  0.00027073692763224244
Batch  61  loss:  0.0005697554443031549
Batch  71  loss:  0.0004062361258547753
Batch  81  loss:  0.00026327534578740597
Batch  91  loss:  0.0002867251168936491
Batch  101  loss:  0.00042358110658824444
Batch  111  loss:  0.00023549071920569986
Batch  121  loss:  0.00028811104129999876
Batch  131  loss:  0.0003265896230004728
Batch  141  loss:  0.0002465711149852723
Batch  151  loss:  0.000242685477132909
Batch  161  loss:  0.00019284030713606626
Batch  171  loss:  0.00022212302428670228
Batch  181  loss:  0.0002941007842309773
Batch  191  loss:  0.00038861698703840375
Validation on real data: 
LOSS supervised-train 0.0002995400818326743, valid 0.0003341250994708389
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.00046430056681856513
Batch  11  loss:  0.00044954821350984275
Batch  21  loss:  0.00020357707398943603
Batch  31  loss:  0.00041165255242958665
Batch  41  loss:  0.000256480387179181
Batch  51  loss:  0.00022943604562897235
Batch  61  loss:  0.000473653431981802
Batch  71  loss:  0.00033120065927505493
Batch  81  loss:  0.00021871039643883705
Batch  91  loss:  0.00031928051612339914
Batch  101  loss:  0.0002455037902109325
Batch  111  loss:  0.0002442189143039286
Batch  121  loss:  0.00023469525331165642
Batch  131  loss:  0.0003436576807871461
Batch  141  loss:  0.0002559497661422938
Batch  151  loss:  0.00020367203978821635
Batch  161  loss:  0.0002023558336077258
Batch  171  loss:  0.00021373487834353
Batch  181  loss:  0.00020417984342202544
Batch  191  loss:  0.0004239856789354235
Validation on real data: 
LOSS supervised-train 0.0002909917953365948, valid 0.00023470025917049497
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00040196164627559483
Batch  11  loss:  0.0003417828702367842
Batch  21  loss:  0.00019736717513296753
Batch  31  loss:  0.0003918508591596037
Batch  41  loss:  0.0002846386923920363
Batch  51  loss:  0.00022192591859493405
Batch  61  loss:  0.0004592668847180903
Batch  71  loss:  0.00044891011202707887
Batch  81  loss:  0.0002094898372888565
Batch  91  loss:  0.00023349307593889534
Batch  101  loss:  0.00023836283071432263
Batch  111  loss:  0.00024074643442872912
Batch  121  loss:  0.0002615849953144789
Batch  131  loss:  0.0003402559377718717
Batch  141  loss:  0.00029359397012740374
Batch  151  loss:  0.0002070809860015288
Batch  161  loss:  0.00022799665748607367
Batch  171  loss:  0.000264310569036752
Batch  181  loss:  0.00023529866302851588
Batch  191  loss:  0.00041616178350523114
Validation on real data: 
LOSS supervised-train 0.0002799028466688469, valid 0.0002098314289469272
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.00036792916944250464
Batch  11  loss:  0.0003252037859056145
Batch  21  loss:  0.00017060860409401357
Batch  31  loss:  0.000374167924746871
Batch  41  loss:  0.00033206783700734377
Batch  51  loss:  0.0001883162185549736
Batch  61  loss:  0.0004081103834323585
Batch  71  loss:  0.0003701927198562771
Batch  81  loss:  0.0001689323253231123
Batch  91  loss:  0.0002281143533764407
Batch  101  loss:  0.00017979198310058564
Batch  111  loss:  0.00022453772544395179
Batch  121  loss:  0.0002506434393581003
Batch  131  loss:  0.00027859030524268746
Batch  141  loss:  0.0002064966392936185
Batch  151  loss:  0.0001914957247208804
Batch  161  loss:  0.00021730239677708596
Batch  171  loss:  0.00027138341101817787
Batch  181  loss:  0.0002457950613461435
Batch  191  loss:  0.0003152603458147496
Validation on real data: 
LOSS supervised-train 0.0002707862765964819, valid 0.00021602924971375614
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.000418932584580034
Batch  11  loss:  0.0003208436828572303
Batch  21  loss:  0.00024464205489493906
Batch  31  loss:  0.0004401408659759909
Batch  41  loss:  0.00027641080669127405
Batch  51  loss:  0.00019274569058325142
Batch  61  loss:  0.0004743101308122277
Batch  71  loss:  0.0005163071327842772
Batch  81  loss:  0.0002489800681360066
Batch  91  loss:  0.0002611521922517568
Batch  101  loss:  0.00025890959659591317
Batch  111  loss:  0.00022423805785365403
Batch  121  loss:  0.00018276504124514759
Batch  131  loss:  0.0002312777505721897
Batch  141  loss:  0.00023754827270749956
Batch  151  loss:  0.00023890080046840012
Batch  161  loss:  0.00022808345966041088
Batch  171  loss:  0.00018563169578555971
Batch  181  loss:  0.00018053648818749934
Batch  191  loss:  0.00046613020822405815
Validation on real data: 
LOSS supervised-train 0.0002670201817818452, valid 0.00017673283582553267
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.00036433219793252647
Batch  11  loss:  0.0003142308269161731
Batch  21  loss:  0.0002347972767893225
Batch  31  loss:  0.0003665079129859805
Batch  41  loss:  0.0003060544840991497
Batch  51  loss:  0.00016629538731649518
Batch  61  loss:  0.0003831913345493376
Batch  71  loss:  0.00031319877598434687
Batch  81  loss:  0.0002566275652498007
Batch  91  loss:  0.0003309274325147271
Batch  101  loss:  0.0002924914297182113
Batch  111  loss:  0.00025091704446822405
Batch  121  loss:  0.0003039516450371593
Batch  131  loss:  0.000268631090875715
Batch  141  loss:  0.00021165169891901314
Batch  151  loss:  0.00021279485372360796
Batch  161  loss:  0.0002803425013553351
Batch  171  loss:  0.00017875852063298225
Batch  181  loss:  0.0002926065935753286
Batch  191  loss:  0.00034136511385440826
Validation on real data: 
LOSS supervised-train 0.0002654806091595674, valid 0.0002156509435735643
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0003210452850908041
Batch  11  loss:  0.0002445252612233162
Batch  21  loss:  0.00020669832883868366
Batch  31  loss:  0.0003538343880791217
Batch  41  loss:  0.00031485530780628324
Batch  51  loss:  0.00021946719789411873
Batch  61  loss:  0.0004896610626019537
Batch  71  loss:  0.0002860128879547119
Batch  81  loss:  0.0002070218324661255
Batch  91  loss:  0.00034175021573901176
Batch  101  loss:  0.00025453633861616254
Batch  111  loss:  0.00031276053050532937
Batch  121  loss:  0.0001726814516587183
Batch  131  loss:  0.00026726521900855005
Batch  141  loss:  0.00019746136968024075
Batch  151  loss:  0.0002001334651140496
Batch  161  loss:  0.00022741875727660954
Batch  171  loss:  0.00015524509944953024
Batch  181  loss:  0.000267108523985371
Batch  191  loss:  0.00029641707078553736
Validation on real data: 
LOSS supervised-train 0.00026827817477169445, valid 0.0001801768084987998
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.00033940240973606706
Batch  11  loss:  0.000313716969685629
Batch  21  loss:  0.0001603404525667429
Batch  31  loss:  0.00033322753733955324
Batch  41  loss:  0.0002499139227438718
Batch  51  loss:  0.0001443449582438916
Batch  61  loss:  0.00043467991054058075
Batch  71  loss:  0.0003197056648787111
Batch  81  loss:  0.00016482282080687582
Batch  91  loss:  0.0002455577196087688
Batch  101  loss:  0.0001868080726126209
Batch  111  loss:  0.00020905765995848924
Batch  121  loss:  0.000236384934396483
Batch  131  loss:  0.00023681434686295688
Batch  141  loss:  0.0001603039272595197
Batch  151  loss:  0.00022889352112542838
Batch  161  loss:  0.00018576646107248962
Batch  171  loss:  0.0002183281467296183
Batch  181  loss:  0.00023705691273789853
Batch  191  loss:  0.00035159901017323136
Validation on real data: 
LOSS supervised-train 0.00025017976506205744, valid 0.00022355577675625682
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.000365200248779729
Batch  11  loss:  0.0002659309539012611
Batch  21  loss:  0.00017391947039868683
Batch  31  loss:  0.00033288373379036784
Batch  41  loss:  0.0002281327178934589
Batch  51  loss:  0.00017162147560156882
Batch  61  loss:  0.0003555194125510752
Batch  71  loss:  0.0003020867588929832
Batch  81  loss:  0.0001776419230736792
Batch  91  loss:  0.00029197640833444893
Batch  101  loss:  0.00020898641378153116
Batch  111  loss:  0.00020825520914513618
Batch  121  loss:  0.00023426297411788255
Batch  131  loss:  0.00026667834026739
Batch  141  loss:  0.0002442281402181834
Batch  151  loss:  0.00014951653429307044
Batch  161  loss:  0.00018858366820495576
Batch  171  loss:  0.00022837291180621833
Batch  181  loss:  0.00020310378749854863
Batch  191  loss:  0.00028273926000110805
Validation on real data: 
LOSS supervised-train 0.0002519319035491208, valid 0.0002155597903765738
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0003363708674442023
Batch  11  loss:  0.0003262578684370965
Batch  21  loss:  0.0001522376696811989
Batch  31  loss:  0.00030251912539824843
Batch  41  loss:  0.00016507288091816008
Batch  51  loss:  0.00016668089665472507
Batch  61  loss:  0.00039291215944103897
Batch  71  loss:  0.00030786555726081133
Batch  81  loss:  0.0001758878497639671
Batch  91  loss:  0.00023208446509670466
Batch  101  loss:  0.0002603049506433308
Batch  111  loss:  0.00016392263933084905
Batch  121  loss:  0.00024121475871652365
Batch  131  loss:  0.0002219396992586553
Batch  141  loss:  0.0001898771442938596
Batch  151  loss:  0.00019388381042517722
Batch  161  loss:  0.00021290355653036386
Batch  171  loss:  0.00021238415502011776
Batch  181  loss:  0.0002026979927904904
Batch  191  loss:  0.00039317941991612315
Validation on real data: 
LOSS supervised-train 0.000238230564063997, valid 0.00016470607079099864
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0003095005522482097
Batch  11  loss:  0.00027733654133044183
Batch  21  loss:  0.0001484080421505496
Batch  31  loss:  0.0003119073808193207
Batch  41  loss:  0.0002028475282713771
Batch  51  loss:  0.00016301088908221573
Batch  61  loss:  0.0003026050399057567
Batch  71  loss:  0.0003453719546087086
Batch  81  loss:  0.000214800049434416
Batch  91  loss:  0.00025941309286281466
Batch  101  loss:  0.00016266180318780243
Batch  111  loss:  0.000213209685171023
Batch  121  loss:  0.00021298084175214171
Batch  131  loss:  0.00024837497039698064
Batch  141  loss:  0.0002017889346461743
Batch  151  loss:  0.00018571382679510862
Batch  161  loss:  0.00018352034385316074
Batch  171  loss:  0.00022177967184688896
Batch  181  loss:  0.0002062007406493649
Batch  191  loss:  0.00027696695178747177
Validation on real data: 
LOSS supervised-train 0.00023498210379329975, valid 0.00018335413187742233
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0003106707881670445
Batch  11  loss:  0.0003046155034098774
Batch  21  loss:  0.00016736076213419437
Batch  31  loss:  0.0004065318207722157
Batch  41  loss:  0.00024094493710435927
Batch  51  loss:  0.00021620074403472245
Batch  61  loss:  0.000336741388309747
Batch  71  loss:  0.000366938766092062
Batch  81  loss:  0.00021077296696603298
Batch  91  loss:  0.00032833003206178546
Batch  101  loss:  0.00016835919814184308
Batch  111  loss:  0.00018547900253906846
Batch  121  loss:  0.00014527990424539894
Batch  131  loss:  0.000239360480918549
Batch  141  loss:  0.00025273632491007447
Batch  151  loss:  0.00019093271112069488
Batch  161  loss:  0.00018613066640682518
Batch  171  loss:  0.00024900774587877095
Batch  181  loss:  0.0002257220767205581
Batch  191  loss:  0.00028871919494122267
Validation on real data: 
LOSS supervised-train 0.00023115186268114486, valid 0.00020755946752615273
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0003033601678907871
Batch  11  loss:  0.0002406502899248153
Batch  21  loss:  0.00017524314171168953
Batch  31  loss:  0.00031497544841840863
Batch  41  loss:  0.00028675177600234747
Batch  51  loss:  0.00014531667693518102
Batch  61  loss:  0.0003172455180902034
Batch  71  loss:  0.0002735403541009873
Batch  81  loss:  0.00016475352458655834
Batch  91  loss:  0.0002404993720119819
Batch  101  loss:  0.0002963310689665377
Batch  111  loss:  0.0001961765083251521
Batch  121  loss:  0.0002255490981042385
Batch  131  loss:  0.0002808959106914699
Batch  141  loss:  0.0002629777300171554
Batch  151  loss:  0.0001807145745260641
Batch  161  loss:  0.00023408373817801476
Batch  171  loss:  0.0001786106440704316
Batch  181  loss:  0.0002367599809076637
Batch  191  loss:  0.0002490496262907982
Validation on real data: 
LOSS supervised-train 0.00023029614127153763, valid 0.00016104461974464357
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.00033299930510111153
Batch  11  loss:  0.00025590613950043917
Batch  21  loss:  0.00014296294830273837
Batch  31  loss:  0.00027393459458835423
Batch  41  loss:  0.00020605452300515026
Batch  51  loss:  0.00017725551151670516
Batch  61  loss:  0.0003033158427570015
Batch  71  loss:  0.00020186988695058972
Batch  81  loss:  0.0001999243104364723
Batch  91  loss:  0.0002412735193502158
Batch  101  loss:  0.00015540306048933417
Batch  111  loss:  0.00015965937927830964
Batch  121  loss:  0.0002076679957099259
Batch  131  loss:  0.00023212879023049027
Batch  141  loss:  0.00018349013407714665
Batch  151  loss:  0.00018537419964559376
Batch  161  loss:  0.00021192239364609122
Batch  171  loss:  0.00017204994219355285
Batch  181  loss:  0.0002389582514297217
Batch  191  loss:  0.00024680656497366726
Validation on real data: 
LOSS supervised-train 0.0002223475279606646, valid 0.0001785981294233352
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00029282853938639164
Batch  11  loss:  0.00033349907607771456
Batch  21  loss:  0.00011501721746753901
Batch  31  loss:  0.00025369226932525635
Batch  41  loss:  0.00019928794063162059
Batch  51  loss:  0.0001379118039039895
Batch  61  loss:  0.00031228436273522675
Batch  71  loss:  0.00022501595958601683
Batch  81  loss:  0.0001638599205762148
Batch  91  loss:  0.0002305400266777724
Batch  101  loss:  0.00014588581689167768
Batch  111  loss:  0.00019246269948780537
Batch  121  loss:  0.000226020478294231
Batch  131  loss:  0.000180590694071725
Batch  141  loss:  0.00018742318206932396
Batch  151  loss:  0.00024231993302237242
Batch  161  loss:  0.00021254573948681355
Batch  171  loss:  0.00019323609012644738
Batch  181  loss:  0.0002058936661342159
Batch  191  loss:  0.00027939360006712377
Validation on real data: 
LOSS supervised-train 0.00022032943707017694, valid 0.0002205509808845818
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.00029737342265434563
Batch  11  loss:  0.00029666657792404294
Batch  21  loss:  0.00017072263290174305
Batch  31  loss:  0.00029767502564936876
Batch  41  loss:  0.00019576071645133197
Batch  51  loss:  0.00016034120926633477
Batch  61  loss:  0.000334931566612795
Batch  71  loss:  0.00031466427026316524
Batch  81  loss:  0.0001844483194872737
Batch  91  loss:  0.0002416234783595428
Batch  101  loss:  0.00015101251483429223
Batch  111  loss:  0.00019280740525573492
Batch  121  loss:  0.0002445732825435698
Batch  131  loss:  0.00022355662076734006
Batch  141  loss:  0.00021423024008981884
Batch  151  loss:  0.00018027095939032733
Batch  161  loss:  0.00019714237714651972
Batch  171  loss:  0.00020077127555850893
Batch  181  loss:  0.00019927143875975162
Batch  191  loss:  0.0002855304628610611
Validation on real data: 
LOSS supervised-train 0.00022429004791774788, valid 0.00016420180327259004
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.00027284875977784395
Batch  11  loss:  0.00025452757836319506
Batch  21  loss:  0.00013808974472340196
Batch  31  loss:  0.00031609187135472894
Batch  41  loss:  0.0002043323911493644
Batch  51  loss:  0.00018749940500129014
Batch  61  loss:  0.00038105808198451996
Batch  71  loss:  0.00025526771787554026
Batch  81  loss:  0.0001639389229239896
Batch  91  loss:  0.0002378166827838868
Batch  101  loss:  0.00027025744202546775
Batch  111  loss:  0.00021401845151558518
Batch  121  loss:  0.00021389711764641106
Batch  131  loss:  0.00022016733419150114
Batch  141  loss:  0.00019732325745280832
Batch  151  loss:  0.00021222128998488188
Batch  161  loss:  0.0001914254971779883
Batch  171  loss:  0.00020732836856041104
Batch  181  loss:  0.0002261951012769714
Batch  191  loss:  0.00024406122975051403
Validation on real data: 
LOSS supervised-train 0.00021574066318862605, valid 0.0001678488915786147
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0003423465823289007
Batch  11  loss:  0.0002669061941560358
Batch  21  loss:  0.00016410424723289907
Batch  31  loss:  0.000269101292360574
Batch  41  loss:  0.00017627424676902592
Batch  51  loss:  0.0001818296586861834
Batch  61  loss:  0.0003829323686659336
Batch  71  loss:  0.0002746684185694903
Batch  81  loss:  0.0001566763676237315
Batch  91  loss:  0.0002637968573253602
Batch  101  loss:  0.00015877000987529755
Batch  111  loss:  0.00021003070287406445
Batch  121  loss:  0.00018094913684763014
Batch  131  loss:  0.00023648611386306584
Batch  141  loss:  0.0001994050689972937
Batch  151  loss:  0.0001563651894684881
Batch  161  loss:  0.0001683641894487664
Batch  171  loss:  0.00015420133422594517
Batch  181  loss:  0.00016764740576036274
Batch  191  loss:  0.00039747325354255736
Validation on real data: 
LOSS supervised-train 0.00021600841748295352, valid 0.00019824583432637155
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.00024095299886539578
Batch  11  loss:  0.0002834536717273295
Batch  21  loss:  0.0001533240865683183
Batch  31  loss:  0.000277182727586478
Batch  41  loss:  0.0001958374778041616
Batch  51  loss:  0.00020870246225968003
Batch  61  loss:  0.0003243216488044709
Batch  71  loss:  0.00024701058282516897
Batch  81  loss:  0.00018918441492132843
Batch  91  loss:  0.00018848892068490386
Batch  101  loss:  0.00020880947704426944
Batch  111  loss:  0.00017172638035845011
Batch  121  loss:  0.00017895098426379263
Batch  131  loss:  0.00022763632296118885
Batch  141  loss:  0.00020192204101476818
Batch  151  loss:  0.00017291103722527623
Batch  161  loss:  0.00021118990844115615
Batch  171  loss:  0.00013891873823013157
Batch  181  loss:  0.00022204432752914727
Batch  191  loss:  0.00033876090310513973
Validation on real data: 
LOSS supervised-train 0.0002095671391725773, valid 0.00017643545288592577
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0002087528700940311
Batch  11  loss:  0.000243486458202824
Batch  21  loss:  0.00015179099864326417
Batch  31  loss:  0.0003085581411141902
Batch  41  loss:  0.00016895409498829395
Batch  51  loss:  0.0001947075070347637
Batch  61  loss:  0.0003692441969178617
Batch  71  loss:  0.00028847355861216784
Batch  81  loss:  0.00016953695740085095
Batch  91  loss:  0.00022643433476332575
Batch  101  loss:  0.00012810925545636564
Batch  111  loss:  0.0001754488912411034
Batch  121  loss:  0.00018083598115481436
Batch  131  loss:  0.00022471044212579727
Batch  141  loss:  0.00022256534430198371
Batch  151  loss:  0.00015096430433914065
Batch  161  loss:  0.00018266751430928707
Batch  171  loss:  0.00017915343050844967
Batch  181  loss:  0.00018965369963552803
Batch  191  loss:  0.00027081018197350204
Validation on real data: 
LOSS supervised-train 0.00020905502380628605, valid 0.00018083488976117224
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0002732919529080391
Batch  11  loss:  0.00019934344163630158
Batch  21  loss:  0.0001456198951927945
Batch  31  loss:  0.00027852930361405015
Batch  41  loss:  0.00017953645146917552
Batch  51  loss:  0.00011874155461555347
Batch  61  loss:  0.0003509539528749883
Batch  71  loss:  0.0002976954565383494
Batch  81  loss:  0.00015179556794464588
Batch  91  loss:  0.0002409621956758201
Batch  101  loss:  0.00016497740580234677
Batch  111  loss:  0.0002459747774992138
Batch  121  loss:  0.00018277399067301303
Batch  131  loss:  0.00018744164844974875
Batch  141  loss:  0.00016277837858069688
Batch  151  loss:  0.0001753894321154803
Batch  161  loss:  0.00013753653911408037
Batch  171  loss:  0.0001520348887424916
Batch  181  loss:  0.0002584790054243058
Batch  191  loss:  0.00022566858388017863
Validation on real data: 
LOSS supervised-train 0.0001999189018897596, valid 0.0001885378296719864
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.000293606222840026
Batch  11  loss:  0.0002759879280347377
Batch  21  loss:  0.00012379707186482847
Batch  31  loss:  0.0002708308456931263
Batch  41  loss:  0.00018503829778637737
Batch  51  loss:  0.0001679569686530158
Batch  61  loss:  0.00033784270635806024
Batch  71  loss:  0.000290660304017365
Batch  81  loss:  0.00013007631059736013
Batch  91  loss:  0.00017096835654228926
Batch  101  loss:  0.0001444274530513212
Batch  111  loss:  0.00016902442439459264
Batch  121  loss:  0.00016783177852630615
Batch  131  loss:  0.0001994641061173752
Batch  141  loss:  0.00016735572717152536
Batch  151  loss:  0.00016105796385090798
Batch  161  loss:  0.00016978215717244893
Batch  171  loss:  0.00015758357767481357
Batch  181  loss:  0.000196661232621409
Batch  191  loss:  0.0002411586174275726
Validation on real data: 
LOSS supervised-train 0.00020099938701605423, valid 0.00014666031347587705
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.00025514193112030625
Batch  11  loss:  0.00022834123228676617
Batch  21  loss:  0.00016834962298162282
Batch  31  loss:  0.0002903097774833441
Batch  41  loss:  0.00020584439334925264
Batch  51  loss:  0.00016434970893897116
Batch  61  loss:  0.0003463505127001554
Batch  71  loss:  0.0002638510486576706
Batch  81  loss:  0.00018320608069188893
Batch  91  loss:  0.00020489182497840375
Batch  101  loss:  0.00015623649233020842
Batch  111  loss:  0.0001824831560952589
Batch  121  loss:  0.00020040074014104903
Batch  131  loss:  0.00021432027278933674
Batch  141  loss:  0.0002113945665769279
Batch  151  loss:  0.00016700883861631155
Batch  161  loss:  0.00017015129560604692
Batch  171  loss:  0.00015018173144198954
Batch  181  loss:  0.00018017689581029117
Batch  191  loss:  0.00024370830215048045
Validation on real data: 
LOSS supervised-train 0.00020220527301717083, valid 0.00014539220137521625
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.00024777790531516075
Batch  11  loss:  0.00022455082216765732
Batch  21  loss:  0.0001312502718064934
Batch  31  loss:  0.00025812609237618744
Batch  41  loss:  0.0002064033324131742
Batch  51  loss:  0.00017027021385729313
Batch  61  loss:  0.0003235077892895788
Batch  71  loss:  0.00018870872736442834
Batch  81  loss:  0.0001385344803566113
Batch  91  loss:  0.00018915836699306965
Batch  101  loss:  0.0002687344385776669
Batch  111  loss:  0.0001585249265190214
Batch  121  loss:  0.00021096147247590125
Batch  131  loss:  0.0001552152243675664
Batch  141  loss:  0.00021951927919872105
Batch  151  loss:  0.00018790025205817074
Batch  161  loss:  0.0001960204099304974
Batch  171  loss:  0.00014901577378623188
Batch  181  loss:  0.00019954846356995404
Batch  191  loss:  0.00028551873401738703
Validation on real data: 
LOSS supervised-train 0.00019187034649803536, valid 0.00018139554595109075
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0002507164317648858
Batch  11  loss:  0.00019070868438575417
Batch  21  loss:  0.00011828937567770481
Batch  31  loss:  0.0002415210474282503
Batch  41  loss:  0.00017618601850699633
Batch  51  loss:  0.0001863747602328658
Batch  61  loss:  0.00032491012825630605
Batch  71  loss:  0.0002601640881039202
Batch  81  loss:  0.00014252591063268483
Batch  91  loss:  0.0001707942719804123
Batch  101  loss:  0.00018346837896388024
Batch  111  loss:  0.00020247857901267707
Batch  121  loss:  0.00016857210721354932
Batch  131  loss:  0.00017543516878504306
Batch  141  loss:  0.000196874636458233
Batch  151  loss:  0.00014899637608323246
Batch  161  loss:  0.00017362128710374236
Batch  171  loss:  0.0001166565707535483
Batch  181  loss:  0.0001844656653702259
Batch  191  loss:  0.00021438932162709534
Validation on real data: 
LOSS supervised-train 0.0001916872943547787, valid 0.000153829125338234
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0002058880781987682
Batch  11  loss:  0.00021562013716902584
Batch  21  loss:  0.0001413736754329875
Batch  31  loss:  0.0002476131194271147
Batch  41  loss:  0.0001659816480241716
Batch  51  loss:  0.00015492660168092698
Batch  61  loss:  0.000276533595751971
Batch  71  loss:  0.00022502373030874878
Batch  81  loss:  0.00014603094314225018
Batch  91  loss:  0.0001830821274779737
Batch  101  loss:  0.00014138928963802755
Batch  111  loss:  0.00016380027227569371
Batch  121  loss:  0.00017460867820773274
Batch  131  loss:  0.00018375692889094353
Batch  141  loss:  0.00022956033353693783
Batch  151  loss:  0.00013422507618088275
Batch  161  loss:  0.0001593277556821704
Batch  171  loss:  0.00015918372082524002
Batch  181  loss:  0.00015939345757942647
Batch  191  loss:  0.00031655002385377884
Validation on real data: 
LOSS supervised-train 0.0001893339196612942, valid 0.00012906812480650842
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.00023398878693114966
Batch  11  loss:  0.00020525472064036876
Batch  21  loss:  0.0001508769200881943
Batch  31  loss:  0.0002475786313880235
Batch  41  loss:  0.0001861471391748637
Batch  51  loss:  0.00015490780060645193
Batch  61  loss:  0.00028698932146653533
Batch  71  loss:  0.00019815776613540947
Batch  81  loss:  0.00012742324906866997
Batch  91  loss:  0.00017756904708221555
Batch  101  loss:  0.000133738576550968
Batch  111  loss:  0.000130599393742159
Batch  121  loss:  0.0001654668158153072
Batch  131  loss:  0.00020025383855681866
Batch  141  loss:  0.00017625780310481787
Batch  151  loss:  0.00014285356155596673
Batch  161  loss:  0.0001598658418515697
Batch  171  loss:  0.0001390793186146766
Batch  181  loss:  0.00015904434258118272
Batch  191  loss:  0.0002423561381874606
Validation on real data: 
LOSS supervised-train 0.00018913051670097048, valid 0.0001689213968347758
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0002480393450241536
Batch  11  loss:  0.00018705216643866152
Batch  21  loss:  0.00014867782010696828
Batch  31  loss:  0.00020682222384493798
Batch  41  loss:  0.00016806960047688335
Batch  51  loss:  0.00017165225290227681
Batch  61  loss:  0.0003284641425125301
Batch  71  loss:  0.00034921339829452336
Batch  81  loss:  0.00012548899394460022
Batch  91  loss:  0.00024724690592847764
Batch  101  loss:  0.0001877316099125892
Batch  111  loss:  0.00015208804688882083
Batch  121  loss:  0.00011852638272102922
Batch  131  loss:  0.00021082199236843735
Batch  141  loss:  0.00014217995340004563
Batch  151  loss:  0.00015580403851345181
Batch  161  loss:  0.00014098943211138248
Batch  171  loss:  0.00013101180957164615
Batch  181  loss:  0.00019358817371539772
Batch  191  loss:  0.0002183038304792717
Validation on real data: 
LOSS supervised-train 0.00018898063452070347, valid 0.00017677851428743452
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.00022247075685299933
Batch  11  loss:  0.00022518109472002834
Batch  21  loss:  0.00013248715549707413
Batch  31  loss:  0.000254736136412248
Batch  41  loss:  0.0001715907856123522
Batch  51  loss:  0.0001418271567672491
Batch  61  loss:  0.00025655608624219894
Batch  71  loss:  0.00022743947920389473
Batch  81  loss:  0.00013110430154483765
Batch  91  loss:  0.00020151901117060333
Batch  101  loss:  0.00013161722745280713
Batch  111  loss:  0.00020608534396160394
Batch  121  loss:  0.0001857094030128792
Batch  131  loss:  0.00016139677609317005
Batch  141  loss:  0.0001133832847699523
Batch  151  loss:  0.00013732429943047464
Batch  161  loss:  0.00015274342149496078
Batch  171  loss:  0.00013245314767118543
Batch  181  loss:  0.00019424404308665544
Batch  191  loss:  0.00023256072017829865
Validation on real data: 
LOSS supervised-train 0.00017608974419999868, valid 0.00013187280273996294
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00024363854026887566
Batch  11  loss:  0.0001958302891580388
Batch  21  loss:  0.0001177845333586447
Batch  31  loss:  0.0002066732558887452
Batch  41  loss:  0.0001513370661996305
Batch  51  loss:  0.000144239587825723
Batch  61  loss:  0.00026592036010697484
Batch  71  loss:  0.00034695223439484835
Batch  81  loss:  0.00016699919069651514
Batch  91  loss:  0.00018497697601560503
Batch  101  loss:  0.0002065344451693818
Batch  111  loss:  0.00017740506154950708
Batch  121  loss:  0.00011206867202417925
Batch  131  loss:  0.0002301426138728857
Batch  141  loss:  0.0001701901783235371
Batch  151  loss:  0.00014831687440164387
Batch  161  loss:  0.0001725902984617278
Batch  171  loss:  0.00014519291289616376
Batch  181  loss:  0.0001819943281589076
Batch  191  loss:  0.000235250307014212
Validation on real data: 
LOSS supervised-train 0.00018251264777063625, valid 0.00012915811385028064
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00027117045829072595
Batch  11  loss:  0.00019766646437346935
Batch  21  loss:  0.00013728834164794534
Batch  31  loss:  0.00028432297403924167
Batch  41  loss:  0.00018228146655019373
Batch  51  loss:  0.00017175154061987996
Batch  61  loss:  0.00024785916320979595
Batch  71  loss:  0.00027143038460053504
Batch  81  loss:  0.00012840540148317814
Batch  91  loss:  0.0002854940539691597
Batch  101  loss:  0.00017542378918733448
Batch  111  loss:  0.00014783813094254583
Batch  121  loss:  0.00015212845755741
Batch  131  loss:  0.00017077219672501087
Batch  141  loss:  0.00021440855925902724
Batch  151  loss:  0.00013556166959460825
Batch  161  loss:  0.0001404163340339437
Batch  171  loss:  0.00015695134061388671
Batch  181  loss:  0.00015327948494814336
Batch  191  loss:  0.0002523635921534151
Validation on real data: 
LOSS supervised-train 0.00018259261934872484, valid 0.00013561168452724814
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0002449148159939796
Batch  11  loss:  0.00019896490266546607
Batch  21  loss:  0.00012854688975494355
Batch  31  loss:  0.00021894372184760869
Batch  41  loss:  0.00018053804524242878
Batch  51  loss:  0.00022299308329820633
Batch  61  loss:  0.0003071237006224692
Batch  71  loss:  0.0002635455166455358
Batch  81  loss:  0.00013871167902834713
Batch  91  loss:  0.00015813318896107376
Batch  101  loss:  0.00015124332276172936
Batch  111  loss:  0.0001446540845790878
Batch  121  loss:  0.00011907139560207725
Batch  131  loss:  0.00019789169891737401
Batch  141  loss:  0.0001931475126184523
Batch  151  loss:  0.0001611228217370808
Batch  161  loss:  0.00018034965614788234
Batch  171  loss:  0.00012146206427132711
Batch  181  loss:  0.0001665753807174042
Batch  191  loss:  0.00019788573263213038
Validation on real data: 
LOSS supervised-train 0.00018585265210276702, valid 0.00013083928206469864
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.00020940332615282387
Batch  11  loss:  0.00020299197058193386
Batch  21  loss:  0.00010839824972208589
Batch  31  loss:  0.00020503401174210012
Batch  41  loss:  0.00019131609587930143
Batch  51  loss:  0.0001358932931907475
Batch  61  loss:  0.0002610176452435553
Batch  71  loss:  0.00019714239169843495
Batch  81  loss:  0.0001323660690104589
Batch  91  loss:  0.00018890589126385748
Batch  101  loss:  0.0001486151450080797
Batch  111  loss:  0.00014052951883058995
Batch  121  loss:  0.00012545639765448868
Batch  131  loss:  0.00017895642668008804
Batch  141  loss:  0.00013737210247199982
Batch  151  loss:  0.0001730193180264905
Batch  161  loss:  0.0001704961759969592
Batch  171  loss:  0.00015889349742792547
Batch  181  loss:  0.0001483430532971397
Batch  191  loss:  0.00022864389757160097
Validation on real data: 
LOSS supervised-train 0.00017592097017768538, valid 0.00014102643763180822
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00015879279817454517
Batch  11  loss:  0.00018348611774854362
Batch  21  loss:  0.00011843207903439179
Batch  31  loss:  0.00022391622769646347
Batch  41  loss:  0.00017584438319317997
Batch  51  loss:  0.00016374430560972542
Batch  61  loss:  0.00022954426822252572
Batch  71  loss:  0.00020364753436297178
Batch  81  loss:  0.00010540337098063901
Batch  91  loss:  0.00021610736439470202
Batch  101  loss:  0.00011879888188559562
Batch  111  loss:  0.00014872272731736302
Batch  121  loss:  0.00015786662697792053
Batch  131  loss:  0.00017507125448901206
Batch  141  loss:  0.00014316772285383195
Batch  151  loss:  0.00013770519581157714
Batch  161  loss:  0.00014429980365093797
Batch  171  loss:  0.00013265844609122723
Batch  181  loss:  0.00015420439012814313
Batch  191  loss:  0.0002162979799322784
Validation on real data: 
LOSS supervised-train 0.0001700009544583736, valid 0.0001564334233989939
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0001962775713764131
Batch  11  loss:  0.0002128940832335502
Batch  21  loss:  0.00013230653712525964
Batch  31  loss:  0.000199401518329978
Batch  41  loss:  0.0001508487039245665
Batch  51  loss:  0.0001191878691315651
Batch  61  loss:  0.0002662978367879987
Batch  71  loss:  0.00018954473489429802
Batch  81  loss:  0.0001068625642801635
Batch  91  loss:  0.00018699010252021253
Batch  101  loss:  0.00014881543756928295
Batch  111  loss:  0.00017684332851786166
Batch  121  loss:  0.00010905248927883804
Batch  131  loss:  0.00018720024672802538
Batch  141  loss:  0.00013142818352207541
Batch  151  loss:  0.00012283744581509382
Batch  161  loss:  0.00014001822273712605
Batch  171  loss:  0.0001346885837847367
Batch  181  loss:  0.00021152538829483092
Batch  191  loss:  0.00021785165881738067
Validation on real data: 
LOSS supervised-train 0.0001727250406111125, valid 0.00048333092126995325
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00025774817913770676
Batch  11  loss:  0.00022050592815503478
Batch  21  loss:  0.00012976719881407917
Batch  31  loss:  0.0002195799897890538
Batch  41  loss:  0.00013820049935020506
Batch  51  loss:  0.00013302045408636332
Batch  61  loss:  0.0002707494713831693
Batch  71  loss:  0.00017551919154357165
Batch  81  loss:  0.00013268738985061646
Batch  91  loss:  0.00015922478633001447
Batch  101  loss:  0.00010398268204880878
Batch  111  loss:  0.00014958882820792496
Batch  121  loss:  0.00012538916780613363
Batch  131  loss:  0.00015053301467560232
Batch  141  loss:  0.00014196256233844906
Batch  151  loss:  0.00012354350474197417
Batch  161  loss:  0.00015221595822367817
Batch  171  loss:  0.00013628402666654438
Batch  181  loss:  0.00013668002793565392
Batch  191  loss:  0.00020206865156069398
Validation on real data: 
LOSS supervised-train 0.00016730805404222338, valid 0.00012145390792284161
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00018734298646450043
Batch  11  loss:  0.00017574802041053772
Batch  21  loss:  0.00012810358020942658
Batch  31  loss:  0.00022190704476088285
Batch  41  loss:  0.00016759592108428478
Batch  51  loss:  0.00013807913637720048
Batch  61  loss:  0.00023020568187348545
Batch  71  loss:  0.0002356318145757541
Batch  81  loss:  0.00013263172877486795
Batch  91  loss:  0.0001460148923797533
Batch  101  loss:  0.00010834112617885694
Batch  111  loss:  0.00015294189506676048
Batch  121  loss:  0.00011593475937843323
Batch  131  loss:  0.0002011478500207886
Batch  141  loss:  0.0001869723346317187
Batch  151  loss:  0.00014270750398281962
Batch  161  loss:  0.0001302828168263659
Batch  171  loss:  0.00010670857591321692
Batch  181  loss:  0.0001466603862354532
Batch  191  loss:  0.00015775649808347225
Validation on real data: 
LOSS supervised-train 0.00016848701547132804, valid 0.00012558158778119832
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0002455587964504957
Batch  11  loss:  0.00017675061826594174
Batch  21  loss:  0.00012303076800890267
Batch  31  loss:  0.00017159355047624558
Batch  41  loss:  0.0001253157970495522
Batch  51  loss:  0.00011023150727851316
Batch  61  loss:  0.00016644356946926564
Batch  71  loss:  0.0002510900376364589
Batch  81  loss:  0.00011156846449011937
Batch  91  loss:  0.00018054048996418715
Batch  101  loss:  0.0001214913500007242
Batch  111  loss:  0.00014927586016710848
Batch  121  loss:  0.0001304061006521806
Batch  131  loss:  0.00017753007705323398
Batch  141  loss:  0.00013532904267776757
Batch  151  loss:  0.00012378218525554985
Batch  161  loss:  0.00016111781587824225
Batch  171  loss:  0.00013759532885160297
Batch  181  loss:  0.00011023826664313674
Batch  191  loss:  0.00018177740275859833
Validation on real data: 
LOSS supervised-train 0.00016397307088482195, valid 0.0001424616202712059
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00020274202688597143
Batch  11  loss:  0.0001994965859921649
Batch  21  loss:  0.00012092109682271257
Batch  31  loss:  0.00020391384896356612
Batch  41  loss:  0.00012736572534777224
Batch  51  loss:  0.00011475000064820051
Batch  61  loss:  0.0002603776811156422
Batch  71  loss:  0.0002159111900255084
Batch  81  loss:  0.00012196697207400575
Batch  91  loss:  0.0002447892038617283
Batch  101  loss:  0.00013274719822220504
Batch  111  loss:  0.00015961100871209055
Batch  121  loss:  0.0001387185329804197
Batch  131  loss:  0.0001427196548320353
Batch  141  loss:  0.000149938598042354
Batch  151  loss:  0.00011049829481635243
Batch  161  loss:  0.00016020100156310946
Batch  171  loss:  0.00012826421880163252
Batch  181  loss:  0.00015848518523853272
Batch  191  loss:  0.00020923871488776058
Validation on real data: 
LOSS supervised-train 0.00016058880599302937, valid 0.00014245143393054605
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00018929196812678128
Batch  11  loss:  0.0002196748391725123
Batch  21  loss:  0.0001328683429164812
Batch  31  loss:  0.00021453840599860996
Batch  41  loss:  0.0001723203604342416
Batch  51  loss:  0.00012957447324879467
Batch  61  loss:  0.00022900279145687819
Batch  71  loss:  0.00019495781452860683
Batch  81  loss:  0.0001208314934046939
Batch  91  loss:  0.0002179637667723
Batch  101  loss:  0.00015692762099206448
Batch  111  loss:  0.00015915886615402997
Batch  121  loss:  0.00013526396651286632
Batch  131  loss:  0.00019499902555253357
Batch  141  loss:  0.00013656713417731225
Batch  151  loss:  0.0001384248025715351
Batch  161  loss:  0.00014757511962670833
Batch  171  loss:  0.00015012182120699435
Batch  181  loss:  0.00017258233856409788
Batch  191  loss:  0.0002596655103843659
Validation on real data: 
LOSS supervised-train 0.00016900625319976825, valid 0.00013215062790550292
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00027982130995951593
Batch  11  loss:  0.00018967475625686347
Batch  21  loss:  0.0001169024471892044
Batch  31  loss:  0.00020156601385679096
Batch  41  loss:  0.00012515632261056453
Batch  51  loss:  0.00010083230881718919
Batch  61  loss:  0.0003010096843354404
Batch  71  loss:  0.00019139768846798688
Batch  81  loss:  0.00013161133392713964
Batch  91  loss:  0.00018464450840838253
Batch  101  loss:  0.0001234865776496008
Batch  111  loss:  0.00013365333143156022
Batch  121  loss:  0.00012861262075603008
Batch  131  loss:  0.00021579589520115405
Batch  141  loss:  0.00018035541870631278
Batch  151  loss:  0.00013938416668679565
Batch  161  loss:  0.00017069006571546197
Batch  171  loss:  0.00011647494102362543
Batch  181  loss:  0.00011928419553441927
Batch  191  loss:  0.00017234828555956483
Validation on real data: 
LOSS supervised-train 0.00016097029660159024, valid 0.00011365771206328645
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00016987133130896837
Batch  11  loss:  0.0001890850398922339
Batch  21  loss:  0.00011408714635763317
Batch  31  loss:  0.00019335384422447532
Batch  41  loss:  0.0001545928098494187
Batch  51  loss:  0.0001444708032067865
Batch  61  loss:  0.00023446175327990204
Batch  71  loss:  0.00022284443548414856
Batch  81  loss:  0.00013730554201174527
Batch  91  loss:  0.00017391466826666147
Batch  101  loss:  0.00033609793172217906
Batch  111  loss:  0.00016531981236767024
Batch  121  loss:  0.00014041717804502696
Batch  131  loss:  0.00019874476129189134
Batch  141  loss:  0.00015929537767078727
Batch  151  loss:  0.00013767991913482547
Batch  161  loss:  0.00014145267778076231
Batch  171  loss:  0.00013549112190958112
Batch  181  loss:  0.00015058586723171175
Batch  191  loss:  0.00018879167328123003
Validation on real data: 
LOSS supervised-train 0.00015922604576189769, valid 0.00014868835569359362
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.00017310511611867696
Batch  11  loss:  0.00017691802349872887
Batch  21  loss:  0.00011442542017903179
Batch  31  loss:  0.00021552701946347952
Batch  41  loss:  0.0001458813640056178
Batch  51  loss:  0.00010722275328589603
Batch  61  loss:  0.0002563957532402128
Batch  71  loss:  0.000185450553544797
Batch  81  loss:  0.00012195746967336163
Batch  91  loss:  0.00019180038361810148
Batch  101  loss:  0.00013577942445408553
Batch  111  loss:  0.00012486438208725303
Batch  121  loss:  0.00016013611457310617
Batch  131  loss:  0.00015246117254719138
Batch  141  loss:  0.00013422682241071016
Batch  151  loss:  0.0001386838121106848
Batch  161  loss:  0.00012779761163983494
Batch  171  loss:  0.00013388792285695672
Batch  181  loss:  0.00019322401203680784
Batch  191  loss:  0.00018432619981467724
Validation on real data: 
LOSS supervised-train 0.00016202300277655014, valid 0.00013377185678109527
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.000241077461396344
Batch  11  loss:  0.00017802960064727813
Batch  21  loss:  0.00011808134149760008
Batch  31  loss:  0.00018846066086553037
Batch  41  loss:  0.00016921163478400558
Batch  51  loss:  0.00014897555229254067
Batch  61  loss:  0.00024572107940912247
Batch  71  loss:  0.00017925120482686907
Batch  81  loss:  0.00011138161062262952
Batch  91  loss:  0.00018933899991679937
Batch  101  loss:  0.00015547554357908666
Batch  111  loss:  0.00014862744137644768
Batch  121  loss:  0.00011309096589684486
Batch  131  loss:  0.0001725888578221202
Batch  141  loss:  0.00017549024778418243
Batch  151  loss:  0.00014788750559091568
Batch  161  loss:  0.00015399372205138206
Batch  171  loss:  0.0001255304377991706
Batch  181  loss:  0.00014087797899264842
Batch  191  loss:  0.00018697205814532936
Validation on real data: 
LOSS supervised-train 0.0001621626448468305, valid 0.00011269698734395206
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0001819288736442104
Batch  11  loss:  0.0001981080713449046
Batch  21  loss:  0.00011166455078637227
Batch  31  loss:  0.0001832444395404309
Batch  41  loss:  0.0001365290372632444
Batch  51  loss:  0.000105312567029614
Batch  61  loss:  0.00020038819639012218
Batch  71  loss:  0.00020089346799068153
Batch  81  loss:  0.00013002692139707506
Batch  91  loss:  0.00017669849330559373
Batch  101  loss:  0.00013247158494777977
Batch  111  loss:  9.08995425561443e-05
Batch  121  loss:  0.00013530071009881794
Batch  131  loss:  0.00018808484310284257
Batch  141  loss:  0.0001415438746334985
Batch  151  loss:  0.000168963146279566
Batch  161  loss:  0.0001348038495052606
Batch  171  loss:  0.00015823125431779772
Batch  181  loss:  0.00015632930444553494
Batch  191  loss:  0.00016517344920430332
Validation on real data: 
LOSS supervised-train 0.00015381344375782645, valid 0.00012263386452104896
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00017656988347880542
Batch  11  loss:  0.00019779558351729065
Batch  21  loss:  0.00010352829121984541
Batch  31  loss:  0.00015875932876951993
Batch  41  loss:  0.00014405134425032884
Batch  51  loss:  0.0001445196394342929
Batch  61  loss:  0.00024517765268683434
Batch  71  loss:  0.00012283361866138875
Batch  81  loss:  0.00010530795407248661
Batch  91  loss:  0.00014114838268142194
Batch  101  loss:  0.00015152838022913784
Batch  111  loss:  0.00011487887968542054
Batch  121  loss:  0.00016717074322514236
Batch  131  loss:  0.00014254383859224617
Batch  141  loss:  0.0001272669469472021
Batch  151  loss:  0.00010695930541260168
Batch  161  loss:  0.00011306914529995993
Batch  171  loss:  0.00011855213961098343
Batch  181  loss:  0.00014292870764620602
Batch  191  loss:  0.00021857630054000765
Validation on real data: 
LOSS supervised-train 0.00015104528560186737, valid 0.00010464369552209973
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.00016085572133306414
Batch  11  loss:  0.0001698191772447899
Batch  21  loss:  0.00011939369869651273
Batch  31  loss:  0.0001744399341987446
Batch  41  loss:  0.0001567408035043627
Batch  51  loss:  0.00010503633529879153
Batch  61  loss:  0.00020260467135813087
Batch  71  loss:  0.0001934626343427226
Batch  81  loss:  0.00011202501627849415
Batch  91  loss:  0.00016721338033676147
Batch  101  loss:  0.00014540314441546798
Batch  111  loss:  0.00012327477452345192
Batch  121  loss:  0.00012890696234535426
Batch  131  loss:  0.0001560173841426149
Batch  141  loss:  0.00011226574861211702
Batch  151  loss:  0.00010076997568830848
Batch  161  loss:  0.0001457366452086717
Batch  171  loss:  0.00011501525295898318
Batch  181  loss:  0.0001852893183240667
Batch  191  loss:  0.00016976169717963785
Validation on real data: 
LOSS supervised-train 0.00014487360989733132, valid 0.0001301511365454644
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.00017622225277591497
Batch  11  loss:  0.0002070531336357817
Batch  21  loss:  0.00012861935829278082
Batch  31  loss:  0.0002154922840418294
Batch  41  loss:  0.00015871625510044396
Batch  51  loss:  0.00011486937728477642
Batch  61  loss:  0.00031010195380076766
Batch  71  loss:  0.00015177919704001397
Batch  81  loss:  9.712684550322592e-05
Batch  91  loss:  0.00017019201186485589
Batch  101  loss:  0.00010246135934721678
Batch  111  loss:  0.00013743992894887924
Batch  121  loss:  0.00012757352669723332
Batch  131  loss:  0.00013833159755449742
Batch  141  loss:  0.00012259490904398263
Batch  151  loss:  0.0001074052051990293
Batch  161  loss:  0.00017870184092316777
Batch  171  loss:  0.00010984653636114672
Batch  181  loss:  0.0001475244789617136
Batch  191  loss:  0.00017223703616764396
Validation on real data: 
LOSS supervised-train 0.0001509421191076399, valid 0.00012804093421436846
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00015042822633404285
Batch  11  loss:  0.00017268606461584568
Batch  21  loss:  9.500684245722368e-05
Batch  31  loss:  0.00015426510071847588
Batch  41  loss:  0.0001502099185017869
Batch  51  loss:  0.0001287342020077631
Batch  61  loss:  0.00018220723723061383
Batch  71  loss:  0.00018730400188360363
Batch  81  loss:  0.00011222533794352785
Batch  91  loss:  0.00017624138854444027
Batch  101  loss:  0.00014414460747502744
Batch  111  loss:  0.00015373025962617248
Batch  121  loss:  0.0001274709647987038
Batch  131  loss:  0.00015693396562710404
Batch  141  loss:  0.00014030272723175585
Batch  151  loss:  0.00012409532791934907
Batch  161  loss:  0.00013471012061927468
Batch  171  loss:  0.00011597439151955768
Batch  181  loss:  0.00017549838230479509
Batch  191  loss:  0.00018049187201540917
Validation on real data: 
LOSS supervised-train 0.00015180502286966658, valid 0.00011072916822740808
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0001358367590000853
Batch  11  loss:  0.0001609300379641354
Batch  21  loss:  0.00012598418106790632
Batch  31  loss:  0.00017352828581351787
Batch  41  loss:  0.00011409709259169176
Batch  51  loss:  0.00014675481361337006
Batch  61  loss:  0.00019764415628742427
Batch  71  loss:  0.00015761583927087486
Batch  81  loss:  0.00012645692913793027
Batch  91  loss:  0.0001575966743985191
Batch  101  loss:  0.0001283676829189062
Batch  111  loss:  0.00012383708963170648
Batch  121  loss:  0.0001425316877430305
Batch  131  loss:  0.00012558854359667748
Batch  141  loss:  0.00012512753892224282
Batch  151  loss:  0.00010460596240591258
Batch  161  loss:  0.00011748948600143194
Batch  171  loss:  0.0001202362182084471
Batch  181  loss:  0.00016637679073028266
Batch  191  loss:  0.00024381440016441047
Validation on real data: 
LOSS supervised-train 0.00014504146412946283, valid 0.0001305556361330673
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00016361217421945184
Batch  11  loss:  0.00013358687283471227
Batch  21  loss:  0.00011844018445117399
Batch  31  loss:  0.000168828439200297
Batch  41  loss:  0.00013687887985724956
Batch  51  loss:  0.00010116471094079316
Batch  61  loss:  0.00018786116561386734
Batch  71  loss:  0.00014614254178013653
Batch  81  loss:  0.00010416845907457173
Batch  91  loss:  0.00014737450692337006
Batch  101  loss:  0.0001274631213163957
Batch  111  loss:  0.00011587992048589513
Batch  121  loss:  0.00010456524614710361
Batch  131  loss:  0.0001284844911424443
Batch  141  loss:  0.00013145232514943928
Batch  151  loss:  8.591541700297967e-05
Batch  161  loss:  0.0001272094959858805
Batch  171  loss:  0.00011073442146880552
Batch  181  loss:  9.557358862366527e-05
Batch  191  loss:  0.00020557678362820297
Validation on real data: 
LOSS supervised-train 0.00014465803258644882, valid 0.00012554808927234262
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00019352664821781218
Batch  11  loss:  0.00022522135986946523
Batch  21  loss:  8.694548159837723e-05
Batch  31  loss:  0.0001763045002007857
Batch  41  loss:  0.00014634877152275294
Batch  51  loss:  0.00016661974950693548
Batch  61  loss:  0.00020313668937887996
Batch  71  loss:  0.00014550685591530055
Batch  81  loss:  0.00010334378021070734
Batch  91  loss:  0.0002051639894489199
Batch  101  loss:  9.971578401746228e-05
Batch  111  loss:  0.0001531255547888577
Batch  121  loss:  0.0001235611707670614
Batch  131  loss:  0.00015971220273058861
Batch  141  loss:  0.00014462231774814427
Batch  151  loss:  0.00010193915659328923
Batch  161  loss:  0.00010510710853850469
Batch  171  loss:  0.00011974895460298285
Batch  181  loss:  0.00015686753613408655
Batch  191  loss:  0.00014972256030887365
Validation on real data: 
LOSS supervised-train 0.00014253495650336844, valid 0.0001240434212377295
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.00019717420218512416
Batch  11  loss:  0.00017158390255644917
Batch  21  loss:  0.0001328510115854442
Batch  31  loss:  0.00018706053378991783
Batch  41  loss:  0.00015075501869432628
Batch  51  loss:  0.00011016915959771723
Batch  61  loss:  0.00031416324782185256
Batch  71  loss:  0.00014878848742228001
Batch  81  loss:  9.320747631136328e-05
Batch  91  loss:  0.00015363756392616779
Batch  101  loss:  0.0001343782350886613
Batch  111  loss:  0.00011026956781279296
Batch  121  loss:  9.606832463759929e-05
Batch  131  loss:  0.00015143273049034178
Batch  141  loss:  0.00012662203516811132
Batch  151  loss:  0.00012606456584762782
Batch  161  loss:  0.00011664407065836713
Batch  171  loss:  0.0001259567798115313
Batch  181  loss:  0.00015285344852600247
Batch  191  loss:  0.00016546565166208893
Validation on real data: 
LOSS supervised-train 0.00014083012931223493, valid 0.00011255725985392928
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00016407083603553474
Batch  11  loss:  0.00016399440937675536
Batch  21  loss:  8.772862929617986e-05
Batch  31  loss:  0.00018552957044448704
Batch  41  loss:  0.00013133243191987276
Batch  51  loss:  0.00013103135279379785
Batch  61  loss:  0.00024055858375504613
Batch  71  loss:  0.00019170224550180137
Batch  81  loss:  0.00014004208787810057
Batch  91  loss:  0.00016391971439588815
Batch  101  loss:  0.0001492997835157439
Batch  111  loss:  0.00011610775982262567
Batch  121  loss:  0.0001437362952856347
Batch  131  loss:  0.00014518182433675975
Batch  141  loss:  0.0001393473648931831
Batch  151  loss:  0.00011206047929590568
Batch  161  loss:  0.0001572833862155676
Batch  171  loss:  0.00011220210581086576
Batch  181  loss:  0.00013263992150314152
Batch  191  loss:  0.00016218397649936378
Validation on real data: 
LOSS supervised-train 0.00014216412346286235, valid 0.00012099307787138969
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0001517261116532609
Batch  11  loss:  0.00019174108456354588
Batch  21  loss:  0.0001116972416639328
Batch  31  loss:  0.00018747466674540192
Batch  41  loss:  0.0001337182620773092
Batch  51  loss:  0.0001396962470607832
Batch  61  loss:  0.0001760734448907897
Batch  71  loss:  0.00017343810759484768
Batch  81  loss:  0.00010124172695213929
Batch  91  loss:  0.00016330364451278
Batch  101  loss:  9.236689220415428e-05
Batch  111  loss:  0.00012382151908241212
Batch  121  loss:  0.00011658314178930596
Batch  131  loss:  0.00013788632350042462
Batch  141  loss:  0.00014310887490864843
Batch  151  loss:  0.0001358717418042943
Batch  161  loss:  0.0001471572177251801
Batch  171  loss:  0.00013102899538353086
Batch  181  loss:  0.0001251204957952723
Batch  191  loss:  0.0001557890063850209
Validation on real data: 
LOSS supervised-train 0.00014228315445507177, valid 0.00011635685223154724
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00016397677245549858
Batch  11  loss:  0.00018419600382912904
Batch  21  loss:  0.00010122892854269594
Batch  31  loss:  0.000176915928022936
Batch  41  loss:  0.00011708575766533613
Batch  51  loss:  0.00011278340389253572
Batch  61  loss:  0.00016748576308600605
Batch  71  loss:  0.00019645407155621797
Batch  81  loss:  0.00014356950123328716
Batch  91  loss:  0.00018949058721773326
Batch  101  loss:  0.00014788468251936138
Batch  111  loss:  0.00012620813504327089
Batch  121  loss:  0.00010424447827972472
Batch  131  loss:  0.0001563881669426337
Batch  141  loss:  0.00010514363384572789
Batch  151  loss:  9.97450842987746e-05
Batch  161  loss:  0.00011636262206593528
Batch  171  loss:  0.00014230750093702227
Batch  181  loss:  0.00012999407772440463
Batch  191  loss:  0.00016601804236415774
Validation on real data: 
LOSS supervised-train 0.00013853295306034852, valid 0.00012538617011159658
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.00021145596110727638
Batch  11  loss:  0.00015657157928217202
Batch  21  loss:  0.00012852919462602586
Batch  31  loss:  0.0001734967081574723
Batch  41  loss:  0.00013023795327171683
Batch  51  loss:  0.00010662693239282817
Batch  61  loss:  0.0002049864997388795
Batch  71  loss:  0.00018615323642734438
Batch  81  loss:  0.00011908697342732921
Batch  91  loss:  0.00014810406719334424
Batch  101  loss:  0.00015440817514900118
Batch  111  loss:  0.00010403802298242226
Batch  121  loss:  0.00011589930363697931
Batch  131  loss:  0.00015850678028073162
Batch  141  loss:  0.00011878879740834236
Batch  151  loss:  0.00010759742872323841
Batch  161  loss:  0.00011567981709958985
Batch  171  loss:  9.197934559779242e-05
Batch  181  loss:  0.0001073147141141817
Batch  191  loss:  0.00014466933498624712
Validation on real data: 
LOSS supervised-train 0.00013905093062930974, valid 9.735043568070978e-05
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.00017848856805358082
Batch  11  loss:  0.00016865841462276876
Batch  21  loss:  0.00010154530900763348
Batch  31  loss:  0.00012318197696004063
Batch  41  loss:  0.0001002745411824435
Batch  51  loss:  0.00010625997674651444
Batch  61  loss:  0.00015677027113270015
Batch  71  loss:  0.00016616718494333327
Batch  81  loss:  8.946143498178571e-05
Batch  91  loss:  0.00014264699711930007
Batch  101  loss:  0.00014731130795553327
Batch  111  loss:  0.00014173690578900278
Batch  121  loss:  0.00011395159526728094
Batch  131  loss:  0.00013700900308322161
Batch  141  loss:  0.00010546566772973165
Batch  151  loss:  0.0001424012880306691
Batch  161  loss:  0.00015902885934337974
Batch  171  loss:  0.00013154302723705769
Batch  181  loss:  0.00016513971786480397
Batch  191  loss:  0.00015986892685759813
Validation on real data: 
LOSS supervised-train 0.0001404730714057223, valid 0.00012723023246508092
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0001627887977520004
Batch  11  loss:  0.00017278634186368436
Batch  21  loss:  0.00015180533227976412
Batch  31  loss:  0.00024803681299090385
Batch  41  loss:  0.00012598444300238043
Batch  51  loss:  0.0001207634704769589
Batch  61  loss:  0.00017090562323573977
Batch  71  loss:  0.000132937595481053
Batch  81  loss:  9.754044003784657e-05
Batch  91  loss:  0.0001470585266361013
Batch  101  loss:  0.0001083181778085418
Batch  111  loss:  0.00010644767462508753
Batch  121  loss:  0.00013336767733562738
Batch  131  loss:  0.0001470335992053151
Batch  141  loss:  0.00014086363080423325
Batch  151  loss:  0.00011892519978573546
Batch  161  loss:  9.20428938115947e-05
Batch  171  loss:  0.00013174903870094568
Batch  181  loss:  0.00015308592992369086
Batch  191  loss:  0.00018169432587455958
Validation on real data: 
LOSS supervised-train 0.00013634360071591801, valid 0.00011615591211011633
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.00016624793352093548
Batch  11  loss:  0.00012460621655918658
Batch  21  loss:  9.136465814663097e-05
Batch  31  loss:  0.00014809072308707982
Batch  41  loss:  0.00012236366455908865
Batch  51  loss:  0.00011222867033211514
Batch  61  loss:  0.0002114439121214673
Batch  71  loss:  0.00022217775403987616
Batch  81  loss:  0.00012337965017650276
Batch  91  loss:  0.00011561941937543452
Batch  101  loss:  0.00014062730770092458
Batch  111  loss:  0.00010312641097698361
Batch  121  loss:  0.0001099709770642221
Batch  131  loss:  0.00016452679119538516
Batch  141  loss:  0.0001420641492586583
Batch  151  loss:  9.608385153114796e-05
Batch  161  loss:  0.00013696881069336087
Batch  171  loss:  9.417530236532912e-05
Batch  181  loss:  0.00013465732627082616
Batch  191  loss:  0.00017398648196831346
Validation on real data: 
LOSS supervised-train 0.00014000061921251472, valid 0.00010697128891479224
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00015585920482408255
Batch  11  loss:  0.00016113094170577824
Batch  21  loss:  0.00010990884038619697
Batch  31  loss:  0.00017135865346062928
Batch  41  loss:  9.859938290901482e-05
Batch  51  loss:  0.00011860060476465151
Batch  61  loss:  0.00019951938884332776
Batch  71  loss:  0.00013343455793801695
Batch  81  loss:  0.00010116818157257512
Batch  91  loss:  0.00016465882072225213
Batch  101  loss:  0.00011782883666455746
Batch  111  loss:  0.00010697924881242216
Batch  121  loss:  0.00011929274478461593
Batch  131  loss:  0.00011779159103753045
Batch  141  loss:  0.00014659891894552857
Batch  151  loss:  0.00011178130080224946
Batch  161  loss:  0.00011173659731866792
Batch  171  loss:  0.00011081215052399784
Batch  181  loss:  0.00013148428115528077
Batch  191  loss:  0.00013124650286044925
Validation on real data: 
LOSS supervised-train 0.00013661825461895206, valid 0.00010259091504849494
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00019668306049425155
Batch  11  loss:  0.0001412786659784615
Batch  21  loss:  0.00010677598766051233
Batch  31  loss:  0.00016253899957519025
Batch  41  loss:  0.00011010628077201545
Batch  51  loss:  0.00014268310042098165
Batch  61  loss:  0.00017587622278369963
Batch  71  loss:  0.0001578064839122817
Batch  81  loss:  9.641363431001082e-05
Batch  91  loss:  0.00013040626072324812
Batch  101  loss:  0.00013127198326401412
Batch  111  loss:  0.00012708066788036376
Batch  121  loss:  9.504866466159001e-05
Batch  131  loss:  0.00016899011097848415
Batch  141  loss:  0.00013259093975648284
Batch  151  loss:  9.331598994322121e-05
Batch  161  loss:  0.0001408823736710474
Batch  171  loss:  0.00011647537030512467
Batch  181  loss:  0.00013930849672760814
Batch  191  loss:  0.00016769136709626764
Validation on real data: 
LOSS supervised-train 0.00013537158778490266, valid 0.00013073737500235438
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.00019578907813411206
Batch  11  loss:  0.00014503115380648524
Batch  21  loss:  0.00011521147098392248
Batch  31  loss:  0.0001259534910786897
Batch  41  loss:  0.00013110676081851125
Batch  51  loss:  0.00010016182204708457
Batch  61  loss:  0.00016075774328783154
Batch  71  loss:  0.00016391088138334453
Batch  81  loss:  9.357630187878385e-05
Batch  91  loss:  0.0001442648353986442
Batch  101  loss:  0.00012102509936084971
Batch  111  loss:  0.00010754564573289827
Batch  121  loss:  8.491764310747385e-05
Batch  131  loss:  0.0001712272787699476
Batch  141  loss:  0.00010625566937960684
Batch  151  loss:  8.811598672764376e-05
Batch  161  loss:  0.0001418782485416159
Batch  171  loss:  0.00012103343033231795
Batch  181  loss:  0.0001201802006107755
Batch  191  loss:  0.000182232353836298
Validation on real data: 
LOSS supervised-train 0.00013619760818983195, valid 0.00011150463251397014
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.00014489743625745177
Batch  11  loss:  0.00015168506070040166
Batch  21  loss:  8.735082519706339e-05
Batch  31  loss:  0.00015304225962609053
Batch  41  loss:  0.0001064847965608351
Batch  51  loss:  9.826837776927277e-05
Batch  61  loss:  0.00019610536401160061
Batch  71  loss:  0.00016701150161679834
Batch  81  loss:  7.922025542939082e-05
Batch  91  loss:  0.00017927156295627356
Batch  101  loss:  0.00010999990627169609
Batch  111  loss:  0.0001329241058556363
Batch  121  loss:  0.0001037912952597253
Batch  131  loss:  0.00011889165034517646
Batch  141  loss:  0.00013019729522056878
Batch  151  loss:  0.00011048123997170478
Batch  161  loss:  0.0001285656908294186
Batch  171  loss:  0.00011145095049869269
Batch  181  loss:  0.00013477973698172718
Batch  191  loss:  0.00016446303925476968
Validation on real data: 
LOSS supervised-train 0.00013236478087492288, valid 0.000113796500954777
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00013336795382201672
Batch  11  loss:  0.00019641636754386127
Batch  21  loss:  9.860580757958815e-05
Batch  31  loss:  0.00014421695959754288
Batch  41  loss:  0.00012896455882582814
Batch  51  loss:  0.00010351125820307061
Batch  61  loss:  0.00016480960766784847
Batch  71  loss:  0.0001586294820299372
Batch  81  loss:  0.00013363569451030344
Batch  91  loss:  0.0001484247768530622
Batch  101  loss:  0.00011821696534752846
Batch  111  loss:  0.00011888210428878665
Batch  121  loss:  9.970970131689683e-05
Batch  131  loss:  0.00014708797971252352
Batch  141  loss:  9.562513878336176e-05
Batch  151  loss:  9.737630898598582e-05
Batch  161  loss:  0.00010569830919848755
Batch  171  loss:  0.00010944522364297882
Batch  181  loss:  0.00013833114644512534
Batch  191  loss:  0.00016779519501142204
Validation on real data: 
LOSS supervised-train 0.00012838225833547768, valid 0.00013526275870390236
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.00015668610285501927
Batch  11  loss:  0.0001290064974455163
Batch  21  loss:  8.46547627588734e-05
Batch  31  loss:  0.0001456632453482598
Batch  41  loss:  9.774727368494496e-05
Batch  51  loss:  7.767669740132987e-05
Batch  61  loss:  0.00018271245062351227
Batch  71  loss:  0.0001852346322266385
Batch  81  loss:  7.939460192574188e-05
Batch  91  loss:  0.00014855283370707184
Batch  101  loss:  9.980785398511216e-05
Batch  111  loss:  0.0001318330760113895
Batch  121  loss:  0.00013659948308486491
Batch  131  loss:  0.0001507590350229293
Batch  141  loss:  0.00015222605725284666
Batch  151  loss:  0.00010359441512264311
Batch  161  loss:  0.00011340107448631898
Batch  171  loss:  9.868208144325763e-05
Batch  181  loss:  0.00012458792480174452
Batch  191  loss:  0.00013948931882623583
Validation on real data: 
LOSS supervised-train 0.00013127573212841524, valid 0.00010286767064826563
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00018184873624704778
Batch  11  loss:  0.0001380669855279848
Batch  21  loss:  0.0001301710872212425
Batch  31  loss:  0.0001799277524696663
Batch  41  loss:  0.00013782990572508425
Batch  51  loss:  0.00012433950905688107
Batch  61  loss:  0.0001978118671104312
Batch  71  loss:  0.00015362097474280745
Batch  81  loss:  0.0001133234181907028
Batch  91  loss:  0.00017821331857703626
Batch  101  loss:  0.00010500151256565005
Batch  111  loss:  0.00010591462341835722
Batch  121  loss:  8.986092871055007e-05
Batch  131  loss:  0.00012689712457358837
Batch  141  loss:  0.00010250267951050773
Batch  151  loss:  8.547158358851448e-05
Batch  161  loss:  0.0001313819084316492
Batch  171  loss:  0.00012258774950169027
Batch  181  loss:  0.00012191930727567524
Batch  191  loss:  0.00015633553266525269
Validation on real data: 
LOSS supervised-train 0.00013253966521006077, valid 0.00013106822734698653
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  airplane ; Model ID: 3db61220251b3c9de719b5362fe06bbb
--------------------
Training baseline regression model:  2022-03-30 12:46:31.779452
Detector:  pointnet
Object:  airplane
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1614643
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.10459724813699722
Batch  11  loss:  0.07029688358306885
Batch  21  loss:  0.05454947426915169
Batch  31  loss:  0.050467900931835175
Batch  41  loss:  0.04574750363826752
Batch  51  loss:  0.04635782539844513
Batch  61  loss:  0.046111173927783966
Batch  71  loss:  0.04002983123064041
Batch  81  loss:  0.04260058328509331
Batch  91  loss:  0.0365230031311512
Validation on real data: 
LOSS supervised-train 0.04922120437026024, valid 0.03408528119325638
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.031093385070562363
Batch  11  loss:  0.032451894134283066
Batch  21  loss:  0.026262333616614342
Batch  31  loss:  0.021237045526504517
Batch  41  loss:  0.01860288344323635
Batch  51  loss:  0.015215925872325897
Batch  61  loss:  0.018689338117837906
Batch  71  loss:  0.01398199051618576
Batch  81  loss:  0.009368520230054855
Batch  91  loss:  0.007004939019680023
Validation on real data: 
LOSS supervised-train 0.0189614974334836, valid 0.006917281076312065
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.006373434327542782
Batch  11  loss:  0.009367686696350574
Batch  21  loss:  0.006080910563468933
Batch  31  loss:  0.006395020987838507
Batch  41  loss:  0.006891570053994656
Batch  51  loss:  0.007023167330771685
Batch  61  loss:  0.008446372114121914
Batch  71  loss:  0.0066188182681798935
Batch  81  loss:  0.00665180804207921
Batch  91  loss:  0.004663921892642975
Validation on real data: 
LOSS supervised-train 0.007411377653479576, valid 0.0033472436480224133
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.004667890258133411
Batch  11  loss:  0.007358639035373926
Batch  21  loss:  0.005095498636364937
Batch  31  loss:  0.0038607039023190737
Batch  41  loss:  0.00480258371680975
Batch  51  loss:  0.00632505165413022
Batch  61  loss:  0.006054938770830631
Batch  71  loss:  0.0054119727574288845
Batch  81  loss:  0.006261733826249838
Batch  91  loss:  0.003719487925991416
Validation on real data: 
LOSS supervised-train 0.005541520291008056, valid 0.0016259291442111135
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0034123598597943783
Batch  11  loss:  0.004370423033833504
Batch  21  loss:  0.0032646399922668934
Batch  31  loss:  0.0032161681447178125
Batch  41  loss:  0.003328738734126091
Batch  51  loss:  0.00564620504155755
Batch  61  loss:  0.006241203285753727
Batch  71  loss:  0.005213563330471516
Batch  81  loss:  0.004909418523311615
Batch  91  loss:  0.003051847219467163
Validation on real data: 
LOSS supervised-train 0.004537909128703177, valid 0.0017347035463899374
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0033451009076088667
Batch  11  loss:  0.0038489496801048517
Batch  21  loss:  0.0037474040873348713
Batch  31  loss:  0.003479433013126254
Batch  41  loss:  0.00336554110981524
Batch  51  loss:  0.003905187826603651
Batch  61  loss:  0.004936577752232552
Batch  71  loss:  0.0036962192971259356
Batch  81  loss:  0.004224151838570833
Batch  91  loss:  0.003709605196490884
Validation on real data: 
LOSS supervised-train 0.0039005591487511993, valid 0.0013306610053405166
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.002890004776418209
Batch  11  loss:  0.00454674893990159
Batch  21  loss:  0.0029592581558972597
Batch  31  loss:  0.0031747871544212103
Batch  41  loss:  0.0033728433772921562
Batch  51  loss:  0.0037865620106458664
Batch  61  loss:  0.0035644785966724157
Batch  71  loss:  0.003848908469080925
Batch  81  loss:  0.0035036620683968067
Batch  91  loss:  0.0025959943886846304
Validation on real data: 
LOSS supervised-train 0.0036527710454538464, valid 0.0012049284996464849
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.002015979727730155
Batch  11  loss:  0.0035005626268684864
Batch  21  loss:  0.0029373830184340477
Batch  31  loss:  0.0028296560049057007
Batch  41  loss:  0.0036961694713681936
Batch  51  loss:  0.003908449783921242
Batch  61  loss:  0.003030674997717142
Batch  71  loss:  0.0037755253724753857
Batch  81  loss:  0.0032929549925029278
Batch  91  loss:  0.0024229930713772774
Validation on real data: 
LOSS supervised-train 0.003283770530251786, valid 0.0013019796460866928
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0024737014900892973
Batch  11  loss:  0.0035723356995731592
Batch  21  loss:  0.0024221742060035467
Batch  31  loss:  0.0028640490490943193
Batch  41  loss:  0.0027365644928067923
Batch  51  loss:  0.005669684149324894
Batch  61  loss:  0.002995716640725732
Batch  71  loss:  0.0028832152020186186
Batch  81  loss:  0.0025240862742066383
Batch  91  loss:  0.0026282649487257004
Validation on real data: 
LOSS supervised-train 0.0030878659337759016, valid 0.0008417625795118511
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0020949249155819416
Batch  11  loss:  0.0028390134684741497
Batch  21  loss:  0.0021693280432373285
Batch  31  loss:  0.002079951809719205
Batch  41  loss:  0.003135905833914876
Batch  51  loss:  0.003942958544939756
Batch  61  loss:  0.004073552321642637
Batch  71  loss:  0.003636531066149473
Batch  81  loss:  0.0026119323447346687
Batch  91  loss:  0.0018828221363946795
Validation on real data: 
LOSS supervised-train 0.0029907413024920972, valid 0.0009257539641112089
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0023325697984546423
Batch  11  loss:  0.0028136877808719873
Batch  21  loss:  0.0024397470988333225
Batch  31  loss:  0.00244839652441442
Batch  41  loss:  0.0025848113000392914
Batch  51  loss:  0.0027313232421875
Batch  61  loss:  0.0031900941394269466
Batch  71  loss:  0.00359653914347291
Batch  81  loss:  0.0021555344574153423
Batch  91  loss:  0.0022365583572536707
Validation on real data: 
LOSS supervised-train 0.0028430617426056415, valid 0.0011418460635468364
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0020983819849789143
Batch  11  loss:  0.0028092171996831894
Batch  21  loss:  0.0022383793257176876
Batch  31  loss:  0.002844330621883273
Batch  41  loss:  0.003503423882648349
Batch  51  loss:  0.003485856344923377
Batch  61  loss:  0.0018876086687669158
Batch  71  loss:  0.003234727308154106
Batch  81  loss:  0.002448322484269738
Batch  91  loss:  0.002644162392243743
Validation on real data: 
LOSS supervised-train 0.002724682189291343, valid 0.0008759009651839733
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0022635527420789003
Batch  11  loss:  0.0021531612146645784
Batch  21  loss:  0.001858395873568952
Batch  31  loss:  0.0019455751171335578
Batch  41  loss:  0.0022299992851912975
Batch  51  loss:  0.003262452082708478
Batch  61  loss:  0.002734282286837697
Batch  71  loss:  0.003898301627486944
Batch  81  loss:  0.00270769651979208
Batch  91  loss:  0.002595416735857725
Validation on real data: 
LOSS supervised-train 0.0026195999048650265, valid 0.0006797995883971453
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.002046534325927496
Batch  11  loss:  0.0024687594268471003
Batch  21  loss:  0.0019761978182941675
Batch  31  loss:  0.0029729001689702272
Batch  41  loss:  0.0025223721750080585
Batch  51  loss:  0.0031825124751776457
Batch  61  loss:  0.003042733296751976
Batch  71  loss:  0.003307158360257745
Batch  81  loss:  0.003043866716325283
Batch  91  loss:  0.0017981609562411904
Validation on real data: 
LOSS supervised-train 0.002504918564809486, valid 0.0007911734282970428
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0016190612222999334
Batch  11  loss:  0.002152578206732869
Batch  21  loss:  0.001984136877581477
Batch  31  loss:  0.0018120607128366828
Batch  41  loss:  0.002075505442917347
Batch  51  loss:  0.0035941381938755512
Batch  61  loss:  0.0030320247169584036
Batch  71  loss:  0.0029918691143393517
Batch  81  loss:  0.0019968366250395775
Batch  91  loss:  0.002039881655946374
Validation on real data: 
LOSS supervised-train 0.002449450058629736, valid 0.0008064870489761233
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0021628327667713165
Batch  11  loss:  0.002347624860703945
Batch  21  loss:  0.0018160941544920206
Batch  31  loss:  0.0028284452855587006
Batch  41  loss:  0.0022113840095698833
Batch  51  loss:  0.002825902309268713
Batch  61  loss:  0.001826565945520997
Batch  71  loss:  0.0026927324943244457
Batch  81  loss:  0.002509400714188814
Batch  91  loss:  0.0013306353939697146
Validation on real data: 
LOSS supervised-train 0.0023194386973045765, valid 0.0005866377614438534
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0017394584137946367
Batch  11  loss:  0.002572089433670044
Batch  21  loss:  0.001516128540970385
Batch  31  loss:  0.0022033737041056156
Batch  41  loss:  0.0021398859098553658
Batch  51  loss:  0.0031996953766793013
Batch  61  loss:  0.0032800822518765926
Batch  71  loss:  0.002944599837064743
Batch  81  loss:  0.0026777652092278004
Batch  91  loss:  0.001906612073071301
Validation on real data: 
LOSS supervised-train 0.002339218507986516, valid 0.0005870374734513462
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.002003979403525591
Batch  11  loss:  0.002406546613201499
Batch  21  loss:  0.0021436517126858234
Batch  31  loss:  0.002743686083704233
Batch  41  loss:  0.0022228779271245003
Batch  51  loss:  0.0028821337036788464
Batch  61  loss:  0.0023068764712661505
Batch  71  loss:  0.002671054331585765
Batch  81  loss:  0.0019565760158002377
Batch  91  loss:  0.0019909110851585865
Validation on real data: 
LOSS supervised-train 0.0023147041583433748, valid 0.0006351025076583028
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0018474854296073318
Batch  11  loss:  0.0018061260925605893
Batch  21  loss:  0.0021557179279625416
Batch  31  loss:  0.0020363882649689913
Batch  41  loss:  0.0022497703321278095
Batch  51  loss:  0.002624900545924902
Batch  61  loss:  0.0024018564727157354
Batch  71  loss:  0.002986286533996463
Batch  81  loss:  0.0017916052602231503
Batch  91  loss:  0.0019319048151373863
Validation on real data: 
LOSS supervised-train 0.002290994813665748, valid 0.0006994780851528049
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0017835925100371242
Batch  11  loss:  0.002538201864808798
Batch  21  loss:  0.001847165054641664
Batch  31  loss:  0.0020532957278192043
Batch  41  loss:  0.001839528908021748
Batch  51  loss:  0.002445521065965295
Batch  61  loss:  0.0016162513056769967
Batch  71  loss:  0.0029371667187660933
Batch  81  loss:  0.0016918843612074852
Batch  91  loss:  0.0019322847947478294
Validation on real data: 
LOSS supervised-train 0.002190152278635651, valid 0.0008282884955406189
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0014043361879885197
Batch  11  loss:  0.0015361768892034888
Batch  21  loss:  0.0016658487729728222
Batch  31  loss:  0.0016826412174850702
Batch  41  loss:  0.001636230619624257
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>PT>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Wed 30 Mar 2022 12:53:20 PM EDT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>POINTNET>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Wed 30 Mar 2022 12:53:20 PM EDT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  airplane ; Model ID: 3db61220251b3c9de719b5362fe06bbb
--------------------
Training baseline regression model:  2022-03-30 12:53:23.085353
Detector:  pointnet
Object:  airplane
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1614643
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.10015765577554703
Batch  11  loss:  0.07124488055706024
Batch  21  loss:  0.055457424372434616
Batch  31  loss:  0.04762062802910805
Batch  41  loss:  0.045801762491464615
Batch  51  loss:  0.04499006271362305
Batch  61  loss:  0.04408454895019531
Batch  71  loss:  0.039005622267723083
Batch  81  loss:  0.042321059852838516
Batch  91  loss:  0.035429105162620544
Validation on real data: 
LOSS supervised-train 0.048842970486730336, valid 0.03210492432117462
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.030151333659887314
Batch  11  loss:  0.03243987262248993
Batch  21  loss:  0.025492843240499496
Batch  31  loss:  0.020883556455373764
Batch  41  loss:  0.01767164282500744
Batch  51  loss:  0.014639290980994701
Batch  61  loss:  0.01578490063548088
Batch  71  loss:  0.01323461253196001
Batch  81  loss:  0.009800976142287254
Batch  91  loss:  0.00831680279225111
Validation on real data: 
LOSS supervised-train 0.018456445564515887, valid 0.00526391202583909
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.007786403875797987
Batch  11  loss:  0.011919667012989521
Batch  21  loss:  0.0061833844520151615
Batch  31  loss:  0.0055205971002578735
Batch  41  loss:  0.006391581147909164
Batch  51  loss:  0.008208394050598145
Batch  61  loss:  0.007904666475951672
Batch  71  loss:  0.006753637455403805
Batch  81  loss:  0.006528310477733612
Batch  91  loss:  0.00535793649032712
Validation on real data: 
LOSS supervised-train 0.007479656282812357, valid 0.0035609626211225986
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.0038868498522788286
Batch  11  loss:  0.007103755138814449
Batch  21  loss:  0.0034982103388756514
Batch  31  loss:  0.00466479267925024
Batch  41  loss:  0.0051024239510297775
Batch  51  loss:  0.006185231264680624
Batch  61  loss:  0.007160917390137911
Batch  71  loss:  0.0061535583809018135
Batch  81  loss:  0.005637030582875013
Batch  91  loss:  0.0033549871295690536
Validation on real data: 
LOSS supervised-train 0.005294934737030417, valid 0.0017081480473279953
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.003344170283526182
Batch  11  loss:  0.004574147053062916
Batch  21  loss:  0.0036429702304303646
Batch  31  loss:  0.004301709588617086
Batch  41  loss:  0.003388628363609314
Batch  51  loss:  0.0058022127486765385
Batch  61  loss:  0.004727524239569902
Batch  71  loss:  0.005338212009519339
Batch  81  loss:  0.004264320246875286
Batch  91  loss:  0.00359401130117476
Validation on real data: 
LOSS supervised-train 0.004307286243420094, valid 0.0014832851011306047
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0030091956723481417
Batch  11  loss:  0.004985275212675333
Batch  21  loss:  0.00263190851546824
Batch  31  loss:  0.0030775100458413363
Batch  41  loss:  0.003809636225923896
Batch  51  loss:  0.0051930868066847324
Batch  61  loss:  0.004998371005058289
Batch  71  loss:  0.005225911736488342
Batch  81  loss:  0.0036744666285812855
Batch  91  loss:  0.0027885171584784985
Validation on real data: 
LOSS supervised-train 0.003940572603605688, valid 0.0015309086302295327
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.002314302371814847
Batch  11  loss:  0.004131698980927467
Batch  21  loss:  0.0033631115220487118
Batch  31  loss:  0.0032817518804222345
Batch  41  loss:  0.0034941653721034527
Batch  51  loss:  0.0034738199319690466
Batch  61  loss:  0.004785092547535896
Batch  71  loss:  0.005488371942192316
Batch  81  loss:  0.003616754896938801
Batch  91  loss:  0.002690196270123124
Validation on real data: 
LOSS supervised-train 0.003624780422542244, valid 0.001113055506721139
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0024216827005147934
Batch  11  loss:  0.003609798848628998
Batch  21  loss:  0.00188090605661273
Batch  31  loss:  0.003462586086243391
Batch  41  loss:  0.0026551370974630117
Batch  51  loss:  0.004869875963777304
Batch  61  loss:  0.003915878944098949
Batch  71  loss:  0.00426592817530036
Batch  81  loss:  0.003019770374521613
Batch  91  loss:  0.0026807496324181557
Validation on real data: 
LOSS supervised-train 0.0032535052951425314, valid 0.0010269255144521594
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0032512557227164507
Batch  11  loss:  0.002834743820130825
Batch  21  loss:  0.002982568694278598
Batch  31  loss:  0.0031440125312656164
Batch  41  loss:  0.0023488083388656378
Batch  51  loss:  0.00249613169580698
Batch  61  loss:  0.0028842368628829718
Batch  71  loss:  0.00479408772662282
Batch  81  loss:  0.0028448360972106457
Batch  91  loss:  0.0017694529378786683
Validation on real data: 
LOSS supervised-train 0.0031541743909474463, valid 0.0010845062788575888
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.002416237723082304
Batch  11  loss:  0.002574001206085086
Batch  21  loss:  0.0020976841915398836
Batch  31  loss:  0.002940189093351364
Batch  41  loss:  0.0026854807510972023
Batch  51  loss:  0.0033595645800232887
Batch  61  loss:  0.002802463946864009
Batch  71  loss:  0.0038754898123443127
Batch  81  loss:  0.002394752809777856
Batch  91  loss:  0.002756045898422599
Validation on real data: 
LOSS supervised-train 0.002903140629641712, valid 0.0006146945524960756
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0017953560454770923
Batch  11  loss:  0.0026598102413117886
Batch  21  loss:  0.002336708130314946
Batch  31  loss:  0.0023653958924114704
Batch  41  loss:  0.002288991352543235
Batch  51  loss:  0.0033186194486916065
Batch  61  loss:  0.002989834174513817
Batch  71  loss:  0.0035340148024260998
Batch  81  loss:  0.002867142204195261
Batch  91  loss:  0.0024531965609639883
Validation on real data: 
LOSS supervised-train 0.0028415177378337832, valid 0.0007113743922673166
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0021098481956869364
Batch  11  loss:  0.002628666814416647
Batch  21  loss:  0.0018240368226543069
Batch  31  loss:  0.0029311052057892084
Batch  41  loss:  0.0021191095001995564
Batch  51  loss:  0.003854715032503009
Batch  61  loss:  0.0023928501177579165
Batch  71  loss:  0.0038715999107807875
Batch  81  loss:  0.0027749112341552973
Batch  91  loss:  0.002092650393024087
Validation on real data: 
LOSS supervised-train 0.0027642435557208955, valid 0.0009876935509964824
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0018558965530246496
Batch  11  loss:  0.0023987265303730965
Batch  21  loss:  0.002001007553189993
Batch  31  loss:  0.0019156717462465167
Batch  41  loss:  0.002480642404407263
Batch  51  loss:  0.004994313232600689
Batch  61  loss:  0.002841564593836665
Batch  71  loss:  0.0035933842882514
Batch  81  loss:  0.0025837731081992388
Batch  91  loss:  0.0024819648824632168
Validation on real data: 
LOSS supervised-train 0.002631339367944747, valid 0.0008798561757430434
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00227848207578063
Batch  11  loss:  0.002481347182765603
Batch  21  loss:  0.0016567365964874625
Batch  31  loss:  0.002788522746413946
Batch  41  loss:  0.0022403739858418703
Batch  51  loss:  0.0032402267679572105
Batch  61  loss:  0.0028721916023641825
Batch  71  loss:  0.003896986832842231
Batch  81  loss:  0.002447070786729455
Batch  91  loss:  0.002596713602542877
Validation on real data: 
LOSS supervised-train 0.002546918095322326, valid 0.0006938245496712625
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0018861960852518678
Batch  11  loss:  0.0023084767162799835
Batch  21  loss:  0.0021842317655682564
Batch  31  loss:  0.0027753144968301058
Batch  41  loss:  0.0024517783895134926
Batch  51  loss:  0.0025272592902183533
Batch  61  loss:  0.002943116705864668
Batch  71  loss:  0.0031894310377538204
Batch  81  loss:  0.0024114095140248537
Batch  91  loss:  0.0023406273685395718
Validation on real data: 
LOSS supervised-train 0.0024366552860010417, valid 0.0006730110035277903
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0018971929093822837
Batch  11  loss:  0.0021745688281953335
Batch  21  loss:  0.0022357895504683256
Batch  31  loss:  0.0018906688783317804
Batch  41  loss:  0.0021461383439600468
Batch  51  loss:  0.0029632721561938524
Batch  61  loss:  0.002240189351141453
Batch  71  loss:  0.002499212743714452
Batch  81  loss:  0.0022769561037421227
Batch  91  loss:  0.002248147502541542
Validation on real data: 
LOSS supervised-train 0.002417219434864819, valid 0.0006426576874218881
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.002230321057140827
Batch  11  loss:  0.0024298124480992556
Batch  21  loss:  0.0020295206923037767
Batch  31  loss:  0.0024450658820569515
Batch  41  loss:  0.0025923799257725477
Batch  51  loss:  0.0029735879506915808
Batch  61  loss:  0.0028020802419632673
Batch  71  loss:  0.002285153605043888
Batch  81  loss:  0.0022554865572601557
Batch  91  loss:  0.002314942190423608
Validation on real data: 
LOSS supervised-train 0.0023183964320924134, valid 0.0007895737653598189
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0020830905996263027
Batch  11  loss:  0.001751132309436798
Batch  21  loss:  0.0017616930417716503
Batch  31  loss:  0.0025215107016265392
Batch  41  loss:  0.0017688783118501306
Batch  51  loss:  0.0031993871089071035
Batch  61  loss:  0.0020262154284864664
Batch  71  loss:  0.002631288720294833
Batch  81  loss:  0.0015982555923983455
Batch  91  loss:  0.0015406408347189426
Validation on real data: 
LOSS supervised-train 0.0022343764826655388, valid 0.0006085574277676642
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.001905063632875681
Batch  11  loss:  0.001984093338251114
Batch  21  loss:  0.001474246964789927
Batch  31  loss:  0.0020470789168030024
Batch  41  loss:  0.001961133675649762
Batch  51  loss:  0.002337311627343297
Batch  61  loss:  0.0023524744901806116
Batch  71  loss:  0.003017187351360917
Batch  81  loss:  0.002223208313807845
Batch  91  loss:  0.002388431690633297
Validation on real data: 
LOSS supervised-train 0.002235412448644638, valid 0.0008861053502187133
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0018507072236388922
Batch  11  loss:  0.0015240147477015853
Batch  21  loss:  0.002139581134542823
Batch  31  loss:  0.0014694231795147061
Batch  41  loss:  0.001528944936580956
Batch  51  loss:  0.002633846364915371
Batch  61  loss:  0.002719832817092538
Batch  71  loss:  0.002495741005986929
Batch  81  loss:  0.0019934261217713356
Batch  91  loss:  0.0019558346830308437
Validation on real data: 
LOSS supervised-train 0.002135147218359634, valid 0.0006228962447494268
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0016199459787458181
Batch  11  loss:  0.0016856988659128547
Batch  21  loss:  0.0012847223551943898
Batch  31  loss:  0.0013030285481363535
Batch  41  loss:  0.0023620808497071266
Batch  51  loss:  0.0025122722145169973
Batch  61  loss:  0.001921951537951827
Batch  71  loss:  0.0022960107307881117
Batch  81  loss:  0.0019054850563406944
Batch  91  loss:  0.0016311013605445623
Validation on real data: 
LOSS supervised-train 0.0020429293694905937, valid 0.0007347630453296006
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.001782428240403533
Batch  11  loss:  0.0017587183974683285
Batch  21  loss:  0.0017473625484853983
Batch  31  loss:  0.001925914199091494
Batch  41  loss:  0.0019771247170865536
Batch  51  loss:  0.002877963474020362
Batch  61  loss:  0.0032309209927916527
Batch  71  loss:  0.0023899751249700785
Batch  81  loss:  0.0018855130765587091
Batch  91  loss:  0.0014932076446712017
Validation on real data: 
LOSS supervised-train 0.0019874622963834553, valid 0.0006637847982347012
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00133175787050277
Batch  11  loss:  0.0017597128171473742
Batch  21  loss:  0.0017963647842407227
Batch  31  loss:  0.001643671072088182
Batch  41  loss:  0.0016444927314296365
Batch  51  loss:  0.0022954712621867657
Batch  61  loss:  0.0018556939903646708
Batch  71  loss:  0.0026594852097332478
Batch  81  loss:  0.0016716205282136798
Batch  91  loss:  0.001485504792071879
Validation on real data: 
LOSS supervised-train 0.0019593483104836194, valid 0.0007051693391986191
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0013780951267108321
Batch  11  loss:  0.0016843128250911832
Batch  21  loss:  0.0016249243635684252
Batch  31  loss:  0.0014315858716145158
Batch  41  loss:  0.0016873597633093596
Batch  51  loss:  0.0026879524812102318
Batch  61  loss:  0.001511088921688497
Batch  71  loss:  0.0028083764482289553
Batch  81  loss:  0.0017737466841936111
Batch  91  loss:  0.002271064789965749
Validation on real data: 
LOSS supervised-train 0.0019609366822987795, valid 0.0007402334013022482
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0014248124789446592
Batch  11  loss:  0.0012983458582311869
Batch  21  loss:  0.0016291813226416707
Batch  31  loss:  0.0017515720101073384
Batch  41  loss:  0.0017619600985199213
Batch  51  loss:  0.0026293874252587557
Batch  61  loss:  0.0022503274958580732
Batch  71  loss:  0.002668773289769888
Batch  81  loss:  0.0018538793083280325
Batch  91  loss:  0.0017215064726769924
Validation on real data: 
LOSS supervised-train 0.0019435465382412077, valid 0.0005107535398565233
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.001337871653959155
Batch  11  loss:  0.0013999446528032422
Batch  21  loss:  0.0013564506080001593
Batch  31  loss:  0.002469463972374797
Batch  41  loss:  0.0018992333207279444
Batch  51  loss:  0.0029178871773183346
Batch  61  loss:  0.0024016101378947496
Batch  71  loss:  0.0020678890869021416
Batch  81  loss:  0.0018117025028914213
Batch  91  loss:  0.0017652591923251748
Validation on real data: 
LOSS supervised-train 0.0019662682415219025, valid 0.0006746893050149083
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0011985297314822674
Batch  11  loss:  0.0017326273955404758
Batch  21  loss:  0.0016901311464607716
Batch  31  loss:  0.0017908259760588408
Batch  41  loss:  0.0018385815201327205
Batch  51  loss:  0.002822345355525613
Batch  61  loss:  0.00211747782304883
Batch  71  loss:  0.002244706731289625
Batch  81  loss:  0.0015465411124750972
Batch  91  loss:  0.0021584229543805122
Validation on real data: 
LOSS supervised-train 0.0018555684760212898, valid 0.0005799062782898545
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.001716588158160448
Batch  11  loss:  0.0017282543703913689
Batch  21  loss:  0.0014533019857481122
Batch  31  loss:  0.0017558493418619037
Batch  41  loss:  0.0014502924168482423
Batch  51  loss:  0.0016979891806840897
Batch  61  loss:  0.0017533903010189533
Batch  71  loss:  0.002167309168726206
Batch  81  loss:  0.0017584499437361956
Batch  91  loss:  0.001649571000598371
Validation on real data: 
LOSS supervised-train 0.0019019790773745625, valid 0.0006721534300595522
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0012947206851094961
Batch  11  loss:  0.0011734856525436044
Batch  21  loss:  0.0016231781337410212
Batch  31  loss:  0.0014468791196122766
Batch  41  loss:  0.0020609002094715834
Batch  51  loss:  0.0017902133986353874
Batch  61  loss:  0.0016169835580512881
Batch  71  loss:  0.001859111711382866
Batch  81  loss:  0.001394872204400599
Batch  91  loss:  0.0015707358252257109
Validation on real data: 
LOSS supervised-train 0.0017538143310230226, valid 0.0006026640767231584
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0014558767434209585
Batch  11  loss:  0.001110013690777123
Batch  21  loss:  0.0015482872258871794
Batch  31  loss:  0.0012380048865452409
Batch  41  loss:  0.0015687727136537433
Batch  51  loss:  0.0017761063063517213
Batch  61  loss:  0.0018728100694715977
Batch  71  loss:  0.0024829290341585875
Batch  81  loss:  0.001422591507434845
Batch  91  loss:  0.0017810013378039002
Validation on real data: 
LOSS supervised-train 0.001726264018798247, valid 0.0006026519113220274
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.001515809679403901
Batch  11  loss:  0.0015031840885058045
Batch  21  loss:  0.0015519337030127645
Batch  31  loss:  0.001843946403823793
Batch  41  loss:  0.0019014881690964103
Batch  51  loss:  0.0024325044360011816
Batch  61  loss:  0.0017229693476110697
Batch  71  loss:  0.0024662725627422333
Batch  81  loss:  0.0019550358410924673
Batch  91  loss:  0.0015881180297583342
Validation on real data: 
LOSS supervised-train 0.001687842708779499, valid 0.0007284886087290943
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0017706311773508787
Batch  11  loss:  0.0013935547322034836
Batch  21  loss:  0.001386972377076745
Batch  31  loss:  0.0018789017340168357
Batch  41  loss:  0.0016322466544806957
Batch  51  loss:  0.0017623476451262832
Batch  61  loss:  0.0016140408115461469
Batch  71  loss:  0.0027091465890407562
Batch  81  loss:  0.0012919005239382386
Batch  91  loss:  0.0022012037225067616
Validation on real data: 
LOSS supervised-train 0.0016904264653567224, valid 0.00047027727123349905
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0013268819311633706
Batch  11  loss:  0.0013393089175224304
Batch  21  loss:  0.0014226329512894154
Batch  31  loss:  0.0015301982639357448
Batch  41  loss:  0.001700433436781168
Batch  51  loss:  0.002060860861092806
Batch  61  loss:  0.0013028077082708478
Batch  71  loss:  0.0029803814832121134
Batch  81  loss:  0.001452457974664867
Batch  91  loss:  0.0017920094542205334
Validation on real data: 
LOSS supervised-train 0.0016721646010410041, valid 0.0007602110272273421
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0012815224472433329
Batch  11  loss:  0.0015809989999979734
Batch  21  loss:  0.0011807455448433757
Batch  31  loss:  0.0015064304461702704
Batch  41  loss:  0.0011193488026037812
Batch  51  loss:  0.0022645771969109774
Batch  61  loss:  0.0020607246551662683
Batch  71  loss:  0.001673162798397243
Batch  81  loss:  0.001809518551453948
Batch  91  loss:  0.0016836660215631127
Validation on real data: 
LOSS supervised-train 0.0016257880884222686, valid 0.0005870038294233382
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0011701027397066355
Batch  11  loss:  0.002361177233979106
Batch  21  loss:  0.001265573431737721
Batch  31  loss:  0.0014788467669859529
Batch  41  loss:  0.0015319290105253458
Batch  51  loss:  0.0017564840381965041
Batch  61  loss:  0.0016056570457294583
Batch  71  loss:  0.002174207242205739
Batch  81  loss:  0.001243965933099389
Batch  91  loss:  0.001315274857915938
Validation on real data: 
LOSS supervised-train 0.0015662559040356427, valid 0.0007143519469536841
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.001138584571890533
Batch  11  loss:  0.0014577563852071762
Batch  21  loss:  0.001228590146638453
Batch  31  loss:  0.001363576971925795
Batch  41  loss:  0.0017709931125864387
Batch  51  loss:  0.0017715956782922149
Batch  61  loss:  0.0014774225419387221
Batch  71  loss:  0.0019216417567804456
Batch  81  loss:  0.0014763721264898777
Batch  91  loss:  0.0011009791633114219
Validation on real data: 
LOSS supervised-train 0.0015906261740019545, valid 0.000569606723729521
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0015228723641484976
Batch  11  loss:  0.001495929667726159
Batch  21  loss:  0.001264616847038269
Batch  31  loss:  0.0014878014335408807
Batch  41  loss:  0.0012729712761938572
Batch  51  loss:  0.0014501326950266957
Batch  61  loss:  0.001737102516926825
Batch  71  loss:  0.0015464650932699442
Batch  81  loss:  0.0014242478646337986
Batch  91  loss:  0.001223783241584897
Validation on real data: 
LOSS supervised-train 0.0015438096842262895, valid 0.0006188328261487186
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0013629592722281814
Batch  11  loss:  0.002233287086710334
Batch  21  loss:  0.0013808321673423052
Batch  31  loss:  0.002230083104223013
Batch  41  loss:  0.0011205130722373724
Batch  51  loss:  0.0016858471790328622
Batch  61  loss:  0.0012285730335861444
Batch  71  loss:  0.0016242576530203223
Batch  81  loss:  0.0013249358162283897
Batch  91  loss:  0.0014639109140262008
Validation on real data: 
LOSS supervised-train 0.0015423878229921684, valid 0.0009978614980354905
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0016765233594924212
Batch  11  loss:  0.0015664097154513001
Batch  21  loss:  0.001094323000870645
Batch  31  loss:  0.0013158763758838177
Batch  41  loss:  0.0018754544435068965
Batch  51  loss:  0.0021077077835798264
Batch  61  loss:  0.0017452038591727614
Batch  71  loss:  0.0025436324067413807
Batch  81  loss:  0.0014694513520225883
Batch  91  loss:  0.0013073396403342485
Validation on real data: 
LOSS supervised-train 0.0016122221341356635, valid 0.0005726900417357683
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0011123211588710546
Batch  11  loss:  0.0013809562660753727
Batch  21  loss:  0.0010306474287062883
Batch  31  loss:  0.0019769531209021807
Batch  41  loss:  0.0013424152275547385
Batch  51  loss:  0.0014262559125199914
Batch  61  loss:  0.0015229913406074047
Batch  71  loss:  0.001870637759566307
Batch  81  loss:  0.0011852973839268088
Batch  91  loss:  0.0016434112330898643
Validation on real data: 
LOSS supervised-train 0.0014792061143089085, valid 0.0005807047127746046
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0012004025047644973
Batch  11  loss:  0.0013441661139950156
Batch  21  loss:  0.0010487961117178202
Batch  31  loss:  0.0013639149256050587
Batch  41  loss:  0.0013688297476619482
Batch  51  loss:  0.002221077913418412
Batch  61  loss:  0.002223944291472435
Batch  71  loss:  0.0015178477624431252
Batch  81  loss:  0.0013927792897447944
Batch  91  loss:  0.0015510829398408532
Validation on real data: 
LOSS supervised-train 0.0015115348307881504, valid 0.000793409941252321
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0009627451072447002
Batch  11  loss:  0.0011522634886205196
Batch  21  loss:  0.001492694253101945
Batch  31  loss:  0.00211784103885293
Batch  41  loss:  0.0011606010375544429
Batch  51  loss:  0.0013322026934474707
Batch  61  loss:  0.0011526879388839006
Batch  71  loss:  0.002274241764098406
Batch  81  loss:  0.0012984302593395114
Batch  91  loss:  0.0015447953483089805
Validation on real data: 
LOSS supervised-train 0.001491485546575859, valid 0.0008002124377526343
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0011249880772083998
Batch  11  loss:  0.0011206166818737984
Batch  21  loss:  0.001240390120074153
Batch  31  loss:  0.0015705376863479614
Batch  41  loss:  0.0010905463714152575
Batch  51  loss:  0.0016972795128822327
Batch  61  loss:  0.0011951056076213717
Batch  71  loss:  0.0014221180463209748
Batch  81  loss:  0.001216390053741634
Batch  91  loss:  0.0012597611639648676
Validation on real data: 
LOSS supervised-train 0.0014442726486595349, valid 0.0008247527875937521
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0011023808037862182
Batch  11  loss:  0.0014588427729904652
Batch  21  loss:  0.001154514029622078
Batch  31  loss:  0.0012326379073783755
Batch  41  loss:  0.0010712944203987718
Batch  51  loss:  0.0016301440773531795
Batch  61  loss:  0.0015307794092223048
Batch  71  loss:  0.002095900708809495
Batch  81  loss:  0.0017199055291712284
Batch  91  loss:  0.0012294863117858768
Validation on real data: 
LOSS supervised-train 0.0014284509961726144, valid 0.000614585995208472
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0016313319792971015
Batch  11  loss:  0.0008596768602728844
Batch  21  loss:  0.0012347155716270208
Batch  31  loss:  0.0011440415401011705
Batch  41  loss:  0.0014088527532294393
Batch  51  loss:  0.0017262237379327416
Batch  61  loss:  0.001739004859700799
Batch  71  loss:  0.002118162577971816
Batch  81  loss:  0.0013966396218165755
Batch  91  loss:  0.001034751650877297
Validation on real data: 
LOSS supervised-train 0.0014414908504113554, valid 0.0007247613975778222
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0012352268677204847
Batch  11  loss:  0.0020906429272145033
Batch  21  loss:  0.0014645974151790142
Batch  31  loss:  0.0013382381293922663
Batch  41  loss:  0.0011190965306013823
Batch  51  loss:  0.0014156815595924854
Batch  61  loss:  0.0013643191196024418
Batch  71  loss:  0.0016794432885944843
Batch  81  loss:  0.0011844004038721323
Batch  91  loss:  0.0011896690120920539
Validation on real data: 
LOSS supervised-train 0.0013788360601756722, valid 0.0006732003530487418
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0013538275379687548
Batch  11  loss:  0.001694754813797772
Batch  21  loss:  0.0014830372529104352
Batch  31  loss:  0.0014171501388773322
Batch  41  loss:  0.0010982671519741416
Batch  51  loss:  0.0009786420268937945
Batch  61  loss:  0.0013153853360563517
Batch  71  loss:  0.0016090760473161936
Batch  81  loss:  0.0014608693309128284
Batch  91  loss:  0.0012605207739397883
Validation on real data: 
LOSS supervised-train 0.0013692664122208953, valid 0.000586759066209197
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0010275061940774322
Batch  11  loss:  0.001133312121964991
Batch  21  loss:  0.0009233657619915903
Batch  31  loss:  0.001252806163392961
Batch  41  loss:  0.0011601457372307777
Batch  51  loss:  0.0017827893607318401
Batch  61  loss:  0.0010010298574343324
Batch  71  loss:  0.0013974806061014533
Batch  81  loss:  0.00154879002366215
Batch  91  loss:  0.0014303792268037796
Validation on real data: 
LOSS supervised-train 0.001348020217847079, valid 0.0006736494251526892
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0010213132482022047
Batch  11  loss:  0.001311590662226081
Batch  21  loss:  0.001202226965688169
Batch  31  loss:  0.0015293946489691734
Batch  41  loss:  0.001404428738169372
Batch  51  loss:  0.0010783616453409195
Batch  61  loss:  0.0012091679964214563
Batch  71  loss:  0.0014306541997939348
Batch  81  loss:  0.0011169323697686195
Batch  91  loss:  0.001037955516949296
Validation on real data: 
LOSS supervised-train 0.001379118677577935, valid 0.0005232133553363383
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0013801814056932926
Batch  11  loss:  0.0012834587832912803
Batch  21  loss:  0.001220431993715465
Batch  31  loss:  0.0009647376136854291
Batch  41  loss:  0.0008636903367005289
Batch  51  loss:  0.0019247159361839294
Batch  61  loss:  0.0018960107117891312
Batch  71  loss:  0.0017500420799478889
Batch  81  loss:  0.0013306541368365288
Batch  91  loss:  0.0015314354095607996
Validation on real data: 
LOSS supervised-train 0.001359901461401023, valid 0.0006361078121699393
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0009312457987107337
Batch  11  loss:  0.001262643258087337
Batch  21  loss:  0.001334727043285966
Batch  31  loss:  0.00118270143866539
Batch  41  loss:  0.0012208614498376846
Batch  51  loss:  0.0013518977211788297
Batch  61  loss:  0.0011447693686932325
Batch  71  loss:  0.0015663525555282831
Batch  81  loss:  0.0012939807493239641
Batch  91  loss:  0.001319308066740632
Validation on real data: 
LOSS supervised-train 0.001332544330507517, valid 0.000671531306579709
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.001332218642346561
Batch  11  loss:  0.0013382311444729567
Batch  21  loss:  0.0013018869794905186
Batch  31  loss:  0.0012268245918676257
Batch  41  loss:  0.0011653538094833493
Batch  51  loss:  0.0016382216708734632
Batch  61  loss:  0.0016782794846221805
Batch  71  loss:  0.0017659219447523355
Batch  81  loss:  0.0012576300650835037
Batch  91  loss:  0.0010875276057049632
Validation on real data: 
LOSS supervised-train 0.0012959262757794932, valid 0.0008278469322249293
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0013858872698619962
Batch  11  loss:  0.0011034250492230058
Batch  21  loss:  0.0011578573612496257
Batch  31  loss:  0.001322227530181408
Batch  41  loss:  0.0012029140489175916
Batch  51  loss:  0.002152274828404188
Batch  61  loss:  0.0012065921910107136
Batch  71  loss:  0.0017390549182891846
Batch  81  loss:  0.0010772945825010538
Batch  91  loss:  0.001357163884676993
Validation on real data: 
LOSS supervised-train 0.0013017304998356849, valid 0.0008067040471360087
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0011142779840156436
Batch  11  loss:  0.0010754329850897193
Batch  21  loss:  0.0010205729631707072
Batch  31  loss:  0.0014829416759312153
Batch  41  loss:  0.0012586192460730672
Batch  51  loss:  0.0012612130958586931
Batch  61  loss:  0.0013096982147544622
Batch  71  loss:  0.0014399361098185182
Batch  81  loss:  0.0009089275263249874
Batch  91  loss:  0.0013044604565948248
Validation on real data: 
LOSS supervised-train 0.0013422251847805455, valid 0.0007355667767114937
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0010062881046906114
Batch  11  loss:  0.0012080144369974732
Batch  21  loss:  0.0012660964857786894
Batch  31  loss:  0.0017880010418593884
Batch  41  loss:  0.0012216289760544896
Batch  51  loss:  0.0010558436624705791
Batch  61  loss:  0.0011831489391624928
Batch  71  loss:  0.0016902340576052666
Batch  81  loss:  0.0011720778420567513
Batch  91  loss:  0.0011810819851234555
Validation on real data: 
LOSS supervised-train 0.0012630324595374986, valid 0.000728194834664464
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0014096852391958237
Batch  11  loss:  0.0014114099321886897
Batch  21  loss:  0.0012626851676031947
Batch  31  loss:  0.001493574003688991
Batch  41  loss:  0.0016563391545787454
Batch  51  loss:  0.001166638219729066
Batch  61  loss:  0.001540058059617877
Batch  71  loss:  0.0014963283902034163
Batch  81  loss:  0.0012919605942443013
Batch  91  loss:  0.0014837048947811127
Validation on real data: 
LOSS supervised-train 0.0012945075548486784, valid 0.0006875271210446954
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.001347881625406444
Batch  11  loss:  0.0012296759523451328
Batch  21  loss:  0.0012294731568545103
Batch  31  loss:  0.0011685641948133707
Batch  41  loss:  0.0010447808308526874
Batch  51  loss:  0.0019040353363379836
Batch  61  loss:  0.0011674659326672554
Batch  71  loss:  0.001829118700698018
Batch  81  loss:  0.0012767278822138906
Batch  91  loss:  0.0009043623576872051
Validation on real data: 
LOSS supervised-train 0.0012976755161071196, valid 0.0006104018539190292
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0008900998509489
Batch  11  loss:  0.0009382591815665364
Batch  21  loss:  0.0010923820082098246
Batch  31  loss:  0.0009520328021608293
Batch  41  loss:  0.0013819488231092691
Batch  51  loss:  0.0015259642386808991
Batch  61  loss:  0.0010559234069660306
Batch  71  loss:  0.0016401070170104504
Batch  81  loss:  0.0010067035909742117
Batch  91  loss:  0.0011513554491102695
Validation on real data: 
LOSS supervised-train 0.0012148912413977087, valid 0.0007828555535525084
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0011911102337762713
Batch  11  loss:  0.001488076290115714
Batch  21  loss:  0.001187377143651247
Batch  31  loss:  0.0009680023649707437
Batch  41  loss:  0.0013064214726909995
Batch  51  loss:  0.001270937267690897
Batch  61  loss:  0.001116319908760488
Batch  71  loss:  0.0013798841973766685
Batch  81  loss:  0.0013817742001265287
Batch  91  loss:  0.0012070268858224154
Validation on real data: 
LOSS supervised-train 0.0012715624802513048, valid 0.0006917773862369359
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0009217874612659216
Batch  11  loss:  0.0009643167722970247
Batch  21  loss:  0.0007497976766899228
Batch  31  loss:  0.0010702679865062237
Batch  41  loss:  0.0010303424205631018
Batch  51  loss:  0.0015221479116007686
Batch  61  loss:  0.0009869120549410582
Batch  71  loss:  0.0013920377241447568
Batch  81  loss:  0.0010492976289242506
Batch  91  loss:  0.00120908475946635
Validation on real data: 
LOSS supervised-train 0.00119967445905786, valid 0.0007418379536829889
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0014014861080795527
Batch  11  loss:  0.0012418717378750443
Batch  21  loss:  0.0009007055195979774
Batch  31  loss:  0.0010986902052536607
Batch  41  loss:  0.0015132817206904292
Batch  51  loss:  0.0012268584687262774
Batch  61  loss:  0.0009740865207277238
Batch  71  loss:  0.0018139479216188192
Batch  81  loss:  0.001012437860481441
Batch  91  loss:  0.001134140300564468
Validation on real data: 
LOSS supervised-train 0.0012262355547863991, valid 0.0007050301064737141
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0009818047983571887
Batch  11  loss:  0.0010846619261428714
Batch  21  loss:  0.0008720995392650366
Batch  31  loss:  0.0009333943598903716
Batch  41  loss:  0.0014144391752779484
Batch  51  loss:  0.0008512870990671217
Batch  61  loss:  0.001202862593345344
Batch  71  loss:  0.0010862983763217926
Batch  81  loss:  0.0011737683089450002
Batch  91  loss:  0.0009219601633958519
Validation on real data: 
LOSS supervised-train 0.0011534494446823374, valid 0.0007555913762189448
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0010126568377017975
Batch  11  loss:  0.00106771569699049
Batch  21  loss:  0.0007561363745480776
Batch  31  loss:  0.0012087535578757524
Batch  41  loss:  0.0013098078779876232
Batch  51  loss:  0.0011292457347735763
Batch  61  loss:  0.0013990449951961637
Batch  71  loss:  0.0011781028006225824
Batch  81  loss:  0.0011063952697440982
Batch  91  loss:  0.0010562888346612453
Validation on real data: 
LOSS supervised-train 0.0012306010955944657, valid 0.0008331663557328284
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0014353125588968396
Batch  11  loss:  0.001220555859617889
Batch  21  loss:  0.0012592175044119358
Batch  31  loss:  0.0007888674736022949
Batch  41  loss:  0.0009582682978361845
Batch  51  loss:  0.001252570073120296
Batch  61  loss:  0.0010922482470050454
Batch  71  loss:  0.001728502451442182
Batch  81  loss:  0.0012296275235712528
Batch  91  loss:  0.0009830164490267634
Validation on real data: 
LOSS supervised-train 0.0012013282562838867, valid 0.0005835990305058658
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0013350553344935179
Batch  11  loss:  0.0011378050548955798
Batch  21  loss:  0.0012087313225492835
Batch  31  loss:  0.0010620264802128077
Batch  41  loss:  0.0014322077622637153
Batch  51  loss:  0.0014633425744250417
Batch  61  loss:  0.001168863265775144
Batch  71  loss:  0.0013523062225431204
Batch  81  loss:  0.0010196201037615538
Batch  91  loss:  0.0012922820169478655
Validation on real data: 
LOSS supervised-train 0.0011715878901304678, valid 0.0007164946291595697
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.001140949665568769
Batch  11  loss:  0.001490255817770958
Batch  21  loss:  0.001104003400541842
Batch  31  loss:  0.0012444874737411737
Batch  41  loss:  0.001241876743733883
Batch  51  loss:  0.0011847765417769551
Batch  61  loss:  0.0014964365400373936
Batch  71  loss:  0.0012627437245100737
Batch  81  loss:  0.0010008149547502398
Batch  91  loss:  0.0009675867040641606
Validation on real data: 
LOSS supervised-train 0.0011834269540850074, valid 0.0005179294385015965
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0009231315343640745
Batch  11  loss:  0.001235499163158238
Batch  21  loss:  0.0010183037957176566
Batch  31  loss:  0.001077101333066821
Batch  41  loss:  0.0011103844735771418
Batch  51  loss:  0.0016118501080200076
Batch  61  loss:  0.0010376241989433765
Batch  71  loss:  0.0013882758794352412
Batch  81  loss:  0.0008887398871593177
Batch  91  loss:  0.0008763701771385968
Validation on real data: 
LOSS supervised-train 0.0011552724643843248, valid 0.0006766091100871563
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.001110330456867814
Batch  11  loss:  0.0011094239307567477
Batch  21  loss:  0.0009530538809485734
Batch  31  loss:  0.0010614129714667797
Batch  41  loss:  0.0008769977139309049
Batch  51  loss:  0.0012177994940429926
Batch  61  loss:  0.0009718299261294305
Batch  71  loss:  0.0013084054226055741
Batch  81  loss:  0.0008611573139205575
Batch  91  loss:  0.0010336455889046192
Validation on real data: 
LOSS supervised-train 0.0011738123401300981, valid 0.00048623414477333426
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0009814177174121141
Batch  11  loss:  0.0010339723667129874
Batch  21  loss:  0.0008468343294225633
Batch  31  loss:  0.0007578795775771141
Batch  41  loss:  0.0009375241352245212
Batch  51  loss:  0.0010274694068357348
Batch  61  loss:  0.0012816053349524736
Batch  71  loss:  0.0012884280877187848
Batch  81  loss:  0.0010196516523137689
Batch  91  loss:  0.0007707980694249272
Validation on real data: 
LOSS supervised-train 0.0011258875083876773, valid 0.0007202890119515359
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0012453004019334912
Batch  11  loss:  0.0009622512734495103
Batch  21  loss:  0.001050977734848857
Batch  31  loss:  0.0011205024784430861
Batch  41  loss:  0.00135910720564425
Batch  51  loss:  0.0011794756865128875
Batch  61  loss:  0.0012199109187349677
Batch  71  loss:  0.0010648089228197932
Batch  81  loss:  0.001136008882895112
Batch  91  loss:  0.0009579856996424496
Validation on real data: 
LOSS supervised-train 0.0011503865488339217, valid 0.000673003785777837
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0008921618573367596
Batch  11  loss:  0.0013566857669502497
Batch  21  loss:  0.0007915148162283003
Batch  31  loss:  0.0010355910053476691
Batch  41  loss:  0.000848626543302089
Batch  51  loss:  0.001203274354338646
Batch  61  loss:  0.0009758799569681287
Batch  71  loss:  0.0009162693750113249
Batch  81  loss:  0.0009358823299407959
Batch  91  loss:  0.0009667760459706187
Validation on real data: 
LOSS supervised-train 0.0011256611644057557, valid 0.0006330765900202096
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0010663020657375455
Batch  11  loss:  0.0011340781347826123
Batch  21  loss:  0.00097870163153857
Batch  31  loss:  0.0013479712652042508
Batch  41  loss:  0.0010627342853695154
Batch  51  loss:  0.0010121989762410522
Batch  61  loss:  0.001280563767068088
Batch  71  loss:  0.0015183370560407639
Batch  81  loss:  0.0011496616061776876
Batch  91  loss:  0.0008825761615298688
Validation on real data: 
LOSS supervised-train 0.0011055534094339236, valid 0.0005992940277792513
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0012386010494083166
Batch  11  loss:  0.0011550076305866241
Batch  21  loss:  0.0009216411272063851
Batch  31  loss:  0.0009174044826067984
Batch  41  loss:  0.0010305902687832713
Batch  51  loss:  0.0011906077852472663
Batch  61  loss:  0.0009917364222928882
Batch  71  loss:  0.0014047053409740329
Batch  81  loss:  0.0008615588885731995
Batch  91  loss:  0.0007807480287738144
Validation on real data: 
LOSS supervised-train 0.0011397563439095393, valid 0.0008477949886582792
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0010303850285708904
Batch  11  loss:  0.0013473647413775325
Batch  21  loss:  0.00075627799378708
Batch  31  loss:  0.0010219718096777797
Batch  41  loss:  0.001199663383886218
Batch  51  loss:  0.001107933814637363
Batch  61  loss:  0.0011182773159816861
Batch  71  loss:  0.001370893558487296
Batch  81  loss:  0.0011258079903200269
Batch  91  loss:  0.0010389392264187336
Validation on real data: 
LOSS supervised-train 0.0011289223009953275, valid 0.0005082107381895185
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0007623218698427081
Batch  11  loss:  0.0008589497301727533
Batch  21  loss:  0.0011829393915832043
Batch  31  loss:  0.0012463325401768088
Batch  41  loss:  0.0010230315383523703
Batch  51  loss:  0.001116768573410809
Batch  61  loss:  0.001585043384693563
Batch  71  loss:  0.001237939577549696
Batch  81  loss:  0.0010488448897376657
Batch  91  loss:  0.001009950996376574
Validation on real data: 
LOSS supervised-train 0.0011181712662801145, valid 0.0008170087239705026
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0009595765150152147
Batch  11  loss:  0.0014283601194620132
Batch  21  loss:  0.000832471065223217
Batch  31  loss:  0.0010721132857725024
Batch  41  loss:  0.0010821799514815211
Batch  51  loss:  0.0008289675461128354
Batch  61  loss:  0.0011623480822890997
Batch  71  loss:  0.0012795409420505166
Batch  81  loss:  0.001024388475343585
Batch  91  loss:  0.0011484434362500906
Validation on real data: 
LOSS supervised-train 0.0011109968536766246, valid 0.0007341303280554712
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0013324993196874857
Batch  11  loss:  0.0012353277998045087
Batch  21  loss:  0.001216619974002242
Batch  31  loss:  0.001236456329934299
Batch  41  loss:  0.0012590870028361678
Batch  51  loss:  0.0012258082861080766
Batch  61  loss:  0.0013173853512853384
Batch  71  loss:  0.0011879135854542255
Batch  81  loss:  0.0009478546562604606
Batch  91  loss:  0.001029336592182517
Validation on real data: 
LOSS supervised-train 0.0010831575555494055, valid 0.0007404036005027592
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0010230636689811945
Batch  11  loss:  0.0010739644058048725
Batch  21  loss:  0.0007080462528392673
Batch  31  loss:  0.0010479057673364878
Batch  41  loss:  0.0009350974578410387
Batch  51  loss:  0.0014007292920723557
Batch  61  loss:  0.0009528223890811205
Batch  71  loss:  0.0013452823041006923
Batch  81  loss:  0.0009551712428219616
Batch  91  loss:  0.0007574169430881739
Validation on real data: 
LOSS supervised-train 0.0010904630355071277, valid 0.0006984702195040882
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0010571543825790286
Batch  11  loss:  0.0010717138648033142
Batch  21  loss:  0.0008593273814767599
Batch  31  loss:  0.0012584221549332142
Batch  41  loss:  0.001031754189170897
Batch  51  loss:  0.0010453339200466871
Batch  61  loss:  0.0009827541653066874
Batch  71  loss:  0.0011504307622089982
Batch  81  loss:  0.001042052055709064
Batch  91  loss:  0.0013165319105610251
Validation on real data: 
LOSS supervised-train 0.0011272255424410104, valid 0.000641652790363878
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0011584071908146143
Batch  11  loss:  0.0009854253148660064
Batch  21  loss:  0.0012063214089721441
Batch  31  loss:  0.0010215871734544635
Batch  41  loss:  0.0012596723390743136
Batch  51  loss:  0.001467586262151599
Batch  61  loss:  0.0013041449710726738
Batch  71  loss:  0.0013570478186011314
Batch  81  loss:  0.001086721895262599
Batch  91  loss:  0.0010106777772307396
Validation on real data: 
LOSS supervised-train 0.0010798637487459929, valid 0.0007048285333439708
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0009530599927529693
Batch  11  loss:  0.001151866395957768
Batch  21  loss:  0.000843181274831295
Batch  31  loss:  0.0009623561636544764
Batch  41  loss:  0.0009241019142791629
Batch  51  loss:  0.0016360533190891147
Batch  61  loss:  0.0010825052158907056
Batch  71  loss:  0.0017658233409747481
Batch  81  loss:  0.001070805243216455
Batch  91  loss:  0.0009388017351739109
Validation on real data: 
LOSS supervised-train 0.001101761973113753, valid 0.0007775918929837644
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0009807508904486895
Batch  11  loss:  0.0008218449074774981
Batch  21  loss:  0.0010207731975242496
Batch  31  loss:  0.0010142071405425668
Batch  41  loss:  0.0009025282924994826
Batch  51  loss:  0.0011687451042234898
Batch  61  loss:  0.00096747069619596
Batch  71  loss:  0.0012274101609364152
Batch  81  loss:  0.000962227990385145
Batch  91  loss:  0.0008551581995561719
Validation on real data: 
LOSS supervised-train 0.0010446003393735736, valid 0.0007268209592439234
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0009481267188675702
Batch  11  loss:  0.001031345222145319
Batch  21  loss:  0.0008948637987487018
Batch  31  loss:  0.0011019179364666343
Batch  41  loss:  0.0013121592346578836
Batch  51  loss:  0.0011241167085245252
Batch  61  loss:  0.0010227232705801725
Batch  71  loss:  0.001228328444994986
Batch  81  loss:  0.0010116349440068007
Batch  91  loss:  0.0009657692862674594
Validation on real data: 
LOSS supervised-train 0.0010655189369572326, valid 0.0005596966366283596
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0010529413120821118
Batch  11  loss:  0.0009739984525367618
Batch  21  loss:  0.0010793589754030108
Batch  31  loss:  0.0011134151136502624
Batch  41  loss:  0.0009694081963971257
Batch  51  loss:  0.0011398578062653542
Batch  61  loss:  0.001480034668929875
Batch  71  loss:  0.0012360909022390842
Batch  81  loss:  0.0010988700669258833
Batch  91  loss:  0.0009465149487368762
Validation on real data: 
LOSS supervised-train 0.0010253481636755169, valid 0.0007996386848390102
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.000931602087803185
Batch  11  loss:  0.001021048054099083
Batch  21  loss:  0.0007529832073487341
Batch  31  loss:  0.0008171846275217831
Batch  41  loss:  0.001077626133337617
Batch  51  loss:  0.0012610027333721519
Batch  61  loss:  0.0008941489504650235
Batch  71  loss:  0.0010521200019866228
Batch  81  loss:  0.0008861641981638968
Batch  91  loss:  0.0008476409711875021
Validation on real data: 
LOSS supervised-train 0.0010127518977969885, valid 0.0005679893656633794
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0009345525177195668
Batch  11  loss:  0.0008911804179660976
Batch  21  loss:  0.0009035095572471619
Batch  31  loss:  0.0010797373251989484
Batch  41  loss:  0.0009400126291438937
Batch  51  loss:  0.0010073819430544972
Batch  61  loss:  0.000974879483692348
Batch  71  loss:  0.001216690638102591
Batch  81  loss:  0.0009563321946188807
Batch  91  loss:  0.000744469347409904
Validation on real data: 
LOSS supervised-train 0.0010259587364271283, valid 0.0005960872513242066
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0008525845478288829
Batch  11  loss:  0.0009048689971677959
Batch  21  loss:  0.0010501255746930838
Batch  31  loss:  0.000769288104493171
Batch  41  loss:  0.001155099249444902
Batch  51  loss:  0.0011763377115130424
Batch  61  loss:  0.001191688934341073
Batch  71  loss:  0.0013312087394297123
Batch  81  loss:  0.001066691125743091
Batch  91  loss:  0.000967441825196147
Validation on real data: 
LOSS supervised-train 0.0010298266657628118, valid 0.0004535990010481328
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0008747181273065507
Batch  11  loss:  0.000999254989437759
Batch  21  loss:  0.0009420024580322206
Batch  31  loss:  0.0009659699280746281
Batch  41  loss:  0.0010690877679735422
Batch  51  loss:  0.001273080357350409
Batch  61  loss:  0.0010449218098074198
Batch  71  loss:  0.0013435309519991279
Batch  81  loss:  0.0011321476195007563
Batch  91  loss:  0.0008976908284239471
Validation on real data: 
LOSS supervised-train 0.0010329180594999344, valid 0.0007114478503353894
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0010638644453138113
Batch  11  loss:  0.001028950558975339
Batch  21  loss:  0.0008427954744547606
Batch  31  loss:  0.0008520023548044264
Batch  41  loss:  0.0010592699982225895
Batch  51  loss:  0.0010438909521326423
Batch  61  loss:  0.0007306521874852479
Batch  71  loss:  0.0014080379623919725
Batch  81  loss:  0.000655925483442843
Batch  91  loss:  0.0009394791559316218
Validation on real data: 
LOSS supervised-train 0.0010188919940264897, valid 0.0008048649178817868
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0011658469447866082
Batch  11  loss:  0.0008066710433922708
Batch  21  loss:  0.0008686640649102628
Batch  31  loss:  0.0009392747888341546
Batch  41  loss:  0.0009455628460273147
Batch  51  loss:  0.0010085011599585414
Batch  61  loss:  0.0014553491491824389
Batch  71  loss:  0.0013928913976997137
Batch  81  loss:  0.0007787169306538999
Batch  91  loss:  0.0008880397654138505
Validation on real data: 
LOSS supervised-train 0.000973677936126478, valid 0.0007050796994008124
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0006824036245234311
Batch  11  loss:  0.0008525406592525542
Batch  21  loss:  0.0006096170982345939
Batch  31  loss:  0.0007441501365974545
Batch  41  loss:  0.0012790104374289513
Batch  51  loss:  0.0009642168879508972
Batch  61  loss:  0.0008963304571807384
Batch  71  loss:  0.0009942686883732677
Batch  81  loss:  0.0008505075820721686
Batch  91  loss:  0.0010337734129279852
Validation on real data: 
LOSS supervised-train 0.001012971229502, valid 0.0005534430965781212
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0006734743365086615
Batch  11  loss:  0.001304343342781067
Batch  21  loss:  0.0008761913632042706
Batch  31  loss:  0.0010005917865782976
Batch  41  loss:  0.001059057074598968
Batch  51  loss:  0.0012117454316467047
Batch  61  loss:  0.0011166060576215386
Batch  71  loss:  0.001444309949874878
Batch  81  loss:  0.0009438134729862213
Batch  91  loss:  0.0005851351306773722
Validation on real data: 
LOSS supervised-train 0.0010070219164481386, valid 0.0008196624694392085
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0009445619652979076
Batch  11  loss:  0.0010312511585652828
Batch  21  loss:  0.0010137093486264348
Batch  31  loss:  0.000784174189902842
Batch  41  loss:  0.000969517685007304
Batch  51  loss:  0.0009271736489608884
Batch  61  loss:  0.0007979893707670271
Batch  71  loss:  0.0015254586469382048
Batch  81  loss:  0.0008172279340215027
Batch  91  loss:  0.0010694111697375774
Validation on real data: 
LOSS supervised-train 0.000980121397296898, valid 0.0005614188266918063
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.001223091036081314
Batch  11  loss:  0.0006735452334396541
Batch  21  loss:  0.0009887279011309147
Batch  31  loss:  0.0010168002918362617
Batch  41  loss:  0.0010402639163658023
Batch  51  loss:  0.0009459236171096563
Batch  61  loss:  0.0009040514705702662
Batch  71  loss:  0.001289474661462009
Batch  81  loss:  0.0012187294196337461
Batch  91  loss:  0.0010459951590746641
Validation on real data: 
LOSS supervised-train 0.0009700350451748818, valid 0.0007378202280960977
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0006779397954232991
Batch  11  loss:  0.0009688204154372215
Batch  21  loss:  0.001015922287479043
Batch  31  loss:  0.0009564710198901594
Batch  41  loss:  0.000834726495668292
Batch  51  loss:  0.0012377389939501882
Batch  61  loss:  0.0010477284668013453
Batch  71  loss:  0.0013131021987646818
Batch  81  loss:  0.0009307753061875701
Batch  91  loss:  0.000817913853097707
Validation on real data: 
LOSS supervised-train 0.0009495896933367476, valid 0.000630743510555476
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0006735671777278185
Batch  11  loss:  0.001149396295659244
Batch  21  loss:  0.000789482262916863
Batch  31  loss:  0.0010133013129234314
Batch  41  loss:  0.0011003285180777311
Batch  51  loss:  0.0012446260079741478
Batch  61  loss:  0.001094832201488316
Batch  71  loss:  0.001667700824327767
Batch  81  loss:  0.0007484753150492907
Batch  91  loss:  0.00085975177353248
Validation on real data: 
LOSS supervised-train 0.000984928043326363, valid 0.0006930052186362445
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0007163492264226079
Batch  11  loss:  0.001267639105208218
Batch  21  loss:  0.0011575056705623865
Batch  31  loss:  0.0010084806708618999
Batch  41  loss:  0.0012510159285739064
Batch  51  loss:  0.0008697417797520757
Batch  61  loss:  0.0011191958328709006
Batch  71  loss:  0.0009925903286784887
Batch  81  loss:  0.0012458838755264878
Batch  91  loss:  0.0009967219084501266
Validation on real data: 
LOSS supervised-train 0.001001931781647727, valid 0.0006320974207483232
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0009810953633859754
Batch  11  loss:  0.0006300316890701652
Batch  21  loss:  0.0008847575518302619
Batch  31  loss:  0.0006822471041232347
Batch  41  loss:  0.0008721157792024314
Batch  51  loss:  0.0012859399430453777
Batch  61  loss:  0.0008861915557645261
Batch  71  loss:  0.0010264600859954953
Batch  81  loss:  0.0007924450910650194
Batch  91  loss:  0.0008339073392562568
Validation on real data: 
LOSS supervised-train 0.0009587398904841393, valid 0.0007149035227485001
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0008369330316781998
Batch  11  loss:  0.0012551010586321354
Batch  21  loss:  0.0007763478206470609
Batch  31  loss:  0.0008152226801030338
Batch  41  loss:  0.0008140391437336802
Batch  51  loss:  0.0008222424075938761
Batch  61  loss:  0.0010875388979911804
Batch  71  loss:  0.0009815440280362964
Batch  81  loss:  0.0007449524709954858
Batch  91  loss:  0.0012967735528945923
Validation on real data: 
LOSS supervised-train 0.0009798045665957034, valid 0.0008473924244754016
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0009591677808202803
Batch  11  loss:  0.0007609478197991848
Batch  21  loss:  0.000828663120046258
Batch  31  loss:  0.001072266953997314
Batch  41  loss:  0.0011406210251152515
Batch  51  loss:  0.0009984156349673867
Batch  61  loss:  0.00113652553409338
Batch  71  loss:  0.0009571684640832245
Batch  81  loss:  0.0009589014807716012
Batch  91  loss:  0.000820677843876183
Validation on real data: 
LOSS supervised-train 0.0009642293205251917, valid 0.0005581641453318298
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  bathtub ; Model ID: 90b6e958b359c1592ad490d4d7fae486
--------------------
Training baseline regression model:  2022-03-30 13:16:34.522959
Detector:  pointnet
Object:  bathtub
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1613101
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.2718954086303711
Batch  11  loss:  0.16591085493564606
Batch  21  loss:  0.13865473866462708
Batch  31  loss:  0.12380951642990112
Batch  41  loss:  0.1186748743057251
Batch  51  loss:  0.1131804808974266
Batch  61  loss:  0.10495235025882721
Batch  71  loss:  0.08759158104658127
Batch  81  loss:  0.08289206027984619
Batch  91  loss:  0.06856057047843933
Validation on real data: 
LOSS supervised-train 0.11606927122920752, valid 0.08896365761756897
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.06552194803953171
Batch  11  loss:  0.054384201765060425
Batch  21  loss:  0.031696248799562454
Batch  31  loss:  0.042130861431360245
Batch  41  loss:  0.023836078122258186
Batch  51  loss:  0.03130028769373894
Batch  61  loss:  0.023847104981541634
Batch  71  loss:  0.017602385953068733
Batch  81  loss:  0.02758767083287239
Batch  91  loss:  0.022526122629642487
Validation on real data: 
LOSS supervised-train 0.02966477763839066, valid 0.013132044114172459
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.027501840144395828
Batch  11  loss:  0.013854016549885273
Batch  21  loss:  0.007707235869020224
Batch  31  loss:  0.012274160049855709
Batch  41  loss:  0.00948195531964302
Batch  51  loss:  0.008504331111907959
Batch  61  loss:  0.009690655395388603
Batch  71  loss:  0.007270289584994316
Batch  81  loss:  0.014060217887163162
Batch  91  loss:  0.01957966573536396
Validation on real data: 
LOSS supervised-train 0.011201401967555284, valid 0.005936376750469208
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.010608631186187267
Batch  11  loss:  0.008950539864599705
Batch  21  loss:  0.004365261178463697
Batch  31  loss:  0.006412389222532511
Batch  41  loss:  0.005015337374061346
Batch  51  loss:  0.006925599649548531
Batch  61  loss:  0.007713737897574902
Batch  71  loss:  0.005320926196873188
Batch  81  loss:  0.012482262216508389
Batch  91  loss:  0.014085715636610985
Validation on real data: 
LOSS supervised-train 0.008066738704219461, valid 0.0035904906690120697
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0073333559557795525
Batch  11  loss:  0.008745551109313965
Batch  21  loss:  0.004022400360554457
Batch  31  loss:  0.006636984646320343
Batch  41  loss:  0.00474057300016284
Batch  51  loss:  0.005708909127861261
Batch  61  loss:  0.006837177090346813
Batch  71  loss:  0.004171385429799557
Batch  81  loss:  0.010842396877706051
Batch  91  loss:  0.013818874023854733
Validation on real data: 
LOSS supervised-train 0.007219166739378125, valid 0.00354630290530622
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.006395835895091295
Batch  11  loss:  0.008849207311868668
Batch  21  loss:  0.00325095746666193
Batch  31  loss:  0.004985167179256678
Batch  41  loss:  0.004516385030001402
Batch  51  loss:  0.005385030992329121
Batch  61  loss:  0.005829613655805588
Batch  71  loss:  0.004189759958535433
Batch  81  loss:  0.009022229351103306
Batch  91  loss:  0.010605809278786182
Validation on real data: 
LOSS supervised-train 0.006350615259725601, valid 0.004852131009101868
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.006271473132073879
Batch  11  loss:  0.006836872082203627
Batch  21  loss:  0.004261213820427656
Batch  31  loss:  0.004249433521181345
Batch  41  loss:  0.004011569079011679
Batch  51  loss:  0.004751006606966257
Batch  61  loss:  0.005727136041969061
Batch  71  loss:  0.003504485357552767
Batch  81  loss:  0.006577501073479652
Batch  91  loss:  0.011092409491539001
Validation on real data: 
LOSS supervised-train 0.005695516890846193, valid 0.003834095550701022
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.005115647334605455
Batch  11  loss:  0.007295478601008654
Batch  21  loss:  0.004139398690313101
Batch  31  loss:  0.004016079939901829
Batch  41  loss:  0.00319067295640707
Batch  51  loss:  0.005000303965061903
Batch  61  loss:  0.00558199267834425
Batch  71  loss:  0.0037698058877140284
Batch  81  loss:  0.008926423266530037
Batch  91  loss:  0.010593828745186329
Validation on real data: 
LOSS supervised-train 0.005393538060598075, valid 0.003762406064197421
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.003624730510637164
Batch  11  loss:  0.007528824266046286
Batch  21  loss:  0.0033611697144806385
Batch  31  loss:  0.00259001343511045
Batch  41  loss:  0.003099138382822275
Batch  51  loss:  0.005162304732948542
Batch  61  loss:  0.004380114376544952
Batch  71  loss:  0.004218826070427895
Batch  81  loss:  0.004982108250260353
Batch  91  loss:  0.01060093380510807
Validation on real data: 
LOSS supervised-train 0.00495234104571864, valid 0.0034803380258381367
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0041074566543102264
Batch  11  loss:  0.006197495386004448
Batch  21  loss:  0.003074429463595152
Batch  31  loss:  0.0026378007605671883
Batch  41  loss:  0.003197640646249056
Batch  51  loss:  0.004199107643216848
Batch  61  loss:  0.0032016735058277845
Batch  71  loss:  0.004309278447180986
Batch  81  loss:  0.00664207199588418
Batch  91  loss:  0.01048694271594286
Validation on real data: 
LOSS supervised-train 0.00489816159941256, valid 0.003748642047867179
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.004213421139866114
Batch  11  loss:  0.0050088390707969666
Batch  21  loss:  0.00341924955137074
Batch  31  loss:  0.003797270590439439
Batch  41  loss:  0.0035055188927799463
Batch  51  loss:  0.003301894525066018
Batch  61  loss:  0.003762229345738888
Batch  71  loss:  0.004503847099840641
Batch  81  loss:  0.004803176503628492
Batch  91  loss:  0.00973863247781992
Validation on real data: 
LOSS supervised-train 0.004602189569268376, valid 0.003741317195817828
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0033330656588077545
Batch  11  loss:  0.006074495147913694
Batch  21  loss:  0.002938698511570692
Batch  31  loss:  0.004111058544367552
Batch  41  loss:  0.0024699412751942873
Batch  51  loss:  0.0034254277125000954
Batch  61  loss:  0.004075464326888323
Batch  71  loss:  0.0041436562314629555
Batch  81  loss:  0.005302757490426302
Batch  91  loss:  0.009025578387081623
Validation on real data: 
LOSS supervised-train 0.004370294655673206, valid 0.003511331044137478
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.004472009371966124
Batch  11  loss:  0.006128943059593439
Batch  21  loss:  0.0026538223028182983
Batch  31  loss:  0.0031013875268399715
Batch  41  loss:  0.003289635293185711
Batch  51  loss:  0.003508863039314747
Batch  61  loss:  0.0034153498709201813
Batch  71  loss:  0.0040539950132369995
Batch  81  loss:  0.003229766385629773
Batch  91  loss:  0.009600222110748291
Validation on real data: 
LOSS supervised-train 0.004281279321294278, valid 0.0029945140704512596
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.002699968870729208
Batch  11  loss:  0.005598593037575483
Batch  21  loss:  0.003560445737093687
Batch  31  loss:  0.003479225793853402
Batch  41  loss:  0.0030880479607731104
Batch  51  loss:  0.0030490641947835684
Batch  61  loss:  0.00391377042979002
Batch  71  loss:  0.003663159441202879
Batch  81  loss:  0.005612439010292292
Batch  91  loss:  0.006883678492158651
Validation on real data: 
LOSS supervised-train 0.004050298762740568, valid 0.0030948533676564693
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0034137004986405373
Batch  11  loss:  0.003790682414546609
Batch  21  loss:  0.0033674922306090593
Batch  31  loss:  0.002488876460120082
Batch  41  loss:  0.0029018793720752
Batch  51  loss:  0.0033547147177159786
Batch  61  loss:  0.0032575835939496756
Batch  71  loss:  0.003663113107904792
Batch  81  loss:  0.004138957243412733
Batch  91  loss:  0.007651560008525848
Validation on real data: 
LOSS supervised-train 0.003923184170853347, valid 0.0030862241983413696
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.002977571915835142
Batch  11  loss:  0.004462636075913906
Batch  21  loss:  0.0026183538138866425
Batch  31  loss:  0.002729903906583786
Batch  41  loss:  0.003588653402402997
Batch  51  loss:  0.0032619056291878223
Batch  61  loss:  0.003626767313107848
Batch  71  loss:  0.003366116201505065
Batch  81  loss:  0.0031184416729956865
Batch  91  loss:  0.007510205265134573
Validation on real data: 
LOSS supervised-train 0.003753572970163077, valid 0.0026003175880759954
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.002821692731231451
Batch  11  loss:  0.005443515256047249
Batch  21  loss:  0.002908945083618164
Batch  31  loss:  0.0024402469862252474
Batch  41  loss:  0.0027554344851523638
Batch  51  loss:  0.0034125347156077623
Batch  61  loss:  0.003274072427302599
Batch  71  loss:  0.003629324957728386
Batch  81  loss:  0.003806072287261486
Batch  91  loss:  0.006854555569589138
Validation on real data: 
LOSS supervised-train 0.0037194941728375852, valid 0.003292086534202099
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0038531215395778418
Batch  11  loss:  0.005055411718785763
Batch  21  loss:  0.0040738461539149284
Batch  31  loss:  0.0026652226224541664
Batch  41  loss:  0.0030814961064606905
Batch  51  loss:  0.0025577859487384558
Batch  61  loss:  0.003073713043704629
Batch  71  loss:  0.004329641815274954
Batch  81  loss:  0.0030675039160996675
Batch  91  loss:  0.006372699048370123
Validation on real data: 
LOSS supervised-train 0.003539020288735628, valid 0.002961972262710333
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0027018277905881405
Batch  11  loss:  0.004071407485753298
Batch  21  loss:  0.0034415600821375847
Batch  31  loss:  0.0023995100054889917
Batch  41  loss:  0.002938383724540472
Batch  51  loss:  0.0032730060629546642
Batch  61  loss:  0.0036334716714918613
Batch  71  loss:  0.0036733876913785934
Batch  81  loss:  0.002871152712032199
Batch  91  loss:  0.00573952728882432
Validation on real data: 
LOSS supervised-train 0.0034708513773512094, valid 0.0029696247074753046
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0027658960316330194
Batch  11  loss:  0.003690345212817192
Batch  21  loss:  0.0030258535407483578
Batch  31  loss:  0.002291313139721751
Batch  41  loss:  0.00226843124255538
Batch  51  loss:  0.0035498407669365406
Batch  61  loss:  0.003643857780843973
Batch  71  loss:  0.0028067082166671753
Batch  81  loss:  0.0029104696586728096
Batch  91  loss:  0.008282573893666267
Validation on real data: 
LOSS supervised-train 0.003434395883232355, valid 0.003322591772302985
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0024749136064201593
Batch  11  loss:  0.00458803866058588
Batch  21  loss:  0.0031507532112300396
Batch  31  loss:  0.002483135089278221
Batch  41  loss:  0.003629240905866027
Batch  51  loss:  0.002494563115760684
Batch  61  loss:  0.0035775217693299055
Batch  71  loss:  0.0032898627687245607
Batch  81  loss:  0.002522497670724988
Batch  91  loss:  0.006365284789353609
Validation on real data: 
LOSS supervised-train 0.00345319160958752, valid 0.0027674357406795025
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0026734662242233753
Batch  11  loss:  0.003849930362775922
Batch  21  loss:  0.003425927134230733
Batch  31  loss:  0.002491210587322712
Batch  41  loss:  0.002244866918772459
Batch  51  loss:  0.0029624775052070618
Batch  61  loss:  0.003119772532954812
Batch  71  loss:  0.003092781873419881
Batch  81  loss:  0.0030734704341739416
Batch  91  loss:  0.005320753902196884
Validation on real data: 
LOSS supervised-train 0.0033211874531116335, valid 0.0027849923353642225
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.003206825815141201
Batch  11  loss:  0.003432887140661478
Batch  21  loss:  0.0025373040698468685
Batch  31  loss:  0.0019367127679288387
Batch  41  loss:  0.003019096562638879
Batch  51  loss:  0.0029096747748553753
Batch  61  loss:  0.0033966628834605217
Batch  71  loss:  0.002474608365446329
Batch  81  loss:  0.002589569892734289
Batch  91  loss:  0.006507979705929756
Validation on real data: 
LOSS supervised-train 0.003218458192422986, valid 0.0026029925793409348
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0024116660933941603
Batch  11  loss:  0.003661890048533678
Batch  21  loss:  0.004087479785084724
Batch  31  loss:  0.0027875325176864862
Batch  41  loss:  0.003215379547327757
Batch  51  loss:  0.003022221615538001
Batch  61  loss:  0.0033122259192168713
Batch  71  loss:  0.002707134932279587
Batch  81  loss:  0.002942787716165185
Batch  91  loss:  0.00748129328712821
Validation on real data: 
LOSS supervised-train 0.003217394386883825, valid 0.00295573053881526
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0027484435122460127
Batch  11  loss:  0.0032746857032179832
Batch  21  loss:  0.0027995966374874115
Batch  31  loss:  0.0027134406846016645
Batch  41  loss:  0.002738277893513441
Batch  51  loss:  0.00211461097933352
Batch  61  loss:  0.003313689958304167
Batch  71  loss:  0.0036867475137114525
Batch  81  loss:  0.0027728204149752855
Batch  91  loss:  0.004837649874389172
Validation on real data: 
LOSS supervised-train 0.0031559074681717903, valid 0.003322436474263668
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0030926226172596216
Batch  11  loss:  0.0032903181854635477
Batch  21  loss:  0.00327413366176188
Batch  31  loss:  0.0017246827483177185
Batch  41  loss:  0.0032301354221999645
Batch  51  loss:  0.0035439867060631514
Batch  61  loss:  0.0033765083644539118
Batch  71  loss:  0.0027813666965812445
Batch  81  loss:  0.0019263291032984853
Batch  91  loss:  0.005765228532254696
Validation on real data: 
LOSS supervised-train 0.0030913915298879146, valid 0.002632648218423128
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0024698469787836075
Batch  11  loss:  0.00378519669175148
Batch  21  loss:  0.004558664746582508
Batch  31  loss:  0.0021177709568291903
Batch  41  loss:  0.0031288384925574064
Batch  51  loss:  0.002623077481985092
Batch  61  loss:  0.002535267500206828
Batch  71  loss:  0.0027666117530316114
Batch  81  loss:  0.002673669485375285
Batch  91  loss:  0.007339467294514179
Validation on real data: 
LOSS supervised-train 0.00308479965897277, valid 0.003093006554991007
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0026321394834667444
Batch  11  loss:  0.004125880543142557
Batch  21  loss:  0.0027449270710349083
Batch  31  loss:  0.0027337120845913887
Batch  41  loss:  0.002505726180970669
Batch  51  loss:  0.0028416344430297613
Batch  61  loss:  0.00289606349542737
Batch  71  loss:  0.0031461475882679224
Batch  81  loss:  0.0023564971052110195
Batch  91  loss:  0.00660300487652421
Validation on real data: 
LOSS supervised-train 0.003107216729549691, valid 0.0026614819653332233
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0028037335723638535
Batch  11  loss:  0.002985322382301092
Batch  21  loss:  0.002040205290541053
Batch  31  loss:  0.0018138771411031485
Batch  41  loss:  0.0023764909710735083
Batch  51  loss:  0.002645316068083048
Batch  61  loss:  0.003654767759144306
Batch  71  loss:  0.0037383942399173975
Batch  81  loss:  0.0022248374298214912
Batch  91  loss:  0.00614300137385726
Validation on real data: 
LOSS supervised-train 0.002913487612968311, valid 0.0029378330800682306
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0022754594683647156
Batch  11  loss:  0.002493689302355051
Batch  21  loss:  0.0029185351449996233
Batch  31  loss:  0.002307747956365347
Batch  41  loss:  0.0032330548856407404
Batch  51  loss:  0.0025302802678197622
Batch  61  loss:  0.003205501474440098
Batch  71  loss:  0.0025235521607100964
Batch  81  loss:  0.002459704177454114
Batch  91  loss:  0.0065542892552912235
Validation on real data: 
LOSS supervised-train 0.002935801486019045, valid 0.002229181118309498
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0031076110899448395
Batch  11  loss:  0.003151785582304001
Batch  21  loss:  0.0036863081622868776
Batch  31  loss:  0.002327984431758523
Batch  41  loss:  0.00378479715436697
Batch  51  loss:  0.0029579277615994215
Batch  61  loss:  0.0024513923563063145
Batch  71  loss:  0.0025947573594748974
Batch  81  loss:  0.0023698098957538605
Batch  91  loss:  0.005071963649243116
Validation on real data: 
LOSS supervised-train 0.0028709940426051616, valid 0.00263400049880147
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.003263779217377305
Batch  11  loss:  0.0025519696064293385
Batch  21  loss:  0.003030077088624239
Batch  31  loss:  0.0016435226425528526
Batch  41  loss:  0.0025264269206672907
Batch  51  loss:  0.0025779204443097115
Batch  61  loss:  0.0029215258546173573
Batch  71  loss:  0.0038116639479994774
Batch  81  loss:  0.002337235724553466
Batch  91  loss:  0.006092413328588009
Validation on real data: 
LOSS supervised-train 0.002938727451255545, valid 0.0023991852067410946
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0019821051973849535
Batch  11  loss:  0.00313382176682353
Batch  21  loss:  0.003309684805572033
Batch  31  loss:  0.0017619553254917264
Batch  41  loss:  0.002434858586639166
Batch  51  loss:  0.002307830611243844
Batch  61  loss:  0.002806413685902953
Batch  71  loss:  0.003648818004876375
Batch  81  loss:  0.002143296878784895
Batch  91  loss:  0.004142582416534424
Validation on real data: 
LOSS supervised-train 0.0028751058725174517, valid 0.0023119160905480385
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.003133367747068405
Batch  11  loss:  0.0030889331828802824
Batch  21  loss:  0.003017962444573641
Batch  31  loss:  0.002239019377157092
Batch  41  loss:  0.0027121363673359156
Batch  51  loss:  0.0020526531152427197
Batch  61  loss:  0.00278273387812078
Batch  71  loss:  0.002842179499566555
Batch  81  loss:  0.0019142864039167762
Batch  91  loss:  0.0036223726347088814
Validation on real data: 
LOSS supervised-train 0.002857059355592355, valid 0.0021209928672760725
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.002620997140184045
Batch  11  loss:  0.002473333617672324
Batch  21  loss:  0.0035756644792854786
Batch  31  loss:  0.0027989132795482874
Batch  41  loss:  0.0018711277516558766
Batch  51  loss:  0.0022755987010896206
Batch  61  loss:  0.0025895824655890465
Batch  71  loss:  0.0024759124498814344
Batch  81  loss:  0.0026095674838870764
Batch  91  loss:  0.005230332724750042
Validation on real data: 
LOSS supervised-train 0.0029214138304814695, valid 0.002283861394971609
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0027551923412829638
Batch  11  loss:  0.002503923373296857
Batch  21  loss:  0.0033709402196109295
Batch  31  loss:  0.0018940394511446357
Batch  41  loss:  0.002410689601674676
Batch  51  loss:  0.0026863792445510626
Batch  61  loss:  0.002536532934755087
Batch  71  loss:  0.003340216586366296
Batch  81  loss:  0.0025155760813504457
Batch  91  loss:  0.004184549208730459
Validation on real data: 
LOSS supervised-train 0.002804303647717461, valid 0.00238259369507432
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0022506548557430506
Batch  11  loss:  0.0027256421744823456
Batch  21  loss:  0.0034929027315229177
Batch  31  loss:  0.002129832748323679
Batch  41  loss:  0.0023627066984772682
Batch  51  loss:  0.0039692348800599575
Batch  61  loss:  0.0023321460466831923
Batch  71  loss:  0.0029045843984931707
Batch  81  loss:  0.002098611555993557
Batch  91  loss:  0.004620605148375034
Validation on real data: 
LOSS supervised-train 0.002837534591089934, valid 0.0018618410686030984
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0020863341633230448
Batch  11  loss:  0.0030599196907132864
Batch  21  loss:  0.0028633964248001575
Batch  31  loss:  0.0014491050969809294
Batch  41  loss:  0.0020329649560153484
Batch  51  loss:  0.0024106465280056
Batch  61  loss:  0.002662904094904661
Batch  71  loss:  0.0030596181750297546
Batch  81  loss:  0.002688335720449686
Batch  91  loss:  0.003637446789070964
Validation on real data: 
LOSS supervised-train 0.002679974347120151, valid 0.002919597551226616
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.002577002625912428
Batch  11  loss:  0.0027912366203963757
Batch  21  loss:  0.0030830237083137035
Batch  31  loss:  0.0018777652876451612
Batch  41  loss:  0.0023323060013353825
Batch  51  loss:  0.0020619011484086514
Batch  61  loss:  0.002341833198443055
Batch  71  loss:  0.003127989126369357
Batch  81  loss:  0.002077138051390648
Batch  91  loss:  0.004695124924182892
Validation on real data: 
LOSS supervised-train 0.0026604321296326817, valid 0.0028749166522175074
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.002520348411053419
Batch  11  loss:  0.0018742525717243552
Batch  21  loss:  0.002391065238043666
Batch  31  loss:  0.0023128101602196693
Batch  41  loss:  0.0018835171358659863
Batch  51  loss:  0.0030969525687396526
Batch  61  loss:  0.002339475555345416
Batch  71  loss:  0.0026768106035888195
Batch  81  loss:  0.001960966270416975
Batch  91  loss:  0.0034896768629550934
Validation on real data: 
LOSS supervised-train 0.0026173808879684656, valid 0.0024745813570916653
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0025686209555715322
Batch  11  loss:  0.0026586141902953386
Batch  21  loss:  0.002717286814004183
Batch  31  loss:  0.0025217575021088123
Batch  41  loss:  0.0027465755119919777
Batch  51  loss:  0.002400786615908146
Batch  61  loss:  0.0028298243414610624
Batch  71  loss:  0.0031353789381682873
Batch  81  loss:  0.0025592839810997248
Batch  91  loss:  0.004117129370570183
Validation on real data: 
LOSS supervised-train 0.0026827262493316086, valid 0.0026009194552898407
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0021923996973782778
Batch  11  loss:  0.002279057865962386
Batch  21  loss:  0.002306978916749358
Batch  31  loss:  0.0028986542019993067
Batch  41  loss:  0.0022318875417113304
Batch  51  loss:  0.002581734675914049
Batch  61  loss:  0.0029123041313141584
Batch  71  loss:  0.002478237496688962
Batch  81  loss:  0.0022604570258408785
Batch  91  loss:  0.002593145938590169
Validation on real data: 
LOSS supervised-train 0.0025247797765769065, valid 0.002276533981785178
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.002939491765573621
Batch  11  loss:  0.0021571586839854717
Batch  21  loss:  0.0024078681599348783
Batch  31  loss:  0.003071768907830119
Batch  41  loss:  0.0021954914554953575
Batch  51  loss:  0.002286325441673398
Batch  61  loss:  0.002682644873857498
Batch  71  loss:  0.002638003556057811
Batch  81  loss:  0.0017437101341784
Batch  91  loss:  0.002790853613987565
Validation on real data: 
LOSS supervised-train 0.002640249151736498, valid 0.0020226975902915
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0023134404327720404
Batch  11  loss:  0.002387602813541889
Batch  21  loss:  0.0025469611864537
Batch  31  loss:  0.0018333719344809651
Batch  41  loss:  0.002347154077142477
Batch  51  loss:  0.0023325185757130384
Batch  61  loss:  0.0035269702784717083
Batch  71  loss:  0.0055451043881475925
Batch  81  loss:  0.0019776432309299707
Batch  91  loss:  0.0031409859657287598
Validation on real data: 
LOSS supervised-train 0.0026856274716556073, valid 0.0024727031122893095
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0020149138290435076
Batch  11  loss:  0.002478176262229681
Batch  21  loss:  0.0032445918768644333
Batch  31  loss:  0.002251697238534689
Batch  41  loss:  0.0022179281804710627
Batch  51  loss:  0.00197673705406487
Batch  61  loss:  0.003048662096261978
Batch  71  loss:  0.00469297356903553
Batch  81  loss:  0.002084320178255439
Batch  91  loss:  0.003411554731428623
Validation on real data: 
LOSS supervised-train 0.0026284995884634553, valid 0.0018808618187904358
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0027659337501972914
Batch  11  loss:  0.0026810334529727697
Batch  21  loss:  0.002682759426534176
Batch  31  loss:  0.0020390977151691914
Batch  41  loss:  0.0026961113326251507
Batch  51  loss:  0.0023568286560475826
Batch  61  loss:  0.003269959706813097
Batch  71  loss:  0.0023488830775022507
Batch  81  loss:  0.002162917982786894
Batch  91  loss:  0.0035355754662305117
Validation on real data: 
LOSS supervised-train 0.002628083494491875, valid 0.002339302795007825
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0019346934277564287
Batch  11  loss:  0.0028194645419716835
Batch  21  loss:  0.0027157103177160025
Batch  31  loss:  0.002480365801602602
Batch  41  loss:  0.0021563214249908924
Batch  51  loss:  0.0028706537559628487
Batch  61  loss:  0.002332072937861085
Batch  71  loss:  0.0027858649846166372
Batch  81  loss:  0.0028168403077870607
Batch  91  loss:  0.00393082806840539
Validation on real data: 
LOSS supervised-train 0.0026519665913656352, valid 0.0022261161357164383
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.002192458836361766
Batch  11  loss:  0.003136490471661091
Batch  21  loss:  0.0023554677609354258
Batch  31  loss:  0.0019259322434663773
Batch  41  loss:  0.001857501221820712
Batch  51  loss:  0.002837167354300618
Batch  61  loss:  0.002916032914072275
Batch  71  loss:  0.0024294781032949686
Batch  81  loss:  0.0020225474145263433
Batch  91  loss:  0.002409435110166669
Validation on real data: 
LOSS supervised-train 0.002540424509206787, valid 0.0022509575355798006
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.002543700858950615
Batch  11  loss:  0.002353809541091323
Batch  21  loss:  0.002038879320025444
Batch  31  loss:  0.002637332072481513
Batch  41  loss:  0.0028664390556514263
Batch  51  loss:  0.002682554302737117
Batch  61  loss:  0.0023156958632171154
Batch  71  loss:  0.002175874775275588
Batch  81  loss:  0.002463995711877942
Batch  91  loss:  0.003232672344893217
Validation on real data: 
LOSS supervised-train 0.002639374415157363, valid 0.002651949180290103
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0018679830245673656
Batch  11  loss:  0.0022685774601995945
Batch  21  loss:  0.0033994566183537245
Batch  31  loss:  0.002300331834703684
Batch  41  loss:  0.002131059067323804
Batch  51  loss:  0.0028679189272224903
Batch  61  loss:  0.002927080262452364
Batch  71  loss:  0.004195807036012411
Batch  81  loss:  0.0022810532245785
Batch  91  loss:  0.003904800396412611
Validation on real data: 
LOSS supervised-train 0.002609723679488525, valid 0.0020065023563802242
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.002853146055713296
Batch  11  loss:  0.002812481950968504
Batch  21  loss:  0.002217961708083749
Batch  31  loss:  0.003196597332134843
Batch  41  loss:  0.0023956228978931904
Batch  51  loss:  0.0023182572331279516
Batch  61  loss:  0.002441701479256153
Batch  71  loss:  0.0030409949831664562
Batch  81  loss:  0.0025052684359252453
Batch  91  loss:  0.0023205906618386507
Validation on real data: 
LOSS supervised-train 0.0025786117766983807, valid 0.0018622841453179717
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0023830512072890997
Batch  11  loss:  0.0030066536273807287
Batch  21  loss:  0.002794538624584675
Batch  31  loss:  0.0025875794235616922
Batch  41  loss:  0.0018491587834432721
Batch  51  loss:  0.0023986459709703922
Batch  61  loss:  0.002325282897800207
Batch  71  loss:  0.0022100810892879963
Batch  81  loss:  0.0017770546255633235
Batch  91  loss:  0.002187276491895318
Validation on real data: 
LOSS supervised-train 0.002482127323746681, valid 0.0023460215888917446
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0019985332619398832
Batch  11  loss:  0.002573547884821892
Batch  21  loss:  0.0032824347727000713
Batch  31  loss:  0.0025339035782963037
Batch  41  loss:  0.0023263180628418922
Batch  51  loss:  0.0026176508981734514
Batch  61  loss:  0.0028304874431341887
Batch  71  loss:  0.0019680096302181482
Batch  81  loss:  0.002765726065263152
Batch  91  loss:  0.002671260852366686
Validation on real data: 
LOSS supervised-train 0.002556252497015521, valid 0.002064736559987068
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0020751405972987413
Batch  11  loss:  0.003039306029677391
Batch  21  loss:  0.0020168512128293514
Batch  31  loss:  0.0020892813336104155
Batch  41  loss:  0.0021345980931073427
Batch  51  loss:  0.0022384992334991693
Batch  61  loss:  0.002228768076747656
Batch  71  loss:  0.00225212424993515
Batch  81  loss:  0.0030944489408284426
Batch  91  loss:  0.0030247760005295277
Validation on real data: 
LOSS supervised-train 0.0026008179446216674, valid 0.0021329205483198166
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0026400145143270493
Batch  11  loss:  0.0026524451095610857
Batch  21  loss:  0.0023548437748104334
Batch  31  loss:  0.002120373770594597
Batch  41  loss:  0.0018711199518293142
Batch  51  loss:  0.002405853010714054
Batch  61  loss:  0.003824210027232766
Batch  71  loss:  0.0020859837532043457
Batch  81  loss:  0.0016091907164081931
Batch  91  loss:  0.002892694901674986
Validation on real data: 
LOSS supervised-train 0.002484814568888396, valid 0.0023786064703017473
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0023073884658515453
Batch  11  loss:  0.003214579541236162
Batch  21  loss:  0.0022522881627082825
Batch  31  loss:  0.002199847251176834
Batch  41  loss:  0.002159950789064169
Batch  51  loss:  0.0024774179328233004
Batch  61  loss:  0.0023912074975669384
Batch  71  loss:  0.0022439390886574984
Batch  81  loss:  0.0017684247577562928
Batch  91  loss:  0.002735850401222706
Validation on real data: 
LOSS supervised-train 0.002576849424513057, valid 0.0023199906572699547
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.003159808926284313
Batch  11  loss:  0.001888072700239718
Batch  21  loss:  0.0015230121789500117
Batch  31  loss:  0.0026293103583157063
Batch  41  loss:  0.003352347295731306
Batch  51  loss:  0.0019116109469905496
Batch  61  loss:  0.0027324717957526445
Batch  71  loss:  0.0020465890411287546
Batch  81  loss:  0.002239339519292116
Batch  91  loss:  0.0019519854104146361
Validation on real data: 
LOSS supervised-train 0.0025645692134276033, valid 0.00223357742652297
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.001791923656128347
Batch  11  loss:  0.002452321583405137
Batch  21  loss:  0.0020409843418747187
Batch  31  loss:  0.002441887743771076
Batch  41  loss:  0.0037670470774173737
Batch  51  loss:  0.0029716843273490667
Batch  61  loss:  0.0029333478305488825
Batch  71  loss:  0.0022638007067143917
Batch  81  loss:  0.002473781118169427
Batch  91  loss:  0.0024869090411812067
Validation on real data: 
LOSS supervised-train 0.00263232113677077, valid 0.0020714672282338142
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0026220178697258234
Batch  11  loss:  0.002590527990832925
Batch  21  loss:  0.0019583371467888355
Batch  31  loss:  0.002958612283691764
Batch  41  loss:  0.00244501163251698
Batch  51  loss:  0.0019076629541814327
Batch  61  loss:  0.002887301379814744
Batch  71  loss:  0.0022530904971063137
Batch  81  loss:  0.0023799294140189886
Batch  91  loss:  0.0016106882831081748
Validation on real data: 
LOSS supervised-train 0.0025739578995853662, valid 0.0025318781845271587
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0019704229198396206
Batch  11  loss:  0.003072561929002404
Batch  21  loss:  0.0021585377398878336
Batch  31  loss:  0.0023743226192891598
Batch  41  loss:  0.0027701512444764376
Batch  51  loss:  0.0017873456235975027
Batch  61  loss:  0.0027130853850394487
Batch  71  loss:  0.0021110486704856157
Batch  81  loss:  0.001978230196982622
Batch  91  loss:  0.002405476290732622
Validation on real data: 
LOSS supervised-train 0.0024545396957546474, valid 0.0022851808462291956
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0016933446750044823
Batch  11  loss:  0.0028417783323675394
Batch  21  loss:  0.0017656665295362473
Batch  31  loss:  0.00271542533300817
Batch  41  loss:  0.0029063820838928223
Batch  51  loss:  0.0025712153874337673
Batch  61  loss:  0.0033792078029364347
Batch  71  loss:  0.0018310662126168609
Batch  81  loss:  0.0018311202293261886
Batch  91  loss:  0.002233001636341214
Validation on real data: 
LOSS supervised-train 0.0026113342179451137, valid 0.0030593746341764927
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0019153336761519313
Batch  11  loss:  0.003431620541960001
Batch  21  loss:  0.0023946750443428755
Batch  31  loss:  0.0026560279075056314
Batch  41  loss:  0.002541743451729417
Batch  51  loss:  0.0023413505405187607
Batch  61  loss:  0.00378815783187747
Batch  71  loss:  0.0028158961795270443
Batch  81  loss:  0.002162667689844966
Batch  91  loss:  0.002059638500213623
Validation on real data: 
LOSS supervised-train 0.002612340443301946, valid 0.0019339495338499546
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.002354746451601386
Batch  11  loss:  0.003565111430361867
Batch  21  loss:  0.0018040090799331665
Batch  31  loss:  0.002313351957127452
Batch  41  loss:  0.0032840704079717398
Batch  51  loss:  0.0017316748853772879
Batch  61  loss:  0.004408227279782295
Batch  71  loss:  0.0020550652407109737
Batch  81  loss:  0.0023746327497065067
Batch  91  loss:  0.002320387400686741
Validation on real data: 
LOSS supervised-train 0.0026230972970370204, valid 0.0019496206659823656
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0016753135714679956
Batch  11  loss:  0.0042474777437746525
Batch  21  loss:  0.002341476734727621
Batch  31  loss:  0.001660709735006094
Batch  41  loss:  0.0022736191749572754
Batch  51  loss:  0.0023749107494950294
Batch  61  loss:  0.0035517325159162283
Batch  71  loss:  0.0018836746457964182
Batch  81  loss:  0.0016074404120445251
Batch  91  loss:  0.0019438409944996238
Validation on real data: 
LOSS supervised-train 0.002635046769864857, valid 0.0018898126436397433
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.002367866924032569
Batch  11  loss:  0.00252494472078979
Batch  21  loss:  0.0024263958912342787
Batch  31  loss:  0.002019476145505905
Batch  41  loss:  0.0030683092772960663
Batch  51  loss:  0.0018855372909456491
Batch  61  loss:  0.0048413705080747604
Batch  71  loss:  0.0025916057638823986
Batch  81  loss:  0.0011499007232487202
Batch  91  loss:  0.0021173881832510233
Validation on real data: 
LOSS supervised-train 0.0025463111174758523, valid 0.00246012257412076
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.002221660455688834
Batch  11  loss:  0.002607006346806884
Batch  21  loss:  0.0019855827558785677
Batch  31  loss:  0.002437810879200697
Batch  41  loss:  0.0021602350752800703
Batch  51  loss:  0.0021735418122261763
Batch  61  loss:  0.004790767095983028
Batch  71  loss:  0.0030323888640850782
Batch  81  loss:  0.0021564580965787172
Batch  91  loss:  0.0026827342808246613
Validation on real data: 
LOSS supervised-train 0.0026431123784277587, valid 0.003099645022302866
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.002554147969931364
Batch  11  loss:  0.0021222776267677546
Batch  21  loss:  0.001919556176289916
Batch  31  loss:  0.002257297048345208
Batch  41  loss:  0.002702397061511874
Batch  51  loss:  0.0018831632332876325
Batch  61  loss:  0.002545233117416501
Batch  71  loss:  0.0028377901762723923
Batch  81  loss:  0.0021052444353699684
Batch  91  loss:  0.0019969490822404623
Validation on real data: 
LOSS supervised-train 0.002665839355904609, valid 0.004582438617944717
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0018636001041159034
Batch  11  loss:  0.002206620294600725
Batch  21  loss:  0.0017435213085263968
Batch  31  loss:  0.002273303922265768
Batch  41  loss:  0.003185173263773322
Batch  51  loss:  0.0022674177307635546
Batch  61  loss:  0.004050122108310461
Batch  71  loss:  0.003267781576141715
Batch  81  loss:  0.001664455747231841
Batch  91  loss:  0.002044153865426779
Validation on real data: 
LOSS supervised-train 0.002649851479800418, valid 0.002346217166632414
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0023323886562138796
Batch  11  loss:  0.00335630401968956
Batch  21  loss:  0.0030109339859336615
Batch  31  loss:  0.001307385740801692
Batch  41  loss:  0.004021778702735901
Batch  51  loss:  0.0018766315188258886
Batch  61  loss:  0.0029424356762319803
Batch  71  loss:  0.002376636490225792
Batch  81  loss:  0.0027690634597092867
Batch  91  loss:  0.0017843132372945547
Validation on real data: 
LOSS supervised-train 0.0026442857028450817, valid 0.00243090046569705
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.002350908238440752
Batch  11  loss:  0.0027379714883863926
Batch  21  loss:  0.0029437406919896603
Batch  31  loss:  0.0019181782845407724
Batch  41  loss:  0.002723594894632697
Batch  51  loss:  0.002867387840524316
Batch  61  loss:  0.003007720923051238
Batch  71  loss:  0.003828086657449603
Batch  81  loss:  0.0023930221796035767
Batch  91  loss:  0.002024278976023197
Validation on real data: 
LOSS supervised-train 0.002745583732612431, valid 0.0023764288052916527
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.002170755760744214
Batch  11  loss:  0.001913397223688662
Batch  21  loss:  0.002987529616802931
Batch  31  loss:  0.002194793662056327
Batch  41  loss:  0.0028333018999546766
Batch  51  loss:  0.00223946082405746
Batch  61  loss:  0.0024021309800446033
Batch  71  loss:  0.0034115680027753115
Batch  81  loss:  0.005029397550970316
Batch  91  loss:  0.002181626856327057
Validation on real data: 
LOSS supervised-train 0.002663700123084709, valid 0.0026718652807176113
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.003051629988476634
Batch  11  loss:  0.002696430077776313
Batch  21  loss:  0.003012811066582799
Batch  31  loss:  0.0017343852669000626
Batch  41  loss:  0.0023295937571674585
Batch  51  loss:  0.0021016448736190796
Batch  61  loss:  0.002398137003183365
Batch  71  loss:  0.0032030686270445585
Batch  81  loss:  0.005079778376966715
Batch  91  loss:  0.0021228911355137825
Validation on real data: 
LOSS supervised-train 0.0025526509992778302, valid 0.002618352649733424
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0024522515013813972
Batch  11  loss:  0.002712610410526395
Batch  21  loss:  0.0027050410863012075
Batch  31  loss:  0.001627309829927981
Batch  41  loss:  0.002589152893051505
Batch  51  loss:  0.003554237773641944
Batch  61  loss:  0.00338666420429945
Batch  71  loss:  0.0028049438260495663
Batch  81  loss:  0.004004767630249262
Batch  91  loss:  0.0018021466676145792
Validation on real data: 
LOSS supervised-train 0.0027501220151316375, valid 0.0030845627188682556
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.003378253197297454
Batch  11  loss:  0.0020483676344156265
Batch  21  loss:  0.0021996989380568266
Batch  31  loss:  0.0015113184927031398
Batch  41  loss:  0.0018776764627546072
Batch  51  loss:  0.002459185430780053
Batch  61  loss:  0.002220884896814823
Batch  71  loss:  0.0030017755925655365
Batch  81  loss:  0.00375837623141706
Batch  91  loss:  0.0020249674562364817
Validation on real data: 
LOSS supervised-train 0.0024864268419332804, valid 0.0024569688830524683
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.003053788561373949
Batch  11  loss:  0.0020702271722257137
Batch  21  loss:  0.0021272001322358847
Batch  31  loss:  0.001960637280717492
Batch  41  loss:  0.0020747599191963673
Batch  51  loss:  0.002987682819366455
Batch  61  loss:  0.002281877910718322
Batch  71  loss:  0.0031923269852995872
Batch  81  loss:  0.0034376319963485003
Batch  91  loss:  0.0019382309401407838
Validation on real data: 
LOSS supervised-train 0.002523400088539347, valid 0.002762045245617628
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0032117401715368032
Batch  11  loss:  0.0019377818098291755
Batch  21  loss:  0.002250581281259656
Batch  31  loss:  0.0015217151958495378
Batch  41  loss:  0.0016080850036814809
Batch  51  loss:  0.0024417887907475233
Batch  61  loss:  0.0019645614083856344
Batch  71  loss:  0.0035656727850437164
Batch  81  loss:  0.003423187416046858
Batch  91  loss:  0.0023977470118552446
Validation on real data: 
LOSS supervised-train 0.0025066275149583815, valid 0.0028017729055136442
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.001985692186281085
Batch  11  loss:  0.0023772618733346462
Batch  21  loss:  0.0015723513206467032
Batch  31  loss:  0.0021589796524494886
Batch  41  loss:  0.001830095425248146
Batch  51  loss:  0.003505009924992919
Batch  61  loss:  0.002145984908565879
Batch  71  loss:  0.00256607448682189
Batch  81  loss:  0.002164020435884595
Batch  91  loss:  0.0027524486649781466
Validation on real data: 
LOSS supervised-train 0.0024821048986632377, valid 0.002619324252009392
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.002752970904111862
Batch  11  loss:  0.0019428296945989132
Batch  21  loss:  0.002384146908298135
Batch  31  loss:  0.0024440614506602287
Batch  41  loss:  0.0021773655898869038
Batch  51  loss:  0.00339148030616343
Batch  61  loss:  0.0030682687647640705
Batch  71  loss:  0.0023770357947796583
Batch  81  loss:  0.0032271514646708965
Batch  91  loss:  0.001757096266373992
Validation on real data: 
LOSS supervised-train 0.002366766430204734, valid 0.002764037810266018
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.003544246545061469
Batch  11  loss:  0.0023017150815576315
Batch  21  loss:  0.0027905143797397614
Batch  31  loss:  0.0023022107779979706
Batch  41  loss:  0.0015437176916748285
Batch  51  loss:  0.0034158253110945225
Batch  61  loss:  0.0019357717828825116
Batch  71  loss:  0.0027512889355421066
Batch  81  loss:  0.002246415475383401
Batch  91  loss:  0.002462228527292609
Validation on real data: 
LOSS supervised-train 0.0023226833471562715, valid 0.0025131171569228172
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0021923945751041174
Batch  11  loss:  0.0028687226586043835
Batch  21  loss:  0.0032381576020270586
Batch  31  loss:  0.002337031066417694
Batch  41  loss:  0.0016789018409326673
Batch  51  loss:  0.0030763382092118263
Batch  61  loss:  0.0019468938698992133
Batch  71  loss:  0.0024410102050751448
Batch  81  loss:  0.003522557206451893
Batch  91  loss:  0.0016516282921656966
Validation on real data: 
LOSS supervised-train 0.002441281881183386, valid 0.0025360258296132088
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0025176273193210363
Batch  11  loss:  0.002222188748419285
Batch  21  loss:  0.002596714533865452
Batch  31  loss:  0.0028051387052983046
Batch  41  loss:  0.001504697953350842
Batch  51  loss:  0.0028031105175614357
Batch  61  loss:  0.0016271567437797785
Batch  71  loss:  0.001547826686874032
Batch  81  loss:  0.003141081891953945
Batch  91  loss:  0.0022317254915833473
Validation on real data: 
LOSS supervised-train 0.0022942619770765305, valid 0.002186591038480401
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0026544653810560703
Batch  11  loss:  0.0032493083272129297
Batch  21  loss:  0.002117432653903961
Batch  31  loss:  0.002408535685390234
Batch  41  loss:  0.002082167426124215
Batch  51  loss:  0.003197108395397663
Batch  61  loss:  0.0016397038707509637
Batch  71  loss:  0.0017328967805951834
Batch  81  loss:  0.003266440937295556
Batch  91  loss:  0.002311357529833913
Validation on real data: 
LOSS supervised-train 0.002302844146033749, valid 0.0026228141505271196
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0020246212370693684
Batch  11  loss:  0.002616466488689184
Batch  21  loss:  0.0027730404399335384
Batch  31  loss:  0.002364341402426362
Batch  41  loss:  0.0016361608868464828
Batch  51  loss:  0.0027606780640780926
Batch  61  loss:  0.0016951425932347775
Batch  71  loss:  0.001856428338214755
Batch  81  loss:  0.002632281742990017
Batch  91  loss:  0.0030469477642327547
Validation on real data: 
LOSS supervised-train 0.0022300999017897992, valid 0.0024249034468084574
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0017332365969195962
Batch  11  loss:  0.002955230651423335
Batch  21  loss:  0.0020996476523578167
Batch  31  loss:  0.0020553250797092915
Batch  41  loss:  0.0019261582056060433
Batch  51  loss:  0.0023372890427708626
Batch  61  loss:  0.001596743706613779
Batch  71  loss:  0.001411147997714579
Batch  81  loss:  0.0026578994002193213
Batch  91  loss:  0.0041822767816483974
Validation on real data: 
LOSS supervised-train 0.0021835695661138743, valid 0.0025103630032390356
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0021039003040641546
Batch  11  loss:  0.002322783460840583
Batch  21  loss:  0.0021451960783451796
Batch  31  loss:  0.002727281069383025
Batch  41  loss:  0.0025638644583523273
Batch  51  loss:  0.002661527832970023
Batch  61  loss:  0.0019966952968388796
Batch  71  loss:  0.0022826003842055798
Batch  81  loss:  0.002305105794221163
Batch  91  loss:  0.002833813428878784
Validation on real data: 
LOSS supervised-train 0.0022407373622991143, valid 0.002012653509154916
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0019526311662048101
Batch  11  loss:  0.0028188254218548536
Batch  21  loss:  0.0020393377635627985
Batch  31  loss:  0.002462146570906043
Batch  41  loss:  0.002280233195051551
Batch  51  loss:  0.0025432389229536057
Batch  61  loss:  0.0021629040129482746
Batch  71  loss:  0.002024858957156539
Batch  81  loss:  0.0036003589630126953
Batch  91  loss:  0.002903879852965474
Validation on real data: 
LOSS supervised-train 0.0022743835835717617, valid 0.002381702186539769
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0020274075213819742
Batch  11  loss:  0.002215593820437789
Batch  21  loss:  0.001717934268526733
Batch  31  loss:  0.00232489756308496
Batch  41  loss:  0.001701437751762569
Batch  51  loss:  0.0021936786361038685
Batch  61  loss:  0.002155499765649438
Batch  71  loss:  0.0018791691400110722
Batch  81  loss:  0.0036978810094296932
Batch  91  loss:  0.0026012787129729986
Validation on real data: 
LOSS supervised-train 0.002200132420985028, valid 0.0021699981298297644
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0023790679406374693
Batch  11  loss:  0.0028047917876392603
Batch  21  loss:  0.001422181841917336
Batch  31  loss:  0.002621284918859601
Batch  41  loss:  0.0021068700589239597
Batch  51  loss:  0.0019117838237434626
Batch  61  loss:  0.0021322756074368954
Batch  71  loss:  0.0017454258631914854
Batch  81  loss:  0.0034336778335273266
Batch  91  loss:  0.0034545105881989002
Validation on real data: 
LOSS supervised-train 0.002154764976585284, valid 0.002324227476492524
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.001645379001274705
Batch  11  loss:  0.002813486149534583
Batch  21  loss:  0.0016622261609882116
Batch  31  loss:  0.00279100495390594
Batch  41  loss:  0.0016583504620939493
Batch  51  loss:  0.002426979597657919
Batch  61  loss:  0.0017629633657634258
Batch  71  loss:  0.0018726950511336327
Batch  81  loss:  0.002633878728374839
Batch  91  loss:  0.003461232176050544
Validation on real data: 
LOSS supervised-train 0.002150069975759834, valid 0.0023034142795950174
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0017936637159436941
Batch  11  loss:  0.002518052700906992
Batch  21  loss:  0.001846995553933084
Batch  31  loss:  0.002961622318252921
Batch  41  loss:  0.002076980657875538
Batch  51  loss:  0.0032114237546920776
Batch  61  loss:  0.002300040330737829
Batch  71  loss:  0.0019990489818155766
Batch  81  loss:  0.003136124461889267
Batch  91  loss:  0.00314853573217988
Validation on real data: 
LOSS supervised-train 0.0021681885910220444, valid 0.0025851239915937185
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0018341948743909597
Batch  11  loss:  0.002911010989919305
Batch  21  loss:  0.0019583343528211117
Batch  31  loss:  0.002661872422322631
Batch  41  loss:  0.001780391321517527
Batch  51  loss:  0.0019275522790849209
Batch  61  loss:  0.0017770465929061174
Batch  71  loss:  0.00164884515106678
Batch  81  loss:  0.002237487817183137
Batch  91  loss:  0.003012040164321661
Validation on real data: 
LOSS supervised-train 0.0020681026263628154, valid 0.002111754845827818
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0013359752483665943
Batch  11  loss:  0.00269871111959219
Batch  21  loss:  0.0017372621223330498
Batch  31  loss:  0.002046019770205021
Batch  41  loss:  0.0018754344200715423
Batch  51  loss:  0.002229053294286132
Batch  61  loss:  0.001707505900412798
Batch  71  loss:  0.0019518702756613493
Batch  81  loss:  0.0023508663289248943
Batch  91  loss:  0.003727932460606098
Validation on real data: 
LOSS supervised-train 0.0020757532853167503, valid 0.0025765676982700825
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0013546949485316873
Batch  11  loss:  0.00310721923597157
Batch  21  loss:  0.001467594294808805
Batch  31  loss:  0.0021185888908803463
Batch  41  loss:  0.002629305003210902
Batch  51  loss:  0.001988888718187809
Batch  61  loss:  0.0020440095104277134
Batch  71  loss:  0.0020200067665427923
Batch  81  loss:  0.0020467147696763277
Batch  91  loss:  0.0031386390328407288
Validation on real data: 
LOSS supervised-train 0.0020993106078822164, valid 0.002040932886302471
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0015556237194687128
Batch  11  loss:  0.0022486024536192417
Batch  21  loss:  0.0019180851522833109
Batch  31  loss:  0.0017038261285051703
Batch  41  loss:  0.0023519459646195173
Batch  51  loss:  0.001704867696389556
Batch  61  loss:  0.0016055218875408173
Batch  71  loss:  0.0016849339008331299
Batch  81  loss:  0.0025522548239678144
Batch  91  loss:  0.0041490173898637295
Validation on real data: 
LOSS supervised-train 0.0020862861117348073, valid 0.0019346856279298663
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0015089401276782155
Batch  11  loss:  0.00230795587413013
Batch  21  loss:  0.0015252154553309083
Batch  31  loss:  0.001841858378611505
Batch  41  loss:  0.0019093311857432127
Batch  51  loss:  0.0019693102221935987
Batch  61  loss:  0.0017855415353551507
Batch  71  loss:  0.0021268706768751144
Batch  81  loss:  0.0014427407877519727
Batch  91  loss:  0.003963547293096781
Validation on real data: 
LOSS supervised-train 0.0019639330939389766, valid 0.0020247537177056074
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0018385114381089807
Batch  11  loss:  0.0020927812438458204
Batch  21  loss:  0.0019165521953254938
Batch  31  loss:  0.002215059008449316
Batch  41  loss:  0.0018532886169850826
Batch  51  loss:  0.0018333884654566646
Batch  61  loss:  0.00222197687253356
Batch  71  loss:  0.002007937990128994
Batch  81  loss:  0.0018935840344056487
Batch  91  loss:  0.0031419419683516026
Validation on real data: 
LOSS supervised-train 0.002017431944841519, valid 0.0019847112707793713
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0017211898230016232
Batch  11  loss:  0.002844309201464057
Batch  21  loss:  0.0017474391497671604
Batch  31  loss:  0.0018324445700272918
Batch  41  loss:  0.002157545415684581
Batch  51  loss:  0.001996447565034032
Batch  61  loss:  0.0016287991311401129
Batch  71  loss:  0.0018452637596055865
Batch  81  loss:  0.001862401724793017
Batch  91  loss:  0.0036000048276036978
Validation on real data: 
LOSS supervised-train 0.001920188816729933, valid 0.002334926277399063
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0017921128310263157
Batch  11  loss:  0.002196810208261013
Batch  21  loss:  0.001875999034382403
Batch  31  loss:  0.001929518417455256
Batch  41  loss:  0.00236430112272501
Batch  51  loss:  0.0016005912330001593
Batch  61  loss:  0.0015667056431993842
Batch  71  loss:  0.0025326537434011698
Batch  81  loss:  0.0016084851231426
Batch  91  loss:  0.005296394694596529
Validation on real data: 
LOSS supervised-train 0.002044164506951347, valid 0.00194847013335675
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.001351365470327437
Batch  11  loss:  0.0022325702011585236
Batch  21  loss:  0.001591704785823822
Batch  31  loss:  0.0015392653876915574
Batch  41  loss:  0.0017426094273105264
Batch  51  loss:  0.0015963668702170253
Batch  61  loss:  0.0017422547098249197
Batch  71  loss:  0.001980961300432682
Batch  81  loss:  0.0015249132411554456
Batch  91  loss:  0.003026885213330388
Validation on real data: 
LOSS supervised-train 0.00192175047355704, valid 0.0018137067090719938
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0016480202320963144
Batch  11  loss:  0.0020444118417799473
Batch  21  loss:  0.002408052561804652
Batch  31  loss:  0.0011224254267290235
Batch  41  loss:  0.0022704419679939747
Batch  51  loss:  0.0015155302826315165
Batch  61  loss:  0.0017552128992974758
Batch  71  loss:  0.0016484548104926944
Batch  81  loss:  0.0017912101466208696
Batch  91  loss:  0.00391275342553854
Validation on real data: 
LOSS supervised-train 0.0019005794497206807, valid 0.0018677061889320612
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  bed ; Model ID: 7c8eb4ab1f2c8bfa2fb46fb8b9b1ac9f
--------------------
Training baseline regression model:  2022-03-30 13:39:32.785500
Detector:  pointnet
Object:  bed
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1611559
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.3792204260826111
Batch  11  loss:  0.21894674003124237
Batch  21  loss:  0.1525459587574005
Batch  31  loss:  0.11629222333431244
Batch  41  loss:  0.08010759204626083
Batch  51  loss:  0.08035760372877121
Batch  61  loss:  0.04899197444319725
Batch  71  loss:  0.03679932653903961
Batch  81  loss:  0.03603067621588707
Batch  91  loss:  0.03546081855893135
Validation on real data: 
LOSS supervised-train 0.10267958732321858, valid 0.014258624985814095
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.02106529101729393
Batch  11  loss:  0.0203426331281662
Batch  21  loss:  0.03141491115093231
Batch  31  loss:  0.01664222776889801
Batch  41  loss:  0.02081616036593914
Batch  51  loss:  0.021454228088259697
Batch  61  loss:  0.0185629203915596
Batch  71  loss:  0.021934164687991142
Batch  81  loss:  0.021713605150580406
Batch  91  loss:  0.023915648460388184
Validation on real data: 
LOSS supervised-train 0.02169708929955959, valid 0.007797589059919119
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.010688910260796547
Batch  11  loss:  0.012768256478011608
Batch  21  loss:  0.015488340519368649
Batch  31  loss:  0.011476087383925915
Batch  41  loss:  0.012481842190027237
Batch  51  loss:  0.01609654165804386
Batch  61  loss:  0.010985738597810268
Batch  71  loss:  0.017829470336437225
Batch  81  loss:  0.01654616743326187
Batch  91  loss:  0.01957470178604126
Validation on real data: 
LOSS supervised-train 0.015476997857913375, valid 0.008914127945899963
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.010151578113436699
Batch  11  loss:  0.01211033295840025
Batch  21  loss:  0.01098950020968914
Batch  31  loss:  0.008449687622487545
Batch  41  loss:  0.013964823447167873
Batch  51  loss:  0.015391667373478413
Batch  61  loss:  0.010983135551214218
Batch  71  loss:  0.018375668674707413
Batch  81  loss:  0.014083144254982471
Batch  91  loss:  0.02109471708536148
Validation on real data: 
LOSS supervised-train 0.013775232629850507, valid 0.006122378166764975
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0063911364413797855
Batch  11  loss:  0.008419059216976166
Batch  21  loss:  0.010545668192207813
Batch  31  loss:  0.007974102161824703
Batch  41  loss:  0.010811371728777885
Batch  51  loss:  0.011902744881808758
Batch  61  loss:  0.00873655453324318
Batch  71  loss:  0.018761150538921356
Batch  81  loss:  0.01534552313387394
Batch  91  loss:  0.017453746870160103
Validation on real data: 
LOSS supervised-train 0.012283827043138445, valid 0.006631777156144381
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.006613461300730705
Batch  11  loss:  0.01055336557328701
Batch  21  loss:  0.00883066188544035
Batch  31  loss:  0.008892746642231941
Batch  41  loss:  0.010043861344456673
Batch  51  loss:  0.011421654373407364
Batch  61  loss:  0.007395447697490454
Batch  71  loss:  0.015258845873177052
Batch  81  loss:  0.01320586446672678
Batch  91  loss:  0.013361637480556965
Validation on real data: 
LOSS supervised-train 0.011021182173863053, valid 0.005435572005808353
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.006339682266116142
Batch  11  loss:  0.010729783214628696
Batch  21  loss:  0.006888903211802244
Batch  31  loss:  0.0086749866604805
Batch  41  loss:  0.010790953412652016
Batch  51  loss:  0.008495639078319073
Batch  61  loss:  0.007314338348805904
Batch  71  loss:  0.01442781649529934
Batch  81  loss:  0.010859810747206211
Batch  91  loss:  0.012915319763123989
Validation on real data: 
LOSS supervised-train 0.01017490859143436, valid 0.004955408629029989
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.007906364277005196
Batch  11  loss:  0.010005657561123371
Batch  21  loss:  0.007608084008097649
Batch  31  loss:  0.0064600505866110325
Batch  41  loss:  0.007811108138412237
Batch  51  loss:  0.008061768487095833
Batch  61  loss:  0.005822347942739725
Batch  71  loss:  0.01355182658880949
Batch  81  loss:  0.009254053235054016
Batch  91  loss:  0.011713671498000622
Validation on real data: 
LOSS supervised-train 0.009397043003700674, valid 0.005135062616318464
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0060038077645003796
Batch  11  loss:  0.007736127357929945
Batch  21  loss:  0.007458458188921213
Batch  31  loss:  0.0066671594977378845
Batch  41  loss:  0.00889553502202034
Batch  51  loss:  0.008245029486715794
Batch  61  loss:  0.005941593553870916
Batch  71  loss:  0.014101528562605381
Batch  81  loss:  0.008297443389892578
Batch  91  loss:  0.012010653503239155
Validation on real data: 
LOSS supervised-train 0.008836308158934117, valid 0.004344410728663206
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0048515889793634415
Batch  11  loss:  0.006348006427288055
Batch  21  loss:  0.00586449122056365
Batch  31  loss:  0.0059205759316682816
Batch  41  loss:  0.008501983247697353
Batch  51  loss:  0.0071099079214036465
Batch  61  loss:  0.006163079757243395
Batch  71  loss:  0.014369095675647259
Batch  81  loss:  0.008640003390610218
Batch  91  loss:  0.014051808975636959
Validation on real data: 
LOSS supervised-train 0.008463956154882908, valid 0.005191667936742306
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.006843435112386942
Batch  11  loss:  0.00785577017813921
Batch  21  loss:  0.0059188841842114925
Batch  31  loss:  0.005206117406487465
Batch  41  loss:  0.007484735455363989
Batch  51  loss:  0.006703498307615519
Batch  61  loss:  0.004324255045503378
Batch  71  loss:  0.012013805098831654
Batch  81  loss:  0.006657285615801811
Batch  91  loss:  0.011868724599480629
Validation on real data: 
LOSS supervised-train 0.00776054285466671, valid 0.0037634270265698433
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.005076421890407801
Batch  11  loss:  0.006841063499450684
Batch  21  loss:  0.006178686395287514
Batch  31  loss:  0.005180917680263519
Batch  41  loss:  0.008289754390716553
Batch  51  loss:  0.006004614755511284
Batch  61  loss:  0.004778056871145964
Batch  71  loss:  0.00944761373102665
Batch  81  loss:  0.006139429286122322
Batch  91  loss:  0.01014427188783884
Validation on real data: 
LOSS supervised-train 0.007370113325305283, valid 0.004043431021273136
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.006267211399972439
Batch  11  loss:  0.007283054292201996
Batch  21  loss:  0.007103388197720051
Batch  31  loss:  0.00437504006549716
Batch  41  loss:  0.00852625910192728
Batch  51  loss:  0.00624114042147994
Batch  61  loss:  0.005020852666348219
Batch  71  loss:  0.011365377344191074
Batch  81  loss:  0.00699582789093256
Batch  91  loss:  0.013619838282465935
Validation on real data: 
LOSS supervised-train 0.007059473842382431, valid 0.003439719555899501
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.004560171160846949
Batch  11  loss:  0.005762514192610979
Batch  21  loss:  0.004487796686589718
Batch  31  loss:  0.006010396406054497
Batch  41  loss:  0.005293937399983406
Batch  51  loss:  0.006599674466997385
Batch  61  loss:  0.006574714090675116
Batch  71  loss:  0.007948645390570164
Batch  81  loss:  0.007126600947231054
Batch  91  loss:  0.010547325015068054
Validation on real data: 
LOSS supervised-train 0.006701467735692859, valid 0.004165294114500284
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.006347285583615303
Batch  11  loss:  0.004572402220219374
Batch  21  loss:  0.005309821106493473
Batch  31  loss:  0.005575565621256828
Batch  41  loss:  0.005446293391287327
Batch  51  loss:  0.00465215602889657
Batch  61  loss:  0.005040850955992937
Batch  71  loss:  0.008428233675658703
Batch  81  loss:  0.005028566811233759
Batch  91  loss:  0.010142856277525425
Validation on real data: 
LOSS supervised-train 0.006500394758768379, valid 0.0033541047014296055
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.004325594287365675
Batch  11  loss:  0.0060510155744850636
Batch  21  loss:  0.004802047275006771
Batch  31  loss:  0.0040727960877120495
Batch  41  loss:  0.0051843891851603985
Batch  51  loss:  0.004182869102805853
Batch  61  loss:  0.004616979975253344
Batch  71  loss:  0.009095712564885616
Batch  81  loss:  0.007249323185533285
Batch  91  loss:  0.009014780633151531
Validation on real data: 
LOSS supervised-train 0.006216842657886445, valid 0.0037535433657467365
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.006440631113946438
Batch  11  loss:  0.0065896580927073956
Batch  21  loss:  0.005121190566569567
Batch  31  loss:  0.004864911083132029
Batch  41  loss:  0.005594765301793814
Batch  51  loss:  0.00505176791921258
Batch  61  loss:  0.005048154853284359
Batch  71  loss:  0.007587580941617489
Batch  81  loss:  0.004924033302813768
Batch  91  loss:  0.009286640211939812
Validation on real data: 
LOSS supervised-train 0.006391084822826088, valid 0.0037158196792006493
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.004726465325802565
Batch  11  loss:  0.00818796455860138
Batch  21  loss:  0.0053090900182724
Batch  31  loss:  0.0035711443051695824
Batch  41  loss:  0.00611522002145648
Batch  51  loss:  0.004921748302876949
Batch  61  loss:  0.005508855450898409
Batch  71  loss:  0.005993408616632223
Batch  81  loss:  0.004933205898851156
Batch  91  loss:  0.00892852433025837
Validation on real data: 
LOSS supervised-train 0.005896121913101524, valid 0.0022279920522123575
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.004852440673857927
Batch  11  loss:  0.00467101251706481
Batch  21  loss:  0.003974542021751404
Batch  31  loss:  0.003989305812865496
Batch  41  loss:  0.0055586048401892185
Batch  51  loss:  0.005688354838639498
Batch  61  loss:  0.0045987628400325775
Batch  71  loss:  0.007553129456937313
Batch  81  loss:  0.005594806745648384
Batch  91  loss:  0.009516378864645958
Validation on real data: 
LOSS supervised-train 0.005620266143232584, valid 0.0029795365408062935
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.005119270645081997
Batch  11  loss:  0.004591980949044228
Batch  21  loss:  0.0043655987828969955
Batch  31  loss:  0.004197441507130861
Batch  41  loss:  0.005086897406727076
Batch  51  loss:  0.0047925179824233055
Batch  61  loss:  0.0039434474892914295
Batch  71  loss:  0.007926939986646175
Batch  81  loss:  0.005560282152146101
Batch  91  loss:  0.008900142274796963
Validation on real data: 
LOSS supervised-train 0.005369473260361701, valid 0.0032067366410046816
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0048541780561208725
Batch  11  loss:  0.005139554850757122
Batch  21  loss:  0.004264002665877342
Batch  31  loss:  0.004576436709612608
Batch  41  loss:  0.006035093683749437
Batch  51  loss:  0.00396505743265152
Batch  61  loss:  0.003373462473973632
Batch  71  loss:  0.005777654703706503
Batch  81  loss:  0.004504517186433077
Batch  91  loss:  0.007964901626110077
Validation on real data: 
LOSS supervised-train 0.0052531537995673715, valid 0.0024563055485486984
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.004244800191372633
Batch  11  loss:  0.004023365210741758
Batch  21  loss:  0.004873240366578102
Batch  31  loss:  0.0037901829928159714
Batch  41  loss:  0.005216752178966999
Batch  51  loss:  0.003983734641224146
Batch  61  loss:  0.004325482062995434
Batch  71  loss:  0.005664653144776821
Batch  81  loss:  0.004301497247070074
Batch  91  loss:  0.0058847167529165745
Validation on real data: 
LOSS supervised-train 0.00524650274310261, valid 0.003142731497064233
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0037687644362449646
Batch  11  loss:  0.005219823680818081
Batch  21  loss:  0.0037532984279096127
Batch  31  loss:  0.00391158415004611
Batch  41  loss:  0.005428474862128496
Batch  51  loss:  0.004178670700639486
Batch  61  loss:  0.003944250755012035
Batch  71  loss:  0.00594195956364274
Batch  81  loss:  0.005312283523380756
Batch  91  loss:  0.006720492150634527
Validation on real data: 
LOSS supervised-train 0.004959390005096793, valid 0.0026998778339475393
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.004426144994795322
Batch  11  loss:  0.003194690914824605
Batch  21  loss:  0.0031275658402591944
Batch  31  loss:  0.0031903882045298815
Batch  41  loss:  0.00432368740439415
Batch  51  loss:  0.005201695952564478
Batch  61  loss:  0.004594946745783091
Batch  71  loss:  0.006150886416435242
Batch  81  loss:  0.004005130846053362
Batch  91  loss:  0.006673300173133612
Validation on real data: 
LOSS supervised-train 0.00467714220052585, valid 0.0027569513767957687
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.004441121593117714
Batch  11  loss:  0.004179804120212793
Batch  21  loss:  0.004413611721247435
Batch  31  loss:  0.004434015601873398
Batch  41  loss:  0.0048563298769295216
Batch  51  loss:  0.004386150278151035
Batch  61  loss:  0.004664069041609764
Batch  71  loss:  0.0056550996378064156
Batch  81  loss:  0.00392335606738925
Batch  91  loss:  0.005763799883425236
Validation on real data: 
LOSS supervised-train 0.0048690249864012, valid 0.0024560438469052315
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.006881429813802242
Batch  11  loss:  0.003844017395749688
Batch  21  loss:  0.002854650840163231
Batch  31  loss:  0.0038457405753433704
Batch  41  loss:  0.004880959168076515
Batch  51  loss:  0.003921942785382271
Batch  61  loss:  0.0038847315590828657
Batch  71  loss:  0.005540891550481319
Batch  81  loss:  0.004688137210905552
Batch  91  loss:  0.006139832548797131
Validation on real data: 
LOSS supervised-train 0.004732943375129253, valid 0.0027855101507157087
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0037911715917289257
Batch  11  loss:  0.0035671405494213104
Batch  21  loss:  0.003662857925519347
Batch  31  loss:  0.004112069495022297
Batch  41  loss:  0.005284452345222235
Batch  51  loss:  0.003916378132998943
Batch  61  loss:  0.0039082542061805725
Batch  71  loss:  0.005544289015233517
Batch  81  loss:  0.004429214634001255
Batch  91  loss:  0.0075655304826796055
Validation on real data: 
LOSS supervised-train 0.00455207726219669, valid 0.0021721816156059504
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.004416066221892834
Batch  11  loss:  0.004000589717179537
Batch  21  loss:  0.002959931967779994
Batch  31  loss:  0.0026715791318565607
Batch  41  loss:  0.004423696547746658
Batch  51  loss:  0.004752307198941708
Batch  61  loss:  0.003870774758979678
Batch  71  loss:  0.006437388714402914
Batch  81  loss:  0.0034603248350322247
Batch  91  loss:  0.005986946634948254
Validation on real data: 
LOSS supervised-train 0.004506942639127373, valid 0.0023094723001122475
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0040300339460372925
Batch  11  loss:  0.004073440097272396
Batch  21  loss:  0.0042333840392529964
Batch  31  loss:  0.003140994580462575
Batch  41  loss:  0.005106677766889334
Batch  51  loss:  0.0039456915110349655
Batch  61  loss:  0.003979881759732962
Batch  71  loss:  0.00593287218362093
Batch  81  loss:  0.0045755961909890175
Batch  91  loss:  0.008250330574810505
Validation on real data: 
LOSS supervised-train 0.004431976401247084, valid 0.002520464826375246
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.00411088764667511
Batch  11  loss:  0.0033055238891392946
Batch  21  loss:  0.003104332135990262
Batch  31  loss:  0.003233171533793211
Batch  41  loss:  0.005336301866918802
Batch  51  loss:  0.003853726200759411
Batch  61  loss:  0.003842875361442566
Batch  71  loss:  0.005589582026004791
Batch  81  loss:  0.0055190143175423145
Batch  91  loss:  0.006142506375908852
Validation on real data: 
LOSS supervised-train 0.004292215367313474, valid 0.0019855403807014227
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.003165110247209668
Batch  11  loss:  0.0038712972309440374
Batch  21  loss:  0.002992118475958705
Batch  31  loss:  0.0035037738271057606
Batch  41  loss:  0.00444892980158329
Batch  51  loss:  0.004976741969585419
Batch  61  loss:  0.0037781160790473223
Batch  71  loss:  0.006353691685944796
Batch  81  loss:  0.00423057796433568
Batch  91  loss:  0.005305938422679901
Validation on real data: 
LOSS supervised-train 0.00434429484186694, valid 0.0029845675453543663
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.003638011636212468
Batch  11  loss:  0.00425163097679615
Batch  21  loss:  0.0038884656969457865
Batch  31  loss:  0.0033093488309532404
Batch  41  loss:  0.0037172832526266575
Batch  51  loss:  0.0039483788423240185
Batch  61  loss:  0.003453128971159458
Batch  71  loss:  0.004975225310772657
Batch  81  loss:  0.003970551770180464
Batch  91  loss:  0.006136841606348753
Validation on real data: 
LOSS supervised-train 0.004258590615354479, valid 0.002418318996205926
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0036968563217669725
Batch  11  loss:  0.0028053417336195707
Batch  21  loss:  0.0031784826423972845
Batch  31  loss:  0.0031153506133705378
Batch  41  loss:  0.0038370590191334486
Batch  51  loss:  0.004064812790602446
Batch  61  loss:  0.00366110703907907
Batch  71  loss:  0.005198291502892971
Batch  81  loss:  0.004282142035663128
Batch  91  loss:  0.005597947631031275
Validation on real data: 
LOSS supervised-train 0.004213345216121525, valid 0.0025802296586334705
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0028653531335294247
Batch  11  loss:  0.0032161488197743893
Batch  21  loss:  0.0031945258378982544
Batch  31  loss:  0.003359472146257758
Batch  41  loss:  0.004180820658802986
Batch  51  loss:  0.004065952729433775
Batch  61  loss:  0.0040542627684772015
Batch  71  loss:  0.0045999460853636265
Batch  81  loss:  0.004616853315383196
Batch  91  loss:  0.006836938671767712
Validation on real data: 
LOSS supervised-train 0.004072681316174566, valid 0.0029830150306224823
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.00330907735042274
Batch  11  loss:  0.004324298817664385
Batch  21  loss:  0.003125426359474659
Batch  31  loss:  0.00355989090166986
Batch  41  loss:  0.004466050770133734
Batch  51  loss:  0.004362559411674738
Batch  61  loss:  0.0034364936873316765
Batch  71  loss:  0.004958983976393938
Batch  81  loss:  0.004369104281067848
Batch  91  loss:  0.006729301065206528
Validation on real data: 
LOSS supervised-train 0.004041912334505468, valid 0.003709725569933653
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.003121493384242058
Batch  11  loss:  0.002951820380985737
Batch  21  loss:  0.0031796139664947987
Batch  31  loss:  0.0032560911495238543
Batch  41  loss:  0.0036975436378270388
Batch  51  loss:  0.004147103521972895
Batch  61  loss:  0.00403309753164649
Batch  71  loss:  0.005553318187594414
Batch  81  loss:  0.004180267918854952
Batch  91  loss:  0.004999504424631596
Validation on real data: 
LOSS supervised-train 0.003951930685434491, valid 0.0030260123312473297
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.003423332003876567
Batch  11  loss:  0.0036420440301299095
Batch  21  loss:  0.0032919065561145544
Batch  31  loss:  0.0030019665136933327
Batch  41  loss:  0.004064425826072693
Batch  51  loss:  0.003485055873170495
Batch  61  loss:  0.004549926146864891
Batch  71  loss:  0.005034689325839281
Batch  81  loss:  0.004663923755288124
Batch  91  loss:  0.006485235411673784
Validation on real data: 
LOSS supervised-train 0.003975281792227179, valid 0.0028047682717442513
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.004002578090876341
Batch  11  loss:  0.003331397660076618
Batch  21  loss:  0.0027260291390120983
Batch  31  loss:  0.0026730219833552837
Batch  41  loss:  0.004508754704147577
Batch  51  loss:  0.00318368268199265
Batch  61  loss:  0.004107233136892319
Batch  71  loss:  0.0037734766956418753
Batch  81  loss:  0.004339288920164108
Batch  91  loss:  0.004704116377979517
Validation on real data: 
LOSS supervised-train 0.004155355587136001, valid 0.002607691567391157
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0038719854783266783
Batch  11  loss:  0.003955432213842869
Batch  21  loss:  0.003304965328425169
Batch  31  loss:  0.002745242090895772
Batch  41  loss:  0.0036786478012800217
Batch  51  loss:  0.0052948580123484135
Batch  61  loss:  0.0042119259014725685
Batch  71  loss:  0.003755553625524044
Batch  81  loss:  0.0039839050732553005
Batch  91  loss:  0.004485852085053921
Validation on real data: 
LOSS supervised-train 0.004016276700422167, valid 0.001714841346256435
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0037530926056206226
Batch  11  loss:  0.004022875335067511
Batch  21  loss:  0.003926741890609264
Batch  31  loss:  0.004633464850485325
Batch  41  loss:  0.004600342363119125
Batch  51  loss:  0.003923488315194845
Batch  61  loss:  0.002776576904579997
Batch  71  loss:  0.004267402924597263
Batch  81  loss:  0.003999949898570776
Batch  91  loss:  0.0047256601974368095
Validation on real data: 
LOSS supervised-train 0.00393432894255966, valid 0.00222550961188972
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.004437103867530823
Batch  11  loss:  0.003972402773797512
Batch  21  loss:  0.003790525021031499
Batch  31  loss:  0.0035847644321620464
Batch  41  loss:  0.0042064920999109745
Batch  51  loss:  0.004005660768598318
Batch  61  loss:  0.003535616910085082
Batch  71  loss:  0.006361129228025675
Batch  81  loss:  0.0032242308370769024
Batch  91  loss:  0.00490984134376049
Validation on real data: 
LOSS supervised-train 0.004051093508023768, valid 0.002475206507369876
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.002954668365418911
Batch  11  loss:  0.002755112014710903
Batch  21  loss:  0.0033318023197352886
Batch  31  loss:  0.0036705087404698133
Batch  41  loss:  0.0038264780305325985
Batch  51  loss:  0.0036342518869787455
Batch  61  loss:  0.004598384257405996
Batch  71  loss:  0.0035733964759856462
Batch  81  loss:  0.00401297165080905
Batch  91  loss:  0.004171205218881369
Validation on real data: 
LOSS supervised-train 0.003825290729291737, valid 0.0028849015943706036
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.003147993702441454
Batch  11  loss:  0.003157585160806775
Batch  21  loss:  0.0030974452383816242
Batch  31  loss:  0.0033623273484408855
Batch  41  loss:  0.0033948945347219706
Batch  51  loss:  0.003919604700058699
Batch  61  loss:  0.004009179305285215
Batch  71  loss:  0.004238955210894346
Batch  81  loss:  0.0043129585683345795
Batch  91  loss:  0.004124179482460022
Validation on real data: 
LOSS supervised-train 0.003883979725651443, valid 0.002986765932291746
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.003373603103682399
Batch  11  loss:  0.0033542844466865063
Batch  21  loss:  0.0032534569036215544
Batch  31  loss:  0.0034913637209683657
Batch  41  loss:  0.002822901587933302
Batch  51  loss:  0.0037193335592746735
Batch  61  loss:  0.003664761083200574
Batch  71  loss:  0.005127718206495047
Batch  81  loss:  0.004229521378874779
Batch  91  loss:  0.005161374807357788
Validation on real data: 
LOSS supervised-train 0.003928187100682407, valid 0.002583518624305725
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.003582586767151952
Batch  11  loss:  0.0031156986951828003
Batch  21  loss:  0.003010259009897709
Batch  31  loss:  0.002807789482176304
Batch  41  loss:  0.0034238784573972225
Batch  51  loss:  0.003933034371584654
Batch  61  loss:  0.00451364042237401
Batch  71  loss:  0.0040480527095496655
Batch  81  loss:  0.006459798663854599
Batch  91  loss:  0.004770897328853607
Validation on real data: 
LOSS supervised-train 0.00395624553784728, valid 0.0022797337733209133
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.004709194879978895
Batch  11  loss:  0.003516840748488903
Batch  21  loss:  0.004225343465805054
Batch  31  loss:  0.0030326431151479483
Batch  41  loss:  0.003527396824210882
Batch  51  loss:  0.0028778251726180315
Batch  61  loss:  0.003924314398318529
Batch  71  loss:  0.004428551532328129
Batch  81  loss:  0.004374383017420769
Batch  91  loss:  0.003793101292103529
Validation on real data: 
LOSS supervised-train 0.0038742536189965903, valid 0.002619934966787696
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0038222174625843763
Batch  11  loss:  0.0033770513255149126
Batch  21  loss:  0.00357218855060637
Batch  31  loss:  0.003447295166552067
Batch  41  loss:  0.0028887188527733088
Batch  51  loss:  0.003880084725096822
Batch  61  loss:  0.003861656179651618
Batch  71  loss:  0.003435336286202073
Batch  81  loss:  0.003827590262517333
Batch  91  loss:  0.004319056868553162
Validation on real data: 
LOSS supervised-train 0.003943252402823418, valid 0.0020536219235509634
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0040284437127411366
Batch  11  loss:  0.0040033720433712006
Batch  21  loss:  0.0037389127537608147
Batch  31  loss:  0.0034326037857681513
Batch  41  loss:  0.0035768640227615833
Batch  51  loss:  0.004121989011764526
Batch  61  loss:  0.003481506137177348
Batch  71  loss:  0.0055602011270821095
Batch  81  loss:  0.002805868396535516
Batch  91  loss:  0.0037800567224621773
Validation on real data: 
LOSS supervised-train 0.0038551344838924704, valid 0.0025733523070812225
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.004126444924622774
Batch  11  loss:  0.005437229759991169
Batch  21  loss:  0.0023896002676337957
Batch  31  loss:  0.0034066764637827873
Batch  41  loss:  0.0035258810967206955
Batch  51  loss:  0.004367454443126917
Batch  61  loss:  0.004699833691120148
Batch  71  loss:  0.005453552585095167
Batch  81  loss:  0.003530631773173809
Batch  91  loss:  0.005716877058148384
Validation on real data: 
LOSS supervised-train 0.003769591513555497, valid 0.0027411305345594883
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.002972999820485711
Batch  11  loss:  0.0052406070753932
Batch  21  loss:  0.0028578494675457478
Batch  31  loss:  0.004425805527716875
Batch  41  loss:  0.003122440306469798
Batch  51  loss:  0.003222032217308879
Batch  61  loss:  0.004358137957751751
Batch  71  loss:  0.0035481303930282593
Batch  81  loss:  0.003404459683224559
Batch  91  loss:  0.004346803296357393
Validation on real data: 
LOSS supervised-train 0.0037660376518033447, valid 0.002454443136230111
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0030367139261215925
Batch  11  loss:  0.0036598993465304375
Batch  21  loss:  0.00380091299302876
Batch  31  loss:  0.003992580808699131
Batch  41  loss:  0.0037182557862251997
Batch  51  loss:  0.0035698299761861563
Batch  61  loss:  0.004151550587266684
Batch  71  loss:  0.003463066415861249
Batch  81  loss:  0.004574954975396395
Batch  91  loss:  0.004024979192763567
Validation on real data: 
LOSS supervised-train 0.0037811958440579474, valid 0.002620152197778225
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.004142588935792446
Batch  11  loss:  0.003223804058507085
Batch  21  loss:  0.0037461286410689354
Batch  31  loss:  0.004842283204197884
Batch  41  loss:  0.00334920990280807
Batch  51  loss:  0.0032725047785788774
Batch  61  loss:  0.004891660064458847
Batch  71  loss:  0.004299868363887072
Batch  81  loss:  0.004238718189299107
Batch  91  loss:  0.004075495060533285
Validation on real data: 
LOSS supervised-train 0.003930051731877029, valid 0.002858371939510107
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.002949619898572564
Batch  11  loss:  0.0035043752286583185
Batch  21  loss:  0.00359504297375679
Batch  31  loss:  0.003313660155981779
Batch  41  loss:  0.0032956262584775686
Batch  51  loss:  0.002929562935605645
Batch  61  loss:  0.005986491683870554
Batch  71  loss:  0.0038719421718269587
Batch  81  loss:  0.004964340012520552
Batch  91  loss:  0.004396277479827404
Validation on real data: 
LOSS supervised-train 0.00395228240871802, valid 0.002448125509545207
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.003411155892536044
Batch  11  loss:  0.003267822787165642
Batch  21  loss:  0.004034634679555893
Batch  31  loss:  0.003900931915268302
Batch  41  loss:  0.003588849911466241
Batch  51  loss:  0.0032200145069509745
Batch  61  loss:  0.00650741346180439
Batch  71  loss:  0.0030752387829124928
Batch  81  loss:  0.003470352152362466
Batch  91  loss:  0.003709859447553754
Validation on real data: 
LOSS supervised-train 0.004018869642168283, valid 0.00244530919007957
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0031985430978238583
Batch  11  loss:  0.003953970968723297
Batch  21  loss:  0.003747105598449707
Batch  31  loss:  0.003305242396891117
Batch  41  loss:  0.003605555510148406
Batch  51  loss:  0.0043540759943425655
Batch  61  loss:  0.005936597008258104
Batch  71  loss:  0.003853763220831752
Batch  81  loss:  0.003379126777872443
Batch  91  loss:  0.005110043101012707
Validation on real data: 
LOSS supervised-train 0.003961121113970876, valid 0.003004853380843997
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0034307921305298805
Batch  11  loss:  0.0056758769787848
Batch  21  loss:  0.00321008893661201
Batch  31  loss:  0.004659667145460844
Batch  41  loss:  0.003009304404258728
Batch  51  loss:  0.0048516010865569115
Batch  61  loss:  0.003311634063720703
Batch  71  loss:  0.006860324181616306
Batch  81  loss:  0.0032588415779173374
Batch  91  loss:  0.004792941268533468
Validation on real data: 
LOSS supervised-train 0.004187227999791503, valid 0.0019885774236172438
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0034242693800479174
Batch  11  loss:  0.005503400228917599
Batch  21  loss:  0.0036627021618187428
Batch  31  loss:  0.0026253974065184593
Batch  41  loss:  0.0029924768023192883
Batch  51  loss:  0.0025691608898341656
Batch  61  loss:  0.002876866143196821
Batch  71  loss:  0.002965337596833706
Batch  81  loss:  0.004860482644289732
Batch  91  loss:  0.007339351810514927
Validation on real data: 
LOSS supervised-train 0.004061544421128929, valid 0.0026665078476071358
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.005131020210683346
Batch  11  loss:  0.006240413524210453
Batch  21  loss:  0.0029339611064642668
Batch  31  loss:  0.002984233433380723
Batch  41  loss:  0.0038665824104100466
Batch  51  loss:  0.0033866751473397017
Batch  61  loss:  0.003082465147599578
Batch  71  loss:  0.003438957268372178
Batch  81  loss:  0.003699676599353552
Batch  91  loss:  0.004828151781111956
Validation on real data: 
LOSS supervised-train 0.004011552119627595, valid 0.0034365111496299505
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.005750754382461309
Batch  11  loss:  0.004090712405741215
Batch  21  loss:  0.004317006096243858
Batch  31  loss:  0.0034257005900144577
Batch  41  loss:  0.003163951216265559
Batch  51  loss:  0.0037517165765166283
Batch  61  loss:  0.002618765691295266
Batch  71  loss:  0.002595453755930066
Batch  81  loss:  0.005036221817135811
Batch  91  loss:  0.004336972255259752
Validation on real data: 
LOSS supervised-train 0.0045416728570126, valid 0.004015122074633837
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.009088661521673203
Batch  11  loss:  0.004689780529588461
Batch  21  loss:  0.006252442020922899
Batch  31  loss:  0.004389069974422455
Batch  41  loss:  0.0033606765791773796
Batch  51  loss:  0.0038147985469549894
Batch  61  loss:  0.005460064392536879
Batch  71  loss:  0.00311292614787817
Batch  81  loss:  0.0037340419366955757
Batch  91  loss:  0.006287245079874992
Validation on real data: 
LOSS supervised-train 0.0044600746757350864, valid 0.004484873265028
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0077036842703819275
Batch  11  loss:  0.0036844557616859674
Batch  21  loss:  0.005724744871258736
Batch  31  loss:  0.0035821532364934683
Batch  41  loss:  0.004713588394224644
Batch  51  loss:  0.004757203161716461
Batch  61  loss:  0.004940835293382406
Batch  71  loss:  0.0031980243511497974
Batch  81  loss:  0.003373837796971202
Batch  91  loss:  0.004754992201924324
Validation on real data: 
LOSS supervised-train 0.004951461383607239, valid 0.004735439084470272
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.004271771293133497
Batch  11  loss:  0.003919513430446386
Batch  21  loss:  0.004764472600072622
Batch  31  loss:  0.00407456373795867
Batch  41  loss:  0.0034155689645558596
Batch  51  loss:  0.0071842968463897705
Batch  61  loss:  0.0051764934323728085
Batch  71  loss:  0.004331901669502258
Batch  81  loss:  0.003978940658271313
Batch  91  loss:  0.005353093147277832
Validation on real data: 
LOSS supervised-train 0.00463532087393105, valid 0.0032796799205243587
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0026453703176230192
Batch  11  loss:  0.004742452409118414
Batch  21  loss:  0.002781959716230631
Batch  31  loss:  0.004763315431773663
Batch  41  loss:  0.0038628808688372374
Batch  51  loss:  0.007702923845499754
Batch  61  loss:  0.004127047024667263
Batch  71  loss:  0.0073730479925870895
Batch  81  loss:  0.0036782510578632355
Batch  91  loss:  0.010500464588403702
Validation on real data: 
LOSS supervised-train 0.004980011743027717, valid 0.003473909106105566
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0043402183800935745
Batch  11  loss:  0.004137519281357527
Batch  21  loss:  0.0028007696382701397
Batch  31  loss:  0.003417826257646084
Batch  41  loss:  0.006415121257305145
Batch  51  loss:  0.0068322475999593735
Batch  61  loss:  0.005552544258534908
Batch  71  loss:  0.005025812424719334
Batch  81  loss:  0.005900155752897263
Batch  91  loss:  0.005382944364100695
Validation on real data: 
LOSS supervised-train 0.005012249059509486, valid 0.004643031395971775
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.007289993576705456
Batch  11  loss:  0.004698045551776886
Batch  21  loss:  0.0049310103058815
Batch  31  loss:  0.004037553444504738
Batch  41  loss:  0.00458898488432169
Batch  51  loss:  0.0038884137757122517
Batch  61  loss:  0.005020692013204098
Batch  71  loss:  0.0033685092348605394
Batch  81  loss:  0.007245374843478203
Batch  91  loss:  0.005518023390322924
Validation on real data: 
LOSS supervised-train 0.004870888965670019, valid 0.0064095095731318
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.015393857844173908
Batch  11  loss:  0.004556912463158369
Batch  21  loss:  0.006715082563459873
Batch  31  loss:  0.003662532428279519
Batch  41  loss:  0.003420336404815316
Batch  51  loss:  0.00404865387827158
Batch  61  loss:  0.0035982958506792784
Batch  71  loss:  0.0034525173250585794
Batch  81  loss:  0.0036961385048925877
Batch  91  loss:  0.006243860814720392
Validation on real data: 
LOSS supervised-train 0.004831249793060124, valid 0.004522970411926508
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.009039140306413174
Batch  11  loss:  0.004728579428046942
Batch  21  loss:  0.005050929728895426
Batch  31  loss:  0.006211187690496445
Batch  41  loss:  0.004387557972222567
Batch  51  loss:  0.00423384178429842
Batch  61  loss:  0.004295783583074808
Batch  71  loss:  0.004369623493403196
Batch  81  loss:  0.003562157740816474
Batch  91  loss:  0.007084717508405447
Validation on real data: 
LOSS supervised-train 0.004867458986118436, valid 0.0047136759385466576
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.004113657400012016
Batch  11  loss:  0.005648328922688961
Batch  21  loss:  0.003142085624858737
Batch  31  loss:  0.006626815535128117
Batch  41  loss:  0.003954143263399601
Batch  51  loss:  0.005833188071846962
Batch  61  loss:  0.003465023823082447
Batch  71  loss:  0.003795245196670294
Batch  81  loss:  0.003421853994950652
Batch  91  loss:  0.006259851157665253
Validation on real data: 
LOSS supervised-train 0.004693274949677289, valid 0.003436286933720112
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.004091056529432535
Batch  11  loss:  0.0083084711804986
Batch  21  loss:  0.002410467714071274
Batch  31  loss:  0.0045874700881540775
Batch  41  loss:  0.0030656466260552406
Batch  51  loss:  0.00397071847692132
Batch  61  loss:  0.0068727764301002026
Batch  71  loss:  0.004251003265380859
Batch  81  loss:  0.004056723788380623
Batch  91  loss:  0.005841885693371296
Validation on real data: 
LOSS supervised-train 0.004853699025698006, valid 0.004020829685032368
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0035064315889030695
Batch  11  loss:  0.005800354760140181
Batch  21  loss:  0.0024014757946133614
Batch  31  loss:  0.004654671065509319
Batch  41  loss:  0.004844742827117443
Batch  51  loss:  0.006056800484657288
Batch  61  loss:  0.0064618512988090515
Batch  71  loss:  0.003467249684035778
Batch  81  loss:  0.004083131905645132
Batch  91  loss:  0.005177708808332682
Validation on real data: 
LOSS supervised-train 0.004947509220801294, valid 0.004053001292049885
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0033536874689161777
Batch  11  loss:  0.0074640726670622826
Batch  21  loss:  0.0033912649378180504
Batch  31  loss:  0.005057759117335081
Batch  41  loss:  0.005775259807705879
Batch  51  loss:  0.005960181355476379
Batch  61  loss:  0.005524948704987764
Batch  71  loss:  0.004479704890400171
Batch  81  loss:  0.005830669309943914
Batch  91  loss:  0.005652217194437981
Validation on real data: 
LOSS supervised-train 0.0053880058252252635, valid 0.00604971032589674
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.004582290071994066
Batch  11  loss:  0.004740032833069563
Batch  21  loss:  0.0045042806304991245
Batch  31  loss:  0.008131755515933037
Batch  41  loss:  0.004664255306124687
Batch  51  loss:  0.005368406884372234
Batch  61  loss:  0.00520580867305398
Batch  71  loss:  0.004818435292690992
Batch  81  loss:  0.0035274301189929247
Batch  91  loss:  0.007357868365943432
Validation on real data: 
LOSS supervised-train 0.0051617800327949225, valid 0.004262816160917282
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.004184558521956205
Batch  11  loss:  0.00867549516260624
Batch  21  loss:  0.0023455412592738867
Batch  31  loss:  0.006707609165459871
Batch  41  loss:  0.005348170176148415
Batch  51  loss:  0.004071551840752363
Batch  61  loss:  0.005061544012278318
Batch  71  loss:  0.005147600080817938
Batch  81  loss:  0.004277249798178673
Batch  91  loss:  0.005304696504026651
Validation on real data: 
LOSS supervised-train 0.005674985337536782, valid 0.003942002542316914
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.002794391941279173
Batch  11  loss:  0.007803345564752817
Batch  21  loss:  0.003498050384223461
Batch  31  loss:  0.006116369739174843
Batch  41  loss:  0.008651734329760075
Batch  51  loss:  0.005224107299000025
Batch  61  loss:  0.006112316157668829
Batch  71  loss:  0.005370398052036762
Batch  81  loss:  0.003930217120796442
Batch  91  loss:  0.005316891707479954
Validation on real data: 
LOSS supervised-train 0.005770893306471407, valid 0.0026778304018080235
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0028072705026715994
Batch  11  loss:  0.007212840020656586
Batch  21  loss:  0.003935521934181452
Batch  31  loss:  0.005403099115937948
Batch  41  loss:  0.008605229668319225
Batch  51  loss:  0.0036957156844437122
Batch  61  loss:  0.005135825835168362
Batch  71  loss:  0.005924555938690901
Batch  81  loss:  0.0037779223639518023
Batch  91  loss:  0.00850030779838562
Validation on real data: 
LOSS supervised-train 0.005902457660995424, valid 0.002590419026091695
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0043896958231925964
Batch  11  loss:  0.005387495271861553
Batch  21  loss:  0.007477655075490475
Batch  31  loss:  0.004473793786019087
Batch  41  loss:  0.009778826497495174
Batch  51  loss:  0.003976787440478802
Batch  61  loss:  0.0033286591060459614
Batch  71  loss:  0.010198722593486309
Batch  81  loss:  0.002755943452939391
Batch  91  loss:  0.011031336151063442
Validation on real data: 
LOSS supervised-train 0.006364657550584525, valid 0.0016679993132129312
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.003967654425650835
Batch  11  loss:  0.006688665132969618
Batch  21  loss:  0.006452195346355438
Batch  31  loss:  0.0024154786951839924
Batch  41  loss:  0.007131458260118961
Batch  51  loss:  0.004137812647968531
Batch  61  loss:  0.0035949659068137407
Batch  71  loss:  0.008858604356646538
Batch  81  loss:  0.006417938973754644
Batch  91  loss:  0.009705322794616222
Validation on real data: 
LOSS supervised-train 0.0060755975800566376, valid 0.0012441071448847651
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00879745278507471
Batch  11  loss:  0.004956542514264584
Batch  21  loss:  0.005955577362328768
Batch  31  loss:  0.0033593028783798218
Batch  41  loss:  0.00804264284670353
Batch  51  loss:  0.0042336066253483295
Batch  61  loss:  0.003630501450970769
Batch  71  loss:  0.008916914463043213
Batch  81  loss:  0.005059974733740091
Batch  91  loss:  0.005305112339556217
Validation on real data: 
LOSS supervised-train 0.006120909303426743, valid 0.001518595963716507
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.008770614862442017
Batch  11  loss:  0.0031820752192288637
Batch  21  loss:  0.010190505534410477
Batch  31  loss:  0.0031652289908379316
Batch  41  loss:  0.009862937964498997
Batch  51  loss:  0.008564316667616367
Batch  61  loss:  0.003756322432309389
Batch  71  loss:  0.015872465446591377
Batch  81  loss:  0.005595153663307428
Batch  91  loss:  0.006381474435329437
Validation on real data: 
LOSS supervised-train 0.007017086611595005, valid 0.0016640224494040012
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.01109624095261097
Batch  11  loss:  0.0036244564689695835
Batch  21  loss:  0.01337570883333683
Batch  31  loss:  0.002107981126755476
Batch  41  loss:  0.007509743329137564
Batch  51  loss:  0.02032976597547531
Batch  61  loss:  0.005759330466389656
Batch  71  loss:  0.0068213436752557755
Batch  81  loss:  0.01971169374883175
Batch  91  loss:  0.0058539677411317825
Validation on real data: 
LOSS supervised-train 0.006732090956065803, valid 0.001957407221198082
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.013647345826029778
Batch  11  loss:  0.0033939452841877937
Batch  21  loss:  0.010813633911311626
Batch  31  loss:  0.002989166881889105
Batch  41  loss:  0.0037818332202732563
Batch  51  loss:  0.008682976476848125
Batch  61  loss:  0.0045781852677464485
Batch  71  loss:  0.006041249725967646
Batch  81  loss:  0.01505558006465435
Batch  91  loss:  0.00458055455237627
Validation on real data: 
LOSS supervised-train 0.006744995904155076, valid 0.001921705319546163
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00888875313103199
Batch  11  loss:  0.004758342634886503
Batch  21  loss:  0.006820995826274157
Batch  31  loss:  0.005155591759830713
Batch  41  loss:  0.0037352691870182753
Batch  51  loss:  0.008878066204488277
Batch  61  loss:  0.005819023586809635
Batch  71  loss:  0.003390758065506816
Batch  81  loss:  0.01095638144761324
Batch  91  loss:  0.003769881557673216
Validation on real data: 
LOSS supervised-train 0.006488280263729393, valid 0.001161338877864182
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.006429755594581366
Batch  11  loss:  0.008872210048139095
Batch  21  loss:  0.005518283694982529
Batch  31  loss:  0.004244562704116106
Batch  41  loss:  0.0043333168141543865
Batch  51  loss:  0.008087260648608208
Batch  61  loss:  0.007118236739188433
Batch  71  loss:  0.003499672980979085
Batch  81  loss:  0.005173924844712019
Batch  91  loss:  0.004973418079316616
Validation on real data: 
LOSS supervised-train 0.006542276467662304, valid 0.0009902666788548231
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0034180122893303633
Batch  11  loss:  0.009873795323073864
Batch  21  loss:  0.003921519964933395
Batch  31  loss:  0.004168130457401276
Batch  41  loss:  0.0035477487836033106
Batch  51  loss:  0.009136206470429897
Batch  61  loss:  0.007957843132317066
Batch  71  loss:  0.0032935170456767082
Batch  81  loss:  0.004703092388808727
Batch  91  loss:  0.004410446155816317
Validation on real data: 
LOSS supervised-train 0.006141334660351276, valid 0.0013058471959084272
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00565029913559556
Batch  11  loss:  0.0072736176662147045
Batch  21  loss:  0.002582690678536892
Batch  31  loss:  0.005455557722598314
Batch  41  loss:  0.0029374617151916027
Batch  51  loss:  0.00872806552797556
Batch  61  loss:  0.009942334145307541
Batch  71  loss:  0.003022362245246768
Batch  81  loss:  0.00617886520922184
Batch  91  loss:  0.003640956012532115
Validation on real data: 
LOSS supervised-train 0.006087112685199827, valid 0.0013837107690051198
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0041583930142223835
Batch  11  loss:  0.0070039378479123116
Batch  21  loss:  0.00335073284804821
Batch  31  loss:  0.008199259638786316
Batch  41  loss:  0.0035423487424850464
Batch  51  loss:  0.0051549505442380905
Batch  61  loss:  0.010579574853181839
Batch  71  loss:  0.0031955342274159193
Batch  81  loss:  0.004863385111093521
Batch  91  loss:  0.004007701296359301
Validation on real data: 
LOSS supervised-train 0.005464159408584237, valid 0.0015710541047155857
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.004651977680623531
Batch  11  loss:  0.006711683701723814
Batch  21  loss:  0.005735698621720076
Batch  31  loss:  0.00460481084883213
Batch  41  loss:  0.0039668274112045765
Batch  51  loss:  0.004232722334563732
Batch  61  loss:  0.00609759334474802
Batch  71  loss:  0.00529855489730835
Batch  81  loss:  0.003924076445400715
Batch  91  loss:  0.004913529846817255
Validation on real data: 
LOSS supervised-train 0.0049608229624573145, valid 0.0016001614276319742
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0041056470945477486
Batch  11  loss:  0.0036064130254089832
Batch  21  loss:  0.005870814900845289
Batch  31  loss:  0.003379242727532983
Batch  41  loss:  0.0032124659046530724
Batch  51  loss:  0.0038042061496526003
Batch  61  loss:  0.004249904304742813
Batch  71  loss:  0.0038357595913112164
Batch  81  loss:  0.004595938604325056
Batch  91  loss:  0.005462605971843004
Validation on real data: 
LOSS supervised-train 0.004572697628755122, valid 0.0022022833582013845
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00392908975481987
Batch  11  loss:  0.0037048046942800283
Batch  21  loss:  0.005924704484641552
Batch  31  loss:  0.00275222840718925
Batch  41  loss:  0.0031919642351567745
Batch  51  loss:  0.00341570726595819
Batch  61  loss:  0.005069830920547247
Batch  71  loss:  0.004284573253244162
Batch  81  loss:  0.004569719545543194
Batch  91  loss:  0.00564133794978261
Validation on real data: 
LOSS supervised-train 0.004341366316657514, valid 0.00317669240757823
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.003222906030714512
Batch  11  loss:  0.00290470477193594
Batch  21  loss:  0.009515808895230293
Batch  31  loss:  0.0038224835880100727
Batch  41  loss:  0.0031801527366042137
Batch  51  loss:  0.004211368039250374
Batch  61  loss:  0.0035265067126601934
Batch  71  loss:  0.004045921377837658
Batch  81  loss:  0.0033025203738361597
Batch  91  loss:  0.004821032285690308
Validation on real data: 
LOSS supervised-train 0.004293485954403878, valid 0.003553747432306409
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.009236255660653114
Batch  11  loss:  0.0038701456505805254
Batch  21  loss:  0.005663848482072353
Batch  31  loss:  0.004422533791512251
Batch  41  loss:  0.00342508009634912
Batch  51  loss:  0.0036662626080214977
Batch  61  loss:  0.003778237383812666
Batch  71  loss:  0.0044982414692640305
Batch  81  loss:  0.003145433496683836
Batch  91  loss:  0.004617441911250353
Validation on real data: 
LOSS supervised-train 0.004194707644637674, valid 0.0038568961899727583
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.006036420818418264
Batch  11  loss:  0.004136283416301012
Batch  21  loss:  0.00820168200880289
Batch  31  loss:  0.003373379586264491
Batch  41  loss:  0.0028546941466629505
Batch  51  loss:  0.0032136812806129456
Batch  61  loss:  0.0037383295129984617
Batch  71  loss:  0.0033377348445355892
Batch  81  loss:  0.0031340448185801506
Batch  91  loss:  0.0050849998369812965
Validation on real data: 
LOSS supervised-train 0.004275100564118475, valid 0.005044171586632729
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.008350865915417671
Batch  11  loss:  0.004093347582966089
Batch  21  loss:  0.00601145951077342
Batch  31  loss:  0.0032451506704092026
Batch  41  loss:  0.0021587181836366653
Batch  51  loss:  0.0029089387971907854
Batch  61  loss:  0.0038416089955717325
Batch  71  loss:  0.0038653232622891665
Batch  81  loss:  0.0032420214265584946
Batch  91  loss:  0.004464247263967991
Validation on real data: 
LOSS supervised-train 0.0040302851423621175, valid 0.0038956329226493835
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0035318636801093817
Batch  11  loss:  0.005766549147665501
Batch  21  loss:  0.00420221546664834
Batch  31  loss:  0.0049997675232589245
Batch  41  loss:  0.0028493942227214575
Batch  51  loss:  0.004013244528323412
Batch  61  loss:  0.0044774156995117664
Batch  71  loss:  0.0027557318098843098
Batch  81  loss:  0.003996002953499556
Batch  91  loss:  0.003726309398189187
Validation on real data: 
LOSS supervised-train 0.004315918833017349, valid 0.004143201746046543
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.006030917167663574
Batch  11  loss:  0.003804787527769804
Batch  21  loss:  0.003802069928497076
Batch  31  loss:  0.004751771688461304
Batch  41  loss:  0.003274709451943636
Batch  51  loss:  0.0033195435535162687
Batch  61  loss:  0.003933149389922619
Batch  71  loss:  0.002562911482527852
Batch  81  loss:  0.004951332230120897
Batch  91  loss:  0.003958704881370068
Validation on real data: 
LOSS supervised-train 0.003962342704180628, valid 0.004451605957001448
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.003071046667173505
Batch  11  loss:  0.004284737631678581
Batch  21  loss:  0.0023852570448070765
Batch  31  loss:  0.0038059549406170845
Batch  41  loss:  0.003270031651481986
Batch  51  loss:  0.0028623449616134167
Batch  61  loss:  0.00354277016595006
Batch  71  loss:  0.003644584445282817
Batch  81  loss:  0.0030961602460592985
Batch  91  loss:  0.005335937719792128
Validation on real data: 
LOSS supervised-train 0.003910194281488657, valid 0.0024303423706442118
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0025137874763458967
Batch  11  loss:  0.005381097085773945
Batch  21  loss:  0.00543720368295908
Batch  31  loss:  0.006614904385060072
Batch  41  loss:  0.006249487400054932
Batch  51  loss:  0.00360286352224648
Batch  61  loss:  0.003830975154414773
Batch  71  loss:  0.0033890726044774055
Batch  81  loss:  0.004203165881335735
Batch  91  loss:  0.004499529954046011
Validation on real data: 
LOSS supervised-train 0.004485899202991277, valid 0.0026590670458972454
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0033964174799621105
Batch  11  loss:  0.003517411882057786
Batch  21  loss:  0.0037044077180325985
Batch  31  loss:  0.0029807398095726967
Batch  41  loss:  0.0060973200015723705
Batch  51  loss:  0.003458951832726598
Batch  61  loss:  0.003941644448786974
Batch  71  loss:  0.004945955239236355
Batch  81  loss:  0.003809129586443305
Batch  91  loss:  0.0063405451364815235
Validation on real data: 
LOSS supervised-train 0.004530313112773001, valid 0.0016137699130922556
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.005252388771623373
Batch  11  loss:  0.003966778516769409
Batch  21  loss:  0.007312755566090345
Batch  31  loss:  0.0024599144235253334
Batch  41  loss:  0.008858874440193176
Batch  51  loss:  0.0029892530292272568
Batch  61  loss:  0.003373913001269102
Batch  71  loss:  0.006114349700510502
Batch  81  loss:  0.0024005258455872536
Batch  91  loss:  0.0036819614470005035
Validation on real data: 
LOSS supervised-train 0.004879656122066081, valid 0.0009995829313993454
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00696616992354393
Batch  11  loss:  0.002381739439442754
Batch  21  loss:  0.00798927340656519
Batch  31  loss:  0.002605247776955366
Batch  41  loss:  0.005216371733695269
Batch  51  loss:  0.006762136239558458
Batch  61  loss:  0.003242305712774396
Batch  71  loss:  0.006603126414120197
Batch  81  loss:  0.0025563575327396393
Batch  91  loss:  0.0040910933166742325
Validation on real data: 
LOSS supervised-train 0.004686371597927064, valid 0.0011693316046148539
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  bottle ; Model ID: 41a2005b595ae783be1868124d5ddbcb
--------------------
Training baseline regression model:  2022-03-30 14:02:29.013290
Detector:  pointnet
Object:  bottle
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1616956
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.32481682300567627
Batch  11  loss:  0.11158999800682068
Batch  21  loss:  0.035977572202682495
Batch  31  loss:  0.020412534475326538
Batch  41  loss:  0.018797041848301888
Batch  51  loss:  0.012829591520130634
Batch  61  loss:  0.010540121234953403
Batch  71  loss:  0.010308614000678062
Batch  81  loss:  0.009715579450130463
Batch  91  loss:  0.009558013640344143
Validation on real data: 
LOSS supervised-train 0.042373806359246376, valid 0.00846124067902565
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.009044877253472805
Batch  11  loss:  0.010653199627995491
Batch  21  loss:  0.010804334655404091
Batch  31  loss:  0.009338694624602795
Batch  41  loss:  0.008615618571639061
Batch  51  loss:  0.009053424932062626
Batch  61  loss:  0.008952206000685692
Batch  71  loss:  0.00931185856461525
Batch  81  loss:  0.00936654582619667
Batch  91  loss:  0.008379886858165264
Validation on real data: 
LOSS supervised-train 0.009185859211720526, valid 0.008926124311983585
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.008315623737871647
Batch  11  loss:  0.009801478125154972
Batch  21  loss:  0.007958758622407913
Batch  31  loss:  0.00807545892894268
Batch  41  loss:  0.008342367596924305
Batch  51  loss:  0.0080204326659441
Batch  61  loss:  0.008082862943410873
Batch  71  loss:  0.008341058157384396
Batch  81  loss:  0.00794017780572176
Batch  91  loss:  0.0076349773444235325
Validation on real data: 
LOSS supervised-train 0.008308285893872381, valid 0.009165456518530846
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.007844752632081509
Batch  11  loss:  0.007861943915486336
Batch  21  loss:  0.007578050717711449
Batch  31  loss:  0.007455817889422178
Batch  41  loss:  0.008187731727957726
Batch  51  loss:  0.007636222522705793
Batch  61  loss:  0.00793074443936348
Batch  71  loss:  0.00797899067401886
Batch  81  loss:  0.008039074949920177
Batch  91  loss:  0.007531147450208664
Validation on real data: 
LOSS supervised-train 0.007995913247577846, valid 0.007560273166745901
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.007664934266358614
Batch  11  loss:  0.00789475254714489
Batch  21  loss:  0.008360297419130802
Batch  31  loss:  0.00775649631395936
Batch  41  loss:  0.008346146903932095
Batch  51  loss:  0.007707693614065647
Batch  61  loss:  0.008064322173595428
Batch  71  loss:  0.007818618789315224
Batch  81  loss:  0.007839378900825977
Batch  91  loss:  0.007497100625187159
Validation on real data: 
LOSS supervised-train 0.00778065147344023, valid 0.007408613804727793
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.008307061158120632
Batch  11  loss:  0.007818548008799553
Batch  21  loss:  0.007632149383425713
Batch  31  loss:  0.007344263605773449
Batch  41  loss:  0.007503288332372904
Batch  51  loss:  0.007272111717611551
Batch  61  loss:  0.00751875014975667
Batch  71  loss:  0.007304053753614426
Batch  81  loss:  0.0072416821494698524
Batch  91  loss:  0.007806556764990091
Validation on real data: 
LOSS supervised-train 0.00756344988476485, valid 0.007999391295015812
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0071837082505226135
Batch  11  loss:  0.007515381556004286
Batch  21  loss:  0.007214180659502745
Batch  31  loss:  0.008106245659291744
Batch  41  loss:  0.008093408308923244
Batch  51  loss:  0.007236094679683447
Batch  61  loss:  0.00794604979455471
Batch  71  loss:  0.007446561940014362
Batch  81  loss:  0.007578047923743725
Batch  91  loss:  0.008127332665026188
Validation on real data: 
LOSS supervised-train 0.007438650219701231, valid 0.007883444428443909
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.007621499244123697
Batch  11  loss:  0.007342247758060694
Batch  21  loss:  0.007691644132137299
Batch  31  loss:  0.007194570265710354
Batch  41  loss:  0.007799917366355658
Batch  51  loss:  0.007218744605779648
Batch  61  loss:  0.007823387160897255
Batch  71  loss:  0.0070300051011145115
Batch  81  loss:  0.007133263163268566
Batch  91  loss:  0.006999699864536524
Validation on real data: 
LOSS supervised-train 0.007339730961248279, valid 0.0071851229295134544
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.007716724649071693
Batch  11  loss:  0.007606129627674818
Batch  21  loss:  0.007141301408410072
Batch  31  loss:  0.007221013307571411
Batch  41  loss:  0.007327438797801733
Batch  51  loss:  0.007539770565927029
Batch  61  loss:  0.007544833701103926
Batch  71  loss:  0.006962427869439125
Batch  81  loss:  0.007198465522378683
Batch  91  loss:  0.0072238571010529995
Validation on real data: 
LOSS supervised-train 0.007308995113708079, valid 0.007744926027953625
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.006963410414755344
Batch  11  loss:  0.006978368386626244
Batch  21  loss:  0.007139405235648155
Batch  31  loss:  0.0071352506056427956
Batch  41  loss:  0.007244385313242674
Batch  51  loss:  0.007362557575106621
Batch  61  loss:  0.007009177003055811
Batch  71  loss:  0.006847890559583902
Batch  81  loss:  0.007025989703834057
Batch  91  loss:  0.007003656588494778
Validation on real data: 
LOSS supervised-train 0.00720165264327079, valid 0.007281179074198008
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.007127449382096529
Batch  11  loss:  0.00756090646609664
Batch  21  loss:  0.006856696680188179
Batch  31  loss:  0.007770212832838297
Batch  41  loss:  0.007147676311433315
Batch  51  loss:  0.007085239049047232
Batch  61  loss:  0.00695975823327899
Batch  71  loss:  0.007368923164904118
Batch  81  loss:  0.007001771591603756
Batch  91  loss:  0.007507263217121363
Validation on real data: 
LOSS supervised-train 0.007198895295150578, valid 0.00753524387255311
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.006741046905517578
Batch  11  loss:  0.007307879626750946
Batch  21  loss:  0.007124120835214853
Batch  31  loss:  0.007184013724327087
Batch  41  loss:  0.007174386642873287
Batch  51  loss:  0.007069013547152281
Batch  61  loss:  0.00711040711030364
Batch  71  loss:  0.0072694518603384495
Batch  81  loss:  0.007047615945339203
Batch  91  loss:  0.007004396989941597
Validation on real data: 
LOSS supervised-train 0.007140844552777708, valid 0.007327508646994829
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.007034176494926214
Batch  11  loss:  0.006867558695375919
Batch  21  loss:  0.007163489703088999
Batch  31  loss:  0.0067734369076788425
Batch  41  loss:  0.007354327477514744
Batch  51  loss:  0.006928736809641123
Batch  61  loss:  0.007113446947187185
Batch  71  loss:  0.007567555643618107
Batch  81  loss:  0.007580935955047607
Batch  91  loss:  0.007632870692759752
Validation on real data: 
LOSS supervised-train 0.0070961013762280345, valid 0.006892985664308071
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.007311825640499592
Batch  11  loss:  0.006879244465380907
Batch  21  loss:  0.006967874243855476
Batch  31  loss:  0.006990064401179552
Batch  41  loss:  0.007011996582150459
Batch  51  loss:  0.006843368988484144
Batch  61  loss:  0.006878380198031664
Batch  71  loss:  0.007292716298252344
Batch  81  loss:  0.007072802633047104
Batch  91  loss:  0.008161922916769981
Validation on real data: 
LOSS supervised-train 0.007082086154259742, valid 0.007045953534543514
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.006745240651071072
Batch  11  loss:  0.0074520655907690525
Batch  21  loss:  0.006846230942755938
Batch  31  loss:  0.006893565412610769
Batch  41  loss:  0.007072105072438717
Batch  51  loss:  0.006904344540089369
Batch  61  loss:  0.0070531913079321384
Batch  71  loss:  0.00727730430662632
Batch  81  loss:  0.006938157137483358
Batch  91  loss:  0.007095170672982931
Validation on real data: 
LOSS supervised-train 0.007025535856373608, valid 0.007005165796726942
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.006811201572418213
Batch  11  loss:  0.006869088858366013
Batch  21  loss:  0.006846319884061813
Batch  31  loss:  0.006884973496198654
Batch  41  loss:  0.0071343714371323586
Batch  51  loss:  0.007161702960729599
Batch  61  loss:  0.007144511677324772
Batch  71  loss:  0.007154402788728476
Batch  81  loss:  0.007160289213061333
Batch  91  loss:  0.007286485750228167
Validation on real data: 
LOSS supervised-train 0.0070186456479132174, valid 0.006922810338437557
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.006817185785621405
Batch  11  loss:  0.007623756770044565
Batch  21  loss:  0.006930510047823191
Batch  31  loss:  0.006999256554991007
Batch  41  loss:  0.007829544134438038
Batch  51  loss:  0.007107867859303951
Batch  61  loss:  0.006913287565112114
Batch  71  loss:  0.007102684583514929
Batch  81  loss:  0.006845517084002495
Batch  91  loss:  0.0069031016901135445
Validation on real data: 
LOSS supervised-train 0.007014812156558037, valid 0.0070764487609267235
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.007836633361876011
Batch  11  loss:  0.007089830469340086
Batch  21  loss:  0.007031470537185669
Batch  31  loss:  0.006850039586424828
Batch  41  loss:  0.00692178588360548
Batch  51  loss:  0.006912193261086941
Batch  61  loss:  0.007069725077599287
Batch  71  loss:  0.006978999357670546
Batch  81  loss:  0.0070084501057863235
Batch  91  loss:  0.007297063712030649
Validation on real data: 
LOSS supervised-train 0.006991502344608307, valid 0.006912766955792904
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.00674678897485137
Batch  11  loss:  0.007043428253382444
Batch  21  loss:  0.006665852386504412
Batch  31  loss:  0.006873027421534061
Batch  41  loss:  0.007005402818322182
Batch  51  loss:  0.0068215117789804935
Batch  61  loss:  0.006932856980711222
Batch  71  loss:  0.006821716204285622
Batch  81  loss:  0.006795598194003105
Batch  91  loss:  0.0068945311941206455
Validation on real data: 
LOSS supervised-train 0.006953240721486509, valid 0.007169757969677448
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.006768016144633293
Batch  11  loss:  0.007379240822046995
Batch  21  loss:  0.006656674202531576
Batch  31  loss:  0.006786669138818979
Batch  41  loss:  0.007267125882208347
Batch  51  loss:  0.006766224279999733
Batch  61  loss:  0.006801680661737919
Batch  71  loss:  0.007094067987054586
Batch  81  loss:  0.00695450883358717
Batch  91  loss:  0.0073557160794734955
Validation on real data: 
LOSS supervised-train 0.0069323935732245445, valid 0.007166150491684675
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.006970992311835289
Batch  11  loss:  0.0067755370400846004
Batch  21  loss:  0.006606291979551315
Batch  31  loss:  0.0070445784367620945
Batch  41  loss:  0.007115587592124939
Batch  51  loss:  0.006854959763586521
Batch  61  loss:  0.006821381859481335
Batch  71  loss:  0.006987111642956734
Batch  81  loss:  0.006636266130954027
Batch  91  loss:  0.007004667539149523
Validation on real data: 
LOSS supervised-train 0.006908071958459914, valid 0.007016106508672237
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.006785265170037746
Batch  11  loss:  0.006910162977874279
Batch  21  loss:  0.006844318471848965
Batch  31  loss:  0.006908029317855835
Batch  41  loss:  0.007429404184222221
Batch  51  loss:  0.006858937442302704
Batch  61  loss:  0.006833517458289862
Batch  71  loss:  0.007193862460553646
Batch  81  loss:  0.006882748566567898
Batch  91  loss:  0.006762420758605003
Validation on real data: 
LOSS supervised-train 0.006891404497437179, valid 0.007262446917593479
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.006732872687280178
Batch  11  loss:  0.00683532515540719
Batch  21  loss:  0.006805475801229477
Batch  31  loss:  0.00683527672663331
Batch  41  loss:  0.007344831712543964
Batch  51  loss:  0.006877456791698933
Batch  61  loss:  0.007041523233056068
Batch  71  loss:  0.007028133142739534
Batch  81  loss:  0.006720518693327904
Batch  91  loss:  0.006836396176367998
Validation on real data: 
LOSS supervised-train 0.006881411485373974, valid 0.007209583185613155
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.006699869874864817
Batch  11  loss:  0.006827297154814005
Batch  21  loss:  0.00664349552243948
Batch  31  loss:  0.00679128197953105
Batch  41  loss:  0.007187919225543737
Batch  51  loss:  0.006957361940294504
Batch  61  loss:  0.006800401024520397
Batch  71  loss:  0.0074708410538733006
Batch  81  loss:  0.007073089946061373
Batch  91  loss:  0.006983340717852116
Validation on real data: 
LOSS supervised-train 0.006897462271153927, valid 0.006981492042541504
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0068406593054533005
Batch  11  loss:  0.006730484776198864
Batch  21  loss:  0.006618658546358347
Batch  31  loss:  0.006779903545975685
Batch  41  loss:  0.007305539678782225
Batch  51  loss:  0.006832679267972708
Batch  61  loss:  0.006974878255277872
Batch  71  loss:  0.007082005962729454
Batch  81  loss:  0.00664721941575408
Batch  91  loss:  0.006811627186834812
Validation on real data: 
LOSS supervised-train 0.006869453373365104, valid 0.006718336138874292
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.006929777562618256
Batch  11  loss:  0.00683475099503994
Batch  21  loss:  0.006603833753615618
Batch  31  loss:  0.006658304948359728
Batch  41  loss:  0.007227922324091196
Batch  51  loss:  0.006844287272542715
Batch  61  loss:  0.006775890477001667
Batch  71  loss:  0.006867025978863239
Batch  81  loss:  0.006694503128528595
Batch  91  loss:  0.0069647496566176414
Validation on real data: 
LOSS supervised-train 0.006849786238744855, valid 0.006723298691213131
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.006892366334795952
Batch  11  loss:  0.006883406545966864
Batch  21  loss:  0.0066152093932032585
Batch  31  loss:  0.006738452706485987
Batch  41  loss:  0.007144301198422909
Batch  51  loss:  0.0067125712521374226
Batch  61  loss:  0.006943201646208763
Batch  71  loss:  0.006794461980462074
Batch  81  loss:  0.006715277675539255
Batch  91  loss:  0.006997519172728062
Validation on real data: 
LOSS supervised-train 0.006842061332426965, valid 0.006989440880715847
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.006805555894970894
Batch  11  loss:  0.0068177455104887486
Batch  21  loss:  0.006552606821060181
Batch  31  loss:  0.006899866741150618
Batch  41  loss:  0.007344256620854139
Batch  51  loss:  0.00687813013792038
Batch  61  loss:  0.006729930639266968
Batch  71  loss:  0.007111901417374611
Batch  81  loss:  0.006819040514528751
Batch  91  loss:  0.006826416123658419
Validation on real data: 
LOSS supervised-train 0.006835140981711447, valid 0.006901409476995468
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00689751235768199
Batch  11  loss:  0.006832537706941366
Batch  21  loss:  0.006665162742137909
Batch  31  loss:  0.006882264278829098
Batch  41  loss:  0.007521152496337891
Batch  51  loss:  0.006797554437071085
Batch  61  loss:  0.00677932845428586
Batch  71  loss:  0.006870611570775509
Batch  81  loss:  0.006765829399228096
Batch  91  loss:  0.006860694848001003
Validation on real data: 
LOSS supervised-train 0.006841861335560679, valid 0.006818833295255899
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.006633891724050045
Batch  11  loss:  0.006689229980111122
Batch  21  loss:  0.006712051574140787
Batch  31  loss:  0.006810083985328674
Batch  41  loss:  0.0070732831954956055
Batch  51  loss:  0.006752775050699711
Batch  61  loss:  0.006718326825648546
Batch  71  loss:  0.006804519798606634
Batch  81  loss:  0.006795029621571302
Batch  91  loss:  0.006923855282366276
Validation on real data: 
LOSS supervised-train 0.0068117302237078545, valid 0.00692364014685154
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.006779271177947521
Batch  11  loss:  0.006669202819466591
Batch  21  loss:  0.006627481430768967
Batch  31  loss:  0.006751587614417076
Batch  41  loss:  0.007471561431884766
Batch  51  loss:  0.006948917172849178
Batch  61  loss:  0.006726366933435202
Batch  71  loss:  0.00677395798265934
Batch  81  loss:  0.006699753925204277
Batch  91  loss:  0.0067382738925516605
Validation on real data: 
LOSS supervised-train 0.006816417882218957, valid 0.00689302571117878
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.006590803153812885
Batch  11  loss:  0.006994960363954306
Batch  21  loss:  0.0066056749783456326
Batch  31  loss:  0.006754841655492783
Batch  41  loss:  0.007270394824445248
Batch  51  loss:  0.006642819382250309
Batch  61  loss:  0.006940730847418308
Batch  71  loss:  0.007067411206662655
Batch  81  loss:  0.00701501127332449
Batch  91  loss:  0.006834235042333603
Validation on real data: 
LOSS supervised-train 0.006820275261998176, valid 0.006834863219410181
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.00685182074084878
Batch  11  loss:  0.006766310892999172
Batch  21  loss:  0.006705040577799082
Batch  31  loss:  0.006812321953475475
Batch  41  loss:  0.0071046981029212475
Batch  51  loss:  0.0067729465663433075
Batch  61  loss:  0.006815074011683464
Batch  71  loss:  0.006758356466889381
Batch  81  loss:  0.007486374117434025
Batch  91  loss:  0.006775635294616222
Validation on real data: 
LOSS supervised-train 0.006838658810593188, valid 0.006912738084793091
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.006759574171155691
Batch  11  loss:  0.006917618215084076
Batch  21  loss:  0.006709425710141659
Batch  31  loss:  0.006655567325651646
Batch  41  loss:  0.006871100049465895
Batch  51  loss:  0.007089228369295597
Batch  61  loss:  0.006768465042114258
Batch  71  loss:  0.006831228733062744
Batch  81  loss:  0.006596971768885851
Batch  91  loss:  0.006855922285467386
Validation on real data: 
LOSS supervised-train 0.006781749124638736, valid 0.006647579371929169
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.006787561811506748
Batch  11  loss:  0.006842494942247868
Batch  21  loss:  0.006516530178487301
Batch  31  loss:  0.006695188581943512
Batch  41  loss:  0.007103470619767904
Batch  51  loss:  0.006838977336883545
Batch  61  loss:  0.006758438888937235
Batch  71  loss:  0.0068823108449578285
Batch  81  loss:  0.006782236509025097
Batch  91  loss:  0.006910074967890978
Validation on real data: 
LOSS supervised-train 0.006760972617194057, valid 0.006731228902935982
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.006621561013162136
Batch  11  loss:  0.006662235129624605
Batch  21  loss:  0.0065943412482738495
Batch  31  loss:  0.006705103907734156
Batch  41  loss:  0.007030264008790255
Batch  51  loss:  0.006824791431427002
Batch  61  loss:  0.0067300572991371155
Batch  71  loss:  0.0068166255950927734
Batch  81  loss:  0.0065879481844604015
Batch  91  loss:  0.006896628998219967
Validation on real data: 
LOSS supervised-train 0.006756156869232655, valid 0.006739367265254259
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.006893553771078587
Batch  11  loss:  0.006688939873129129
Batch  21  loss:  0.006602985318750143
Batch  31  loss:  0.006833275314420462
Batch  41  loss:  0.007033242378383875
Batch  51  loss:  0.006705502513796091
Batch  61  loss:  0.006766158156096935
Batch  71  loss:  0.0068473368883132935
Batch  81  loss:  0.006608631461858749
Batch  91  loss:  0.0067766085267066956
Validation on real data: 
LOSS supervised-train 0.0067709775222465395, valid 0.006789886858314276
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.00660977466031909
Batch  11  loss:  0.006651912350207567
Batch  21  loss:  0.0067917355336248875
Batch  31  loss:  0.006680644117295742
Batch  41  loss:  0.0069833779707551
Batch  51  loss:  0.006740395911037922
Batch  61  loss:  0.006683811545372009
Batch  71  loss:  0.006766929291188717
Batch  81  loss:  0.006789620965719223
Batch  91  loss:  0.006835716776549816
Validation on real data: 
LOSS supervised-train 0.006748734526336193, valid 0.0066810306161642075
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.006632092408835888
Batch  11  loss:  0.006700505036860704
Batch  21  loss:  0.006568855606019497
Batch  31  loss:  0.006700652185827494
Batch  41  loss:  0.006923794746398926
Batch  51  loss:  0.006939246319234371
Batch  61  loss:  0.006705108564347029
Batch  71  loss:  0.006754525937139988
Batch  81  loss:  0.006832491140812635
Batch  91  loss:  0.0068243881687521935
Validation on real data: 
LOSS supervised-train 0.006760267582722008, valid 0.006716910284012556
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.007110584992915392
Batch  11  loss:  0.006696235854178667
Batch  21  loss:  0.006478076800704002
Batch  31  loss:  0.006775441113859415
Batch  41  loss:  0.006734996102750301
Batch  51  loss:  0.00677897222340107
Batch  61  loss:  0.006858645007014275
Batch  71  loss:  0.006814712658524513
Batch  81  loss:  0.006664587184786797
Batch  91  loss:  0.006822648458182812
Validation on real data: 
LOSS supervised-train 0.006744088572449982, valid 0.006644720211625099
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.006635535974055529
Batch  11  loss:  0.006677751895040274
Batch  21  loss:  0.0066732061095535755
Batch  31  loss:  0.006722237914800644
Batch  41  loss:  0.007269259542226791
Batch  51  loss:  0.0066923703998327255
Batch  61  loss:  0.006864815019071102
Batch  71  loss:  0.006919398903846741
Batch  81  loss:  0.00671380152925849
Batch  91  loss:  0.007023022510111332
Validation on real data: 
LOSS supervised-train 0.006798012140206993, valid 0.006776455324143171
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.006745160557329655
Batch  11  loss:  0.006832257844507694
Batch  21  loss:  0.006533048115670681
Batch  31  loss:  0.006914303172379732
Batch  41  loss:  0.0071038054302334785
Batch  51  loss:  0.006677036639302969
Batch  61  loss:  0.0066430214792490005
Batch  71  loss:  0.006998982280492783
Batch  81  loss:  0.006637162528932095
Batch  91  loss:  0.006916520651429892
Validation on real data: 
LOSS supervised-train 0.006711447290144861, valid 0.006641237065196037
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.007699858862906694
Batch  11  loss:  0.006807120982557535
Batch  21  loss:  0.0065133702009916306
Batch  31  loss:  0.006716234143823385
Batch  41  loss:  0.006830365397036076
Batch  51  loss:  0.006739844102412462
Batch  61  loss:  0.006702997721731663
Batch  71  loss:  0.007302079815417528
Batch  81  loss:  0.006839598063379526
Batch  91  loss:  0.006724099162966013
Validation on real data: 
LOSS supervised-train 0.0067536132084205745, valid 0.006721684709191322
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.006803443655371666
Batch  11  loss:  0.006649565417319536
Batch  21  loss:  0.006604478694498539
Batch  31  loss:  0.006761462427675724
Batch  41  loss:  0.006957483012229204
Batch  51  loss:  0.006570857483893633
Batch  61  loss:  0.00671527860686183
Batch  71  loss:  0.006694421637803316
Batch  81  loss:  0.006675219163298607
Batch  91  loss:  0.006872018799185753
Validation on real data: 
LOSS supervised-train 0.006718439459800721, valid 0.0066177090629935265
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.007245199289172888
Batch  11  loss:  0.006643644534051418
Batch  21  loss:  0.006626592017710209
Batch  31  loss:  0.006753979716449976
Batch  41  loss:  0.006880253553390503
Batch  51  loss:  0.006752559915184975
Batch  61  loss:  0.006678573787212372
Batch  71  loss:  0.006691541522741318
Batch  81  loss:  0.006597839295864105
Batch  91  loss:  0.006867786403745413
Validation on real data: 
LOSS supervised-train 0.006728578410111367, valid 0.006830650381743908
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.006561788264662027
Batch  11  loss:  0.006534462329000235
Batch  21  loss:  0.00666837627068162
Batch  31  loss:  0.006712906062602997
Batch  41  loss:  0.007040961179882288
Batch  51  loss:  0.006684108171612024
Batch  61  loss:  0.006888570263981819
Batch  71  loss:  0.007740006782114506
Batch  81  loss:  0.006541902665048838
Batch  91  loss:  0.006755310110747814
Validation on real data: 
LOSS supervised-train 0.006723829945549369, valid 0.006719894241541624
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.006723909173160791
Batch  11  loss:  0.006627836264669895
Batch  21  loss:  0.006483602803200483
Batch  31  loss:  0.007055777125060558
Batch  41  loss:  0.00713006965816021
Batch  51  loss:  0.006628010887652636
Batch  61  loss:  0.006680733989924192
Batch  71  loss:  0.006731020286679268
Batch  81  loss:  0.006717699579894543
Batch  91  loss:  0.006720334757119417
Validation on real data: 
LOSS supervised-train 0.006710800402797759, valid 0.006754702888429165
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0066207521595060825
Batch  11  loss:  0.006682032253593206
Batch  21  loss:  0.006559340283274651
Batch  31  loss:  0.007432418409734964
Batch  41  loss:  0.007343331351876259
Batch  51  loss:  0.00668119452893734
Batch  61  loss:  0.006720712408423424
Batch  71  loss:  0.0070178937166929245
Batch  81  loss:  0.006669709458947182
Batch  91  loss:  0.00674423249438405
Validation on real data: 
LOSS supervised-train 0.006714689540676773, valid 0.006696608383208513
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.006594253703951836
Batch  11  loss:  0.006680834107100964
Batch  21  loss:  0.006472407374531031
Batch  31  loss:  0.006660505197942257
Batch  41  loss:  0.007407905999571085
Batch  51  loss:  0.006934460252523422
Batch  61  loss:  0.00692626042291522
Batch  71  loss:  0.006617305334657431
Batch  81  loss:  0.006648489739745855
Batch  91  loss:  0.006882375106215477
Validation on real data: 
LOSS supervised-train 0.006715417713858187, valid 0.006634038407355547
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0067245839163661
Batch  11  loss:  0.006579960230737925
Batch  21  loss:  0.00666608102619648
Batch  31  loss:  0.006809297017753124
Batch  41  loss:  0.0067847128957509995
Batch  51  loss:  0.006724341306835413
Batch  61  loss:  0.006778359413146973
Batch  71  loss:  0.006835535634309053
Batch  81  loss:  0.006588709075003862
Batch  91  loss:  0.0069398474879562855
Validation on real data: 
LOSS supervised-train 0.0066941525181755425, valid 0.00656114099547267
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.006576493848115206
Batch  11  loss:  0.00669186981394887
Batch  21  loss:  0.006593149155378342
Batch  31  loss:  0.006915293633937836
Batch  41  loss:  0.006944139953702688
Batch  51  loss:  0.0067247021943330765
Batch  61  loss:  0.006815408356487751
Batch  71  loss:  0.006798611022531986
Batch  81  loss:  0.006588479038327932
Batch  91  loss:  0.006828831508755684
Validation on real data: 
LOSS supervised-train 0.00668887825217098, valid 0.006614592391997576
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.006735070608556271
Batch  11  loss:  0.0066084545105695724
Batch  21  loss:  0.006521030329167843
Batch  31  loss:  0.006686168722808361
Batch  41  loss:  0.006786026991903782
Batch  51  loss:  0.006657796446233988
Batch  61  loss:  0.00667321914806962
Batch  71  loss:  0.006843241397291422
Batch  81  loss:  0.006837374530732632
Batch  91  loss:  0.0067775920033454895
Validation on real data: 
LOSS supervised-train 0.006683104219846427, valid 0.006696194410324097
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.006506416480988264
Batch  11  loss:  0.006722139194607735
Batch  21  loss:  0.006489394698292017
Batch  31  loss:  0.006694783456623554
Batch  41  loss:  0.00714319571852684
Batch  51  loss:  0.0065984646789729595
Batch  61  loss:  0.0066924188286066055
Batch  71  loss:  0.006788642145693302
Batch  81  loss:  0.0067669907584786415
Batch  91  loss:  0.0067728012800216675
Validation on real data: 
LOSS supervised-train 0.006681354753673077, valid 0.006602945737540722
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.007244253996759653
Batch  11  loss:  0.006545880809426308
Batch  21  loss:  0.00653111794963479
Batch  31  loss:  0.006633590441197157
Batch  41  loss:  0.006939681712538004
Batch  51  loss:  0.0067011332139372826
Batch  61  loss:  0.006661045365035534
Batch  71  loss:  0.006820633076131344
Batch  81  loss:  0.006712279282510281
Batch  91  loss:  0.006847886834293604
Validation on real data: 
LOSS supervised-train 0.006686312560923398, valid 0.006810823921114206
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.006744133308529854
Batch  11  loss:  0.006751243490725756
Batch  21  loss:  0.006494182627648115
Batch  31  loss:  0.006715794559568167
Batch  41  loss:  0.006894377525895834
Batch  51  loss:  0.006743176840245724
Batch  61  loss:  0.006692970637232065
Batch  71  loss:  0.0067749894224107265
Batch  81  loss:  0.00666821701452136
Batch  91  loss:  0.006792311090976
Validation on real data: 
LOSS supervised-train 0.006678906674496829, valid 0.006751344073563814
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.006635423284024
Batch  11  loss:  0.006679084151983261
Batch  21  loss:  0.006548835895955563
Batch  31  loss:  0.006641449872404337
Batch  41  loss:  0.006894790567457676
Batch  51  loss:  0.00657467870041728
Batch  61  loss:  0.006742286495864391
Batch  71  loss:  0.007470987271517515
Batch  81  loss:  0.006567684467881918
Batch  91  loss:  0.006886630319058895
Validation on real data: 
LOSS supervised-train 0.006681481460109353, valid 0.006731011904776096
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.006594735197722912
Batch  11  loss:  0.006607137620449066
Batch  21  loss:  0.006512768100947142
Batch  31  loss:  0.006659925915300846
Batch  41  loss:  0.007372100837528706
Batch  51  loss:  0.006627288181334734
Batch  61  loss:  0.006743211764842272
Batch  71  loss:  0.006663001142442226
Batch  81  loss:  0.00674764858558774
Batch  91  loss:  0.006974248681217432
Validation on real data: 
LOSS supervised-train 0.006672224099747837, valid 0.006557515822350979
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.006594362203031778
Batch  11  loss:  0.006577581632882357
Batch  21  loss:  0.006529037840664387
Batch  31  loss:  0.006627598777413368
Batch  41  loss:  0.007279056590050459
Batch  51  loss:  0.006738992873579264
Batch  61  loss:  0.006667624693363905
Batch  71  loss:  0.006795160006731749
Batch  81  loss:  0.00658569298684597
Batch  91  loss:  0.006940442603081465
Validation on real data: 
LOSS supervised-train 0.00667340521235019, valid 0.006582658272236586
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.006801181938499212
Batch  11  loss:  0.006529068574309349
Batch  21  loss:  0.0064680324867367744
Batch  31  loss:  0.0066772159188985825
Batch  41  loss:  0.006717231124639511
Batch  51  loss:  0.006707766558974981
Batch  61  loss:  0.006749126594513655
Batch  71  loss:  0.006833885330706835
Batch  81  loss:  0.006572004407644272
Batch  91  loss:  0.006707166321575642
Validation on real data: 
LOSS supervised-train 0.006658407268114388, valid 0.006568771321326494
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.007091008126735687
Batch  11  loss:  0.00668261619284749
Batch  21  loss:  0.006562468130141497
Batch  31  loss:  0.007931879721581936
Batch  41  loss:  0.006882121320813894
Batch  51  loss:  0.006705723237246275
Batch  61  loss:  0.006627337075769901
Batch  71  loss:  0.006768904160708189
Batch  81  loss:  0.006694694049656391
Batch  91  loss:  0.00669743400067091
Validation on real data: 
LOSS supervised-train 0.0066629873868078, valid 0.006570967845618725
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.006529586389660835
Batch  11  loss:  0.006545350421220064
Batch  21  loss:  0.006605243310332298
Batch  31  loss:  0.006737386807799339
Batch  41  loss:  0.006882601883262396
Batch  51  loss:  0.006727806758135557
Batch  61  loss:  0.0066775428131222725
Batch  71  loss:  0.006739599630236626
Batch  81  loss:  0.00658215070143342
Batch  91  loss:  0.0069795954041182995
Validation on real data: 
LOSS supervised-train 0.006672517089173198, valid 0.006572720129042864
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.006634815596044064
Batch  11  loss:  0.006602266803383827
Batch  21  loss:  0.006570775993168354
Batch  31  loss:  0.006716843228787184
Batch  41  loss:  0.006850806530565023
Batch  51  loss:  0.006709516979753971
Batch  61  loss:  0.006687670014798641
Batch  71  loss:  0.006748102139681578
Batch  81  loss:  0.006672298070043325
Batch  91  loss:  0.006971195340156555
Validation on real data: 
LOSS supervised-train 0.006672898135147989, valid 0.006531897466629744
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.006550525315105915
Batch  11  loss:  0.0066582500003278255
Batch  21  loss:  0.00682184100151062
Batch  31  loss:  0.0067492020316421986
Batch  41  loss:  0.006998339667916298
Batch  51  loss:  0.006821551825851202
Batch  61  loss:  0.006634771823883057
Batch  71  loss:  0.00668044900521636
Batch  81  loss:  0.006656905170530081
Batch  91  loss:  0.006811400409787893
Validation on real data: 
LOSS supervised-train 0.006652709394693375, valid 0.006804772652685642
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.006573283579200506
Batch  11  loss:  0.006568035110831261
Batch  21  loss:  0.006438535172492266
Batch  31  loss:  0.006701928097754717
Batch  41  loss:  0.006770046427845955
Batch  51  loss:  0.006646367721259594
Batch  61  loss:  0.006771515589207411
Batch  71  loss:  0.0068578654900193214
Batch  81  loss:  0.006735680624842644
Batch  91  loss:  0.00677140336483717
Validation on real data: 
LOSS supervised-train 0.0066469393344596025, valid 0.006762307602912188
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0068757799454033375
Batch  11  loss:  0.006557677872478962
Batch  21  loss:  0.006535759195685387
Batch  31  loss:  0.006608376279473305
Batch  41  loss:  0.007045157253742218
Batch  51  loss:  0.00665639340877533
Batch  61  loss:  0.006596206687390804
Batch  71  loss:  0.0068230885080993176
Batch  81  loss:  0.006574003491550684
Batch  91  loss:  0.006758890580385923
Validation on real data: 
LOSS supervised-train 0.006640712209045887, valid 0.006730989553034306
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.006783653516322374
Batch  11  loss:  0.006508497521281242
Batch  21  loss:  0.006446798797696829
Batch  31  loss:  0.006712965201586485
Batch  41  loss:  0.00668809749186039
Batch  51  loss:  0.00663309870287776
Batch  61  loss:  0.006620573345571756
Batch  71  loss:  0.0069897654466331005
Batch  81  loss:  0.006659161765128374
Batch  91  loss:  0.006930864416062832
Validation on real data: 
LOSS supervised-train 0.006642361944541335, valid 0.0065713319927453995
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.006610244512557983
Batch  11  loss:  0.006586831528693438
Batch  21  loss:  0.006415438372641802
Batch  31  loss:  0.006643960252404213
Batch  41  loss:  0.006752283312380314
Batch  51  loss:  0.006693175993859768
Batch  61  loss:  0.0067787072621285915
Batch  71  loss:  0.0066769178956747055
Batch  81  loss:  0.006504295393824577
Batch  91  loss:  0.006664914544671774
Validation on real data: 
LOSS supervised-train 0.006630211593583226, valid 0.006541403476148844
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0067522875033319
Batch  11  loss:  0.00653412938117981
Batch  21  loss:  0.006514090113341808
Batch  31  loss:  0.006993694230914116
Batch  41  loss:  0.0068828389048576355
Batch  51  loss:  0.006727224215865135
Batch  61  loss:  0.006652553100138903
Batch  71  loss:  0.006809362210333347
Batch  81  loss:  0.0065933349542319775
Batch  91  loss:  0.006694823503494263
Validation on real data: 
LOSS supervised-train 0.0066392120067030195, valid 0.006563827861100435
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.006567665375769138
Batch  11  loss:  0.006524778436869383
Batch  21  loss:  0.006647557020187378
Batch  31  loss:  0.006714930292218924
Batch  41  loss:  0.006808679085224867
Batch  51  loss:  0.00662860507145524
Batch  61  loss:  0.006691759452223778
Batch  71  loss:  0.006695211865007877
Batch  81  loss:  0.0065946029499173164
Batch  91  loss:  0.00672726659104228
Validation on real data: 
LOSS supervised-train 0.006639508288353682, valid 0.0066456422209739685
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.006583061069250107
Batch  11  loss:  0.006566976197063923
Batch  21  loss:  0.006695031188428402
Batch  31  loss:  0.0070357546210289
Batch  41  loss:  0.006711242720484734
Batch  51  loss:  0.006649539805948734
Batch  61  loss:  0.00669392105191946
Batch  71  loss:  0.006729815620929003
Batch  81  loss:  0.006597522180527449
Batch  91  loss:  0.006769864819943905
Validation on real data: 
LOSS supervised-train 0.00663698447868228, valid 0.006593179423362017
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.006692687515169382
Batch  11  loss:  0.006614034995436668
Batch  21  loss:  0.006501576863229275
Batch  31  loss:  0.006604901980608702
Batch  41  loss:  0.006800512317568064
Batch  51  loss:  0.00666025560349226
Batch  61  loss:  0.00663283234462142
Batch  71  loss:  0.006797614973038435
Batch  81  loss:  0.006538664922118187
Batch  91  loss:  0.006775555666536093
Validation on real data: 
LOSS supervised-train 0.006636106134392321, valid 0.0066120135597884655
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.006795802153646946
Batch  11  loss:  0.006611932534724474
Batch  21  loss:  0.006470208056271076
Batch  31  loss:  0.006579072214663029
Batch  41  loss:  0.006941176950931549
Batch  51  loss:  0.006647233851253986
Batch  61  loss:  0.006609362084418535
Batch  71  loss:  0.006797267124056816
Batch  81  loss:  0.0065704104490578175
Batch  91  loss:  0.006742563564330339
Validation on real data: 
LOSS supervised-train 0.006647870778106153, valid 0.006529028061777353
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.006465250160545111
Batch  11  loss:  0.006461929064244032
Batch  21  loss:  0.006445819512009621
Batch  31  loss:  0.006683914456516504
Batch  41  loss:  0.0069029866717755795
Batch  51  loss:  0.006758193951100111
Batch  61  loss:  0.006698020733892918
Batch  71  loss:  0.0067075942642986774
Batch  81  loss:  0.006773652508854866
Batch  91  loss:  0.006848904769867659
Validation on real data: 
LOSS supervised-train 0.00662858278490603, valid 0.006697255186736584
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.006540049333125353
Batch  11  loss:  0.006691645830869675
Batch  21  loss:  0.006453510373830795
Batch  31  loss:  0.006652830634266138
Batch  41  loss:  0.006714611314237118
Batch  51  loss:  0.006629529874771833
Batch  61  loss:  0.006706167943775654
Batch  71  loss:  0.0067718601785600185
Batch  81  loss:  0.00661971652880311
Batch  91  loss:  0.006757860071957111
Validation on real data: 
LOSS supervised-train 0.006635685935616493, valid 0.00659173121675849
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.006562585476785898
Batch  11  loss:  0.00660377973690629
Batch  21  loss:  0.006591335870325565
Batch  31  loss:  0.006706535816192627
Batch  41  loss:  0.0067230998538434505
Batch  51  loss:  0.006916446145623922
Batch  61  loss:  0.0067849126644432545
Batch  71  loss:  0.006803384982049465
Batch  81  loss:  0.006562831345945597
Batch  91  loss:  0.006822670344263315
Validation on real data: 
LOSS supervised-train 0.0066246636537835, valid 0.006563645321875811
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0066923657432198524
Batch  11  loss:  0.006634429097175598
Batch  21  loss:  0.006466836202889681
Batch  31  loss:  0.0067092678509652615
Batch  41  loss:  0.006851722486317158
Batch  51  loss:  0.006611342076212168
Batch  61  loss:  0.00667186314240098
Batch  71  loss:  0.006820425856858492
Batch  81  loss:  0.006633348297327757
Batch  91  loss:  0.006646163295954466
Validation on real data: 
LOSS supervised-train 0.006616102531552315, valid 0.006628234405070543
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.006512053310871124
Batch  11  loss:  0.006436748430132866
Batch  21  loss:  0.006415489129722118
Batch  31  loss:  0.006968983449041843
Batch  41  loss:  0.006728462874889374
Batch  51  loss:  0.006671613082289696
Batch  61  loss:  0.006714885588735342
Batch  71  loss:  0.006723928265273571
Batch  81  loss:  0.006569249089807272
Batch  91  loss:  0.006584801711142063
Validation on real data: 
LOSS supervised-train 0.006634180028922856, valid 0.006570115685462952
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.006593348458409309
Batch  11  loss:  0.006531697232276201
Batch  21  loss:  0.006441513076424599
Batch  31  loss:  0.006680165883153677
Batch  41  loss:  0.006984279025346041
Batch  51  loss:  0.0065450165420770645
Batch  61  loss:  0.006781282369047403
Batch  71  loss:  0.006748376414179802
Batch  81  loss:  0.006628104485571384
Batch  91  loss:  0.00671863229945302
Validation on real data: 
LOSS supervised-train 0.006615220136009157, valid 0.006737404968589544
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0064954874105751514
Batch  11  loss:  0.006517310161143541
Batch  21  loss:  0.006462326273322105
Batch  31  loss:  0.0069684553891420364
Batch  41  loss:  0.0067505971528589725
Batch  51  loss:  0.006642878986895084
Batch  61  loss:  0.006683591287583113
Batch  71  loss:  0.006881895940750837
Batch  81  loss:  0.006672439631074667
Batch  91  loss:  0.00673813559114933
Validation on real data: 
LOSS supervised-train 0.0066223920090124015, valid 0.006764393765479326
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.006610604003071785
Batch  11  loss:  0.0066050635650753975
Batch  21  loss:  0.0064458283595740795
Batch  31  loss:  0.006663772743195295
Batch  41  loss:  0.006776375230401754
Batch  51  loss:  0.006686113774776459
Batch  61  loss:  0.006632050033658743
Batch  71  loss:  0.006767448503524065
Batch  81  loss:  0.006590803153812885
Batch  91  loss:  0.006781945005059242
Validation on real data: 
LOSS supervised-train 0.006615265440195799, valid 0.006539505440741777
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0065592327155172825
Batch  11  loss:  0.0064583453349769115
Batch  21  loss:  0.006464794278144836
Batch  31  loss:  0.006642663385719061
Batch  41  loss:  0.006670141592621803
Batch  51  loss:  0.006529301404953003
Batch  61  loss:  0.00664972048252821
Batch  71  loss:  0.006770688574761152
Batch  81  loss:  0.006566762924194336
Batch  91  loss:  0.006648506969213486
Validation on real data: 
LOSS supervised-train 0.006600682931020856, valid 0.006687428802251816
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.006665452383458614
Batch  11  loss:  0.0064905849285423756
Batch  21  loss:  0.006444725673645735
Batch  31  loss:  0.006691081915050745
Batch  41  loss:  0.006801930721849203
Batch  51  loss:  0.006627095863223076
Batch  61  loss:  0.0067376065999269485
Batch  71  loss:  0.006777515634894371
Batch  81  loss:  0.006567429285496473
Batch  91  loss:  0.006696746218949556
Validation on real data: 
LOSS supervised-train 0.006621699058450758, valid 0.006506183184683323
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.006538279354572296
Batch  11  loss:  0.0065335663966834545
Batch  21  loss:  0.006389507558196783
Batch  31  loss:  0.006706178188323975
Batch  41  loss:  0.006800863426178694
Batch  51  loss:  0.006595181301236153
Batch  61  loss:  0.006719455122947693
Batch  71  loss:  0.006696865428239107
Batch  81  loss:  0.006614328362047672
Batch  91  loss:  0.0066435085609555244
Validation on real data: 
LOSS supervised-train 0.006606890140101313, valid 0.0066014803014695644
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.006501233205199242
Batch  11  loss:  0.006526455748826265
Batch  21  loss:  0.006463135126978159
Batch  31  loss:  0.0065658423118293285
Batch  41  loss:  0.006633545737713575
Batch  51  loss:  0.0066400752402842045
Batch  61  loss:  0.006754932459443808
Batch  71  loss:  0.0068152891471982
Batch  81  loss:  0.0066207884810864925
Batch  91  loss:  0.0066773127764463425
Validation on real data: 
LOSS supervised-train 0.006590390056371689, valid 0.006523753050714731
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0068618496879935265
Batch  11  loss:  0.006490618921816349
Batch  21  loss:  0.006449050735682249
Batch  31  loss:  0.006737816613167524
Batch  41  loss:  0.006744787096977234
Batch  51  loss:  0.006636237725615501
Batch  61  loss:  0.0067806849256157875
Batch  71  loss:  0.006707302760332823
Batch  81  loss:  0.006728896871209145
Batch  91  loss:  0.006739513017237186
Validation on real data: 
LOSS supervised-train 0.0065997687866911296, valid 0.006774961948394775
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.006498797796666622
Batch  11  loss:  0.006538696121424437
Batch  21  loss:  0.006450937129557133
Batch  31  loss:  0.006672350224107504
Batch  41  loss:  0.006808792240917683
Batch  51  loss:  0.006640257779508829
Batch  61  loss:  0.006625292357057333
Batch  71  loss:  0.006890425458550453
Batch  81  loss:  0.006561912130564451
Batch  91  loss:  0.006677293684333563
Validation on real data: 
LOSS supervised-train 0.006604725616052747, valid 0.00657473411411047
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.006676601245999336
Batch  11  loss:  0.006533678621053696
Batch  21  loss:  0.006575238425284624
Batch  31  loss:  0.0070131816901266575
Batch  41  loss:  0.006677199620753527
Batch  51  loss:  0.006609269417822361
Batch  61  loss:  0.006715221330523491
Batch  71  loss:  0.006890644785016775
Batch  81  loss:  0.006552143953740597
Batch  91  loss:  0.006689131259918213
Validation on real data: 
LOSS supervised-train 0.006596657391637564, valid 0.006752160843461752
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00653955340385437
Batch  11  loss:  0.006497579626739025
Batch  21  loss:  0.006407669745385647
Batch  31  loss:  0.006786012556403875
Batch  41  loss:  0.006714197341352701
Batch  51  loss:  0.006582416128367186
Batch  61  loss:  0.0067212749272584915
Batch  71  loss:  0.006792282219976187
Batch  81  loss:  0.006577194202691317
Batch  91  loss:  0.0067612179554998875
Validation on real data: 
LOSS supervised-train 0.006596066006459296, valid 0.006670445669442415
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.006536595523357391
Batch  11  loss:  0.00656093517318368
Batch  21  loss:  0.0065706404857337475
Batch  31  loss:  0.00667957728728652
Batch  41  loss:  0.00664976891130209
Batch  51  loss:  0.0065636723302304745
Batch  61  loss:  0.006659138016402721
Batch  71  loss:  0.006769076455384493
Batch  81  loss:  0.006623797118663788
Batch  91  loss:  0.006653456017374992
Validation on real data: 
LOSS supervised-train 0.00659038444980979, valid 0.006568292621523142
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0067396401427686214
Batch  11  loss:  0.006525460164994001
Batch  21  loss:  0.006443451624363661
Batch  31  loss:  0.006671365816146135
Batch  41  loss:  0.006686782464385033
Batch  51  loss:  0.006578652188181877
Batch  61  loss:  0.006676473654806614
Batch  71  loss:  0.00688186613842845
Batch  81  loss:  0.006639591418206692
Batch  91  loss:  0.006673325318843126
Validation on real data: 
LOSS supervised-train 0.0066039181128144265, valid 0.006792875938117504
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.006532097700983286
Batch  11  loss:  0.006594430655241013
Batch  21  loss:  0.006406841333955526
Batch  31  loss:  0.00661374069750309
Batch  41  loss:  0.0068382504396140575
Batch  51  loss:  0.006700982805341482
Batch  61  loss:  0.006689118221402168
Batch  71  loss:  0.006788681726902723
Batch  81  loss:  0.006633772514760494
Batch  91  loss:  0.00661388598382473
Validation on real data: 
LOSS supervised-train 0.006597866797819734, valid 0.006547826807945967
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.006521021947264671
Batch  11  loss:  0.0065005142241716385
Batch  21  loss:  0.00646473141387105
Batch  31  loss:  0.006730669643729925
Batch  41  loss:  0.006615284364670515
Batch  51  loss:  0.006628958508372307
Batch  61  loss:  0.006748524960130453
Batch  71  loss:  0.006805839482694864
Batch  81  loss:  0.006541446316987276
Batch  91  loss:  0.006710713729262352
Validation on real data: 
LOSS supervised-train 0.006594033055007458, valid 0.0065650176256895065
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.006538198329508305
Batch  11  loss:  0.006483547389507294
Batch  21  loss:  0.006422168575227261
Batch  31  loss:  0.006666188593953848
Batch  41  loss:  0.0067872353829443455
Batch  51  loss:  0.00661276513710618
Batch  61  loss:  0.006650386843830347
Batch  71  loss:  0.006751141510903835
Batch  81  loss:  0.006577176507562399
Batch  91  loss:  0.00659514032304287
Validation on real data: 
LOSS supervised-train 0.006582917463965714, valid 0.006588093936443329
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.006735614966601133
Batch  11  loss:  0.006468254141509533
Batch  21  loss:  0.006393569987267256
Batch  31  loss:  0.006567896343767643
Batch  41  loss:  0.006805801298469305
Batch  51  loss:  0.0066039469093084335
Batch  61  loss:  0.006824295502156019
Batch  71  loss:  0.006810068152844906
Batch  81  loss:  0.00660172663629055
Batch  91  loss:  0.006756568793207407
Validation on real data: 
LOSS supervised-train 0.006592700509354472, valid 0.006461466196924448
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0065500508062541485
Batch  11  loss:  0.006441933568567038
Batch  21  loss:  0.0064145722426474094
Batch  31  loss:  0.006692233495414257
Batch  41  loss:  0.006698355544358492
Batch  51  loss:  0.00655062822625041
Batch  61  loss:  0.006604986730962992
Batch  71  loss:  0.006702786311507225
Batch  81  loss:  0.006537403911352158
Batch  91  loss:  0.006762183737009764
Validation on real data: 
LOSS supervised-train 0.006587779265828431, valid 0.006461627781391144
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.006679060403257608
Batch  11  loss:  0.006529038306325674
Batch  21  loss:  0.006490368861705065
Batch  31  loss:  0.006616473663598299
Batch  41  loss:  0.006674765609204769
Batch  51  loss:  0.006632095202803612
Batch  61  loss:  0.006716499105095863
Batch  71  loss:  0.006795451510697603
Batch  81  loss:  0.006691389717161655
Batch  91  loss:  0.00665197754278779
Validation on real data: 
LOSS supervised-train 0.006574419983662665, valid 0.006671013776212931
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.006717226933687925
Batch  11  loss:  0.006559675093740225
Batch  21  loss:  0.006408534944057465
Batch  31  loss:  0.006671652663499117
Batch  41  loss:  0.006687551736831665
Batch  51  loss:  0.0066585848107934
Batch  61  loss:  0.006673840340226889
Batch  71  loss:  0.006738409865647554
Batch  81  loss:  0.0065945968963205814
Batch  91  loss:  0.0066823516972362995
Validation on real data: 
LOSS supervised-train 0.006596681158989668, valid 0.006783023942261934
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.006724477279931307
Batch  11  loss:  0.006615558639168739
Batch  21  loss:  0.006345746573060751
Batch  31  loss:  0.006599372252821922
Batch  41  loss:  0.0066531263291835785
Batch  51  loss:  0.006642489228397608
Batch  61  loss:  0.006575599778443575
Batch  71  loss:  0.006639042403548956
Batch  81  loss:  0.006557995453476906
Batch  91  loss:  0.006745295599102974
Validation on real data: 
LOSS supervised-train 0.006569380494765937, valid 0.006477712653577328
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.006573695223778486
Batch  11  loss:  0.006440313998609781
Batch  21  loss:  0.006337704602628946
Batch  31  loss:  0.006733180023729801
Batch  41  loss:  0.006725499872118235
Batch  51  loss:  0.006718716584146023
Batch  61  loss:  0.00658559612929821
Batch  71  loss:  0.006692342460155487
Batch  81  loss:  0.006563802249729633
Batch  91  loss:  0.0066873542964458466
Validation on real data: 
LOSS supervised-train 0.006583258993923664, valid 0.0064339302480220795
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.006632077042013407
Batch  11  loss:  0.006465343292802572
Batch  21  loss:  0.006455086171627045
Batch  31  loss:  0.006653486751019955
Batch  41  loss:  0.006704835686832666
Batch  51  loss:  0.0065825628116726875
Batch  61  loss:  0.006641005165874958
Batch  71  loss:  0.006942054256796837
Batch  81  loss:  0.006579159293323755
Batch  91  loss:  0.006745155900716782
Validation on real data: 
LOSS supervised-train 0.0065742793073877695, valid 0.006531613878905773
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  cap ; Model ID: 3dec0d851cba045fbf444790f25ea3db
--------------------
Training baseline regression model:  2022-03-30 14:25:32.014400
Detector:  pointnet
Object:  cap
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1608475
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.2091745287179947
Batch  11  loss:  0.11728931963443756
Batch  21  loss:  0.11975659430027008
Batch  31  loss:  0.06360673159360886
Batch  41  loss:  0.06074278801679611
Batch  51  loss:  0.04923667386174202
Batch  61  loss:  0.04047662764787674
Batch  71  loss:  0.02753715217113495
Batch  81  loss:  0.02770787663757801
Batch  91  loss:  0.02050967700779438
Validation on real data: 
LOSS supervised-train 0.0602190694026649, valid 0.01538015902042389
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.016139183193445206
Batch  11  loss:  0.018748583272099495
Batch  21  loss:  0.014520947821438313
Batch  31  loss:  0.017133617773652077
Batch  41  loss:  0.01199028454720974
Batch  51  loss:  0.02203395962715149
Batch  61  loss:  0.012855928391218185
Batch  71  loss:  0.013413218781352043
Batch  81  loss:  0.017145860940217972
Batch  91  loss:  0.011505944654345512
Validation on real data: 
LOSS supervised-train 0.015479079224169254, valid 0.005474216770380735
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.009938538074493408
Batch  11  loss:  0.010637912899255753
Batch  21  loss:  0.013581447303295135
Batch  31  loss:  0.00882946327328682
Batch  41  loss:  0.008471344597637653
Batch  51  loss:  0.016314832493662834
Batch  61  loss:  0.010162836872041225
Batch  71  loss:  0.008362951688468456
Batch  81  loss:  0.017119091004133224
Batch  91  loss:  0.008139039389789104
Validation on real data: 
LOSS supervised-train 0.010608961754478514, valid 0.004922854248434305
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.007706036325544119
Batch  11  loss:  0.006125340703874826
Batch  21  loss:  0.010584108531475067
Batch  31  loss:  0.007469211705029011
Batch  41  loss:  0.006985957268625498
Batch  51  loss:  0.014350952580571175
Batch  61  loss:  0.007747436407953501
Batch  71  loss:  0.006738761905580759
Batch  81  loss:  0.012023450806736946
Batch  91  loss:  0.007493192795664072
Validation on real data: 
LOSS supervised-train 0.0088845839118585, valid 0.0029531281907111406
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.007065386511385441
Batch  11  loss:  0.004710559267550707
Batch  21  loss:  0.011184358038008213
Batch  31  loss:  0.007690207567065954
Batch  41  loss:  0.008753103204071522
Batch  51  loss:  0.010729610919952393
Batch  61  loss:  0.007865889929234982
Batch  71  loss:  0.006844036281108856
Batch  81  loss:  0.00957144983112812
Batch  91  loss:  0.006434366572648287
Validation on real data: 
LOSS supervised-train 0.007822708571329712, valid 0.002134433714672923
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.005879443604499102
Batch  11  loss:  0.00662353727966547
Batch  21  loss:  0.008511900901794434
Batch  31  loss:  0.006998362485319376
Batch  41  loss:  0.007129260338842869
Batch  51  loss:  0.010953186079859734
Batch  61  loss:  0.009352747350931168
Batch  71  loss:  0.0050085303373634815
Batch  81  loss:  0.011333838105201721
Batch  91  loss:  0.005249290261417627
Validation on real data: 
LOSS supervised-train 0.007052607438527048, valid 0.0022549524437636137
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.007022038102149963
Batch  11  loss:  0.006008952856063843
Batch  21  loss:  0.009269573725759983
Batch  31  loss:  0.006588778458535671
Batch  41  loss:  0.007764006499201059
Batch  51  loss:  0.009146900847554207
Batch  61  loss:  0.007061431184411049
Batch  71  loss:  0.004303173162043095
Batch  81  loss:  0.009081360884010792
Batch  91  loss:  0.005844664294272661
Validation on real data: 
LOSS supervised-train 0.006673940639011562, valid 0.0021433094516396523
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.00534266559407115
Batch  11  loss:  0.004599219653755426
Batch  21  loss:  0.008837803266942501
Batch  31  loss:  0.004675595555454493
Batch  41  loss:  0.007125032600015402
Batch  51  loss:  0.00995672307908535
Batch  61  loss:  0.007478818763047457
Batch  71  loss:  0.004564729519188404
Batch  81  loss:  0.01075432077050209
Batch  91  loss:  0.00527262594550848
Validation on real data: 
LOSS supervised-train 0.0060419883439317345, valid 0.001486323308199644
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0052939532324671745
Batch  11  loss:  0.005614285357296467
Batch  21  loss:  0.006894620601087809
Batch  31  loss:  0.00425707595422864
Batch  41  loss:  0.00593140535056591
Batch  51  loss:  0.009366527199745178
Batch  61  loss:  0.005977142136543989
Batch  71  loss:  0.005126128904521465
Batch  81  loss:  0.010404123924672604
Batch  91  loss:  0.00406733388081193
Validation on real data: 
LOSS supervised-train 0.005822325630579144, valid 0.0017594817327335477
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.004233409184962511
Batch  11  loss:  0.0035583495628088713
Batch  21  loss:  0.006745693739503622
Batch  31  loss:  0.004623087123036385
Batch  41  loss:  0.0051390305161476135
Batch  51  loss:  0.006728181149810553
Batch  61  loss:  0.005659893155097961
Batch  71  loss:  0.0034552323631942272
Batch  81  loss:  0.010212772525846958
Batch  91  loss:  0.004155059810727835
Validation on real data: 
LOSS supervised-train 0.005465712146833539, valid 0.0015331604517996311
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0039020038675516844
Batch  11  loss:  0.003841554978862405
Batch  21  loss:  0.006797668058425188
Batch  31  loss:  0.004236103035509586
Batch  41  loss:  0.005133574362844229
Batch  51  loss:  0.008000167086720467
Batch  61  loss:  0.0054726931266486645
Batch  71  loss:  0.004997594747692347
Batch  81  loss:  0.009695334360003471
Batch  91  loss:  0.00491167139261961
Validation on real data: 
LOSS supervised-train 0.0053097137925215065, valid 0.0018178621539846063
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.004625989124178886
Batch  11  loss:  0.0029521179385483265
Batch  21  loss:  0.008683319203555584
Batch  31  loss:  0.003962076269090176
Batch  41  loss:  0.007276132702827454
Batch  51  loss:  0.008199954405426979
Batch  61  loss:  0.006760819815099239
Batch  71  loss:  0.004278257489204407
Batch  81  loss:  0.006210465915501118
Batch  91  loss:  0.004321066662669182
Validation on real data: 
LOSS supervised-train 0.005446178060956299, valid 0.0018021988216787577
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.004041752777993679
Batch  11  loss:  0.003786452580243349
Batch  21  loss:  0.008972415700554848
Batch  31  loss:  0.004840576555579901
Batch  41  loss:  0.005804155953228474
Batch  51  loss:  0.0063978442922234535
Batch  61  loss:  0.0052095926366746426
Batch  71  loss:  0.005802163388580084
Batch  81  loss:  0.007233722601085901
Batch  91  loss:  0.005256981123238802
Validation on real data: 
LOSS supervised-train 0.005065206305589527, valid 0.001157338498160243
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.004086588975042105
Batch  11  loss:  0.0028899163007736206
Batch  21  loss:  0.005410373210906982
Batch  31  loss:  0.0034084366634488106
Batch  41  loss:  0.004985649138689041
Batch  51  loss:  0.008120589889585972
Batch  61  loss:  0.006175479851663113
Batch  71  loss:  0.004530405625700951
Batch  81  loss:  0.0067322514951229095
Batch  91  loss:  0.003507169894874096
Validation on real data: 
LOSS supervised-train 0.005051246283110231, valid 0.0015474100364372134
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.00317959301173687
Batch  11  loss:  0.00290870713070035
Batch  21  loss:  0.006236204877495766
Batch  31  loss:  0.003279950236901641
Batch  41  loss:  0.006388561800122261
Batch  51  loss:  0.006825583055615425
Batch  61  loss:  0.006354873068630695
Batch  71  loss:  0.0031050960533320904
Batch  81  loss:  0.00764229753986001
Batch  91  loss:  0.003337205620482564
Validation on real data: 
LOSS supervised-train 0.004896026132628322, valid 0.0017922049155458808
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.004940181504935026
Batch  11  loss:  0.0032837921753525734
Batch  21  loss:  0.006189321167767048
Batch  31  loss:  0.00386691908352077
Batch  41  loss:  0.005228768568485975
Batch  51  loss:  0.008768976666033268
Batch  61  loss:  0.00441873911768198
Batch  71  loss:  0.0034630619920790195
Batch  81  loss:  0.007463162764906883
Batch  91  loss:  0.003678079228848219
Validation on real data: 
LOSS supervised-train 0.004638423172291368, valid 0.0020289176609367132
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.004217360634356737
Batch  11  loss:  0.0034411665983498096
Batch  21  loss:  0.0057554892264306545
Batch  31  loss:  0.0036306490655988455
Batch  41  loss:  0.0033594786655157804
Batch  51  loss:  0.006406895816326141
Batch  61  loss:  0.0044309538789093494
Batch  71  loss:  0.0034687768202275038
Batch  81  loss:  0.0064154742285609245
Batch  91  loss:  0.004018514882773161
Validation on real data: 
LOSS supervised-train 0.0044844776019454, valid 0.0014569706982001662
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0029028826393187046
Batch  11  loss:  0.004668374080210924
Batch  21  loss:  0.006402110680937767
Batch  31  loss:  0.004703029524534941
Batch  41  loss:  0.004902814049273729
Batch  51  loss:  0.007662312593311071
Batch  61  loss:  0.006998538970947266
Batch  71  loss:  0.0033393369521945715
Batch  81  loss:  0.007208834867924452
Batch  91  loss:  0.0052732801996171474
Validation on real data: 
LOSS supervised-train 0.0045131856226362285, valid 0.0013148344587534666
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.003970661200582981
Batch  11  loss:  0.003346099518239498
Batch  21  loss:  0.004613872151821852
Batch  31  loss:  0.004120966885238886
Batch  41  loss:  0.0037532958667725325
Batch  51  loss:  0.007071086671203375
Batch  61  loss:  0.004669547080993652
Batch  71  loss:  0.003997697029262781
Batch  81  loss:  0.006049552001059055
Batch  91  loss:  0.0036187099758535624
Validation on real data: 
LOSS supervised-train 0.004410181180574, valid 0.0013019624166190624
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.004306322894990444
Batch  11  loss:  0.0026022603269666433
Batch  21  loss:  0.0044504436664283276
Batch  31  loss:  0.003450510324910283
Batch  41  loss:  0.004011469893157482
Batch  51  loss:  0.005411054007709026
Batch  61  loss:  0.00548434117808938
Batch  71  loss:  0.0030333767645061016
Batch  81  loss:  0.007278412580490112
Batch  91  loss:  0.0034965628292411566
Validation on real data: 
LOSS supervised-train 0.0041534924576990305, valid 0.0014516442315652966
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0040693990886211395
Batch  11  loss:  0.0036173455882817507
Batch  21  loss:  0.004441082943230867
Batch  31  loss:  0.0028047834057360888
Batch  41  loss:  0.005049906671047211
Batch  51  loss:  0.005664357915520668
Batch  61  loss:  0.004397268872708082
Batch  71  loss:  0.0033687762916088104
Batch  81  loss:  0.007532321847975254
Batch  91  loss:  0.0033329299185425043
Validation on real data: 
LOSS supervised-train 0.004101941953413188, valid 0.0011656899005174637
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.003640069393441081
Batch  11  loss:  0.0023452325258404016
Batch  21  loss:  0.0030235457234084606
Batch  31  loss:  0.003099107416346669
Batch  41  loss:  0.004555300809442997
Batch  51  loss:  0.005852525122463703
Batch  61  loss:  0.005289780907332897
Batch  71  loss:  0.0025926989037543535
Batch  81  loss:  0.005833481438457966
Batch  91  loss:  0.004251136910170317
Validation on real data: 
LOSS supervised-train 0.004048815197311342, valid 0.001424922957085073
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.002828400582075119
Batch  11  loss:  0.002902879612520337
Batch  21  loss:  0.004466542508453131
Batch  31  loss:  0.002929277718067169
Batch  41  loss:  0.004142065066844225
Batch  51  loss:  0.0056807855144143105
Batch  61  loss:  0.003760686842724681
Batch  71  loss:  0.003393744118511677
Batch  81  loss:  0.006575974635779858
Batch  91  loss:  0.003408099291846156
Validation on real data: 
LOSS supervised-train 0.003964529733639211, valid 0.0012335141655057669
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.004941015969961882
Batch  11  loss:  0.0034929977264255285
Batch  21  loss:  0.0034459768794476986
Batch  31  loss:  0.0022261939011514187
Batch  41  loss:  0.003676776308566332
Batch  51  loss:  0.005580488126724958
Batch  61  loss:  0.0046142591163516045
Batch  71  loss:  0.002769928891211748
Batch  81  loss:  0.008407747372984886
Batch  91  loss:  0.004487151280045509
Validation on real data: 
LOSS supervised-train 0.0039005985530093314, valid 0.0012241103686392307
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.003438665997236967
Batch  11  loss:  0.003486980451270938
Batch  21  loss:  0.004061307292431593
Batch  31  loss:  0.0030282309744507074
Batch  41  loss:  0.003012874396517873
Batch  51  loss:  0.006821617484092712
Batch  61  loss:  0.005063605960458517
Batch  71  loss:  0.0030932396184653044
Batch  81  loss:  0.005332814529538155
Batch  91  loss:  0.0034053060226142406
Validation on real data: 
LOSS supervised-train 0.0038451764336787163, valid 0.001741780317388475
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0040922826156020164
Batch  11  loss:  0.0027675526216626167
Batch  21  loss:  0.004221161361783743
Batch  31  loss:  0.0025765507016330957
Batch  41  loss:  0.0035080304369330406
Batch  51  loss:  0.005189645104110241
Batch  61  loss:  0.00440948037430644
Batch  71  loss:  0.002870112657546997
Batch  81  loss:  0.006749965250492096
Batch  91  loss:  0.0029552846681326628
Validation on real data: 
LOSS supervised-train 0.0037152660195715727, valid 0.0013905748492106795
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.003482202300801873
Batch  11  loss:  0.002420346951112151
Batch  21  loss:  0.003766737412661314
Batch  31  loss:  0.0030861203558743
Batch  41  loss:  0.005386621691286564
Batch  51  loss:  0.005184618290513754
Batch  61  loss:  0.004653917625546455
Batch  71  loss:  0.002593105426058173
Batch  81  loss:  0.005603779572993517
Batch  91  loss:  0.00278655462898314
Validation on real data: 
LOSS supervised-train 0.003669533308129758, valid 0.0017293260898441076
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.004734830930829048
Batch  11  loss:  0.002468074904754758
Batch  21  loss:  0.005070290993899107
Batch  31  loss:  0.0024978446308523417
Batch  41  loss:  0.004637828096747398
Batch  51  loss:  0.005146929994225502
Batch  61  loss:  0.004714449401944876
Batch  71  loss:  0.0026597215328365564
Batch  81  loss:  0.004218182060867548
Batch  91  loss:  0.003233651164919138
Validation on real data: 
LOSS supervised-train 0.0035858027427457272, valid 0.0015416230307891965
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0044355690479278564
Batch  11  loss:  0.002068811794742942
Batch  21  loss:  0.0034780323039740324
Batch  31  loss:  0.0030477046966552734
Batch  41  loss:  0.004491567611694336
Batch  51  loss:  0.004941470455378294
Batch  61  loss:  0.0036834399215877056
Batch  71  loss:  0.0029087592847645283
Batch  81  loss:  0.005387136712670326
Batch  91  loss:  0.0025098794139921665
Validation on real data: 
LOSS supervised-train 0.0035657401033677163, valid 0.0013508863048627973
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0038138132076710463
Batch  11  loss:  0.0023222099989652634
Batch  21  loss:  0.002833098405972123
Batch  31  loss:  0.0024836575612425804
Batch  41  loss:  0.0029986558947712183
Batch  51  loss:  0.005159868858754635
Batch  61  loss:  0.003328918246552348
Batch  71  loss:  0.00403501046821475
Batch  81  loss:  0.005297715775668621
Batch  91  loss:  0.0024496889673173428
Validation on real data: 
LOSS supervised-train 0.0033538128179498015, valid 0.0012389573967084289
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.003527312772348523
Batch  11  loss:  0.0029084226116538048
Batch  21  loss:  0.003458678023889661
Batch  31  loss:  0.0025355324614793062
Batch  41  loss:  0.004902584478259087
Batch  51  loss:  0.004886511247605085
Batch  61  loss:  0.003337165340781212
Batch  71  loss:  0.003288209903985262
Batch  81  loss:  0.005908194463700056
Batch  91  loss:  0.002529806224629283
Validation on real data: 
LOSS supervised-train 0.003399946467252448, valid 0.001243449398316443
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.004134331364184618
Batch  11  loss:  0.002039205050095916
Batch  21  loss:  0.003676285268738866
Batch  31  loss:  0.002967681735754013
Batch  41  loss:  0.00396649818867445
Batch  51  loss:  0.005799698643386364
Batch  61  loss:  0.0034055134747177362
Batch  71  loss:  0.003260999917984009
Batch  81  loss:  0.005980820395052433
Batch  91  loss:  0.004223020747303963
Validation on real data: 
LOSS supervised-train 0.0034186246362514793, valid 0.001183892018161714
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0036435299552977085
Batch  11  loss:  0.002391027519479394
Batch  21  loss:  0.003996883984655142
Batch  31  loss:  0.0022211954928934574
Batch  41  loss:  0.0022235752549022436
Batch  51  loss:  0.005973055958747864
Batch  61  loss:  0.0027007018215954304
Batch  71  loss:  0.0032164149452000856
Batch  81  loss:  0.0048968857154250145
Batch  91  loss:  0.0024112567771226168
Validation on real data: 
LOSS supervised-train 0.003214267735602334, valid 0.0016841974575072527
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0024956457782536745
Batch  11  loss:  0.002072905655950308
Batch  21  loss:  0.003001654054969549
Batch  31  loss:  0.002768835285678506
Batch  41  loss:  0.002737795701250434
Batch  51  loss:  0.004469776526093483
Batch  61  loss:  0.0045046377927064896
Batch  71  loss:  0.0030306673143059015
Batch  81  loss:  0.005108006298542023
Batch  91  loss:  0.0032430009450763464
Validation on real data: 
LOSS supervised-train 0.0032295205269474536, valid 0.0012716880301013589
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.002629444934427738
Batch  11  loss:  0.002500095870345831
Batch  21  loss:  0.0036114752292633057
Batch  31  loss:  0.0038012948352843523
Batch  41  loss:  0.003968787845224142
Batch  51  loss:  0.00525185652077198
Batch  61  loss:  0.0035889819264411926
Batch  71  loss:  0.003518406767398119
Batch  81  loss:  0.005964826326817274
Batch  91  loss:  0.002621117513626814
Validation on real data: 
LOSS supervised-train 0.003305158307775855, valid 0.0011585524771362543
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.004076420795172453
Batch  11  loss:  0.0017634483519941568
Batch  21  loss:  0.0030678058974444866
Batch  31  loss:  0.002353097777813673
Batch  41  loss:  0.003146001137793064
Batch  51  loss:  0.004098470788449049
Batch  61  loss:  0.0038322191685438156
Batch  71  loss:  0.0024701873771846294
Batch  81  loss:  0.0043596732430160046
Batch  91  loss:  0.0030540761072188616
Validation on real data: 
LOSS supervised-train 0.0031402449135202915, valid 0.0013761956943199039
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0035317742731422186
Batch  11  loss:  0.0024323072284460068
Batch  21  loss:  0.0031450996175408363
Batch  31  loss:  0.0026903811376541853
Batch  41  loss:  0.0034409675281494856
Batch  51  loss:  0.004174502566456795
Batch  61  loss:  0.002703332807868719
Batch  71  loss:  0.0029450058937072754
Batch  81  loss:  0.006088040303438902
Batch  91  loss:  0.0024889777414500713
Validation on real data: 
LOSS supervised-train 0.0030452781810890884, valid 0.0012950939126312733
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.002608169801533222
Batch  11  loss:  0.00232391688041389
Batch  21  loss:  0.0029912730678915977
Batch  31  loss:  0.003643721342086792
Batch  41  loss:  0.0025934537407010794
Batch  51  loss:  0.0042002201080322266
Batch  61  loss:  0.0032932076137512922
Batch  71  loss:  0.0021749285515397787
Batch  81  loss:  0.00404565641656518
Batch  91  loss:  0.0023163380101323128
Validation on real data: 
LOSS supervised-train 0.003018837639829144, valid 0.0012946885544806719
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0038422048091888428
Batch  11  loss:  0.0017665611812844872
Batch  21  loss:  0.0033391229808330536
Batch  31  loss:  0.001837815623730421
Batch  41  loss:  0.0031715473160147667
Batch  51  loss:  0.004829113371670246
Batch  61  loss:  0.002983659505844116
Batch  71  loss:  0.002916579134762287
Batch  81  loss:  0.0064154621213674545
Batch  91  loss:  0.0034257268998771906
Validation on real data: 
LOSS supervised-train 0.003036931356182322, valid 0.001152922515757382
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0021886243484914303
Batch  11  loss:  0.002541788388043642
Batch  21  loss:  0.0020462197717279196
Batch  31  loss:  0.0029188799671828747
Batch  41  loss:  0.0027700720820575953
Batch  51  loss:  0.0036874671932309866
Batch  61  loss:  0.0030017411336302757
Batch  71  loss:  0.0019693016074597836
Batch  81  loss:  0.003348124213516712
Batch  91  loss:  0.002059265971183777
Validation on real data: 
LOSS supervised-train 0.002895540768513456, valid 0.0015378139214590192
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.003368747653439641
Batch  11  loss:  0.002013622783124447
Batch  21  loss:  0.003007057821378112
Batch  31  loss:  0.0021352185867726803
Batch  41  loss:  0.0024977049324661493
Batch  51  loss:  0.00484223198145628
Batch  61  loss:  0.002328435890376568
Batch  71  loss:  0.002482411451637745
Batch  81  loss:  0.004557934124022722
Batch  91  loss:  0.0019395719282329082
Validation on real data: 
LOSS supervised-train 0.002869038216304034, valid 0.001185150700621307
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0024173124693334103
Batch  11  loss:  0.0018429348710924387
Batch  21  loss:  0.0024483371526002884
Batch  31  loss:  0.002212911145761609
Batch  41  loss:  0.0037356780376285315
Batch  51  loss:  0.0029534962959587574
Batch  61  loss:  0.003030132269486785
Batch  71  loss:  0.002826934913173318
Batch  81  loss:  0.004963156767189503
Batch  91  loss:  0.002560650696977973
Validation on real data: 
LOSS supervised-train 0.0028534388798289, valid 0.0014092990895733237
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.002697916002944112
Batch  11  loss:  0.002495751716196537
Batch  21  loss:  0.0023211371153593063
Batch  31  loss:  0.0021230168640613556
Batch  41  loss:  0.002904631430283189
Batch  51  loss:  0.004001768305897713
Batch  61  loss:  0.002600764622911811
Batch  71  loss:  0.00268705771304667
Batch  81  loss:  0.005110634490847588
Batch  91  loss:  0.002740368479862809
Validation on real data: 
LOSS supervised-train 0.0027830839017406106, valid 0.0010221354896202683
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.002075263997539878
Batch  11  loss:  0.002213497646152973
Batch  21  loss:  0.003612268716096878
Batch  31  loss:  0.0034809743519872427
Batch  41  loss:  0.002910659881308675
Batch  51  loss:  0.003928859252482653
Batch  61  loss:  0.003480500541627407
Batch  71  loss:  0.003510372946038842
Batch  81  loss:  0.0042914715595543385
Batch  91  loss:  0.0026883739046752453
Validation on real data: 
LOSS supervised-train 0.002838582267286256, valid 0.001210611080750823
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0018936797278001904
Batch  11  loss:  0.001905394485220313
Batch  21  loss:  0.002485329285264015
Batch  31  loss:  0.0017376485047861934
Batch  41  loss:  0.0032044637482613325
Batch  51  loss:  0.003468239912763238
Batch  61  loss:  0.0026996503584086895
Batch  71  loss:  0.003372662700712681
Batch  81  loss:  0.0037450208328664303
Batch  91  loss:  0.002112740185111761
Validation on real data: 
LOSS supervised-train 0.0026978848851285874, valid 0.0012351891491562128
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0021364528220146894
Batch  11  loss:  0.0019462298369035125
Batch  21  loss:  0.002101077465340495
Batch  31  loss:  0.0023925171699374914
Batch  41  loss:  0.0031952178105711937
Batch  51  loss:  0.0036208515521138906
Batch  61  loss:  0.0036944770254194736
Batch  71  loss:  0.0030628221575170755
Batch  81  loss:  0.004390969406813383
Batch  91  loss:  0.002673068083822727
Validation on real data: 
LOSS supervised-train 0.002688668657792732, valid 0.001172824064269662
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0022509333211928606
Batch  11  loss:  0.0022125013638287783
Batch  21  loss:  0.0019361829617992043
Batch  31  loss:  0.0022229603491723537
Batch  41  loss:  0.002533016260713339
Batch  51  loss:  0.0028426230419427156
Batch  61  loss:  0.0025423418264836073
Batch  71  loss:  0.003047160804271698
Batch  81  loss:  0.004252357874065638
Batch  91  loss:  0.0031054173596203327
Validation on real data: 
LOSS supervised-train 0.00262947405455634, valid 0.0011944714933633804
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0036708619445562363
Batch  11  loss:  0.0018197305034846067
Batch  21  loss:  0.002744365483522415
Batch  31  loss:  0.0021655901800841093
Batch  41  loss:  0.0035385601222515106
Batch  51  loss:  0.003582098986953497
Batch  61  loss:  0.0030998496804386377
Batch  71  loss:  0.0026440040674060583
Batch  81  loss:  0.0038925879634916782
Batch  91  loss:  0.002399925608187914
Validation on real data: 
LOSS supervised-train 0.002697881953790784, valid 0.0013710653875023127
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0020463557448238134
Batch  11  loss:  0.001931175822392106
Batch  21  loss:  0.0017754733562469482
Batch  31  loss:  0.0026117663364857435
Batch  41  loss:  0.002837992273271084
Batch  51  loss:  0.0036578583531081676
Batch  61  loss:  0.0032050006557255983
Batch  71  loss:  0.001843760022893548
Batch  81  loss:  0.004963792860507965
Batch  91  loss:  0.001715265796519816
Validation on real data: 
LOSS supervised-train 0.002529750819085166, valid 0.0011028657900169492
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0016458603786304593
Batch  11  loss:  0.002337105106562376
Batch  21  loss:  0.002524701412767172
Batch  31  loss:  0.002401531208306551
Batch  41  loss:  0.0025503020733594894
Batch  51  loss:  0.0030628973618149757
Batch  61  loss:  0.003340217750519514
Batch  71  loss:  0.0025710249319672585
Batch  81  loss:  0.004657499026507139
Batch  91  loss:  0.002552849007770419
Validation on real data: 
LOSS supervised-train 0.0025986905884929, valid 0.0013893285067752004
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0037226087879389524
Batch  11  loss:  0.001710087526589632
Batch  21  loss:  0.002311516087502241
Batch  31  loss:  0.002013863530009985
Batch  41  loss:  0.0035669845528900623
Batch  51  loss:  0.0032298408914357424
Batch  61  loss:  0.0033393693156540394
Batch  71  loss:  0.0024521106388419867
Batch  81  loss:  0.004362222272902727
Batch  91  loss:  0.00288134953007102
Validation on real data: 
LOSS supervised-train 0.002559148872969672, valid 0.001258199685253203
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.002117358846589923
Batch  11  loss:  0.0019987672567367554
Batch  21  loss:  0.002594785066321492
Batch  31  loss:  0.002306319074705243
Batch  41  loss:  0.0028525376692414284
Batch  51  loss:  0.0026809561531990767
Batch  61  loss:  0.002454339759424329
Batch  71  loss:  0.0019229130120947957
Batch  81  loss:  0.003700470319017768
Batch  91  loss:  0.0019545319955796003
Validation on real data: 
LOSS supervised-train 0.0025018737686332317, valid 0.0010939367348328233
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.003292578039690852
Batch  11  loss:  0.0018132810946553946
Batch  21  loss:  0.002426195191219449
Batch  31  loss:  0.0017805317183956504
Batch  41  loss:  0.0025382637977600098
Batch  51  loss:  0.0032103871926665306
Batch  61  loss:  0.003144631627947092
Batch  71  loss:  0.0018199466867372394
Batch  81  loss:  0.003807542845606804
Batch  91  loss:  0.0038512866012752056
Validation on real data: 
LOSS supervised-train 0.002491345707094297, valid 0.0014228960499167442
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.002183096017688513
Batch  11  loss:  0.0021585263311862946
Batch  21  loss:  0.0020659631118178368
Batch  31  loss:  0.0018813785864040256
Batch  41  loss:  0.0027906286995857954
Batch  51  loss:  0.0031866198405623436
Batch  61  loss:  0.0026308202650398016
Batch  71  loss:  0.0018031054642051458
Batch  81  loss:  0.0033618120942264795
Batch  91  loss:  0.0026925322599709034
Validation on real data: 
LOSS supervised-train 0.002430649154121056, valid 0.0014972425997257233
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.002144017955288291
Batch  11  loss:  0.0021420272532850504
Batch  21  loss:  0.0034161543007940054
Batch  31  loss:  0.00208865269087255
Batch  41  loss:  0.0026069949381053448
Batch  51  loss:  0.0037583443336188793
Batch  61  loss:  0.0030974613036960363
Batch  71  loss:  0.0017808484844863415
Batch  81  loss:  0.004331653006374836
Batch  91  loss:  0.002400712575763464
Validation on real data: 
LOSS supervised-train 0.002563176430994645, valid 0.0012434254167601466
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.002223397372290492
Batch  11  loss:  0.0023687495850026608
Batch  21  loss:  0.0022866788785904646
Batch  31  loss:  0.0024450281634926796
Batch  41  loss:  0.00222682673484087
Batch  51  loss:  0.002399511868134141
Batch  61  loss:  0.0033692275173962116
Batch  71  loss:  0.003391987644135952
Batch  81  loss:  0.003723523346707225
Batch  91  loss:  0.002870558761060238
Validation on real data: 
LOSS supervised-train 0.00245373340905644, valid 0.0011328990804031491
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0021606029476970434
Batch  11  loss:  0.002089105313643813
Batch  21  loss:  0.0019033584976568818
Batch  31  loss:  0.00238044117577374
Batch  41  loss:  0.0020286557264626026
Batch  51  loss:  0.004133838694542646
Batch  61  loss:  0.003209671238437295
Batch  71  loss:  0.002266218885779381
Batch  81  loss:  0.003523425664752722
Batch  91  loss:  0.0028002371545881033
Validation on real data: 
LOSS supervised-train 0.0024459165229927748, valid 0.001720559666864574
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0025993790477514267
Batch  11  loss:  0.0019523128867149353
Batch  21  loss:  0.002413660753518343
Batch  31  loss:  0.0014962799614295363
Batch  41  loss:  0.0019066166132688522
Batch  51  loss:  0.0025397497229278088
Batch  61  loss:  0.002408791333436966
Batch  71  loss:  0.0018635623855516315
Batch  81  loss:  0.0039553153328597546
Batch  91  loss:  0.002145325532183051
Validation on real data: 
LOSS supervised-train 0.0023353642283473162, valid 0.0013485495001077652
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0019592621829360723
Batch  11  loss:  0.0022912987042218447
Batch  21  loss:  0.001744234235957265
Batch  31  loss:  0.0021334029734134674
Batch  41  loss:  0.002381549682468176
Batch  51  loss:  0.002819967223331332
Batch  61  loss:  0.0023908582516014576
Batch  71  loss:  0.0019930729176849127
Batch  81  loss:  0.0030077113769948483
Batch  91  loss:  0.0028173148166388273
Validation on real data: 
LOSS supervised-train 0.00236001918441616, valid 0.0011965566081926227
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0021336073987185955
Batch  11  loss:  0.001832104753702879
Batch  21  loss:  0.002890826901420951
Batch  31  loss:  0.0016169934533536434
Batch  41  loss:  0.002519938861951232
Batch  51  loss:  0.003083097282797098
Batch  61  loss:  0.0031583840027451515
Batch  71  loss:  0.001934477942995727
Batch  81  loss:  0.0044372063130140305
Batch  91  loss:  0.0022404969204217196
Validation on real data: 
LOSS supervised-train 0.002403036643518135, valid 0.00132694689091295
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.002473714528605342
Batch  11  loss:  0.002104793442413211
Batch  21  loss:  0.0024437480606138706
Batch  31  loss:  0.0020297167357057333
Batch  41  loss:  0.0029130748007446527
Batch  51  loss:  0.0029270732775330544
Batch  61  loss:  0.0028383529279381037
Batch  71  loss:  0.002168050967156887
Batch  81  loss:  0.0035709368530660868
Batch  91  loss:  0.002039711456745863
Validation on real data: 
LOSS supervised-train 0.0022879796754568817, valid 0.0010229548206552863
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0018720984226092696
Batch  11  loss:  0.0019565841648727655
Batch  21  loss:  0.002196993911638856
Batch  31  loss:  0.0021096535492688417
Batch  41  loss:  0.0023104590363800526
Batch  51  loss:  0.002868734300136566
Batch  61  loss:  0.001631403574720025
Batch  71  loss:  0.006208330392837524
Batch  81  loss:  0.002691583475098014
Batch  91  loss:  0.0022913403809070587
Validation on real data: 
LOSS supervised-train 0.0022806156578008083, valid 0.0013706105528399348
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0023372124414891005
Batch  11  loss:  0.001783053856343031
Batch  21  loss:  0.0020379135385155678
Batch  31  loss:  0.0022561128716915846
Batch  41  loss:  0.00244381302036345
Batch  51  loss:  0.004230237565934658
Batch  61  loss:  0.0027517955750226974
Batch  71  loss:  0.0018947386415675282
Batch  81  loss:  0.003765555564314127
Batch  91  loss:  0.002624302636831999
Validation on real data: 
LOSS supervised-train 0.0023168648418504744, valid 0.0017031104071065784
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.002701549557968974
Batch  11  loss:  0.0024436423555016518
Batch  21  loss:  0.002269004937261343
Batch  31  loss:  0.0018138452433049679
Batch  41  loss:  0.002271353965625167
Batch  51  loss:  0.0032351436093449593
Batch  61  loss:  0.003312370041385293
Batch  71  loss:  0.001854910864494741
Batch  81  loss:  0.0029997124802321196
Batch  91  loss:  0.002085197251290083
Validation on real data: 
LOSS supervised-train 0.0022046592400874945, valid 0.0020686627831310034
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0028118195477873087
Batch  11  loss:  0.002637188881635666
Batch  21  loss:  0.0015906330663710833
Batch  31  loss:  0.0022214502096176147
Batch  41  loss:  0.0033957897685468197
Batch  51  loss:  0.002922894898802042
Batch  61  loss:  0.0021708442363888025
Batch  71  loss:  0.002617437159642577
Batch  81  loss:  0.0031260198447853327
Batch  91  loss:  0.002185678109526634
Validation on real data: 
LOSS supervised-train 0.002270402822177857, valid 0.0014534485526382923
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.002580078551545739
Batch  11  loss:  0.0017052452312782407
Batch  21  loss:  0.0018497765995562077
Batch  31  loss:  0.001530404668301344
Batch  41  loss:  0.003675388405099511
Batch  51  loss:  0.002664195140823722
Batch  61  loss:  0.0021521730814129114
Batch  71  loss:  0.0017150386702269316
Batch  81  loss:  0.003370382124558091
Batch  91  loss:  0.0025960749480873346
Validation on real data: 
LOSS supervised-train 0.002181885689496994, valid 0.0015962549950927496
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0018518287688493729
Batch  11  loss:  0.0016664504073560238
Batch  21  loss:  0.00211107125505805
Batch  31  loss:  0.00239947228692472
Batch  41  loss:  0.001972030848264694
Batch  51  loss:  0.0027210204862058163
Batch  61  loss:  0.002782287308946252
Batch  71  loss:  0.0014908271841704845
Batch  81  loss:  0.0025931219570338726
Batch  91  loss:  0.0029844006057828665
Validation on real data: 
LOSS supervised-train 0.0022370196925476193, valid 0.0013647070154547691
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0029768208041787148
Batch  11  loss:  0.002031691838055849
Batch  21  loss:  0.0024727224372327328
Batch  31  loss:  0.0018721976084634662
Batch  41  loss:  0.0022336794063448906
Batch  51  loss:  0.002677474170923233
Batch  61  loss:  0.0020478887017816305
Batch  71  loss:  0.00279799266718328
Batch  81  loss:  0.0035501541569828987
Batch  91  loss:  0.002716801129281521
Validation on real data: 
LOSS supervised-train 0.0022202511748764665, valid 0.001964925555512309
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.002057161182165146
Batch  11  loss:  0.0018172813579440117
Batch  21  loss:  0.0023195839021354914
Batch  31  loss:  0.002539070090278983
Batch  41  loss:  0.0022745004389435053
Batch  51  loss:  0.002592098666355014
Batch  61  loss:  0.002482964424416423
Batch  71  loss:  0.0017280301544815302
Batch  81  loss:  0.0031013579573482275
Batch  91  loss:  0.0017528501339256763
Validation on real data: 
LOSS supervised-train 0.0021668525342829524, valid 0.0013106584083288908
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.002704238286241889
Batch  11  loss:  0.003161937929689884
Batch  21  loss:  0.0020160407293587923
Batch  31  loss:  0.0016712879296392202
Batch  41  loss:  0.0018078174907714128
Batch  51  loss:  0.002682808320969343
Batch  61  loss:  0.002333554206416011
Batch  71  loss:  0.0024406586308032274
Batch  81  loss:  0.003389653516933322
Batch  91  loss:  0.0019107700791209936
Validation on real data: 
LOSS supervised-train 0.0022292857454158364, valid 0.0010673350188881159
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0019637676887214184
Batch  11  loss:  0.0018814589129760861
Batch  21  loss:  0.0019470378756523132
Batch  31  loss:  0.0012064998736605048
Batch  41  loss:  0.0027558181900531054
Batch  51  loss:  0.002752474742010236
Batch  61  loss:  0.002161386888474226
Batch  71  loss:  0.0015207980759441853
Batch  81  loss:  0.00399381760507822
Batch  91  loss:  0.0022494380827993155
Validation on real data: 
LOSS supervised-train 0.002080770377069712, valid 0.0014014840126037598
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.001937375869601965
Batch  11  loss:  0.0014960166299715638
Batch  21  loss:  0.001717947656288743
Batch  31  loss:  0.0016494733281433582
Batch  41  loss:  0.0023523957934230566
Batch  51  loss:  0.0029888018034398556
Batch  61  loss:  0.002061778912320733
Batch  71  loss:  0.0018049482023343444
Batch  81  loss:  0.0029021042864769697
Batch  91  loss:  0.0019757093396037817
Validation on real data: 
LOSS supervised-train 0.00207494102709461, valid 0.0012239704374223948
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0028475855942815542
Batch  11  loss:  0.001994298305362463
Batch  21  loss:  0.002276004757732153
Batch  31  loss:  0.0015517303254455328
Batch  41  loss:  0.001446730806492269
Batch  51  loss:  0.002774046501144767
Batch  61  loss:  0.001895920024253428
Batch  71  loss:  0.001532775117084384
Batch  81  loss:  0.004329307936131954
Batch  91  loss:  0.0021795383654534817
Validation on real data: 
LOSS supervised-train 0.0020562419400084763, valid 0.0013791328528895974
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.00263666152022779
Batch  11  loss:  0.0017131147906184196
Batch  21  loss:  0.002067821566015482
Batch  31  loss:  0.0020423911046236753
Batch  41  loss:  0.002431656001135707
Batch  51  loss:  0.0031807594932615757
Batch  61  loss:  0.0021083189640194178
Batch  71  loss:  0.0015518160071223974
Batch  81  loss:  0.003416566178202629
Batch  91  loss:  0.002328363945707679
Validation on real data: 
LOSS supervised-train 0.0021010513207875194, valid 0.0018596452428027987
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0016938828630372882
Batch  11  loss:  0.0012475477997213602
Batch  21  loss:  0.00233936938457191
Batch  31  loss:  0.001704831956885755
Batch  41  loss:  0.0022970044519752264
Batch  51  loss:  0.002284059301018715
Batch  61  loss:  0.002733172383159399
Batch  71  loss:  0.001777086639776826
Batch  81  loss:  0.0037383204326033592
Batch  91  loss:  0.002213981468230486
Validation on real data: 
LOSS supervised-train 0.0020713830459862946, valid 0.0014983125729486346
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0018879335839301348
Batch  11  loss:  0.0016345500480383635
Batch  21  loss:  0.0018974145641550422
Batch  31  loss:  0.001502188970334828
Batch  41  loss:  0.002384244929999113
Batch  51  loss:  0.002371829701587558
Batch  61  loss:  0.002111010020598769
Batch  71  loss:  0.0011958424001932144
Batch  81  loss:  0.0021922329906374216
Batch  91  loss:  0.0022693939972668886
Validation on real data: 
LOSS supervised-train 0.0020984771579969676, valid 0.0019363586325198412
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.002343390602618456
Batch  11  loss:  0.001822692691348493
Batch  21  loss:  0.0019823715556412935
Batch  31  loss:  0.0018233201699331403
Batch  41  loss:  0.002546735107898712
Batch  51  loss:  0.002352616749703884
Batch  61  loss:  0.0034389107022434473
Batch  71  loss:  0.0018603617791086435
Batch  81  loss:  0.003427570452913642
Batch  91  loss:  0.002055894350633025
Validation on real data: 
LOSS supervised-train 0.002071751720504835, valid 0.0015663953963667154
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0018029843922704458
Batch  11  loss:  0.0013796236598864198
Batch  21  loss:  0.0017352835275232792
Batch  31  loss:  0.0022891859989613295
Batch  41  loss:  0.0024405098520219326
Batch  51  loss:  0.0022366789635270834
Batch  61  loss:  0.002624315209686756
Batch  71  loss:  0.0025182294193655252
Batch  81  loss:  0.003223581239581108
Batch  91  loss:  0.002130885375663638
Validation on real data: 
LOSS supervised-train 0.0020073449972551317, valid 0.0015318930381909013
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.002418621676042676
Batch  11  loss:  0.001861902535893023
Batch  21  loss:  0.001916092587634921
Batch  31  loss:  0.001959978835657239
Batch  41  loss:  0.0018251992296427488
Batch  51  loss:  0.00267808698117733
Batch  61  loss:  0.0031174784526228905
Batch  71  loss:  0.0022173102479428053
Batch  81  loss:  0.0034830051008611917
Batch  91  loss:  0.0022609394509345293
Validation on real data: 
LOSS supervised-train 0.002027500238036737, valid 0.001521421829238534
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.002042527077719569
Batch  11  loss:  0.002347612287849188
Batch  21  loss:  0.002142024924978614
Batch  31  loss:  0.0015858826227486134
Batch  41  loss:  0.00201159599237144
Batch  51  loss:  0.002296724123880267
Batch  61  loss:  0.0021530785597860813
Batch  71  loss:  0.0018025264143943787
Batch  81  loss:  0.002965620718896389
Batch  91  loss:  0.0019245675066486
Validation on real data: 
LOSS supervised-train 0.001979423153679818, valid 0.0015121885808184743
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.002983990591019392
Batch  11  loss:  0.0017759418115019798
Batch  21  loss:  0.001851091394200921
Batch  31  loss:  0.0015652993461117148
Batch  41  loss:  0.00195726053789258
Batch  51  loss:  0.0019345538457855582
Batch  61  loss:  0.0021839013788849115
Batch  71  loss:  0.0014455887721851468
Batch  81  loss:  0.0027993295807391405
Batch  91  loss:  0.0016857960727065802
Validation on real data: 
LOSS supervised-train 0.0019446861057076602, valid 0.0014020871603861451
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.002308703027665615
Batch  11  loss:  0.001721487264148891
Batch  21  loss:  0.0017921477556228638
Batch  31  loss:  0.001990577206015587
Batch  41  loss:  0.0024314490146934986
Batch  51  loss:  0.002623381558805704
Batch  61  loss:  0.0018120259046554565
Batch  71  loss:  0.0013185528805479407
Batch  81  loss:  0.0033340638037770987
Batch  91  loss:  0.002110018627718091
Validation on real data: 
LOSS supervised-train 0.001977811306715012, valid 0.0013964050449430943
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0028340991120785475
Batch  11  loss:  0.0016699738334864378
Batch  21  loss:  0.0017907907022163272
Batch  31  loss:  0.002161759650334716
Batch  41  loss:  0.002116425661370158
Batch  51  loss:  0.002769154729321599
Batch  61  loss:  0.002282228320837021
Batch  71  loss:  0.0011878067161887884
Batch  81  loss:  0.002889269031584263
Batch  91  loss:  0.00239012623205781
Validation on real data: 
LOSS supervised-train 0.002050187879940495, valid 0.0016318880952894688
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0028777429834008217
Batch  11  loss:  0.001784991822205484
Batch  21  loss:  0.0015649728011339903
Batch  31  loss:  0.001393186510540545
Batch  41  loss:  0.0023623474407941103
Batch  51  loss:  0.002321231411769986
Batch  61  loss:  0.0023107160814106464
Batch  71  loss:  0.0019589848816394806
Batch  81  loss:  0.0035819478798657656
Batch  91  loss:  0.002235876163467765
Validation on real data: 
LOSS supervised-train 0.001993833305314183, valid 0.0010702254949137568
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.002586888615041971
Batch  11  loss:  0.0017049139132723212
Batch  21  loss:  0.0018655541352927685
Batch  31  loss:  0.0019467463716864586
Batch  41  loss:  0.0017609689384698868
Batch  51  loss:  0.0033133993856608868
Batch  61  loss:  0.0027640527114272118
Batch  71  loss:  0.0018146827351301908
Batch  81  loss:  0.002799788024276495
Batch  91  loss:  0.0020901132375001907
Validation on real data: 
LOSS supervised-train 0.0019321174325887113, valid 0.0013988742139190435
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.001798412762582302
Batch  11  loss:  0.0012815878726541996
Batch  21  loss:  0.0020098534878343344
Batch  31  loss:  0.0014398349449038506
Batch  41  loss:  0.0016840213211253285
Batch  51  loss:  0.00252464204095304
Batch  61  loss:  0.0020509371533989906
Batch  71  loss:  0.0012745150597766042
Batch  81  loss:  0.0029306274373084307
Batch  91  loss:  0.0016703372821211815
Validation on real data: 
LOSS supervised-train 0.0018853158212732524, valid 0.0017371163703501225
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0018883331213146448
Batch  11  loss:  0.0017231088131666183
Batch  21  loss:  0.001977655803784728
Batch  31  loss:  0.001675460604019463
Batch  41  loss:  0.0015042942250147462
Batch  51  loss:  0.002095296746119857
Batch  61  loss:  0.0021591929253190756
Batch  71  loss:  0.0018629389815032482
Batch  81  loss:  0.0033731304574757814
Batch  91  loss:  0.002637689933180809
Validation on real data: 
LOSS supervised-train 0.0019473018345888705, valid 0.0013988811988383532
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0019621415995061398
Batch  11  loss:  0.0014966443413868546
Batch  21  loss:  0.001833950518630445
Batch  31  loss:  0.0021185483783483505
Batch  41  loss:  0.001858500181697309
Batch  51  loss:  0.0024917651899158955
Batch  61  loss:  0.0033437665551900864
Batch  71  loss:  0.0017478328663855791
Batch  81  loss:  0.00418449379503727
Batch  91  loss:  0.002276016166433692
Validation on real data: 
LOSS supervised-train 0.001963698490289971, valid 0.001685988623648882
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.001889010425657034
Batch  11  loss:  0.0014495383948087692
Batch  21  loss:  0.0021465790923684835
Batch  31  loss:  0.0015027991030365229
Batch  41  loss:  0.0025166228879243135
Batch  51  loss:  0.0018723016837611794
Batch  61  loss:  0.002342262538149953
Batch  71  loss:  0.0010276598623022437
Batch  81  loss:  0.003032579319551587
Batch  91  loss:  0.0015325401909649372
Validation on real data: 
LOSS supervised-train 0.0019253909797407687, valid 0.0013586480636149645
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0015657123876735568
Batch  11  loss:  0.001921307877637446
Batch  21  loss:  0.0021212652791291475
Batch  31  loss:  0.0017462405376136303
Batch  41  loss:  0.0015255501493811607
Batch  51  loss:  0.0024068267084658146
Batch  61  loss:  0.002260490320622921
Batch  71  loss:  0.0014337343163788319
Batch  81  loss:  0.0034868489019572735
Batch  91  loss:  0.0018427224131301045
Validation on real data: 
LOSS supervised-train 0.0018803066096734255, valid 0.0018432385986670852
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0028010779060423374
Batch  11  loss:  0.0015003475127741694
Batch  21  loss:  0.0015556317521259189
Batch  31  loss:  0.002006904222071171
Batch  41  loss:  0.0017271378310397267
Batch  51  loss:  0.002032242715358734
Batch  61  loss:  0.002293790690600872
Batch  71  loss:  0.0017781363567337394
Batch  81  loss:  0.0026994552463293076
Batch  91  loss:  0.0018828256288543344
Validation on real data: 
LOSS supervised-train 0.0018776402482762933, valid 0.0014936542138457298
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.002513935323804617
Batch  11  loss:  0.0013644977007061243
Batch  21  loss:  0.001815468305721879
Batch  31  loss:  0.0027986359782516956
Batch  41  loss:  0.002653867471963167
Batch  51  loss:  0.0019432947738096118
Batch  61  loss:  0.0027146213687956333
Batch  71  loss:  0.0013539724750444293
Batch  81  loss:  0.0027255662716925144
Batch  91  loss:  0.0020660816226154566
Validation on real data: 
LOSS supervised-train 0.0019597797491587698, valid 0.0015726506244391203
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0022138510830700397
Batch  11  loss:  0.0020084776915609837
Batch  21  loss:  0.001645636628381908
Batch  31  loss:  0.001988752046599984
Batch  41  loss:  0.0015731551684439182
Batch  51  loss:  0.0027905027382075787
Batch  61  loss:  0.002524598268792033
Batch  71  loss:  0.00150199921336025
Batch  81  loss:  0.0033132596872746944
Batch  91  loss:  0.0016455271979793906
Validation on real data: 
LOSS supervised-train 0.0018713405507151037, valid 0.001148549490608275
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0018112617544829845
Batch  11  loss:  0.0017828246345743537
Batch  21  loss:  0.0016436115838587284
Batch  31  loss:  0.002644906286150217
Batch  41  loss:  0.0018291327869519591
Batch  51  loss:  0.0018340280512347817
Batch  61  loss:  0.0019446784863248467
Batch  71  loss:  0.0016165872802957892
Batch  81  loss:  0.0037310095503926277
Batch  91  loss:  0.001748464535921812
Validation on real data: 
LOSS supervised-train 0.0018928308732574805, valid 0.0012768511660397053
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0023623667657375336
Batch  11  loss:  0.0014852728927507997
Batch  21  loss:  0.001994980499148369
Batch  31  loss:  0.0021518876310437918
Batch  41  loss:  0.0019417485455051064
Batch  51  loss:  0.0026760301552712917
Batch  61  loss:  0.0016222455305978656
Batch  71  loss:  0.0016829830128699541
Batch  81  loss:  0.0021436563692986965
Batch  91  loss:  0.0016298609552904963
Validation on real data: 
LOSS supervised-train 0.001873253260855563, valid 0.0016064782394096255
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0017212332459166646
Batch  11  loss:  0.0018026831094175577
Batch  21  loss:  0.0025183914694935083
Batch  31  loss:  0.0018749439623206854
Batch  41  loss:  0.0017283079214394093
Batch  51  loss:  0.0027320021763443947
Batch  61  loss:  0.0021625009831041098
Batch  71  loss:  0.001225701067596674
Batch  81  loss:  0.0032977822702378035
Batch  91  loss:  0.0017554525984451175
Validation on real data: 
LOSS supervised-train 0.0018481439945753663, valid 0.0016160474624484777
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0013635320356115699
Batch  11  loss:  0.0020819702185690403
Batch  21  loss:  0.001462142332457006
Batch  31  loss:  0.0018574055284261703
Batch  41  loss:  0.0013863202184438705
Batch  51  loss:  0.00209107156842947
Batch  61  loss:  0.0015954013215377927
Batch  71  loss:  0.0019867364317178726
Batch  81  loss:  0.0027334755286574364
Batch  91  loss:  0.0018254335736855865
Validation on real data: 
LOSS supervised-train 0.0018461838155053557, valid 0.001439198269508779
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.002521287649869919
Batch  11  loss:  0.0014901034301146865
Batch  21  loss:  0.001569109270349145
Batch  31  loss:  0.001369047095067799
Batch  41  loss:  0.0019670077599585056
Batch  51  loss:  0.0020885758567601442
Batch  61  loss:  0.0021781057585030794
Batch  71  loss:  0.0016638098750263453
Batch  81  loss:  0.003119782079011202
Batch  91  loss:  0.0017997348913922906
Validation on real data: 
LOSS supervised-train 0.0017728182335849851, valid 0.0013770509976893663
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.002423164900392294
Batch  11  loss:  0.0018337807850912213
Batch  21  loss:  0.0011710433755069971
Batch  31  loss:  0.0017400988144800067
Batch  41  loss:  0.0018396320519968867
Batch  51  loss:  0.0022704829461872578
Batch  61  loss:  0.0023089342284947634
Batch  71  loss:  0.0009738844237290323
Batch  81  loss:  0.0018497415585443377
Batch  91  loss:  0.0017546463059261441
Validation on real data: 
LOSS supervised-train 0.0017500162037322297, valid 0.0017375621246173978
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.002162396442145109
Batch  11  loss:  0.0019108158303424716
Batch  21  loss:  0.0014427596470341086
Batch  31  loss:  0.0020415440667420626
Batch  41  loss:  0.0014733114512637258
Batch  51  loss:  0.002374305622652173
Batch  61  loss:  0.0024574310518801212
Batch  71  loss:  0.0012147767702117562
Batch  81  loss:  0.0035573169589042664
Batch  91  loss:  0.0015624490333721042
Validation on real data: 
LOSS supervised-train 0.0018205036222934722, valid 0.0015584961511194706
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  car ; Model ID: ad45b2d40c7801ef2074a73831d8a3a2
--------------------
Training baseline regression model:  2022-03-30 14:48:21.872103
Detector:  pointnet
Object:  car
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1620811
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.20462611317634583
Batch  11  loss:  0.15103362500667572
Batch  21  loss:  0.09861674904823303
Batch  31  loss:  0.08681469410657883
Batch  41  loss:  0.08248403668403625
Batch  51  loss:  0.080253005027771
Batch  61  loss:  0.06575888395309448
Batch  71  loss:  0.061367545276880264
Batch  81  loss:  0.052035897970199585
Batch  91  loss:  0.056007400155067444
Validation on real data: 
LOSS supervised-train 0.0906134943664074, valid 0.06328120827674866
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.04593047499656677
Batch  11  loss:  0.053688954561948776
Batch  21  loss:  0.035202205181121826
Batch  31  loss:  0.02797079086303711
Batch  41  loss:  0.02279282920062542
Batch  51  loss:  0.0225637499243021
Batch  61  loss:  0.015451759099960327
Batch  71  loss:  0.015163912437856197
Batch  81  loss:  0.01831728033721447
Batch  91  loss:  0.014580249786376953
Validation on real data: 
LOSS supervised-train 0.027031925544142722, valid 0.008785109035670757
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.012930558994412422
Batch  11  loss:  0.02344893105328083
Batch  21  loss:  0.012558834627270699
Batch  31  loss:  0.014568841084837914
Batch  41  loss:  0.00940996129065752
Batch  51  loss:  0.011261837556958199
Batch  61  loss:  0.018770525231957436
Batch  71  loss:  0.010694282129406929
Batch  81  loss:  0.01223558560013771
Batch  91  loss:  0.009760064072906971
Validation on real data: 
LOSS supervised-train 0.01238138969987631, valid 0.00551208108663559
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.010014912113547325
Batch  11  loss:  0.019152097404003143
Batch  21  loss:  0.009249670431017876
Batch  31  loss:  0.007922656834125519
Batch  41  loss:  0.0071290223859250546
Batch  51  loss:  0.006619840860366821
Batch  61  loss:  0.009728914126753807
Batch  71  loss:  0.0071923560462892056
Batch  81  loss:  0.0082848584279418
Batch  91  loss:  0.007788618560880423
Validation on real data: 
LOSS supervised-train 0.009065984161570667, valid 0.0046746134757995605
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.006090850103646517
Batch  11  loss:  0.01227461826056242
Batch  21  loss:  0.007080191280692816
Batch  31  loss:  0.008898397907614708
Batch  41  loss:  0.00419546477496624
Batch  51  loss:  0.004896434955298901
Batch  61  loss:  0.00833966862410307
Batch  71  loss:  0.005097821354866028
Batch  81  loss:  0.008810306899249554
Batch  91  loss:  0.00628875894472003
Validation on real data: 
LOSS supervised-train 0.007040355377830565, valid 0.003701851237565279
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.006202858407050371
Batch  11  loss:  0.008162490092217922
Batch  21  loss:  0.005338386166840792
Batch  31  loss:  0.007385064382106066
Batch  41  loss:  0.004836262669414282
Batch  51  loss:  0.005091733764857054
Batch  61  loss:  0.005534287542104721
Batch  71  loss:  0.005976741202175617
Batch  81  loss:  0.008795412257313728
Batch  91  loss:  0.005150840152055025
Validation on real data: 
LOSS supervised-train 0.006038707522675395, valid 0.003050559666007757
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0058807372115552425
Batch  11  loss:  0.00711766118183732
Batch  21  loss:  0.005037855356931686
Batch  31  loss:  0.006192035507410765
Batch  41  loss:  0.0033912062644958496
Batch  51  loss:  0.0037920353934168816
Batch  61  loss:  0.004947777837514877
Batch  71  loss:  0.005355330184102058
Batch  81  loss:  0.0065717617981135845
Batch  91  loss:  0.004648593720048666
Validation on real data: 
LOSS supervised-train 0.005623096043709665, valid 0.0026414936874061823
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.004422270692884922
Batch  11  loss:  0.006312449462711811
Batch  21  loss:  0.005692620296031237
Batch  31  loss:  0.0064715053886175156
Batch  41  loss:  0.0036775614134967327
Batch  51  loss:  0.004139254800975323
Batch  61  loss:  0.004751615226268768
Batch  71  loss:  0.004399303812533617
Batch  81  loss:  0.005048565566539764
Batch  91  loss:  0.0037447293289005756
Validation on real data: 
LOSS supervised-train 0.0049838966783136126, valid 0.002841977635398507
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.004540572874248028
Batch  11  loss:  0.006995505653321743
Batch  21  loss:  0.00529527198523283
Batch  31  loss:  0.005692000035196543
Batch  41  loss:  0.003105896059423685
Batch  51  loss:  0.0033479349222034216
Batch  61  loss:  0.004331804346293211
Batch  71  loss:  0.0028948974795639515
Batch  81  loss:  0.004266965202987194
Batch  91  loss:  0.00430131983011961
Validation on real data: 
LOSS supervised-train 0.004741353618446738, valid 0.0028685443103313446
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0036259673070162535
Batch  11  loss:  0.006260395981371403
Batch  21  loss:  0.006953804288059473
Batch  31  loss:  0.0048715840093791485
Batch  41  loss:  0.004095734562724829
Batch  51  loss:  0.003380104433745146
Batch  61  loss:  0.004418014083057642
Batch  71  loss:  0.00426461873576045
Batch  81  loss:  0.004599509295076132
Batch  91  loss:  0.0029584183357656
Validation on real data: 
LOSS supervised-train 0.0043598639988340435, valid 0.00451356265693903
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0035305730998516083
Batch  11  loss:  0.005861008074134588
Batch  21  loss:  0.004820857662707567
Batch  31  loss:  0.0047601512633264065
Batch  41  loss:  0.002844330621883273
Batch  51  loss:  0.0026123623829334974
Batch  61  loss:  0.004660643171519041
Batch  71  loss:  0.0036309808492660522
Batch  81  loss:  0.004814359359443188
Batch  91  loss:  0.003270100336521864
Validation on real data: 
LOSS supervised-train 0.004213695633225143, valid 0.0029285314958542585
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0038560323882848024
Batch  11  loss:  0.006035162135958672
Batch  21  loss:  0.006321412976831198
Batch  31  loss:  0.004357710015028715
Batch  41  loss:  0.0026875012554228306
Batch  51  loss:  0.0025298341643065214
Batch  61  loss:  0.004038587212562561
Batch  71  loss:  0.003722808323800564
Batch  81  loss:  0.005072332452982664
Batch  91  loss:  0.00306527316570282
Validation on real data: 
LOSS supervised-train 0.004073094401974231, valid 0.0019177698995918036
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.003176358761265874
Batch  11  loss:  0.005189191084355116
Batch  21  loss:  0.004582494497299194
Batch  31  loss:  0.005109131336212158
Batch  41  loss:  0.0032117038499563932
Batch  51  loss:  0.002238383051007986
Batch  61  loss:  0.002970361150801182
Batch  71  loss:  0.0031929761171340942
Batch  81  loss:  0.0037727474700659513
Batch  91  loss:  0.003702113637700677
Validation on real data: 
LOSS supervised-train 0.0037023498956114055, valid 0.002354735042899847
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.003435649210587144
Batch  11  loss:  0.006123964674770832
Batch  21  loss:  0.004755125381052494
Batch  31  loss:  0.0035609209444373846
Batch  41  loss:  0.002959250705316663
Batch  51  loss:  0.002784768817946315
Batch  61  loss:  0.0032212825026363134
Batch  71  loss:  0.003323544282466173
Batch  81  loss:  0.003308006562292576
Batch  91  loss:  0.003020926844328642
Validation on real data: 
LOSS supervised-train 0.0036699977330863476, valid 0.0020502167753875256
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.003169327275827527
Batch  11  loss:  0.00456196628510952
Batch  21  loss:  0.0038672881200909615
Batch  31  loss:  0.0040107122622430325
Batch  41  loss:  0.0030107039492577314
Batch  51  loss:  0.0031069840770214796
Batch  61  loss:  0.0037574348971247673
Batch  71  loss:  0.002782026771456003
Batch  81  loss:  0.003547082422301173
Batch  91  loss:  0.0030753277242183685
Validation on real data: 
LOSS supervised-train 0.0033911230135709046, valid 0.002106917090713978
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.003016039729118347
Batch  11  loss:  0.004133736249059439
Batch  21  loss:  0.0038711566012352705
Batch  31  loss:  0.004157093353569508
Batch  41  loss:  0.0026738292071968317
Batch  51  loss:  0.0026040843222290277
Batch  61  loss:  0.002873920137062669
Batch  71  loss:  0.003252978902310133
Batch  81  loss:  0.00431642634794116
Batch  91  loss:  0.0033288532868027687
Validation on real data: 
LOSS supervised-train 0.0034229864971712233, valid 0.001982680754736066
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.002680998994037509
Batch  11  loss:  0.004492908716201782
Batch  21  loss:  0.003923265729099512
Batch  31  loss:  0.003466084599494934
Batch  41  loss:  0.002173474058508873
Batch  51  loss:  0.0027781780809164047
Batch  61  loss:  0.0028632949106395245
Batch  71  loss:  0.0026723681949079037
Batch  81  loss:  0.004015864804387093
Batch  91  loss:  0.0023700306192040443
Validation on real data: 
LOSS supervised-train 0.0032972226943820716, valid 0.00222056289203465
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0025597428902983665
Batch  11  loss:  0.004564585164189339
Batch  21  loss:  0.004265426192432642
Batch  31  loss:  0.0034890365786850452
Batch  41  loss:  0.0021737448405474424
Batch  51  loss:  0.001958172768354416
Batch  61  loss:  0.0028012224938720465
Batch  71  loss:  0.0034683221019804478
Batch  81  loss:  0.00429878942668438
Batch  91  loss:  0.002265159972012043
Validation on real data: 
LOSS supervised-train 0.0031728164665400984, valid 0.0018694320460781455
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0031952473800629377
Batch  11  loss:  0.004206966143101454
Batch  21  loss:  0.0036441050469875336
Batch  31  loss:  0.003688221797347069
Batch  41  loss:  0.0024280922953039408
Batch  51  loss:  0.0023666953202337027
Batch  61  loss:  0.0025627831928431988
Batch  71  loss:  0.002996728988364339
Batch  81  loss:  0.0033083457965403795
Batch  91  loss:  0.002426584716886282
Validation on real data: 
LOSS supervised-train 0.0030356267176102846, valid 0.001735668396577239
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0026068012230098248
Batch  11  loss:  0.0034461277537047863
Batch  21  loss:  0.0031048101373016834
Batch  31  loss:  0.00406087189912796
Batch  41  loss:  0.00236229388974607
Batch  51  loss:  0.0021678272169083357
Batch  61  loss:  0.0027859085239470005
Batch  71  loss:  0.0033197675365954638
Batch  81  loss:  0.004088061396032572
Batch  91  loss:  0.0027260291390120983
Validation on real data: 
LOSS supervised-train 0.0029904647066723556, valid 0.0020997985266149044
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.002193812280893326
Batch  11  loss:  0.0035871858708560467
Batch  21  loss:  0.002725868718698621
Batch  31  loss:  0.002933710115030408
Batch  41  loss:  0.002190536120906472
Batch  51  loss:  0.002186465309932828
Batch  61  loss:  0.003214186290279031
Batch  71  loss:  0.002718348754569888
Batch  81  loss:  0.0036043040454387665
Batch  91  loss:  0.0021624856162816286
Validation on real data: 
LOSS supervised-train 0.0028058349166531116, valid 0.0017831220757216215
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.002232726663351059
Batch  11  loss:  0.003653554944321513
Batch  21  loss:  0.0033877023961395025
Batch  31  loss:  0.004231576342135668
Batch  41  loss:  0.002495446940883994
Batch  51  loss:  0.002254236489534378
Batch  61  loss:  0.0021384297870099545
Batch  71  loss:  0.003393955063074827
Batch  81  loss:  0.002278176136314869
Batch  91  loss:  0.002739995950832963
Validation on real data: 
LOSS supervised-train 0.0028464384074322877, valid 0.0021464829333126545
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00212023314088583
Batch  11  loss:  0.0034711735788732767
Batch  21  loss:  0.003629016689956188
Batch  31  loss:  0.0038503354880958796
Batch  41  loss:  0.0024447431787848473
Batch  51  loss:  0.0022572374437004328
Batch  61  loss:  0.0022300388664007187
Batch  71  loss:  0.0030589730013161898
Batch  81  loss:  0.003047922858968377
Batch  91  loss:  0.002182894852012396
Validation on real data: 
LOSS supervised-train 0.002805942241102457, valid 0.0016791715752333403
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0016983987297862768
Batch  11  loss:  0.0026283927727490664
Batch  21  loss:  0.002890936331823468
Batch  31  loss:  0.0039252787828445435
Batch  41  loss:  0.002855557482689619
Batch  51  loss:  0.002465997589752078
Batch  61  loss:  0.0022842020262032747
Batch  71  loss:  0.0027596885338425636
Batch  81  loss:  0.002714530099183321
Batch  91  loss:  0.002810502192005515
Validation on real data: 
LOSS supervised-train 0.002685233918018639, valid 0.0019010513788089156
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.002580159343779087
Batch  11  loss:  0.0028244033455848694
Batch  21  loss:  0.0031645623967051506
Batch  31  loss:  0.0031676923390477896
Batch  41  loss:  0.002037509111687541
Batch  51  loss:  0.0027555986307561398
Batch  61  loss:  0.0026583748403936625
Batch  71  loss:  0.0025784610770642757
Batch  81  loss:  0.003571316134184599
Batch  91  loss:  0.0022212809417396784
Validation on real data: 
LOSS supervised-train 0.002637309262063354, valid 0.002022786997258663
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0022477852180600166
Batch  11  loss:  0.002916321624070406
Batch  21  loss:  0.002801004331558943
Batch  31  loss:  0.0033288300037384033
Batch  41  loss:  0.0021016227547079325
Batch  51  loss:  0.0021382682025432587
Batch  61  loss:  0.0030554949771612883
Batch  71  loss:  0.0024340511299669743
Batch  81  loss:  0.002686937339603901
Batch  91  loss:  0.002159933792427182
Validation on real data: 
LOSS supervised-train 0.0025826178630813956, valid 0.0016455954173579812
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0018663766095414758
Batch  11  loss:  0.003003964899107814
Batch  21  loss:  0.0028176987543702126
Batch  31  loss:  0.0035114127676934004
Batch  41  loss:  0.0021385187283158302
Batch  51  loss:  0.001848729094490409
Batch  61  loss:  0.0023868894204497337
Batch  71  loss:  0.002989425789564848
Batch  81  loss:  0.0033296835608780384
Batch  91  loss:  0.001935478881932795
Validation on real data: 
LOSS supervised-train 0.0025691331387497486, valid 0.0017605654429644346
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0016289837658405304
Batch  11  loss:  0.0031979994382709265
Batch  21  loss:  0.0032210221979767084
Batch  31  loss:  0.0030438669491559267
Batch  41  loss:  0.0023912228643894196
Batch  51  loss:  0.0021221071947366
Batch  61  loss:  0.0016801428282633424
Batch  71  loss:  0.003435341641306877
Batch  81  loss:  0.003124440787360072
Batch  91  loss:  0.0032898341305553913
Validation on real data: 
LOSS supervised-train 0.002358140686992556, valid 0.001611831015907228
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.002287548501044512
Batch  11  loss:  0.0023415314499288797
Batch  21  loss:  0.0038895970210433006
Batch  31  loss:  0.0034268200397491455
Batch  41  loss:  0.0019030887633562088
Batch  51  loss:  0.0023822772782295942
Batch  61  loss:  0.0028168230783194304
Batch  71  loss:  0.0022462953347712755
Batch  81  loss:  0.0025068430695682764
Batch  91  loss:  0.0017577284015715122
Validation on real data: 
LOSS supervised-train 0.002473709888290614, valid 0.001763687003403902
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.002286540577188134
Batch  11  loss:  0.0029117208905518055
Batch  21  loss:  0.002668209606781602
Batch  31  loss:  0.0025706945452839136
Batch  41  loss:  0.0015935945557430387
Batch  51  loss:  0.002197123598307371
Batch  61  loss:  0.002754134824499488
Batch  71  loss:  0.0027037602849304676
Batch  81  loss:  0.002614489756524563
Batch  91  loss:  0.002179111121222377
Validation on real data: 
LOSS supervised-train 0.002418718981789425, valid 0.0014276937581598759
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0017940524267032743
Batch  11  loss:  0.003239545738324523
Batch  21  loss:  0.002815213054418564
Batch  31  loss:  0.003151609795168042
Batch  41  loss:  0.002034035511314869
Batch  51  loss:  0.0013919654302299023
Batch  61  loss:  0.0018153064884245396
Batch  71  loss:  0.002731876913458109
Batch  81  loss:  0.0029631536453962326
Batch  91  loss:  0.0020224431063979864
Validation on real data: 
LOSS supervised-train 0.0023375146137550474, valid 0.0020210088696330786
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0021642236970365047
Batch  11  loss:  0.0027741496451199055
Batch  21  loss:  0.0028220610693097115
Batch  31  loss:  0.0026631311047822237
Batch  41  loss:  0.0017049446469172835
Batch  51  loss:  0.001801192294806242
Batch  61  loss:  0.0030765479896217585
Batch  71  loss:  0.002615398494526744
Batch  81  loss:  0.001745631918311119
Batch  91  loss:  0.0024816852528601885
Validation on real data: 
LOSS supervised-train 0.002283320319838822, valid 0.0014688068768009543
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0018076318083330989
Batch  11  loss:  0.0022621627431362867
Batch  21  loss:  0.003182423999533057
Batch  31  loss:  0.002754091750830412
Batch  41  loss:  0.0017510727047920227
Batch  51  loss:  0.002483871765434742
Batch  61  loss:  0.002273747231811285
Batch  71  loss:  0.0025451481342315674
Batch  81  loss:  0.003872738452628255
Batch  91  loss:  0.0024200784973800182
Validation on real data: 
LOSS supervised-train 0.002324682712787762, valid 0.001373564125970006
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0017068894812837243
Batch  11  loss:  0.002800530754029751
Batch  21  loss:  0.002155998023226857
Batch  31  loss:  0.002956512151286006
Batch  41  loss:  0.0021421057172119617
Batch  51  loss:  0.002140918979421258
Batch  61  loss:  0.002053842879831791
Batch  71  loss:  0.0018613383872434497
Batch  81  loss:  0.002390077104791999
Batch  91  loss:  0.0017376383766531944
Validation on real data: 
LOSS supervised-train 0.002286977267358452, valid 0.0014252514811232686
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0017076913500204682
Batch  11  loss:  0.0018518607830628753
Batch  21  loss:  0.002537201624363661
Batch  31  loss:  0.002826667856425047
Batch  41  loss:  0.0014913552440702915
Batch  51  loss:  0.0016305430326610804
Batch  61  loss:  0.001948004588484764
Batch  71  loss:  0.0027696830220520496
Batch  81  loss:  0.0021446470636874437
Batch  91  loss:  0.002733162371441722
Validation on real data: 
LOSS supervised-train 0.002195302901091054, valid 0.0015337069053202868
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.00153475278057158
Batch  11  loss:  0.002477687317878008
Batch  21  loss:  0.003658812493085861
Batch  31  loss:  0.0029178964905440807
Batch  41  loss:  0.0018800749676302075
Batch  51  loss:  0.0016278994735330343
Batch  61  loss:  0.002891185926273465
Batch  71  loss:  0.002849223790690303
Batch  81  loss:  0.0021798897068947554
Batch  91  loss:  0.00219299946911633
Validation on real data: 
LOSS supervised-train 0.002159007609589025, valid 0.0014123028377071023
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0020125394221395254
Batch  11  loss:  0.0021159732714295387
Batch  21  loss:  0.0029977618250995874
Batch  31  loss:  0.00253913551568985
Batch  41  loss:  0.0015068232314661145
Batch  51  loss:  0.002193174324929714
Batch  61  loss:  0.0024250883143395185
Batch  71  loss:  0.002065986627712846
Batch  81  loss:  0.0018405262380838394
Batch  91  loss:  0.0018907975172623992
Validation on real data: 
LOSS supervised-train 0.0021142870583571493, valid 0.001665999530814588
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0015996742295101285
Batch  11  loss:  0.002424979116767645
Batch  21  loss:  0.002188152400776744
Batch  31  loss:  0.003145958296954632
Batch  41  loss:  0.0015645441599190235
Batch  51  loss:  0.0020970774348825216
Batch  61  loss:  0.0022687537129968405
Batch  71  loss:  0.0023593376390635967
Batch  81  loss:  0.002144318539649248
Batch  91  loss:  0.002285813447088003
Validation on real data: 
LOSS supervised-train 0.0021104172093328087, valid 0.0015938767464831471
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0014285051729530096
Batch  11  loss:  0.0026811412535607815
Batch  21  loss:  0.0023377202451229095
Batch  31  loss:  0.0027945691253989935
Batch  41  loss:  0.0019246304873377085
Batch  51  loss:  0.002096773823723197
Batch  61  loss:  0.0016907938988879323
Batch  71  loss:  0.0024704623501747847
Batch  81  loss:  0.0019446613732725382
Batch  91  loss:  0.001970473909750581
Validation on real data: 
LOSS supervised-train 0.0021135466848500074, valid 0.0013542142696678638
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0017266508657485247
Batch  11  loss:  0.0015399951953440905
Batch  21  loss:  0.0023899024818092585
Batch  31  loss:  0.0031481708865612745
Batch  41  loss:  0.0016310103237628937
Batch  51  loss:  0.0016857152804732323
Batch  61  loss:  0.0020087098237127066
Batch  71  loss:  0.002119197743013501
Batch  81  loss:  0.0032836534082889557
Batch  91  loss:  0.0022082305513322353
Validation on real data: 
LOSS supervised-train 0.00206911125802435, valid 0.0012319311499595642
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.001590461703017354
Batch  11  loss:  0.0020176798570901155
Batch  21  loss:  0.002199765294790268
Batch  31  loss:  0.0027758809737861156
Batch  41  loss:  0.0013610126916319132
Batch  51  loss:  0.002084457315504551
Batch  61  loss:  0.0020670639351010323
Batch  71  loss:  0.0017481065588071942
Batch  81  loss:  0.0020359070040285587
Batch  91  loss:  0.0022931129205971956
Validation on real data: 
LOSS supervised-train 0.0020227499422617256, valid 0.001423107460141182
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0018765402492135763
Batch  11  loss:  0.002219484420493245
Batch  21  loss:  0.0030518281273543835
Batch  31  loss:  0.0024013742804527283
Batch  41  loss:  0.0016134659526869655
Batch  51  loss:  0.0015092686517164111
Batch  61  loss:  0.002052361611276865
Batch  71  loss:  0.0021240285132080317
Batch  81  loss:  0.002267588395625353
Batch  91  loss:  0.0017348078545182943
Validation on real data: 
LOSS supervised-train 0.001955026660580188, valid 0.001347784767858684
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0018933411920443177
Batch  11  loss:  0.0016995117766782641
Batch  21  loss:  0.0020740823820233345
Batch  31  loss:  0.0024332567118108273
Batch  41  loss:  0.001543705235235393
Batch  51  loss:  0.001524822087958455
Batch  61  loss:  0.002126098610460758
Batch  71  loss:  0.0017312078271061182
Batch  81  loss:  0.002037876285612583
Batch  91  loss:  0.0017211331287398934
Validation on real data: 
LOSS supervised-train 0.001981033394113183, valid 0.0017019372899085283
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0014623230090364814
Batch  11  loss:  0.001978497253730893
Batch  21  loss:  0.0025436454452574253
Batch  31  loss:  0.0027214805595576763
Batch  41  loss:  0.0017671574605628848
Batch  51  loss:  0.0017299436731263995
Batch  61  loss:  0.0018761551473289728
Batch  71  loss:  0.001718494575470686
Batch  81  loss:  0.0025302686262875795
Batch  91  loss:  0.0018573851557448506
Validation on real data: 
LOSS supervised-train 0.0019970409059897067, valid 0.0014178165001794696
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0017039887607097626
Batch  11  loss:  0.002021330175921321
Batch  21  loss:  0.0020397996995598078
Batch  31  loss:  0.002727747429162264
Batch  41  loss:  0.0019328562775626779
Batch  51  loss:  0.0017761552007868886
Batch  61  loss:  0.0017408960266038775
Batch  71  loss:  0.0018869034247472882
Batch  81  loss:  0.0021964709740132093
Batch  91  loss:  0.0019308283226564527
Validation on real data: 
LOSS supervised-train 0.002000495282700285, valid 0.0011251906398683786
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0014152704970911145
Batch  11  loss:  0.0019012059783563018
Batch  21  loss:  0.0022178334183990955
Batch  31  loss:  0.0020393359009176493
Batch  41  loss:  0.0014388775452971458
Batch  51  loss:  0.0015320759266614914
Batch  61  loss:  0.0018788683228194714
Batch  71  loss:  0.0019031019182875752
Batch  81  loss:  0.0019348099594935775
Batch  91  loss:  0.0020712928380817175
Validation on real data: 
LOSS supervised-train 0.0019498916645534336, valid 0.001464451546780765
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0014966201269999146
Batch  11  loss:  0.002139653544872999
Batch  21  loss:  0.0020586929749697447
Batch  31  loss:  0.0028146107215434313
Batch  41  loss:  0.0018614273285493255
Batch  51  loss:  0.0015820926055312157
Batch  61  loss:  0.0019758320413529873
Batch  71  loss:  0.002523372182622552
Batch  81  loss:  0.002203719224780798
Batch  91  loss:  0.0017683141632005572
Validation on real data: 
LOSS supervised-train 0.001995566046098247, valid 0.0014637758722528815
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0017059551319107413
Batch  11  loss:  0.0016551377484574914
Batch  21  loss:  0.001944945426657796
Batch  31  loss:  0.0024177581071853638
Batch  41  loss:  0.0015470318030565977
Batch  51  loss:  0.0014518509851768613
Batch  61  loss:  0.0026067118160426617
Batch  71  loss:  0.0020504500716924667
Batch  81  loss:  0.0020146616734564304
Batch  91  loss:  0.0018973792903125286
Validation on real data: 
LOSS supervised-train 0.0019131007755640893, valid 0.0013693325454369187
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0015122093027457595
Batch  11  loss:  0.0018507494824007154
Batch  21  loss:  0.0018717654747888446
Batch  31  loss:  0.002496349858120084
Batch  41  loss:  0.0020652725361287594
Batch  51  loss:  0.0014268949162214994
Batch  61  loss:  0.002163219964131713
Batch  71  loss:  0.002282976172864437
Batch  81  loss:  0.0017730399267747998
Batch  91  loss:  0.0016895385924726725
Validation on real data: 
LOSS supervised-train 0.001893990256357938, valid 0.0011242422042414546
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0015719336224719882
Batch  11  loss:  0.002346812980249524
Batch  21  loss:  0.0017087608575820923
Batch  31  loss:  0.002925684442743659
Batch  41  loss:  0.0020922834519296885
Batch  51  loss:  0.001548354048281908
Batch  61  loss:  0.001205940032377839
Batch  71  loss:  0.0018885380122810602
Batch  81  loss:  0.00154887733515352
Batch  91  loss:  0.0015655300812795758
Validation on real data: 
LOSS supervised-train 0.0018226096942089499, valid 0.0011595236137509346
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0014626114862039685
Batch  11  loss:  0.002016793703660369
Batch  21  loss:  0.002103823469951749
Batch  31  loss:  0.002134065143764019
Batch  41  loss:  0.001628597965463996
Batch  51  loss:  0.0015087735373526812
Batch  61  loss:  0.0023153922520577908
Batch  71  loss:  0.0016336271073669195
Batch  81  loss:  0.002275363076478243
Batch  91  loss:  0.0015700866933912039
Validation on real data: 
LOSS supervised-train 0.0018623192748054862, valid 0.0012538012815639377
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.001677891006693244
Batch  11  loss:  0.0016439738683402538
Batch  21  loss:  0.0017263535410165787
Batch  31  loss:  0.0018529824446886778
Batch  41  loss:  0.0016248601023107767
Batch  51  loss:  0.0012757704826071858
Batch  61  loss:  0.002115643350407481
Batch  71  loss:  0.0019758150447160006
Batch  81  loss:  0.0018848994513973594
Batch  91  loss:  0.0015857256948947906
Validation on real data: 
LOSS supervised-train 0.001866566821699962, valid 0.0012344705173745751
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0018724268302321434
Batch  11  loss:  0.0018803998827934265
Batch  21  loss:  0.0017928072484210134
Batch  31  loss:  0.0026643150486052036
Batch  41  loss:  0.0015849327901378274
Batch  51  loss:  0.0016084326198324561
Batch  61  loss:  0.0019511942518875003
Batch  71  loss:  0.001881332602351904
Batch  81  loss:  0.0022928989492356777
Batch  91  loss:  0.0019298868719488382
Validation on real data: 
LOSS supervised-train 0.0018456309207249433, valid 0.0012422373984009027
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0013465348165482283
Batch  11  loss:  0.0024950613733381033
Batch  21  loss:  0.0018391598714515567
Batch  31  loss:  0.0020541043486446142
Batch  41  loss:  0.0015457661356776953
Batch  51  loss:  0.0012064760085195303
Batch  61  loss:  0.0017419910291209817
Batch  71  loss:  0.0020130586344748735
Batch  81  loss:  0.0017559971893206239
Batch  91  loss:  0.0018011003267019987
Validation on real data: 
LOSS supervised-train 0.0017915698292199523, valid 0.001377996290102601
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0014962032437324524
Batch  11  loss:  0.0019325274042785168
Batch  21  loss:  0.0018169760005548596
Batch  31  loss:  0.0026168348267674446
Batch  41  loss:  0.0017209609504789114
Batch  51  loss:  0.0015560142928734422
Batch  61  loss:  0.0013917440082877874
Batch  71  loss:  0.0018873166991397738
Batch  81  loss:  0.0020540866535156965
Batch  91  loss:  0.0021036528050899506
Validation on real data: 
LOSS supervised-train 0.0018562565988395363, valid 0.0015240181237459183
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.001626316225156188
Batch  11  loss:  0.0021023110020905733
Batch  21  loss:  0.001556056085973978
Batch  31  loss:  0.002148063387721777
Batch  41  loss:  0.001615638379007578
Batch  51  loss:  0.001452544005587697
Batch  61  loss:  0.00149441952817142
Batch  71  loss:  0.002121664583683014
Batch  81  loss:  0.0022374053951352835
Batch  91  loss:  0.0014177605044096708
Validation on real data: 
LOSS supervised-train 0.0017315048945602029, valid 0.0013776656705886126
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0016179734375327826
Batch  11  loss:  0.0016523655503988266
Batch  21  loss:  0.001955529907718301
Batch  31  loss:  0.0018231701105833054
Batch  41  loss:  0.0019522756338119507
Batch  51  loss:  0.0016106023686006665
Batch  61  loss:  0.0018574901623651385
Batch  71  loss:  0.0018485432956367731
Batch  81  loss:  0.00221492862328887
Batch  91  loss:  0.001710274606011808
Validation on real data: 
LOSS supervised-train 0.0017604086059145629, valid 0.0013724364107474685
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.001650431426241994
Batch  11  loss:  0.0014197203563526273
Batch  21  loss:  0.002109542256221175
Batch  31  loss:  0.0019549953285604715
Batch  41  loss:  0.0017068175366148353
Batch  51  loss:  0.001360845984891057
Batch  61  loss:  0.001969172852113843
Batch  71  loss:  0.0020036110654473305
Batch  81  loss:  0.002230585552752018
Batch  91  loss:  0.001680858084000647
Validation on real data: 
LOSS supervised-train 0.0017146534670609982, valid 0.0015363612910732627
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0015607563545927405
Batch  11  loss:  0.0016190173337236047
Batch  21  loss:  0.0015307646244764328
Batch  31  loss:  0.0020577511750161648
Batch  41  loss:  0.0015306788263842463
Batch  51  loss:  0.0012464176397770643
Batch  61  loss:  0.001686472212895751
Batch  71  loss:  0.0016886938828974962
Batch  81  loss:  0.001829274813644588
Batch  91  loss:  0.002010249998420477
Validation on real data: 
LOSS supervised-train 0.0016961162840016186, valid 0.0010612686164677143
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0013180153910070658
Batch  11  loss:  0.0021448121406137943
Batch  21  loss:  0.0017591220093891025
Batch  31  loss:  0.0018562771147117019
Batch  41  loss:  0.001894598244689405
Batch  51  loss:  0.0015659010969102383
Batch  61  loss:  0.001648339326493442
Batch  71  loss:  0.0021902944426983595
Batch  81  loss:  0.0020485427230596542
Batch  91  loss:  0.001915216213092208
Validation on real data: 
LOSS supervised-train 0.0017082959844265133, valid 0.0013912711292505264
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0015797724481672049
Batch  11  loss:  0.0015315767377614975
Batch  21  loss:  0.0013493001461029053
Batch  31  loss:  0.0018695954931899905
Batch  41  loss:  0.0013498027110472322
Batch  51  loss:  0.0016761991428211331
Batch  61  loss:  0.0017514565261080861
Batch  71  loss:  0.0013793280813843012
Batch  81  loss:  0.001833917573094368
Batch  91  loss:  0.0015086644561961293
Validation on real data: 
LOSS supervised-train 0.0016879119770601392, valid 0.0012111810501664877
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0014001595554873347
Batch  11  loss:  0.0018580153118818998
Batch  21  loss:  0.0021753220353275537
Batch  31  loss:  0.0022304996382445097
Batch  41  loss:  0.001216756529174745
Batch  51  loss:  0.0011455491185188293
Batch  61  loss:  0.0019280367996543646
Batch  71  loss:  0.0012831126805394888
Batch  81  loss:  0.001561263925395906
Batch  91  loss:  0.001483693951740861
Validation on real data: 
LOSS supervised-train 0.001687492416240275, valid 0.0011456427164375782
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0012955334968864918
Batch  11  loss:  0.0018409504555165768
Batch  21  loss:  0.001309471088461578
Batch  31  loss:  0.0016445337096229196
Batch  41  loss:  0.0014373312005773187
Batch  51  loss:  0.0014831371372565627
Batch  61  loss:  0.0014875800115987659
Batch  71  loss:  0.001573676709085703
Batch  81  loss:  0.002509517129510641
Batch  91  loss:  0.0016612043837085366
Validation on real data: 
LOSS supervised-train 0.0016499498509801925, valid 0.0013008208479732275
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0016122665256261826
Batch  11  loss:  0.0015750703169032931
Batch  21  loss:  0.002154011745005846
Batch  31  loss:  0.0014279065653681755
Batch  41  loss:  0.0015161107294261456
Batch  51  loss:  0.0012162328930571675
Batch  61  loss:  0.0015020542778074741
Batch  71  loss:  0.0014945981092751026
Batch  81  loss:  0.0017894699703902006
Batch  91  loss:  0.0015048080822452903
Validation on real data: 
LOSS supervised-train 0.0016733083548024297, valid 0.0011688293889164925
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.001135694794356823
Batch  11  loss:  0.001754538039676845
Batch  21  loss:  0.0019209057791158557
Batch  31  loss:  0.002221412258222699
Batch  41  loss:  0.0014519186224788427
Batch  51  loss:  0.0013534431345760822
Batch  61  loss:  0.0018451204523444176
Batch  71  loss:  0.0016499836929142475
Batch  81  loss:  0.0013398018199950457
Batch  91  loss:  0.002082608640193939
Validation on real data: 
LOSS supervised-train 0.0016649514436721802, valid 0.001085697440430522
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.001263386569917202
Batch  11  loss:  0.0018081828020513058
Batch  21  loss:  0.0014176403637975454
Batch  31  loss:  0.0019856051076203585
Batch  41  loss:  0.0015923548489809036
Batch  51  loss:  0.0015219884226098657
Batch  61  loss:  0.0017086996231228113
Batch  71  loss:  0.00177623494528234
Batch  81  loss:  0.0017950348556041718
Batch  91  loss:  0.0014750361442565918
Validation on real data: 
LOSS supervised-train 0.0016657183051574974, valid 0.001308112172409892
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.001117551582865417
Batch  11  loss:  0.001553800655528903
Batch  21  loss:  0.0024501271545886993
Batch  31  loss:  0.0019986291881650686
Batch  41  loss:  0.0012661300133913755
Batch  51  loss:  0.0011880665551871061
Batch  61  loss:  0.0016051733400672674
Batch  71  loss:  0.001829782035201788
Batch  81  loss:  0.0018771592294797301
Batch  91  loss:  0.0016210111789405346
Validation on real data: 
LOSS supervised-train 0.001651592447888106, valid 0.0009511400712653995
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0012808784376829863
Batch  11  loss:  0.001540465047582984
Batch  21  loss:  0.0020001858938485384
Batch  31  loss:  0.002166311489418149
Batch  41  loss:  0.0013176932698115706
Batch  51  loss:  0.0012344929855316877
Batch  61  loss:  0.0014671259559690952
Batch  71  loss:  0.0017914974596351385
Batch  81  loss:  0.0016680866247043014
Batch  91  loss:  0.0014355790335685015
Validation on real data: 
LOSS supervised-train 0.001647455431520939, valid 0.0012417190009728074
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0011906244326382875
Batch  11  loss:  0.0018246042309328914
Batch  21  loss:  0.0016041439957916737
Batch  31  loss:  0.0018924501491710544
Batch  41  loss:  0.0018400752451270819
Batch  51  loss:  0.0012592850252985954
Batch  61  loss:  0.0020666855853050947
Batch  71  loss:  0.0016151999589055777
Batch  81  loss:  0.0021074924152344465
Batch  91  loss:  0.0017717555165290833
Validation on real data: 
LOSS supervised-train 0.0016287551994901151, valid 0.0011194482212886214
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0015432159416377544
Batch  11  loss:  0.001665298012085259
Batch  21  loss:  0.0014877025969326496
Batch  31  loss:  0.0017367128748446703
Batch  41  loss:  0.001214794465340674
Batch  51  loss:  0.001319498405791819
Batch  61  loss:  0.0017151759238913655
Batch  71  loss:  0.0013290533097460866
Batch  81  loss:  0.0019565289840102196
Batch  91  loss:  0.0014752426650375128
Validation on real data: 
LOSS supervised-train 0.0016136842337436974, valid 0.0011092378990724683
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.001404903014190495
Batch  11  loss:  0.0017341990023851395
Batch  21  loss:  0.0017989333719015121
Batch  31  loss:  0.0016931681893765926
Batch  41  loss:  0.0010062181390821934
Batch  51  loss:  0.001485549728386104
Batch  61  loss:  0.0014105763984844089
Batch  71  loss:  0.0013674646615982056
Batch  81  loss:  0.0017879073275253177
Batch  91  loss:  0.0016519727651029825
Validation on real data: 
LOSS supervised-train 0.001593597485916689, valid 0.0011229256633669138
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0016882180934771895
Batch  11  loss:  0.002029729774221778
Batch  21  loss:  0.0018074082909151912
Batch  31  loss:  0.002011161530390382
Batch  41  loss:  0.0017026992281898856
Batch  51  loss:  0.0013292264193296432
Batch  61  loss:  0.0017933165654540062
Batch  71  loss:  0.001483422820456326
Batch  81  loss:  0.001896696980111301
Batch  91  loss:  0.0016431781696155667
Validation on real data: 
LOSS supervised-train 0.0016326523176394402, valid 0.0011477561201900244
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.001542967977002263
Batch  11  loss:  0.001625890377908945
Batch  21  loss:  0.001410011202096939
Batch  31  loss:  0.0019991029985249043
Batch  41  loss:  0.0014442643150687218
Batch  51  loss:  0.0012626394163817167
Batch  61  loss:  0.0015473352978006005
Batch  71  loss:  0.0015122992917895317
Batch  81  loss:  0.001866661012172699
Batch  91  loss:  0.0013910916168242693
Validation on real data: 
LOSS supervised-train 0.0015663741633761673, valid 0.0011459288652986288
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0013585202395915985
Batch  11  loss:  0.001702341134659946
Batch  21  loss:  0.0014843610115349293
Batch  31  loss:  0.001816081115975976
Batch  41  loss:  0.0017289300449192524
Batch  51  loss:  0.0015525022754445672
Batch  61  loss:  0.0017278972081840038
Batch  71  loss:  0.0017504120478406549
Batch  81  loss:  0.002062529092654586
Batch  91  loss:  0.001470377086661756
Validation on real data: 
LOSS supervised-train 0.0015519411501009018, valid 0.0011836262419819832
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0011839166982099414
Batch  11  loss:  0.0017155128298327327
Batch  21  loss:  0.0016730445204302669
Batch  31  loss:  0.002501807641237974
Batch  41  loss:  0.001308400183916092
Batch  51  loss:  0.0015009755734354258
Batch  61  loss:  0.0017586186295375228
Batch  71  loss:  0.001329102204181254
Batch  81  loss:  0.001584684825502336
Batch  91  loss:  0.0012679799692705274
Validation on real data: 
LOSS supervised-train 0.0015871072985464707, valid 0.0016451844712719321
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0014703684719279408
Batch  11  loss:  0.001621108502149582
Batch  21  loss:  0.0016558263450860977
Batch  31  loss:  0.0014640287263318896
Batch  41  loss:  0.0015049936482682824
Batch  51  loss:  0.0011381562799215317
Batch  61  loss:  0.0016374452970921993
Batch  71  loss:  0.00138957763556391
Batch  81  loss:  0.0017971224151551723
Batch  91  loss:  0.0016506469110026956
Validation on real data: 
LOSS supervised-train 0.0015727441059425473, valid 0.00134942471049726
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0010688677430152893
Batch  11  loss:  0.001883177668787539
Batch  21  loss:  0.0017110401531681418
Batch  31  loss:  0.0014386232942342758
Batch  41  loss:  0.0011554397642612457
Batch  51  loss:  0.0012695541372522712
Batch  61  loss:  0.0015416619135066867
Batch  71  loss:  0.0010783030884340405
Batch  81  loss:  0.0015765774296596646
Batch  91  loss:  0.0016705362359061837
Validation on real data: 
LOSS supervised-train 0.001540256526786834, valid 0.0011718812165781856
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0014587723417207599
Batch  11  loss:  0.0017881400417536497
Batch  21  loss:  0.0016906908713281155
Batch  31  loss:  0.001596964430063963
Batch  41  loss:  0.0012949067167937756
Batch  51  loss:  0.001651361002586782
Batch  61  loss:  0.0015160414623096585
Batch  71  loss:  0.001566450810059905
Batch  81  loss:  0.001458619488403201
Batch  91  loss:  0.0015304955886676908
Validation on real data: 
LOSS supervised-train 0.0015387389599345624, valid 0.0014852010644972324
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0013334618415683508
Batch  11  loss:  0.001244544517248869
Batch  21  loss:  0.0018411553464829922
Batch  31  loss:  0.0015021807048469782
Batch  41  loss:  0.0015400872798636556
Batch  51  loss:  0.0018791351467370987
Batch  61  loss:  0.0021729881409555674
Batch  71  loss:  0.001685381750576198
Batch  81  loss:  0.001655084197409451
Batch  91  loss:  0.0018223518272861838
Validation on real data: 
LOSS supervised-train 0.0015113427274627612, valid 0.00108815124258399
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0011895966017618775
Batch  11  loss:  0.0015040754806250334
Batch  21  loss:  0.0015625229571014643
Batch  31  loss:  0.001787786721251905
Batch  41  loss:  0.0014837440103292465
Batch  51  loss:  0.0016036602901294827
Batch  61  loss:  0.001550512621179223
Batch  71  loss:  0.00182828598190099
Batch  81  loss:  0.0014356396859511733
Batch  91  loss:  0.0019504828378558159
Validation on real data: 
LOSS supervised-train 0.00152460515324492, valid 0.001114606042392552
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0012785064755007625
Batch  11  loss:  0.0013760268921032548
Batch  21  loss:  0.0019760450813919306
Batch  31  loss:  0.0014029222074896097
Batch  41  loss:  0.001235958538018167
Batch  51  loss:  0.0009926168713718653
Batch  61  loss:  0.0018600135808810592
Batch  71  loss:  0.0013570549199357629
Batch  81  loss:  0.0016656153602525592
Batch  91  loss:  0.001472759642638266
Validation on real data: 
LOSS supervised-train 0.0015481029986403883, valid 0.0014705309877172112
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.001741877757012844
Batch  11  loss:  0.0015025862958282232
Batch  21  loss:  0.0014617874985560775
Batch  31  loss:  0.001938022207468748
Batch  41  loss:  0.0014585336903110147
Batch  51  loss:  0.001062067924067378
Batch  61  loss:  0.0012910820078104734
Batch  71  loss:  0.0015315100317820907
Batch  81  loss:  0.001701605855487287
Batch  91  loss:  0.0012800892582163215
Validation on real data: 
LOSS supervised-train 0.001505108114797622, valid 0.0010427297092974186
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0013483738293871284
Batch  11  loss:  0.001495129894465208
Batch  21  loss:  0.0011478032683953643
Batch  31  loss:  0.0015621936181560159
Batch  41  loss:  0.0013958335621282458
Batch  51  loss:  0.0014605721225962043
Batch  61  loss:  0.001187168643809855
Batch  71  loss:  0.0016849514795467257
Batch  81  loss:  0.0015851850621402264
Batch  91  loss:  0.0017097077798098326
Validation on real data: 
LOSS supervised-train 0.0015241009392775596, valid 0.001254135393537581
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0014373986050486565
Batch  11  loss:  0.0013907334068790078
Batch  21  loss:  0.0018086722120642662
Batch  31  loss:  0.0017273672856390476
Batch  41  loss:  0.0014380373759195209
Batch  51  loss:  0.001209221431054175
Batch  61  loss:  0.0016557207563892007
Batch  71  loss:  0.0013241887791082263
Batch  81  loss:  0.0016810974339023232
Batch  91  loss:  0.0013441434130072594
Validation on real data: 
LOSS supervised-train 0.0014573251968249678, valid 0.0010647568851709366
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0016252916539087892
Batch  11  loss:  0.0019324937602505088
Batch  21  loss:  0.001650194637477398
Batch  31  loss:  0.0019479509210214019
Batch  41  loss:  0.001576066599227488
Batch  51  loss:  0.0015764135168865323
Batch  61  loss:  0.0015884735621511936
Batch  71  loss:  0.0012470409274101257
Batch  81  loss:  0.001613967353478074
Batch  91  loss:  0.0016043346840888262
Validation on real data: 
LOSS supervised-train 0.0015018632891587913, valid 0.0012196603929623961
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0012955787824466825
Batch  11  loss:  0.001440151478163898
Batch  21  loss:  0.0018187641398981214
Batch  31  loss:  0.0014623505994677544
Batch  41  loss:  0.0013543660752475262
Batch  51  loss:  0.0015045484760776162
Batch  61  loss:  0.0013277916004881263
Batch  71  loss:  0.00123487482778728
Batch  81  loss:  0.0013114288449287415
Batch  91  loss:  0.0015716972993686795
Validation on real data: 
LOSS supervised-train 0.0014653981605079025, valid 0.0012206684332340956
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0012941878521814942
Batch  11  loss:  0.0019390515517443419
Batch  21  loss:  0.0017885854467749596
Batch  31  loss:  0.0016479080077260733
Batch  41  loss:  0.0014396993210539222
Batch  51  loss:  0.0011899566743522882
Batch  61  loss:  0.001322810654528439
Batch  71  loss:  0.0012474010000005364
Batch  81  loss:  0.001460975967347622
Batch  91  loss:  0.0014100134139880538
Validation on real data: 
LOSS supervised-train 0.0014607626415090635, valid 0.001308882492594421
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0011562047293409705
Batch  11  loss:  0.001327984151430428
Batch  21  loss:  0.0013866627123206854
Batch  31  loss:  0.001576591981574893
Batch  41  loss:  0.001336261397227645
Batch  51  loss:  0.0015816104132682085
Batch  61  loss:  0.0018011311767622828
Batch  71  loss:  0.001521131256595254
Batch  81  loss:  0.0015123834600672126
Batch  91  loss:  0.001555935712531209
Validation on real data: 
LOSS supervised-train 0.0014627161039970815, valid 0.0014382757944986224
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0013466804521158338
Batch  11  loss:  0.0012181747006252408
Batch  21  loss:  0.0013401306932792068
Batch  31  loss:  0.001485000248067081
Batch  41  loss:  0.001152064767666161
Batch  51  loss:  0.0015505884075537324
Batch  61  loss:  0.0016781099839136004
Batch  71  loss:  0.0010388443479314446
Batch  81  loss:  0.0014269087696447968
Batch  91  loss:  0.0011187122436240315
Validation on real data: 
LOSS supervised-train 0.0014274098171154037, valid 0.0011730999685823917
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0013244450092315674
Batch  11  loss:  0.0018443926237523556
Batch  21  loss:  0.0020374113228172064
Batch  31  loss:  0.0017740019829943776
Batch  41  loss:  0.0012597462628036737
Batch  51  loss:  0.000997082912363112
Batch  61  loss:  0.0015165754593908787
Batch  71  loss:  0.0015301951207220554
Batch  81  loss:  0.0016536313341930509
Batch  91  loss:  0.0012702607782557607
Validation on real data: 
LOSS supervised-train 0.0014914493344258518, valid 0.001240793033502996
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0010528330458328128
Batch  11  loss:  0.0015409054467454553
Batch  21  loss:  0.0012438285630196333
Batch  31  loss:  0.0017646956257522106
Batch  41  loss:  0.0013389169471338391
Batch  51  loss:  0.0020715000573545694
Batch  61  loss:  0.0018485785694792867
Batch  71  loss:  0.001648582168854773
Batch  81  loss:  0.0021200880873948336
Batch  91  loss:  0.0016009723767638206
Validation on real data: 
LOSS supervised-train 0.0014346744742942973, valid 0.0010734978131949902
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0016013747081160545
Batch  11  loss:  0.00186526111792773
Batch  21  loss:  0.0013159748632460833
Batch  31  loss:  0.0015050888760015368
Batch  41  loss:  0.0011766289826482534
Batch  51  loss:  0.0012050216319039464
Batch  61  loss:  0.0016080504283308983
Batch  71  loss:  0.0014749985421076417
Batch  81  loss:  0.001447192276827991
Batch  91  loss:  0.0013391756219789386
Validation on real data: 
LOSS supervised-train 0.0014490773383295163, valid 0.0010773991234600544
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0012608201941475272
Batch  11  loss:  0.0011851036688312888
Batch  21  loss:  0.0015021400758996606
Batch  31  loss:  0.0018200598424300551
Batch  41  loss:  0.0010959733044728637
Batch  51  loss:  0.001112364698201418
Batch  61  loss:  0.0013541692169383168
Batch  71  loss:  0.001672789454460144
Batch  81  loss:  0.0018291001906618476
Batch  91  loss:  0.0012445271713659167
Validation on real data: 
LOSS supervised-train 0.0014188988669775427, valid 0.0009386971942149103
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0012236953480169177
Batch  11  loss:  0.0014724385691806674
Batch  21  loss:  0.001811724272556603
Batch  31  loss:  0.0011253476841375232
Batch  41  loss:  0.0011983844451606274
Batch  51  loss:  0.0009254633914679289
Batch  61  loss:  0.0016226571751758456
Batch  71  loss:  0.0017825019313022494
Batch  81  loss:  0.0017260313034057617
Batch  91  loss:  0.0010932311415672302
Validation on real data: 
LOSS supervised-train 0.0013928105926606805, valid 0.0010351333767175674
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0014512741472572088
Batch  11  loss:  0.0012811812339350581
Batch  21  loss:  0.0014854713808745146
Batch  31  loss:  0.0016550077125430107
Batch  41  loss:  0.0015130954561755061
Batch  51  loss:  0.001440637861378491
Batch  61  loss:  0.0014872702304273844
Batch  71  loss:  0.0011594005627557635
Batch  81  loss:  0.0018143006600439548
Batch  91  loss:  0.0014118212275207043
Validation on real data: 
LOSS supervised-train 0.001409995516296476, valid 0.0012985607609152794
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0010564292315393686
Batch  11  loss:  0.0015951440436765552
Batch  21  loss:  0.001438261941075325
Batch  31  loss:  0.0013517009792849422
Batch  41  loss:  0.0011193474056199193
Batch  51  loss:  0.0013591658789664507
Batch  61  loss:  0.0014158415142446756
Batch  71  loss:  0.0013525014510378242
Batch  81  loss:  0.001890399376861751
Batch  91  loss:  0.0010429846588522196
Validation on real data: 
LOSS supervised-train 0.0014112293039215728, valid 0.0011270323302596807
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0013746534241363406
Batch  11  loss:  0.0014591669896617532
Batch  21  loss:  0.0013357141287997365
Batch  31  loss:  0.0016546028200536966
Batch  41  loss:  0.0012536817230284214
Batch  51  loss:  0.0012107740622013807
Batch  61  loss:  0.0016645959112793207
Batch  71  loss:  0.0014338040491566062
Batch  81  loss:  0.0014625431504100561
Batch  91  loss:  0.0013875120785087347
Validation on real data: 
LOSS supervised-train 0.0014149925473611802, valid 0.001132198958657682
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0010631219483911991
Batch  11  loss:  0.0013124605175107718
Batch  21  loss:  0.001372053986415267
Batch  31  loss:  0.0015562470071017742
Batch  41  loss:  0.0014089848846197128
Batch  51  loss:  0.0012081359745934606
Batch  61  loss:  0.0015393629437312484
Batch  71  loss:  0.0017388734268024564
Batch  81  loss:  0.0012817239621654153
Batch  91  loss:  0.0015554476995021105
Validation on real data: 
LOSS supervised-train 0.0014016962738242, valid 0.001435738173313439
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0011092792265117168
Batch  11  loss:  0.0013603770639747381
Batch  21  loss:  0.0017018243670463562
Batch  31  loss:  0.0013724733144044876
Batch  41  loss:  0.0013274260563775897
Batch  51  loss:  0.0012165094958618283
Batch  61  loss:  0.0015102006727829576
Batch  71  loss:  0.0010836200090125203
Batch  81  loss:  0.0016597381327301264
Batch  91  loss:  0.001346903620287776
Validation on real data: 
LOSS supervised-train 0.0014353765081614256, valid 0.0011094113579019904
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0011065900325775146
Batch  11  loss:  0.001064041629433632
Batch  21  loss:  0.0015114442212507129
Batch  31  loss:  0.001639824127778411
Batch  41  loss:  0.001054316759109497
Batch  51  loss:  0.0012620827183127403
Batch  61  loss:  0.0012261888477951288
Batch  71  loss:  0.0019053089199587703
Batch  81  loss:  0.0018721955129876733
Batch  91  loss:  0.0017124420264735818
Validation on real data: 
LOSS supervised-train 0.0013901835156138987, valid 0.001041633659042418
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  chair ; Model ID: 1cc6f2ed3d684fa245f213b8994b4a04
--------------------
Training baseline regression model:  2022-03-30 15:11:24.154479
Detector:  pointnet
Object:  chair
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1611559
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.2852618396282196
Batch  11  loss:  0.074216827750206
Batch  21  loss:  0.12104106694459915
Batch  31  loss:  0.03295650705695152
Batch  41  loss:  0.0459391288459301
Batch  51  loss:  0.025742916390299797
Batch  61  loss:  0.013966092839837074
Batch  71  loss:  0.03965315967798233
Batch  81  loss:  0.01215489860624075
Batch  91  loss:  0.010224433615803719
Validation on real data: 
LOSS supervised-train 0.04532153500244021, valid 0.012644483707845211
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.007246260531246662
Batch  11  loss:  0.011332232505083084
Batch  21  loss:  0.013462386094033718
Batch  31  loss:  0.007951316423714161
Batch  41  loss:  0.01192392222583294
Batch  51  loss:  0.00531357154250145
Batch  61  loss:  0.005945016164332628
Batch  71  loss:  0.006979710888117552
Batch  81  loss:  0.006009635049849749
Batch  91  loss:  0.007115927059203386
Validation on real data: 
LOSS supervised-train 0.007429290874861181, valid 0.004421469289809465
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.00553036667406559
Batch  11  loss:  0.007892584428191185
Batch  21  loss:  0.007951205596327782
Batch  31  loss:  0.00551625294610858
Batch  41  loss:  0.009055592119693756
Batch  51  loss:  0.006449310574680567
Batch  61  loss:  0.004572814796119928
Batch  71  loss:  0.005666437558829784
Batch  81  loss:  0.004053112585097551
Batch  91  loss:  0.004199684131890535
Validation on real data: 
LOSS supervised-train 0.005676694698631763, valid 0.0028633701149374247
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.004420910961925983
Batch  11  loss:  0.00603772746399045
Batch  21  loss:  0.005885659251362085
Batch  31  loss:  0.005767931696027517
Batch  41  loss:  0.009250589646399021
Batch  51  loss:  0.004274128004908562
Batch  61  loss:  0.00422147661447525
Batch  71  loss:  0.00425625778734684
Batch  81  loss:  0.0031401030719280243
Batch  91  loss:  0.0037187456618994474
Validation on real data: 
LOSS supervised-train 0.004686597522813827, valid 0.003963326569646597
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.004225841257721186
Batch  11  loss:  0.00561119057238102
Batch  21  loss:  0.004300463479012251
Batch  31  loss:  0.0038600265979766846
Batch  41  loss:  0.009964684955775738
Batch  51  loss:  0.0034352648071944714
Batch  61  loss:  0.003199330996721983
Batch  71  loss:  0.0035848640836775303
Batch  81  loss:  0.0025006006471812725
Batch  91  loss:  0.0031759273260831833
Validation on real data: 
LOSS supervised-train 0.004123518485575914, valid 0.00234776153229177
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0034256884828209877
Batch  11  loss:  0.0037792047951370478
Batch  21  loss:  0.003951807040721178
Batch  31  loss:  0.004963749088346958
Batch  41  loss:  0.007764953654259443
Batch  51  loss:  0.003876470262184739
Batch  61  loss:  0.0026980796828866005
Batch  71  loss:  0.004617420956492424
Batch  81  loss:  0.003763623535633087
Batch  91  loss:  0.003238306613638997
Validation on real data: 
LOSS supervised-train 0.0037040379690006374, valid 0.0020651284139603376
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.002974586095660925
Batch  11  loss:  0.004254616796970367
Batch  21  loss:  0.0033874630462378263
Batch  31  loss:  0.00440463051199913
Batch  41  loss:  0.007772104348987341
Batch  51  loss:  0.002486444776877761
Batch  61  loss:  0.0026297341100871563
Batch  71  loss:  0.0036970870569348335
Batch  81  loss:  0.0026523731648921967
Batch  91  loss:  0.0027429526671767235
Validation on real data: 
LOSS supervised-train 0.0035488169803284107, valid 0.002228408819064498
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.001917235553264618
Batch  11  loss:  0.0035845832899212837
Batch  21  loss:  0.0033459907863289118
Batch  31  loss:  0.004210556857287884
Batch  41  loss:  0.008138610050082207
Batch  51  loss:  0.002931386698037386
Batch  61  loss:  0.001981386449187994
Batch  71  loss:  0.0035058825742453337
Batch  81  loss:  0.0026132522616535425
Batch  91  loss:  0.003627089550718665
Validation on real data: 
LOSS supervised-train 0.0033139899279922246, valid 0.0018489371286705136
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0020282662007957697
Batch  11  loss:  0.0038477969355881214
Batch  21  loss:  0.003003970952704549
Batch  31  loss:  0.003536358941346407
Batch  41  loss:  0.008355318568646908
Batch  51  loss:  0.003807883244007826
Batch  61  loss:  0.002207414247095585
Batch  71  loss:  0.003581488039344549
Batch  81  loss:  0.0018575831782072783
Batch  91  loss:  0.0027213965076953173
Validation on real data: 
LOSS supervised-train 0.0030237124976702032, valid 0.0015744739212095737
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0023726073559373617
Batch  11  loss:  0.0032958867959678173
Batch  21  loss:  0.0034142041113227606
Batch  31  loss:  0.00395421776920557
Batch  41  loss:  0.005831239279359579
Batch  51  loss:  0.0019776728004217148
Batch  61  loss:  0.0019561953376978636
Batch  71  loss:  0.0026646077167242765
Batch  81  loss:  0.0024948380887508392
Batch  91  loss:  0.0026906488928943872
Validation on real data: 
LOSS supervised-train 0.0029344549984671175, valid 0.001072303275577724
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0024112712126225233
Batch  11  loss:  0.0028282874263823032
Batch  21  loss:  0.0023712217807769775
Batch  31  loss:  0.004928361624479294
Batch  41  loss:  0.006085092667490244
Batch  51  loss:  0.002103323582559824
Batch  61  loss:  0.002695550210773945
Batch  71  loss:  0.002508648671209812
Batch  81  loss:  0.0016855173744261265
Batch  91  loss:  0.0027861392591148615
Validation on real data: 
LOSS supervised-train 0.0026870770240202545, valid 0.001014753244817257
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0020983214490115643
Batch  11  loss:  0.0032659200951457024
Batch  21  loss:  0.0019882447086274624
Batch  31  loss:  0.0035050329752266407
Batch  41  loss:  0.00584090081974864
Batch  51  loss:  0.0026278975419700146
Batch  61  loss:  0.0020870899315923452
Batch  71  loss:  0.003377141896635294
Batch  81  loss:  0.002071966417133808
Batch  91  loss:  0.0024075352121144533
Validation on real data: 
LOSS supervised-train 0.002674839576939121, valid 0.0013281869469210505
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0021980705205351114
Batch  11  loss:  0.0022812967654317617
Batch  21  loss:  0.0026645432226359844
Batch  31  loss:  0.0029597121756523848
Batch  41  loss:  0.0038594084326177835
Batch  51  loss:  0.001860776450484991
Batch  61  loss:  0.0017138704424723983
Batch  71  loss:  0.003302701050415635
Batch  81  loss:  0.0018634790321812034
Batch  91  loss:  0.003311234526336193
Validation on real data: 
LOSS supervised-train 0.002626061941264197, valid 0.0007826620130799711
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0021754885092377663
Batch  11  loss:  0.0028707049787044525
Batch  21  loss:  0.0026418662164360285
Batch  31  loss:  0.0028116931207478046
Batch  41  loss:  0.006133145187050104
Batch  51  loss:  0.0016675959341228008
Batch  61  loss:  0.0023411617148667574
Batch  71  loss:  0.002622077940031886
Batch  81  loss:  0.0019488079706206918
Batch  91  loss:  0.0017871076706796885
Validation on real data: 
LOSS supervised-train 0.0025193138991016894, valid 0.0008163861930370331
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0022691688500344753
Batch  11  loss:  0.0041722883470356464
Batch  21  loss:  0.002901726868003607
Batch  31  loss:  0.003122557420283556
Batch  41  loss:  0.004367413930594921
Batch  51  loss:  0.0019976506009697914
Batch  61  loss:  0.002374129369854927
Batch  71  loss:  0.0021887102629989386
Batch  81  loss:  0.0024525693152099848
Batch  91  loss:  0.002737321425229311
Validation on real data: 
LOSS supervised-train 0.002466360256075859, valid 0.0007183569250628352
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.00197541993111372
Batch  11  loss:  0.002525149844586849
Batch  21  loss:  0.0025566357653588057
Batch  31  loss:  0.002663475926965475
Batch  41  loss:  0.006916110869497061
Batch  51  loss:  0.0021323123946785927
Batch  61  loss:  0.0018537620780989528
Batch  71  loss:  0.002209206111729145
Batch  81  loss:  0.001575692556798458
Batch  91  loss:  0.002450522966682911
Validation on real data: 
LOSS supervised-train 0.002429846411105245, valid 0.0006149833207018673
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0024300713557749987
Batch  11  loss:  0.002577137667685747
Batch  21  loss:  0.002019562292844057
Batch  31  loss:  0.0019720131531357765
Batch  41  loss:  0.005013767164200544
Batch  51  loss:  0.0015134885907173157
Batch  61  loss:  0.0014296586159616709
Batch  71  loss:  0.0024607405066490173
Batch  81  loss:  0.001734436140395701
Batch  91  loss:  0.002261450281366706
Validation on real data: 
LOSS supervised-train 0.002355248931562528, valid 0.0007380728493444622
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0014864474069327116
Batch  11  loss:  0.002871840726584196
Batch  21  loss:  0.0022964610252529383
Batch  31  loss:  0.0030910803470760584
Batch  41  loss:  0.004321020096540451
Batch  51  loss:  0.001534117734991014
Batch  61  loss:  0.0018199842888861895
Batch  71  loss:  0.0022321583237499
Batch  81  loss:  0.0016202771803364158
Batch  91  loss:  0.0020945146679878235
Validation on real data: 
LOSS supervised-train 0.002238899413496256, valid 0.0011939550749957561
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0017106389859691262
Batch  11  loss:  0.00302676809951663
Batch  21  loss:  0.0019263275898993015
Batch  31  loss:  0.0024405126459896564
Batch  41  loss:  0.005509558599442244
Batch  51  loss:  0.001328449696302414
Batch  61  loss:  0.0014576943358406425
Batch  71  loss:  0.0023507806472480297
Batch  81  loss:  0.0012766183353960514
Batch  91  loss:  0.0020157387480139732
Validation on real data: 
LOSS supervised-train 0.002186005171388388, valid 0.0007198635139502585
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0018946918426081538
Batch  11  loss:  0.0021788054145872593
Batch  21  loss:  0.0018148567760363221
Batch  31  loss:  0.0028499651234596968
Batch  41  loss:  0.005845747888088226
Batch  51  loss:  0.0019385562045499682
Batch  61  loss:  0.0019221986876800656
Batch  71  loss:  0.0016581633826717734
Batch  81  loss:  0.0015653203008696437
Batch  91  loss:  0.0017956193769350648
Validation on real data: 
LOSS supervised-train 0.0021862535283435135, valid 0.0005917696398682892
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0016439234605059028
Batch  11  loss:  0.0024734847247600555
Batch  21  loss:  0.0025717359967529774
Batch  31  loss:  0.002789431717246771
Batch  41  loss:  0.006150730885565281
Batch  51  loss:  0.0014528515748679638
Batch  61  loss:  0.0016508917324244976
Batch  71  loss:  0.0026014274917542934
Batch  81  loss:  0.001987369731068611
Batch  91  loss:  0.0031281006522476673
Validation on real data: 
LOSS supervised-train 0.0022179168777074665, valid 0.0008233403787016869
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0018342420225962996
Batch  11  loss:  0.0022907897364348173
Batch  21  loss:  0.0021576720755547285
Batch  31  loss:  0.00243155169300735
Batch  41  loss:  0.006719070486724377
Batch  51  loss:  0.0012131427647545934
Batch  61  loss:  0.0017581024440005422
Batch  71  loss:  0.002798263682052493
Batch  81  loss:  0.0014637891435995698
Batch  91  loss:  0.0016917524626478553
Validation on real data: 
LOSS supervised-train 0.0020749718614388258, valid 0.0007975567132234573
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0016743832966312766
Batch  11  loss:  0.0018407288007438183
Batch  21  loss:  0.0019233989296481013
Batch  31  loss:  0.0021069692447781563
Batch  41  loss:  0.006755165755748749
Batch  51  loss:  0.0017210292862728238
Batch  61  loss:  0.0019357630517333746
Batch  71  loss:  0.0022896192967891693
Batch  81  loss:  0.0014397678896784782
Batch  91  loss:  0.002164660720154643
Validation on real data: 
LOSS supervised-train 0.002085303892381489, valid 0.0007045198581181467
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0015294437762349844
Batch  11  loss:  0.0017507770098745823
Batch  21  loss:  0.0028342371806502342
Batch  31  loss:  0.002324806759133935
Batch  41  loss:  0.004812266211956739
Batch  51  loss:  0.0015524789923802018
Batch  61  loss:  0.0016029708785936236
Batch  71  loss:  0.0015848365146666765
Batch  81  loss:  0.001556751667521894
Batch  91  loss:  0.0015752987237647176
Validation on real data: 
LOSS supervised-train 0.001984713915735483, valid 0.0006884804461151361
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.001913090469315648
Batch  11  loss:  0.0016222416888922453
Batch  21  loss:  0.0016477906610816717
Batch  31  loss:  0.002016258193179965
Batch  41  loss:  0.004605887923389673
Batch  51  loss:  0.0011819713981822133
Batch  61  loss:  0.0014989585615694523
Batch  71  loss:  0.0018570973770692945
Batch  81  loss:  0.0014552712673321366
Batch  91  loss:  0.0015342168044298887
Validation on real data: 
LOSS supervised-train 0.001971454567974433, valid 0.00061732780886814
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.001603399752639234
Batch  11  loss:  0.0020447480492293835
Batch  21  loss:  0.0020302352495491505
Batch  31  loss:  0.002129464177414775
Batch  41  loss:  0.006309026386588812
Batch  51  loss:  0.0013967887498438358
Batch  61  loss:  0.0017727019730955362
Batch  71  loss:  0.002334672724828124
Batch  81  loss:  0.0011625002371147275
Batch  91  loss:  0.0019154599867761135
Validation on real data: 
LOSS supervised-train 0.0019210134050808847, valid 0.0005477683153003454
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0014703837223351002
Batch  11  loss:  0.0021698621567338705
Batch  21  loss:  0.002420692704617977
Batch  31  loss:  0.0019753584638237953
Batch  41  loss:  0.008243421092629433
Batch  51  loss:  0.001353053143247962
Batch  61  loss:  0.0014900017995387316
Batch  71  loss:  0.0018380354158580303
Batch  81  loss:  0.001436114078387618
Batch  91  loss:  0.0018486719345673919
Validation on real data: 
LOSS supervised-train 0.0018569189857225866, valid 0.0007105707773007452
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0015240275533869863
Batch  11  loss:  0.0017540865810588002
Batch  21  loss:  0.0016049843979999423
Batch  31  loss:  0.0020047430880367756
Batch  41  loss:  0.004097744356840849
Batch  51  loss:  0.0012149304384365678
Batch  61  loss:  0.0016114949248731136
Batch  71  loss:  0.0025091622956097126
Batch  81  loss:  0.00139829539693892
Batch  91  loss:  0.001830462133511901
Validation on real data: 
LOSS supervised-train 0.0019170432433020324, valid 0.0006212601438164711
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.001314504537731409
Batch  11  loss:  0.0012009934289380908
Batch  21  loss:  0.0016514485469087958
Batch  31  loss:  0.002257046289741993
Batch  41  loss:  0.005940825212746859
Batch  51  loss:  0.001244274084456265
Batch  61  loss:  0.0009760350803844631
Batch  71  loss:  0.002284072106704116
Batch  81  loss:  0.001339490874670446
Batch  91  loss:  0.0014323341893032193
Validation on real data: 
LOSS supervised-train 0.0018392489803954958, valid 0.0005547168548218906
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0012776588555425406
Batch  11  loss:  0.0015728091821074486
Batch  21  loss:  0.00205849576741457
Batch  31  loss:  0.0021480515133589506
Batch  41  loss:  0.004737446550279856
Batch  51  loss:  0.0011637823190540075
Batch  61  loss:  0.0013033865252509713
Batch  71  loss:  0.0020378364715725183
Batch  81  loss:  0.0014526229351758957
Batch  91  loss:  0.001715191756375134
Validation on real data: 
LOSS supervised-train 0.0018513100768905134, valid 0.0005568880005739629
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0012727450812235475
Batch  11  loss:  0.0020144470036029816
Batch  21  loss:  0.0014917738735675812
Batch  31  loss:  0.0023941132239997387
Batch  41  loss:  0.0047638677060604095
Batch  51  loss:  0.001060326467268169
Batch  61  loss:  0.0013265300076454878
Batch  71  loss:  0.0021761548705399036
Batch  81  loss:  0.0017833765596151352
Batch  91  loss:  0.001625236589461565
Validation on real data: 
LOSS supervised-train 0.0017704165587201714, valid 0.00042749301064759493
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0015558534068986773
Batch  11  loss:  0.001589498482644558
Batch  21  loss:  0.002074355958029628
Batch  31  loss:  0.001869182800874114
Batch  41  loss:  0.003133657155558467
Batch  51  loss:  0.0013264401350170374
Batch  61  loss:  0.0017997442046180367
Batch  71  loss:  0.0022511747665703297
Batch  81  loss:  0.0012544800993055105
Batch  91  loss:  0.0015196343883872032
Validation on real data: 
LOSS supervised-train 0.0017657521174987778, valid 0.0006767537561245263
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0013509473064914346
Batch  11  loss:  0.0024324855767190456
Batch  21  loss:  0.0013425571378320456
Batch  31  loss:  0.001713902922347188
Batch  41  loss:  0.004252714570611715
Batch  51  loss:  0.001409644610248506
Batch  61  loss:  0.0012564928038045764
Batch  71  loss:  0.001513634342700243
Batch  81  loss:  0.0011049900203943253
Batch  91  loss:  0.001745170564390719
Validation on real data: 
LOSS supervised-train 0.0016799676610389724, valid 0.0007221069536171854
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0013417100999504328
Batch  11  loss:  0.0013699696864932775
Batch  21  loss:  0.0013487335527315736
Batch  31  loss:  0.0022830653470009565
Batch  41  loss:  0.002954239724203944
Batch  51  loss:  0.0012912192614749074
Batch  61  loss:  0.0013130703009665012
Batch  71  loss:  0.0013717798283323646
Batch  81  loss:  0.0016989867435768247
Batch  91  loss:  0.001820711768232286
Validation on real data: 
LOSS supervised-train 0.0017036989220650866, valid 0.0006275523919612169
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0013432332780212164
Batch  11  loss:  0.0017126263119280338
Batch  21  loss:  0.001989514334127307
Batch  31  loss:  0.0014050944009795785
Batch  41  loss:  0.0038327265065163374
Batch  51  loss:  0.0011638910509645939
Batch  61  loss:  0.0012083170004189014
Batch  71  loss:  0.001872133812867105
Batch  81  loss:  0.0020502556581050158
Batch  91  loss:  0.0017083920538425446
Validation on real data: 
LOSS supervised-train 0.0016786953440168873, valid 0.0006914395489729941
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.001260683755390346
Batch  11  loss:  0.0016372865065932274
Batch  21  loss:  0.0017532117199152708
Batch  31  loss:  0.001406148192472756
Batch  41  loss:  0.005339233670383692
Batch  51  loss:  0.0010870271362364292
Batch  61  loss:  0.0014131474308669567
Batch  71  loss:  0.0017642062157392502
Batch  81  loss:  0.0019310161005705595
Batch  91  loss:  0.0013990119332447648
Validation on real data: 
LOSS supervised-train 0.0016583490459015594, valid 0.0006794380024075508
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0019988692365586758
Batch  11  loss:  0.0017657732823863626
Batch  21  loss:  0.0014577360125258565
Batch  31  loss:  0.0020866282284259796
Batch  41  loss:  0.004239359404891729
Batch  51  loss:  0.000964521721471101
Batch  61  loss:  0.001088294549845159
Batch  71  loss:  0.0016255880473181605
Batch  81  loss:  0.0013284072047099471
Batch  91  loss:  0.0013654500944539905
Validation on real data: 
LOSS supervised-train 0.0016026114998385311, valid 0.0005390599835664034
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0011945818550884724
Batch  11  loss:  0.0018944862531498075
Batch  21  loss:  0.0021581014152616262
Batch  31  loss:  0.002449170919135213
Batch  41  loss:  0.003063059411942959
Batch  51  loss:  0.0010547038400545716
Batch  61  loss:  0.0009241070947609842
Batch  71  loss:  0.00197435705922544
Batch  81  loss:  0.0013545170659199357
Batch  91  loss:  0.0011935618240386248
Validation on real data: 
LOSS supervised-train 0.001621646913117729, valid 0.0005419377121143043
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0013995713088661432
Batch  11  loss:  0.0015543184708803892
Batch  21  loss:  0.0011580423451960087
Batch  31  loss:  0.0019553094170987606
Batch  41  loss:  0.005793864373117685
Batch  51  loss:  0.0007861932390369475
Batch  61  loss:  0.0013830664101988077
Batch  71  loss:  0.002286425093188882
Batch  81  loss:  0.0015921450685709715
Batch  91  loss:  0.0018859666306525469
Validation on real data: 
LOSS supervised-train 0.001664408024516888, valid 0.0005899796378798783
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0016242153942584991
Batch  11  loss:  0.0014909501187503338
Batch  21  loss:  0.0013554042670875788
Batch  31  loss:  0.002150209853425622
Batch  41  loss:  0.0033317182678729296
Batch  51  loss:  0.0009850646601989865
Batch  61  loss:  0.0012066637864336371
Batch  71  loss:  0.002238360233604908
Batch  81  loss:  0.0012291810708120465
Batch  91  loss:  0.0014864670811221004
Validation on real data: 
LOSS supervised-train 0.001591020958730951, valid 0.0004839141620323062
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0014311640989035368
Batch  11  loss:  0.0015457761473953724
Batch  21  loss:  0.001780544058419764
Batch  31  loss:  0.001593851251527667
Batch  41  loss:  0.006278307177126408
Batch  51  loss:  0.0011732142884284258
Batch  61  loss:  0.0011746310628950596
Batch  71  loss:  0.002023719483986497
Batch  81  loss:  0.0015439887065440416
Batch  91  loss:  0.0014105126028880477
Validation on real data: 
LOSS supervised-train 0.0015972359367879107, valid 0.0005382497911341488
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0012832570355385542
Batch  11  loss:  0.001107585383579135
Batch  21  loss:  0.0016273229848593473
Batch  31  loss:  0.0015848904149606824
Batch  41  loss:  0.005493451375514269
Batch  51  loss:  0.0013433718122541904
Batch  61  loss:  0.0014667403884232044
Batch  71  loss:  0.0016308763297274709
Batch  81  loss:  0.001261856989003718
Batch  91  loss:  0.0015842076390981674
Validation on real data: 
LOSS supervised-train 0.0015316772181540727, valid 0.0006534825079143047
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0012103193439543247
Batch  11  loss:  0.0011643223697319627
Batch  21  loss:  0.0016640592366456985
Batch  31  loss:  0.0022317804396152496
Batch  41  loss:  0.0032109494786709547
Batch  51  loss:  0.0012028987985104322
Batch  61  loss:  0.0013018279569223523
Batch  71  loss:  0.0018938505090773106
Batch  81  loss:  0.001052946550771594
Batch  91  loss:  0.0011635401751846075
Validation on real data: 
LOSS supervised-train 0.001539095230982639, valid 0.000618243997450918
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0011835682671517134
Batch  11  loss:  0.0013868986861780286
Batch  21  loss:  0.0013634867500513792
Batch  31  loss:  0.0018952455138787627
Batch  41  loss:  0.004234306048601866
Batch  51  loss:  0.0009452954982407391
Batch  61  loss:  0.0013800563756376505
Batch  71  loss:  0.0015115338610485196
Batch  81  loss:  0.0011028337758034468
Batch  91  loss:  0.0019547799602150917
Validation on real data: 
LOSS supervised-train 0.0015157662983983754, valid 0.000414730777265504
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0012834547087550163
Batch  11  loss:  0.001294851303100586
Batch  21  loss:  0.0015588823007419705
Batch  31  loss:  0.0012044130126014352
Batch  41  loss:  0.002958116354420781
Batch  51  loss:  0.001198994810692966
Batch  61  loss:  0.0014807477127760649
Batch  71  loss:  0.0013825177447870374
Batch  81  loss:  0.0008516544476151466
Batch  91  loss:  0.0015765835996717215
Validation on real data: 
LOSS supervised-train 0.0014927894581342116, valid 0.00049851048970595
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0008966093882918358
Batch  11  loss:  0.0020367291290313005
Batch  21  loss:  0.0012887261109426618
Batch  31  loss:  0.001528301858343184
Batch  41  loss:  0.003634206485003233
Batch  51  loss:  0.0011084320722147822
Batch  61  loss:  0.0012108501978218555
Batch  71  loss:  0.001713656121864915
Batch  81  loss:  0.001081653987057507
Batch  91  loss:  0.0018553808331489563
Validation on real data: 
LOSS supervised-train 0.0015319236705545337, valid 0.0005015345523133874
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0014153950614854693
Batch  11  loss:  0.0012903310125693679
Batch  21  loss:  0.0014198962599039078
Batch  31  loss:  0.0014899076195433736
Batch  41  loss:  0.0031790630891919136
Batch  51  loss:  0.0010467490646988153
Batch  61  loss:  0.0010679163970053196
Batch  71  loss:  0.0017359155463054776
Batch  81  loss:  0.0012615344021469355
Batch  91  loss:  0.0015607374953106046
Validation on real data: 
LOSS supervised-train 0.0014475610881345346, valid 0.0006508048973046243
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0012183473445475101
Batch  11  loss:  0.0013163071125745773
Batch  21  loss:  0.001197833800688386
Batch  31  loss:  0.0019128581043332815
Batch  41  loss:  0.003050619037821889
Batch  51  loss:  0.001161788823083043
Batch  61  loss:  0.0010756038827821612
Batch  71  loss:  0.002035519341006875
Batch  81  loss:  0.0011776962783187628
Batch  91  loss:  0.0015102182514965534
Validation on real data: 
LOSS supervised-train 0.0014417247514938935, valid 0.00039151168311946094
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0010402906918898225
Batch  11  loss:  0.00120028224773705
Batch  21  loss:  0.001634979504160583
Batch  31  loss:  0.0012455818941816688
Batch  41  loss:  0.0045506758615374565
Batch  51  loss:  0.0007254136144183576
Batch  61  loss:  0.0014419334474951029
Batch  71  loss:  0.0015106494538486004
Batch  81  loss:  0.001612867577932775
Batch  91  loss:  0.00136522704269737
Validation on real data: 
LOSS supervised-train 0.0014513410697691142, valid 0.00062071398133412
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0012514961417764425
Batch  11  loss:  0.0009040010045282543
Batch  21  loss:  0.0014964661095291376
Batch  31  loss:  0.001656550564803183
Batch  41  loss:  0.003406004747375846
Batch  51  loss:  0.0008144743042066693
Batch  61  loss:  0.0010365303605794907
Batch  71  loss:  0.002164620440453291
Batch  81  loss:  0.0014613686362281442
Batch  91  loss:  0.0013981973752379417
Validation on real data: 
LOSS supervised-train 0.0013822220952715725, valid 0.00033022143179550767
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0010648185852915049
Batch  11  loss:  0.0017583940643817186
Batch  21  loss:  0.001090714242309332
Batch  31  loss:  0.0018102085450664163
Batch  41  loss:  0.002539993729442358
Batch  51  loss:  0.0010473697911947966
Batch  61  loss:  0.0012224545935168862
Batch  71  loss:  0.0016885210061445832
Batch  81  loss:  0.001369171659462154
Batch  91  loss:  0.001299152965657413
Validation on real data: 
LOSS supervised-train 0.001394223810057156, valid 0.00040417464333586395
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0012179015902802348
Batch  11  loss:  0.0015673260204494
Batch  21  loss:  0.0013346053892746568
Batch  31  loss:  0.0021359899546951056
Batch  41  loss:  0.002720676362514496
Batch  51  loss:  0.0009116724831983447
Batch  61  loss:  0.0010339856380596757
Batch  71  loss:  0.0014931093901395798
Batch  81  loss:  0.0010068657575175166
Batch  91  loss:  0.001507267472334206
Validation on real data: 
LOSS supervised-train 0.0013612638443009928, valid 0.00041235776734538376
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0011201364686712623
Batch  11  loss:  0.0010487082181498408
Batch  21  loss:  0.001016519614495337
Batch  31  loss:  0.0016615060158073902
Batch  41  loss:  0.0032533055637031794
Batch  51  loss:  0.0008344116504304111
Batch  61  loss:  0.0011692242696881294
Batch  71  loss:  0.001149306888692081
Batch  81  loss:  0.001075421809218824
Batch  91  loss:  0.0015781726688146591
Validation on real data: 
LOSS supervised-train 0.0013980851782253011, valid 0.0004977416829206049
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0010334032122045755
Batch  11  loss:  0.0013813285622745752
Batch  21  loss:  0.001543958205729723
Batch  31  loss:  0.0012068256037309766
Batch  41  loss:  0.0026982503477483988
Batch  51  loss:  0.0010139085352420807
Batch  61  loss:  0.0009858413832262158
Batch  71  loss:  0.0018169238464906812
Batch  81  loss:  0.0009217722690664232
Batch  91  loss:  0.0012625515228137374
Validation on real data: 
LOSS supervised-train 0.0013518850790569558, valid 0.0005781037034466863
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0012594637228175998
Batch  11  loss:  0.0012092425022274256
Batch  21  loss:  0.0010817183647304773
Batch  31  loss:  0.0013737845001742244
Batch  41  loss:  0.0026132494676858187
Batch  51  loss:  0.0010220955591648817
Batch  61  loss:  0.0009742079419083893
Batch  71  loss:  0.0014239626470953226
Batch  81  loss:  0.0009943837067112327
Batch  91  loss:  0.0012661066139116883
Validation on real data: 
LOSS supervised-train 0.0013358445756603032, valid 0.0005959218251518905
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.001375319086946547
Batch  11  loss:  0.001470524468459189
Batch  21  loss:  0.001196465571410954
Batch  31  loss:  0.0014656122075393796
Batch  41  loss:  0.003588890889659524
Batch  51  loss:  0.0007000495097599924
Batch  61  loss:  0.0010066789109259844
Batch  71  loss:  0.0018120978493243456
Batch  81  loss:  0.0010582992108538747
Batch  91  loss:  0.0013509495183825493
Validation on real data: 
LOSS supervised-train 0.00136993128224276, valid 0.0007198866223916411
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0013055537128821015
Batch  11  loss:  0.0011360897915437818
Batch  21  loss:  0.0017550741322338581
Batch  31  loss:  0.0014020323287695646
Batch  41  loss:  0.0035924625117331743
Batch  51  loss:  0.00115267897490412
Batch  61  loss:  0.001061962335370481
Batch  71  loss:  0.0013134179171174765
Batch  81  loss:  0.0016447172965854406
Batch  91  loss:  0.0011934736976400018
Validation on real data: 
LOSS supervised-train 0.0014074664586223662, valid 0.0005221293540671468
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.001419906271621585
Batch  11  loss:  0.0011045291321352124
Batch  21  loss:  0.0014161799335852265
Batch  31  loss:  0.0010597910732030869
Batch  41  loss:  0.0024061936419457197
Batch  51  loss:  0.000832037883810699
Batch  61  loss:  0.0015813716454431415
Batch  71  loss:  0.001331910490989685
Batch  81  loss:  0.001024459139443934
Batch  91  loss:  0.0012719569494947791
Validation on real data: 
LOSS supervised-train 0.0013428712845779955, valid 0.00047257289406843483
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0008925243746489286
Batch  11  loss:  0.001040209666825831
Batch  21  loss:  0.0012396613601595163
Batch  31  loss:  0.0015956265851855278
Batch  41  loss:  0.0025986635591834784
Batch  51  loss:  0.000597791513428092
Batch  61  loss:  0.0011933111818507314
Batch  71  loss:  0.0018623482901602983
Batch  81  loss:  0.0009266220731660724
Batch  91  loss:  0.0016119977226480842
Validation on real data: 
LOSS supervised-train 0.0013110246771248057, valid 0.000444226898252964
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0010603793198242784
Batch  11  loss:  0.0013857840094715357
Batch  21  loss:  0.0012663454981520772
Batch  31  loss:  0.0016153869219124317
Batch  41  loss:  0.0034040932077914476
Batch  51  loss:  0.0007271886570379138
Batch  61  loss:  0.00131032126955688
Batch  71  loss:  0.0015130029059946537
Batch  81  loss:  0.0012544103665277362
Batch  91  loss:  0.0011727199889719486
Validation on real data: 
LOSS supervised-train 0.0012840940942987799, valid 0.0003729757445398718
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0009631244465708733
Batch  11  loss:  0.001266819192096591
Batch  21  loss:  0.0009183517540805042
Batch  31  loss:  0.0014317515306174755
Batch  41  loss:  0.0020601481664925814
Batch  51  loss:  0.0010945984395220876
Batch  61  loss:  0.0011218341533094645
Batch  71  loss:  0.0017156096873804927
Batch  81  loss:  0.000980547396466136
Batch  91  loss:  0.0012636338360607624
Validation on real data: 
LOSS supervised-train 0.0012828026327770204, valid 0.0004832702688872814
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0008632229873910546
Batch  11  loss:  0.0011877448996528983
Batch  21  loss:  0.001251696958206594
Batch  31  loss:  0.0014351207064464688
Batch  41  loss:  0.004592537879943848
Batch  51  loss:  0.0011680263560265303
Batch  61  loss:  0.0009029631037265062
Batch  71  loss:  0.0012183192884549499
Batch  81  loss:  0.0009942720644176006
Batch  91  loss:  0.0010962040396407247
Validation on real data: 
LOSS supervised-train 0.0012853915675077588, valid 0.0006089912494644523
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0009494823752902448
Batch  11  loss:  0.0010124915279448032
Batch  21  loss:  0.0011014619376510382
Batch  31  loss:  0.0010948233539238572
Batch  41  loss:  0.002764341188594699
Batch  51  loss:  0.0009237863123416901
Batch  61  loss:  0.0008561223512515426
Batch  71  loss:  0.0013054204173386097
Batch  81  loss:  0.0014627778436988592
Batch  91  loss:  0.0010085991816595197
Validation on real data: 
LOSS supervised-train 0.0012579287163680419, valid 0.0005668392404913902
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0010732188820838928
Batch  11  loss:  0.0011463811388239264
Batch  21  loss:  0.001329353777691722
Batch  31  loss:  0.0014188983477652073
Batch  41  loss:  0.0033045043237507343
Batch  51  loss:  0.0009877545526251197
Batch  61  loss:  0.0011566334869712591
Batch  71  loss:  0.0009074658155441284
Batch  81  loss:  0.0009602908394299448
Batch  91  loss:  0.001157600898295641
Validation on real data: 
LOSS supervised-train 0.0012860730604734273, valid 0.000578045379370451
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0011665336787700653
Batch  11  loss:  0.0010588275035843253
Batch  21  loss:  0.001066609867848456
Batch  31  loss:  0.0009114979766309261
Batch  41  loss:  0.002708714921027422
Batch  51  loss:  0.0010339388391003013
Batch  61  loss:  0.0009655620669946074
Batch  71  loss:  0.0016455741133540869
Batch  81  loss:  0.00096346769714728
Batch  91  loss:  0.0008254728745669127
Validation on real data: 
LOSS supervised-train 0.001304928752942942, valid 0.0005383716197684407
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0010260710259899497
Batch  11  loss:  0.0012171887792646885
Batch  21  loss:  0.0011153965024277568
Batch  31  loss:  0.001267863903194666
Batch  41  loss:  0.0030685928650200367
Batch  51  loss:  0.0009491746895946562
Batch  61  loss:  0.0008181725279428065
Batch  71  loss:  0.0010382269974797964
Batch  81  loss:  0.0009866016916930676
Batch  91  loss:  0.001265516970306635
Validation on real data: 
LOSS supervised-train 0.0012618421338265761, valid 0.0005925497389398515
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0013289919588714838
Batch  11  loss:  0.0014471329050138593
Batch  21  loss:  0.0015452910447493196
Batch  31  loss:  0.0011195025872439146
Batch  41  loss:  0.0027418104000389576
Batch  51  loss:  0.0012520035961642861
Batch  61  loss:  0.0009966791840270162
Batch  71  loss:  0.0011865866836160421
Batch  81  loss:  0.0009483352187089622
Batch  91  loss:  0.0013281152350828052
Validation on real data: 
LOSS supervised-train 0.0012137779203476385, valid 0.0005655809072777629
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0012540221214294434
Batch  11  loss:  0.0009457388077862561
Batch  21  loss:  0.0012231710134074092
Batch  31  loss:  0.0012333710910752416
Batch  41  loss:  0.002535380655899644
Batch  51  loss:  0.0011864930856972933
Batch  61  loss:  0.00106198457069695
Batch  71  loss:  0.001226126914843917
Batch  81  loss:  0.0007966068806126714
Batch  91  loss:  0.0010873198043555021
Validation on real data: 
LOSS supervised-train 0.001204284408595413, valid 0.0005101830465719104
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.001369020319543779
Batch  11  loss:  0.0009522947366349399
Batch  21  loss:  0.0009114122367464006
Batch  31  loss:  0.001402235240675509
Batch  41  loss:  0.002075671451166272
Batch  51  loss:  0.000674846931360662
Batch  61  loss:  0.0008691180846653879
Batch  71  loss:  0.001049908809363842
Batch  81  loss:  0.001052215346135199
Batch  91  loss:  0.0012357495725154877
Validation on real data: 
LOSS supervised-train 0.0011837210116209462, valid 0.0005815637996420264
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0012217473704367876
Batch  11  loss:  0.00156111060641706
Batch  21  loss:  0.0010671349009498954
Batch  31  loss:  0.0011091999476775527
Batch  41  loss:  0.00222562113776803
Batch  51  loss:  0.000880891049746424
Batch  61  loss:  0.0007420137408189476
Batch  71  loss:  0.00106249435339123
Batch  81  loss:  0.0008797829505056143
Batch  91  loss:  0.001452362397685647
Validation on real data: 
LOSS supervised-train 0.0011805950407870114, valid 0.0004819443274755031
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.001112916273996234
Batch  11  loss:  0.0009409059421159327
Batch  21  loss:  0.001304212142713368
Batch  31  loss:  0.0018360276008024812
Batch  41  loss:  0.00323209585621953
Batch  51  loss:  0.0011498573003336787
Batch  61  loss:  0.0007400343311019242
Batch  71  loss:  0.0017495513893663883
Batch  81  loss:  0.0010671878699213266
Batch  91  loss:  0.0011484446004033089
Validation on real data: 
LOSS supervised-train 0.00123429132218007, valid 0.0004434601869434118
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.000959470693487674
Batch  11  loss:  0.0010540507500991225
Batch  21  loss:  0.0011623966274783015
Batch  31  loss:  0.0014041882241144776
Batch  41  loss:  0.0019467988749966025
Batch  51  loss:  0.0008143524755723774
Batch  61  loss:  0.0010472654830664396
Batch  71  loss:  0.0012768530286848545
Batch  81  loss:  0.001175746787339449
Batch  91  loss:  0.0008449181914329529
Validation on real data: 
LOSS supervised-train 0.001198015606496483, valid 0.0003060246235691011
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.001292133703827858
Batch  11  loss:  0.0010394708951935172
Batch  21  loss:  0.00116547173820436
Batch  31  loss:  0.0010527678532525897
Batch  41  loss:  0.0034837459679692984
Batch  51  loss:  0.0008927530725486577
Batch  61  loss:  0.0009163160575553775
Batch  71  loss:  0.0011356205213814974
Batch  81  loss:  0.0007583722472190857
Batch  91  loss:  0.001233258517459035
Validation on real data: 
LOSS supervised-train 0.0011898949497845023, valid 0.0004841371555812657
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0012468353379517794
Batch  11  loss:  0.0010898106265813112
Batch  21  loss:  0.001427777810022235
Batch  31  loss:  0.0013742427108809352
Batch  41  loss:  0.0020861325319856405
Batch  51  loss:  0.0008616260020062327
Batch  61  loss:  0.0009200022905133665
Batch  71  loss:  0.001219899975694716
Batch  81  loss:  0.0012420774437487125
Batch  91  loss:  0.000980272307060659
Validation on real data: 
LOSS supervised-train 0.0011710395949194208, valid 0.000511554186232388
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0007169125019572675
Batch  11  loss:  0.0011800217907875776
Batch  21  loss:  0.0009044361650012434
Batch  31  loss:  0.0012000270653516054
Batch  41  loss:  0.0024993594270199537
Batch  51  loss:  0.0008478930685669184
Batch  61  loss:  0.0005884604179300368
Batch  71  loss:  0.0009203262161463499
Batch  81  loss:  0.000969657558016479
Batch  91  loss:  0.0009733503684401512
Validation on real data: 
LOSS supervised-train 0.0011223508493276313, valid 0.00044422788778319955
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0010803467594087124
Batch  11  loss:  0.0011012500617653131
Batch  21  loss:  0.000988530577160418
Batch  31  loss:  0.0010218870593234897
Batch  41  loss:  0.002999073127284646
Batch  51  loss:  0.0010891541605815291
Batch  61  loss:  0.001527907676063478
Batch  71  loss:  0.0013162350514903665
Batch  81  loss:  0.0009317646035924554
Batch  91  loss:  0.0009987079538404942
Validation on real data: 
LOSS supervised-train 0.001175505758728832, valid 0.0005747037357650697
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0008200482116080821
Batch  11  loss:  0.0008804918034002185
Batch  21  loss:  0.0010630061151459813
Batch  31  loss:  0.0012698185164481401
Batch  41  loss:  0.001671071513555944
Batch  51  loss:  0.0010010538389906287
Batch  61  loss:  0.001050254562869668
Batch  71  loss:  0.0009368808823637664
Batch  81  loss:  0.0009289098088629544
Batch  91  loss:  0.0012429254129529
Validation on real data: 
LOSS supervised-train 0.0011103816149989143, valid 0.0004957932978868484
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0010136403143405914
Batch  11  loss:  0.000998303177766502
Batch  21  loss:  0.001204990316182375
Batch  31  loss:  0.0017552375793457031
Batch  41  loss:  0.0017791315913200378
Batch  51  loss:  0.0009386712335981429
Batch  61  loss:  0.0010250314371660352
Batch  71  loss:  0.0011984141310676932
Batch  81  loss:  0.0008922542911022902
Batch  91  loss:  0.001364011550322175
Validation on real data: 
LOSS supervised-train 0.0011570891266455873, valid 0.0004707606858573854
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0010292873485013843
Batch  11  loss:  0.0011643251636996865
Batch  21  loss:  0.0010097975609824061
Batch  31  loss:  0.0012569536920636892
Batch  41  loss:  0.0018935881089419127
Batch  51  loss:  0.0011298913741484284
Batch  61  loss:  0.0012268905993551016
Batch  71  loss:  0.0011359609197825193
Batch  81  loss:  0.0010900885099545121
Batch  91  loss:  0.0012230010470375419
Validation on real data: 
LOSS supervised-train 0.001135116868535988, valid 0.0005334825254976749
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0012044317554682493
Batch  11  loss:  0.0009761720430105925
Batch  21  loss:  0.0009150870610028505
Batch  31  loss:  0.001254642615094781
Batch  41  loss:  0.0023637444246560335
Batch  51  loss:  0.0009438101551495492
Batch  61  loss:  0.0009484989568591118
Batch  71  loss:  0.001108201453462243
Batch  81  loss:  0.0013596173375844955
Batch  91  loss:  0.0009861065773293376
Validation on real data: 
LOSS supervised-train 0.0011521452554734423, valid 0.00038039550418034196
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0009679512586444616
Batch  11  loss:  0.0009803693974390626
Batch  21  loss:  0.0010109728900715709
Batch  31  loss:  0.0013854646822437644
Batch  41  loss:  0.0031548854894936085
Batch  51  loss:  0.0008860474335961044
Batch  61  loss:  0.0007765659247525036
Batch  71  loss:  0.0014205806655809283
Batch  81  loss:  0.0008631551172584295
Batch  91  loss:  0.0012384458677843213
Validation on real data: 
LOSS supervised-train 0.0011533126136055215, valid 0.0005261003389023244
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0014342755312100053
Batch  11  loss:  0.0010111206211149693
Batch  21  loss:  0.000956959614995867
Batch  31  loss:  0.0012431007344275713
Batch  41  loss:  0.0023096688091754913
Batch  51  loss:  0.0009484618785791099
Batch  61  loss:  0.0007706052856519818
Batch  71  loss:  0.001016688416711986
Batch  81  loss:  0.0008404079126194119
Batch  91  loss:  0.0008325831149704754
Validation on real data: 
LOSS supervised-train 0.001104499552748166, valid 0.0004594565834850073
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.00114028702955693
Batch  11  loss:  0.001062449417077005
Batch  21  loss:  0.0009989755926653743
Batch  31  loss:  0.0010049337288364768
Batch  41  loss:  0.0018037160625681281
Batch  51  loss:  0.0009343875572085381
Batch  61  loss:  0.0008905610884539783
Batch  71  loss:  0.0013874262804165483
Batch  81  loss:  0.0008492040215060115
Batch  91  loss:  0.0008730851113796234
Validation on real data: 
LOSS supervised-train 0.001093618509476073, valid 0.0005361207295209169
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0007078720373101532
Batch  11  loss:  0.0011119855334982276
Batch  21  loss:  0.0009516922873444855
Batch  31  loss:  0.0013402702752500772
Batch  41  loss:  0.003164685331285
Batch  51  loss:  0.0010874924482777715
Batch  61  loss:  0.0008022637339308858
Batch  71  loss:  0.001201295992359519
Batch  81  loss:  0.0012065161718055606
Batch  91  loss:  0.0008692481205798686
Validation on real data: 
LOSS supervised-train 0.0011544178309850395, valid 0.0005694321589544415
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0008482283446937799
Batch  11  loss:  0.000881667947396636
Batch  21  loss:  0.001086436677724123
Batch  31  loss:  0.001053673680871725
Batch  41  loss:  0.0017932744231075048
Batch  51  loss:  0.0006991027039475739
Batch  61  loss:  0.0008102466235868633
Batch  71  loss:  0.0011201167944818735
Batch  81  loss:  0.0008434075862169266
Batch  91  loss:  0.0009669954888522625
Validation on real data: 
LOSS supervised-train 0.001078446171595715, valid 0.0004099110374227166
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0010038291802629828
Batch  11  loss:  0.0012638804037123919
Batch  21  loss:  0.0007080676732584834
Batch  31  loss:  0.0013235409278422594
Batch  41  loss:  0.0023315807338804007
Batch  51  loss:  0.001146296039223671
Batch  61  loss:  0.0006330477772280574
Batch  71  loss:  0.0011305803200230002
Batch  81  loss:  0.0012933751568198204
Batch  91  loss:  0.001681373338215053
Validation on real data: 
LOSS supervised-train 0.0011053540516877548, valid 0.0005188757786527276
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0009443092276342213
Batch  11  loss:  0.0011731096310541034
Batch  21  loss:  0.0006918252911418676
Batch  31  loss:  0.0012937714345753193
Batch  41  loss:  0.002352676820009947
Batch  51  loss:  0.0010044313967227936
Batch  61  loss:  0.0006732544279657304
Batch  71  loss:  0.0011527218157425523
Batch  81  loss:  0.0009862385923042893
Batch  91  loss:  0.0010758962016552687
Validation on real data: 
LOSS supervised-train 0.0010549938475014641, valid 0.0004484012897592038
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0009678853093646467
Batch  11  loss:  0.0009715139749459922
Batch  21  loss:  0.0010103877866640687
Batch  31  loss:  0.0008150225621648133
Batch  41  loss:  0.0024098847061395645
Batch  51  loss:  0.0010820095194503665
Batch  61  loss:  0.0007414797437377274
Batch  71  loss:  0.0011858184589073062
Batch  81  loss:  0.0008793887100182474
Batch  91  loss:  0.001346257864497602
Validation on real data: 
LOSS supervised-train 0.0011074950953479856, valid 0.0003703396359924227
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0009137029992416501
Batch  11  loss:  0.001157851773314178
Batch  21  loss:  0.0009405098389834166
Batch  31  loss:  0.0009416307439096272
Batch  41  loss:  0.0027534360997378826
Batch  51  loss:  0.00100832583848387
Batch  61  loss:  0.00116652250289917
Batch  71  loss:  0.0010947827249765396
Batch  81  loss:  0.0009423985029570758
Batch  91  loss:  0.0010998822981491685
Validation on real data: 
LOSS supervised-train 0.0010415779770119116, valid 0.00048149071517400444
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.000723289093002677
Batch  11  loss:  0.0010652217315509915
Batch  21  loss:  0.0009870791109278798
Batch  31  loss:  0.0013266674941405654
Batch  41  loss:  0.0021332022733986378
Batch  51  loss:  0.0005921361735090613
Batch  61  loss:  0.0008361748186871409
Batch  71  loss:  0.0009984328644350171
Batch  81  loss:  0.0007034120499156415
Batch  91  loss:  0.0009707151330076158
Validation on real data: 
LOSS supervised-train 0.0010576556308660656, valid 0.0005747387185692787
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0009968443773686886
Batch  11  loss:  0.0011131304781883955
Batch  21  loss:  0.0006956228171475232
Batch  31  loss:  0.0008642077445983887
Batch  41  loss:  0.002181732328608632
Batch  51  loss:  0.0010047220857813954
Batch  61  loss:  0.0007788821239955723
Batch  71  loss:  0.0011334053706377745
Batch  81  loss:  0.0012057289713993669
Batch  91  loss:  0.0010659220861271024
Validation on real data: 
LOSS supervised-train 0.0010464352293638513, valid 0.0005398462526500225
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0012856924440711737
Batch  11  loss:  0.0008805495453998446
Batch  21  loss:  0.0011320877820253372
Batch  31  loss:  0.0010278611443936825
Batch  41  loss:  0.002104572718963027
Batch  51  loss:  0.0008938106475397944
Batch  61  loss:  0.001023586723022163
Batch  71  loss:  0.0009653787710703909
Batch  81  loss:  0.0009655450121499598
Batch  91  loss:  0.0009337906376458704
Validation on real data: 
LOSS supervised-train 0.001082954503945075, valid 0.0005270651308819652
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0009141123155131936
Batch  11  loss:  0.0009905567858368158
Batch  21  loss:  0.0008897469961084425
Batch  31  loss:  0.0009598439792171121
Batch  41  loss:  0.0020922033581882715
Batch  51  loss:  0.0009872184600681067
Batch  61  loss:  0.0008298337925225496
Batch  71  loss:  0.0008455134811811149
Batch  81  loss:  0.0010679252445697784
Batch  91  loss:  0.001119459280744195
Validation on real data: 
LOSS supervised-train 0.0010566995677072555, valid 0.00044778792653232813
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0009692804305814207
Batch  11  loss:  0.0014320904156193137
Batch  21  loss:  0.0008096089586615562
Batch  31  loss:  0.0007436892483383417
Batch  41  loss:  0.002578804502263665
Batch  51  loss:  0.0007045496604405344
Batch  61  loss:  0.00096884899539873
Batch  71  loss:  0.0007797424914315343
Batch  81  loss:  0.0009201206848956645
Batch  91  loss:  0.0012007057666778564
Validation on real data: 
LOSS supervised-train 0.0010262039012741298, valid 0.0005011295434087515
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0007854416035115719
Batch  11  loss:  0.001211780239827931
Batch  21  loss:  0.0013743918389081955
Batch  31  loss:  0.0008073622011579573
Batch  41  loss:  0.002200875198468566
Batch  51  loss:  0.0007605653372593224
Batch  61  loss:  0.0006537187146022916
Batch  71  loss:  0.0008358511258848011
Batch  81  loss:  0.0007087250123731792
Batch  91  loss:  0.0009030866203829646
Validation on real data: 
LOSS supervised-train 0.0010525646450696513, valid 0.00033753193565644324
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.001096541527658701
Batch  11  loss:  0.001408601412549615
Batch  21  loss:  0.0009402785217389464
Batch  31  loss:  0.0011111387284472585
Batch  41  loss:  0.0019509918056428432
Batch  51  loss:  0.0008668554946780205
Batch  61  loss:  0.0007782219909131527
Batch  71  loss:  0.0009640229982323945
Batch  81  loss:  0.001323162461631
Batch  91  loss:  0.0008715408621355891
Validation on real data: 
LOSS supervised-train 0.0010238585266051814, valid 0.0004165822465438396
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0007927822880446911
Batch  11  loss:  0.0010124292457476258
Batch  21  loss:  0.0009222707012668252
Batch  31  loss:  0.0007267401670105755
Batch  41  loss:  0.0022818606812506914
Batch  51  loss:  0.0006501905736513436
Batch  61  loss:  0.0011986675672233105
Batch  71  loss:  0.0013126626145094633
Batch  81  loss:  0.0008726962259970605
Batch  91  loss:  0.0007092386367730796
Validation on real data: 
LOSS supervised-train 0.0010001248633489013, valid 0.0005713051650673151
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0009051154484041035
Batch  11  loss:  0.0010842561023309827
Batch  21  loss:  0.0009652686421759427
Batch  31  loss:  0.0008139815763570368
Batch  41  loss:  0.0027016287203878164
Batch  51  loss:  0.0007616395596414804
Batch  61  loss:  0.0009507135255262256
Batch  71  loss:  0.0016230985056608915
Batch  81  loss:  0.0007027077372185886
Batch  91  loss:  0.0009757737861946225
Validation on real data: 
LOSS supervised-train 0.0010241134656826035, valid 0.0004461328499019146
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0009897758718580008
Batch  11  loss:  0.000989602762274444
Batch  21  loss:  0.000822888279799372
Batch  31  loss:  0.0008368599228560925
Batch  41  loss:  0.0025823076721280813
Batch  51  loss:  0.0006980939069762826
Batch  61  loss:  0.0007263290463015437
Batch  71  loss:  0.0011839732760563493
Batch  81  loss:  0.0008809756836853921
Batch  91  loss:  0.001013877335935831
Validation on real data: 
LOSS supervised-train 0.0010192067665047945, valid 0.0005089931655675173
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0009701225208118558
Batch  11  loss:  0.0009954095585271716
Batch  21  loss:  0.001129538519307971
Batch  31  loss:  0.0012284863041713834
Batch  41  loss:  0.0017536289524286985
Batch  51  loss:  0.0008657071739435196
Batch  61  loss:  0.0008615725091658533
Batch  71  loss:  0.0012203946243971586
Batch  81  loss:  0.0009043511818163097
Batch  91  loss:  0.0008023030823096633
Validation on real data: 
LOSS supervised-train 0.0010102506581461056, valid 0.00042767563718371093
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  guitar ; Model ID: 5df08ba7af60e7bfe72db292d4e13056
--------------------
Training baseline regression model:  2022-03-30 15:34:07.211113
Detector:  pointnet
Object:  guitar
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1610788
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.12641088664531708
Batch  11  loss:  0.033838752657175064
Batch  21  loss:  0.05851558595895767
Batch  31  loss:  0.020001888275146484
Batch  41  loss:  0.020455684512853622
Batch  51  loss:  0.013576695695519447
Batch  61  loss:  0.01719610206782818
Batch  71  loss:  0.011664162389934063
Batch  81  loss:  0.010320045985281467
Batch  91  loss:  0.011027997359633446
Validation on real data: 
LOSS supervised-train 0.027100816844031216, valid 0.012953215278685093
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.009794558398425579
Batch  11  loss:  0.009961261413991451
Batch  21  loss:  0.010652322322130203
Batch  31  loss:  0.008521800860762596
Batch  41  loss:  0.00850692018866539
Batch  51  loss:  0.009211517870426178
Batch  61  loss:  0.008548231795430183
Batch  71  loss:  0.00868410523980856
Batch  81  loss:  0.007824872620403767
Batch  91  loss:  0.010631837882101536
Validation on real data: 
LOSS supervised-train 0.009181889034807681, valid 0.012162506580352783
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.011166143231093884
Batch  11  loss:  0.006929227616637945
Batch  21  loss:  0.00813253503292799
Batch  31  loss:  0.006100704427808523
Batch  41  loss:  0.0059546809643507
Batch  51  loss:  0.006524022668600082
Batch  61  loss:  0.005854045040905476
Batch  71  loss:  0.00608684541657567
Batch  81  loss:  0.005498967599123716
Batch  91  loss:  0.0060928333550691605
Validation on real data: 
LOSS supervised-train 0.006686767125502229, valid 0.007084168028086424
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.006301526911556721
Batch  11  loss:  0.005526100285351276
Batch  21  loss:  0.005676144268363714
Batch  31  loss:  0.003848515683785081
Batch  41  loss:  0.004215847235172987
Batch  51  loss:  0.005592302419245243
Batch  61  loss:  0.0034986301325261593
Batch  71  loss:  0.004013628698885441
Batch  81  loss:  0.003637902904301882
Batch  91  loss:  0.004086507484316826
Validation on real data: 
LOSS supervised-train 0.004507506780792028, valid 0.0027282238006591797
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.004233925603330135
Batch  11  loss:  0.003498126519843936
Batch  21  loss:  0.0041423942893743515
Batch  31  loss:  0.002796040615066886
Batch  41  loss:  0.003133683232590556
Batch  51  loss:  0.004050296265631914
Batch  61  loss:  0.0026229312643408775
Batch  71  loss:  0.0034532425925135612
Batch  81  loss:  0.003452760400250554
Batch  91  loss:  0.003803550498560071
Validation on real data: 
LOSS supervised-train 0.003391154045239091, valid 0.0030887757893651724
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.0032038134522736073
Batch  11  loss:  0.002579065738245845
Batch  21  loss:  0.0034964047372341156
Batch  31  loss:  0.0018737135687842965
Batch  41  loss:  0.0031786751933395863
Batch  51  loss:  0.0031595949549227953
Batch  61  loss:  0.002167231636121869
Batch  71  loss:  0.0036552033852785826
Batch  81  loss:  0.0020626294426620007
Batch  91  loss:  0.0036456547677516937
Validation on real data: 
LOSS supervised-train 0.0029752677609212698, valid 0.003846909385174513
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.0024802102707326412
Batch  11  loss:  0.00229552760720253
Batch  21  loss:  0.0031313602812588215
Batch  31  loss:  0.0018058171262964606
Batch  41  loss:  0.0022944866213947535
Batch  51  loss:  0.00519197341054678
Batch  61  loss:  0.0019836053252220154
Batch  71  loss:  0.0030835019424557686
Batch  81  loss:  0.0027236968744546175
Batch  91  loss:  0.0031635838095098734
Validation on real data: 
LOSS supervised-train 0.0027698625973425805, valid 0.0024600194301456213
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.0019065480446442962
Batch  11  loss:  0.0022667113225907087
Batch  21  loss:  0.0033083425369113684
Batch  31  loss:  0.002055586315691471
Batch  41  loss:  0.0019538619089871645
Batch  51  loss:  0.0026460837107151747
Batch  61  loss:  0.0018242853693664074
Batch  71  loss:  0.0036349426954984665
Batch  81  loss:  0.0030512618832290173
Batch  91  loss:  0.003233263734728098
Validation on real data: 
LOSS supervised-train 0.002602622846607119, valid 0.0027742444071918726
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0019122104858979583
Batch  11  loss:  0.0035123976413160563
Batch  21  loss:  0.002856815466657281
Batch  31  loss:  0.0018120186869055033
Batch  41  loss:  0.0031304950825870037
Batch  51  loss:  0.00272870110347867
Batch  61  loss:  0.0016082347137853503
Batch  71  loss:  0.002255195053294301
Batch  81  loss:  0.0017554756486788392
Batch  91  loss:  0.0036680821795016527
Validation on real data: 
LOSS supervised-train 0.0024895873316563666, valid 0.0024308168794959784
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0019741710275411606
Batch  11  loss:  0.0036687094252556562
Batch  21  loss:  0.002788734622299671
Batch  31  loss:  0.0014120793202891946
Batch  41  loss:  0.0027625656221061945
Batch  51  loss:  0.0028130768332630396
Batch  61  loss:  0.001728863688185811
Batch  71  loss:  0.0024222410283982754
Batch  81  loss:  0.002253933111205697
Batch  91  loss:  0.0026553587522357702
Validation on real data: 
LOSS supervised-train 0.002349091647192836, valid 0.0016844141064211726
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.001416837447322905
Batch  11  loss:  0.0020736430305987597
Batch  21  loss:  0.0024150297977030277
Batch  31  loss:  0.0014672430697828531
Batch  41  loss:  0.002104298211634159
Batch  51  loss:  0.0021942046005278826
Batch  61  loss:  0.001636832021176815
Batch  71  loss:  0.002754978369921446
Batch  81  loss:  0.0022938463371247053
Batch  91  loss:  0.0022059453185647726
Validation on real data: 
LOSS supervised-train 0.0022984354267828167, valid 0.0025339669082313776
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0020891353487968445
Batch  11  loss:  0.0026172723155468702
Batch  21  loss:  0.0023454490583389997
Batch  31  loss:  0.0014472302282229066
Batch  41  loss:  0.0015939256409183145
Batch  51  loss:  0.0022551300935447216
Batch  61  loss:  0.0015699573559686542
Batch  71  loss:  0.003336830995976925
Batch  81  loss:  0.0023265699855983257
Batch  91  loss:  0.0018946663476526737
Validation on real data: 
LOSS supervised-train 0.00215456836274825, valid 0.002209535101428628
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.001474060700275004
Batch  11  loss:  0.0017693490954115987
Batch  21  loss:  0.002256552455946803
Batch  31  loss:  0.001643811585381627
Batch  41  loss:  0.0016141206724569201
Batch  51  loss:  0.002363982144743204
Batch  61  loss:  0.0013418704038485885
Batch  71  loss:  0.0021620802581310272
Batch  81  loss:  0.0016428424278274179
Batch  91  loss:  0.002571073127910495
Validation on real data: 
LOSS supervised-train 0.0019397753913654014, valid 0.0023779375478625298
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0014670194359496236
Batch  11  loss:  0.0018158312886953354
Batch  21  loss:  0.002145800506696105
Batch  31  loss:  0.0015039832796901464
Batch  41  loss:  0.0017724325880408287
Batch  51  loss:  0.0016862862976267934
Batch  61  loss:  0.0010723239975050092
Batch  71  loss:  0.0025350642390549183
Batch  81  loss:  0.0014746846863999963
Batch  91  loss:  0.0017323842039331794
Validation on real data: 
LOSS supervised-train 0.0019322532205842435, valid 0.002040485618636012
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0016492342110723257
Batch  11  loss:  0.0014499452663585544
Batch  21  loss:  0.0020928983576595783
Batch  31  loss:  0.001400032197125256
Batch  41  loss:  0.00151808129157871
Batch  51  loss:  0.0020442353561520576
Batch  61  loss:  0.001221277634613216
Batch  71  loss:  0.0026114475913345814
Batch  81  loss:  0.002363752108067274
Batch  91  loss:  0.0022278667893260717
Validation on real data: 
LOSS supervised-train 0.0018704291398171336, valid 0.00283959973603487
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0017696823924779892
Batch  11  loss:  0.0013598137302324176
Batch  21  loss:  0.0017997160321101546
Batch  31  loss:  0.0013179450761526823
Batch  41  loss:  0.0017152094515040517
Batch  51  loss:  0.0017395075410604477
Batch  61  loss:  0.0011000987142324448
Batch  71  loss:  0.0015704032266512513
Batch  81  loss:  0.0018739331280812621
Batch  91  loss:  0.0018125849310308695
Validation on real data: 
LOSS supervised-train 0.0018108269269578158, valid 0.001879826420918107
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.001152054755948484
Batch  11  loss:  0.0011403068201616406
Batch  21  loss:  0.0020540556870400906
Batch  31  loss:  0.0011727847158908844
Batch  41  loss:  0.0013673226349055767
Batch  51  loss:  0.0018019428243860602
Batch  61  loss:  0.0011352509027346969
Batch  71  loss:  0.002366154920309782
Batch  81  loss:  0.0016057626344263554
Batch  91  loss:  0.0017570077907294035
Validation on real data: 
LOSS supervised-train 0.0017241542204283178, valid 0.0017352434806525707
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0015333598712459207
Batch  11  loss:  0.0012174794683232903
Batch  21  loss:  0.0017920086393132806
Batch  31  loss:  0.0010750512592494488
Batch  41  loss:  0.0014074114151299
Batch  51  loss:  0.001923529664054513
Batch  61  loss:  0.001150591648183763
Batch  71  loss:  0.001751351053826511
Batch  81  loss:  0.0015771436737850308
Batch  91  loss:  0.0015916561242192984
Validation on real data: 
LOSS supervised-train 0.0016210163454525174, valid 0.0012760634999722242
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0012141489423811436
Batch  11  loss:  0.0014549079351127148
Batch  21  loss:  0.0019071637652814388
Batch  31  loss:  0.0013422972988337278
Batch  41  loss:  0.0010883852373808622
Batch  51  loss:  0.0014190414221957326
Batch  61  loss:  0.0007884670631028712
Batch  71  loss:  0.0017057573422789574
Batch  81  loss:  0.0015907242195680737
Batch  91  loss:  0.0020499159581959248
Validation on real data: 
LOSS supervised-train 0.001586516536772251, valid 0.001755134086124599
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0012506190687417984
Batch  11  loss:  0.00102660758420825
Batch  21  loss:  0.001916576293297112
Batch  31  loss:  0.002171857049688697
Batch  41  loss:  0.0015746511053293943
Batch  51  loss:  0.0015530592063441873
Batch  61  loss:  0.0010040800552815199
Batch  71  loss:  0.0016015565488487482
Batch  81  loss:  0.00124267372302711
Batch  91  loss:  0.0016555049223825336
Validation on real data: 
LOSS supervised-train 0.0015645065036369488, valid 0.0014336638851091266
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0014128113398328424
Batch  11  loss:  0.0011835754849016666
Batch  21  loss:  0.0018397956155240536
Batch  31  loss:  0.0012074412079527974
Batch  41  loss:  0.000972808338701725
Batch  51  loss:  0.0011908350279554725
Batch  61  loss:  0.0011384187964722514
Batch  71  loss:  0.0019237288506701589
Batch  81  loss:  0.001555818598717451
Batch  91  loss:  0.0016167191788554192
Validation on real data: 
LOSS supervised-train 0.0014958683302393183, valid 0.002413415350019932
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0010593754705041647
Batch  11  loss:  0.0009370410698466003
Batch  21  loss:  0.002058877842500806
Batch  31  loss:  0.0011231967946514487
Batch  41  loss:  0.0016991057200357318
Batch  51  loss:  0.001440898166038096
Batch  61  loss:  0.0011547172907739878
Batch  71  loss:  0.001515342853963375
Batch  81  loss:  0.001810105750337243
Batch  91  loss:  0.001201713690534234
Validation on real data: 
LOSS supervised-train 0.0014463011268526316, valid 0.001444170717149973
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.0012190552661195397
Batch  11  loss:  0.0011355987517163157
Batch  21  loss:  0.0014953195350244641
Batch  31  loss:  0.0009470531949773431
Batch  41  loss:  0.001683829934336245
Batch  51  loss:  0.0010624045971781015
Batch  61  loss:  0.0010310146026313305
Batch  71  loss:  0.0016758863348513842
Batch  81  loss:  0.001935780281201005
Batch  91  loss:  0.0013563730753958225
Validation on real data: 
LOSS supervised-train 0.001451058898237534, valid 0.0017420216463506222
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.001011973712593317
Batch  11  loss:  0.0009549693786539137
Batch  21  loss:  0.0017976111266762018
Batch  31  loss:  0.0011979436967521906
Batch  41  loss:  0.001166828442364931
Batch  51  loss:  0.0014363034861162305
Batch  61  loss:  0.0017146409954875708
Batch  71  loss:  0.0014698827872052789
Batch  81  loss:  0.001114926883019507
Batch  91  loss:  0.000991667271591723
Validation on real data: 
LOSS supervised-train 0.0014121611916925758, valid 0.0014001433737576008
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0010799120645970106
Batch  11  loss:  0.0010692483047023416
Batch  21  loss:  0.0017520739929750562
Batch  31  loss:  0.0009841211140155792
Batch  41  loss:  0.0010028911055997014
Batch  51  loss:  0.0013480677735060453
Batch  61  loss:  0.0009809656767174602
Batch  71  loss:  0.0013669399777427316
Batch  81  loss:  0.0012238642666488886
Batch  91  loss:  0.0014119383413344622
Validation on real data: 
LOSS supervised-train 0.001344720473862253, valid 0.0012651676079258323
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0010754331015050411
Batch  11  loss:  0.0011057276278734207
Batch  21  loss:  0.0019702313002198935
Batch  31  loss:  0.0010140970116481185
Batch  41  loss:  0.0010996314231306314
Batch  51  loss:  0.0014203315367922187
Batch  61  loss:  0.0008580124122090638
Batch  71  loss:  0.0019540046341717243
Batch  81  loss:  0.001216574339196086
Batch  91  loss:  0.0011783776571974158
Validation on real data: 
LOSS supervised-train 0.001395430623088032, valid 0.0011367494007572532
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0011659981682896614
Batch  11  loss:  0.0009071237873286009
Batch  21  loss:  0.00155059399548918
Batch  31  loss:  0.0012501892633736134
Batch  41  loss:  0.0011235573329031467
Batch  51  loss:  0.0014512824127450585
Batch  61  loss:  0.0010721230646595359
Batch  71  loss:  0.0016253271605819464
Batch  81  loss:  0.0012348919408395886
Batch  91  loss:  0.0008772375877015293
Validation on real data: 
LOSS supervised-train 0.001381788260769099, valid 0.0012266897829249501
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0012697072234004736
Batch  11  loss:  0.0012246256228536367
Batch  21  loss:  0.002083910396322608
Batch  31  loss:  0.0012158877216279507
Batch  41  loss:  0.0011899308301508427
Batch  51  loss:  0.0013471802230924368
Batch  61  loss:  0.001009010011330247
Batch  71  loss:  0.0018364244606345892
Batch  81  loss:  0.0016656017396599054
Batch  91  loss:  0.0013161213137209415
Validation on real data: 
LOSS supervised-train 0.0013585348444757983, valid 0.0011854300973936915
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.001263519749045372
Batch  11  loss:  0.0013660502154380083
Batch  21  loss:  0.0015784514835104346
Batch  31  loss:  0.0008313377038575709
Batch  41  loss:  0.0010835417779162526
Batch  51  loss:  0.0008558600675314665
Batch  61  loss:  0.0008464253041893244
Batch  71  loss:  0.0011708561796694994
Batch  81  loss:  0.0012483920436352491
Batch  91  loss:  0.0011833608150482178
Validation on real data: 
LOSS supervised-train 0.001308681225636974, valid 0.0010036058956757188
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0010884952498599887
Batch  11  loss:  0.0009047387866303325
Batch  21  loss:  0.0014558067778125405
Batch  31  loss:  0.0008837242494337261
Batch  41  loss:  0.00105516507755965
Batch  51  loss:  0.0012401220155879855
Batch  61  loss:  0.0010880717309191823
Batch  71  loss:  0.0017601526342332363
Batch  81  loss:  0.000903771782759577
Batch  91  loss:  0.0010053549194708467
Validation on real data: 
LOSS supervised-train 0.0012641337868990377, valid 0.001079869456589222
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0010933330049738288
Batch  11  loss:  0.001134335994720459
Batch  21  loss:  0.0017578569240868092
Batch  31  loss:  0.0010135378688573837
Batch  41  loss:  0.0014645442133769393
Batch  51  loss:  0.001266056322492659
Batch  61  loss:  0.0009001704747788608
Batch  71  loss:  0.0013369146035984159
Batch  81  loss:  0.000994822010397911
Batch  91  loss:  0.0012440175050869584
Validation on real data: 
LOSS supervised-train 0.0012935402296716347, valid 0.001588446437381208
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0011231957469135523
Batch  11  loss:  0.0009928402723744512
Batch  21  loss:  0.0017867827555164695
Batch  31  loss:  0.0011459242086857557
Batch  41  loss:  0.0018573617562651634
Batch  51  loss:  0.001190386014059186
Batch  61  loss:  0.0009682432864792645
Batch  71  loss:  0.002104097744449973
Batch  81  loss:  0.0010211457265540957
Batch  91  loss:  0.0015332956099882722
Validation on real data: 
LOSS supervised-train 0.001250968156964518, valid 0.0014513616915792227
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0007271194481290877
Batch  11  loss:  0.0009676180779933929
Batch  21  loss:  0.0017040623351931572
Batch  31  loss:  0.0010485703824087977
Batch  41  loss:  0.0010086464462801814
Batch  51  loss:  0.0013236877275630832
Batch  61  loss:  0.0010901177302002907
Batch  71  loss:  0.0013595281634479761
Batch  81  loss:  0.0011930845212191343
Batch  91  loss:  0.0008893873309716582
Validation on real data: 
LOSS supervised-train 0.001272519623162225, valid 0.0009128265082836151
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0009298897930420935
Batch  11  loss:  0.0011405724799260497
Batch  21  loss:  0.001461771666072309
Batch  31  loss:  0.0013064302038401365
Batch  41  loss:  0.0014462649123743176
Batch  51  loss:  0.0011165455216541886
Batch  61  loss:  0.0008450892637483776
Batch  71  loss:  0.0011981634888797998
Batch  81  loss:  0.0009508641087450087
Batch  91  loss:  0.0008846099954098463
Validation on real data: 
LOSS supervised-train 0.0012258738972013817, valid 0.0009601183701306581
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0009024533210322261
Batch  11  loss:  0.0009202183573506773
Batch  21  loss:  0.0012812706409022212
Batch  31  loss:  0.0010285215685144067
Batch  41  loss:  0.001050571445375681
Batch  51  loss:  0.0009092654217965901
Batch  61  loss:  0.0009692118037492037
Batch  71  loss:  0.001619839807972312
Batch  81  loss:  0.0012316216016188264
Batch  91  loss:  0.0011691225226968527
Validation on real data: 
LOSS supervised-train 0.001229195017949678, valid 0.0006601720815524459
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0014972412027418613
Batch  11  loss:  0.0009284053812734783
Batch  21  loss:  0.0015028462512418628
Batch  31  loss:  0.0007911482243798673
Batch  41  loss:  0.0009855382377281785
Batch  51  loss:  0.0012874483363702893
Batch  61  loss:  0.0007398645393550396
Batch  71  loss:  0.0020354462321847677
Batch  81  loss:  0.0007866272353567183
Batch  91  loss:  0.0009167314856313169
Validation on real data: 
LOSS supervised-train 0.001219853782095015, valid 0.0008517851238138974
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0007992179016582668
Batch  11  loss:  0.0009495812701061368
Batch  21  loss:  0.0020013360772281885
Batch  31  loss:  0.0010130652226507664
Batch  41  loss:  0.0016960472567006946
Batch  51  loss:  0.00079638249007985
Batch  61  loss:  0.0009019325952976942
Batch  71  loss:  0.0012124228524044156
Batch  81  loss:  0.0012446505716070533
Batch  91  loss:  0.0012585746590048075
Validation on real data: 
LOSS supervised-train 0.0012076287073432468, valid 0.0009539764723740518
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0009979847818613052
Batch  11  loss:  0.0009739722008816898
Batch  21  loss:  0.001418525236658752
Batch  31  loss:  0.0011622400488704443
Batch  41  loss:  0.001580297015607357
Batch  51  loss:  0.000979024451225996
Batch  61  loss:  0.0008282759808935225
Batch  71  loss:  0.0018713661702349782
Batch  81  loss:  0.0008617365965619683
Batch  91  loss:  0.0010928965639322996
Validation on real data: 
LOSS supervised-train 0.0011695240921108052, valid 0.0009930561063811183
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0009766407310962677
Batch  11  loss:  0.0009524092893116176
Batch  21  loss:  0.0016038648318499327
Batch  31  loss:  0.0009801031555980444
Batch  41  loss:  0.001018513459712267
Batch  51  loss:  0.0008142597507685423
Batch  61  loss:  0.0009135865839198232
Batch  71  loss:  0.0013581530656665564
Batch  81  loss:  0.0008330502896569669
Batch  91  loss:  0.0011886288411915302
Validation on real data: 
LOSS supervised-train 0.0011325336492154748, valid 0.001024169148877263
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0007909748237580061
Batch  11  loss:  0.0007693901425227523
Batch  21  loss:  0.0015894820680841804
Batch  31  loss:  0.0009228441631421447
Batch  41  loss:  0.001571554341353476
Batch  51  loss:  0.0008503526332788169
Batch  61  loss:  0.0007655682857148349
Batch  71  loss:  0.0011309388792142272
Batch  81  loss:  0.0013545190449804068
Batch  91  loss:  0.00096005795057863
Validation on real data: 
LOSS supervised-train 0.001138004254316911, valid 0.0010178174125030637
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0007631851476617157
Batch  11  loss:  0.0009255875484086573
Batch  21  loss:  0.0014374949969351292
Batch  31  loss:  0.0008704811334609985
Batch  41  loss:  0.0012686318950727582
Batch  51  loss:  0.0009827035246416926
Batch  61  loss:  0.0008625451009720564
Batch  71  loss:  0.0012789746979251504
Batch  81  loss:  0.0013786047929897904
Batch  91  loss:  0.0008368070120923221
Validation on real data: 
LOSS supervised-train 0.0011187707452336327, valid 0.0007906572427600622
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0007931723957881331
Batch  11  loss:  0.0008804152603261173
Batch  21  loss:  0.00184896111022681
Batch  31  loss:  0.0010213032364845276
Batch  41  loss:  0.0008854997577145696
Batch  51  loss:  0.0008052219054661691
Batch  61  loss:  0.0006616415921598673
Batch  71  loss:  0.001673211227171123
Batch  81  loss:  0.0008816455374471843
Batch  91  loss:  0.0012117524165660143
Validation on real data: 
LOSS supervised-train 0.001183498008758761, valid 0.0011028962908312678
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0009779967367649078
Batch  11  loss:  0.0009667682461440563
Batch  21  loss:  0.0016260432312265038
Batch  31  loss:  0.0008408983703702688
Batch  41  loss:  0.0007003219216130674
Batch  51  loss:  0.0011312111746519804
Batch  61  loss:  0.0006886111223138869
Batch  71  loss:  0.0011914923088625073
Batch  81  loss:  0.001021520234644413
Batch  91  loss:  0.0012803979916498065
Validation on real data: 
LOSS supervised-train 0.0011181468417635187, valid 0.000832010293379426
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0007213677163235843
Batch  11  loss:  0.0008929652976803482
Batch  21  loss:  0.0014117377577349544
Batch  31  loss:  0.0008598617860116065
Batch  41  loss:  0.0010779981967061758
Batch  51  loss:  0.0009761137771420181
Batch  61  loss:  0.0007033618167042732
Batch  71  loss:  0.0010877775494009256
Batch  81  loss:  0.0010058453772217035
Batch  91  loss:  0.0010837927693501115
Validation on real data: 
LOSS supervised-train 0.0011070395476417616, valid 0.001183820771984756
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0008451208123005927
Batch  11  loss:  0.0008681838517077267
Batch  21  loss:  0.001442427164874971
Batch  31  loss:  0.000921953993383795
Batch  41  loss:  0.0011556518729776144
Batch  51  loss:  0.0010677544632926583
Batch  61  loss:  0.0006442812155000865
Batch  71  loss:  0.001452757278457284
Batch  81  loss:  0.0009919932344928384
Batch  91  loss:  0.0010372258257120848
Validation on real data: 
LOSS supervised-train 0.0011381600267486647, valid 0.0010826033540070057
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0007461675559170544
Batch  11  loss:  0.0009945518104359508
Batch  21  loss:  0.0013547842390835285
Batch  31  loss:  0.0007918850751593709
Batch  41  loss:  0.0010551607701927423
Batch  51  loss:  0.0011614689137786627
Batch  61  loss:  0.0006317793158814311
Batch  71  loss:  0.0017570755444467068
Batch  81  loss:  0.001577445538714528
Batch  91  loss:  0.0010422866325825453
Validation on real data: 
LOSS supervised-train 0.0010773867959505878, valid 0.0008614021353423595
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0006034598336555064
Batch  11  loss:  0.0012904880568385124
Batch  21  loss:  0.0013766740448772907
Batch  31  loss:  0.0009731647442094982
Batch  41  loss:  0.0006810746854171157
Batch  51  loss:  0.0010723305167630315
Batch  61  loss:  0.0010641436092555523
Batch  71  loss:  0.0011840916704386473
Batch  81  loss:  0.0010358315194025636
Batch  91  loss:  0.0009916548151522875
Validation on real data: 
LOSS supervised-train 0.0010713678353931756, valid 0.0009228853159584105
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0008485729922540486
Batch  11  loss:  0.0009969975799322128
Batch  21  loss:  0.0011737348977476358
Batch  31  loss:  0.0009570109541527927
Batch  41  loss:  0.0010168531443923712
Batch  51  loss:  0.0008892665500752628
Batch  61  loss:  0.0005419065128080547
Batch  71  loss:  0.001332526677288115
Batch  81  loss:  0.0007321632583625615
Batch  91  loss:  0.0008554233354516327
Validation on real data: 
LOSS supervised-train 0.001023640696075745, valid 0.0010328968055546284
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0008504103752784431
Batch  11  loss:  0.001086583943106234
Batch  21  loss:  0.001299348776228726
Batch  31  loss:  0.001206528744660318
Batch  41  loss:  0.0009988283272832632
Batch  51  loss:  0.0011117560788989067
Batch  61  loss:  0.000780142960138619
Batch  71  loss:  0.0010228862520307302
Batch  81  loss:  0.0008060845430009067
Batch  91  loss:  0.0010708948830142617
Validation on real data: 
LOSS supervised-train 0.0010394080582773312, valid 0.0009830640628933907
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0005981918657198548
Batch  11  loss:  0.0007998059736564755
Batch  21  loss:  0.0014336822787299752
Batch  31  loss:  0.0010193090420216322
Batch  41  loss:  0.0011242845794185996
Batch  51  loss:  0.0008592616068199277
Batch  61  loss:  0.0007913084118627012
Batch  71  loss:  0.0011477329535409808
Batch  81  loss:  0.000763639051001519
Batch  91  loss:  0.0007707729819230735
Validation on real data: 
LOSS supervised-train 0.0010329892943263985, valid 0.0008402987150475383
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0005615494446828961
Batch  11  loss:  0.0006275023915804923
Batch  21  loss:  0.0012454550014808774
Batch  31  loss:  0.000927596352994442
Batch  41  loss:  0.0011529171606525779
Batch  51  loss:  0.000782298855483532
Batch  61  loss:  0.0005826609558425844
Batch  71  loss:  0.0008853767649270594
Batch  81  loss:  0.0006052738754078746
Batch  91  loss:  0.000812907179351896
Validation on real data: 
LOSS supervised-train 0.0009965932063641957, valid 0.0013120209332555532
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0005962268332950771
Batch  11  loss:  0.0008159434655681252
Batch  21  loss:  0.0014241894241422415
Batch  31  loss:  0.0009027544874697924
Batch  41  loss:  0.0008234087727032602
Batch  51  loss:  0.0007949019782245159
Batch  61  loss:  0.0009679764043539762
Batch  71  loss:  0.001084198011085391
Batch  81  loss:  0.0007664449512958527
Batch  91  loss:  0.0009674620232544839
Validation on real data: 
LOSS supervised-train 0.001018974634935148, valid 0.0005563588929362595
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0006022541201673448
Batch  11  loss:  0.0006208625854924321
Batch  21  loss:  0.0015430027851834893
Batch  31  loss:  0.0007784871850162745
Batch  41  loss:  0.0009203509544022381
Batch  51  loss:  0.000834990874864161
Batch  61  loss:  0.0006451387307606637
Batch  71  loss:  0.0010221967240795493
Batch  81  loss:  0.001317943329922855
Batch  91  loss:  0.0008354705641977489
Validation on real data: 
LOSS supervised-train 0.0010201002540998162, valid 0.0008417832432314754
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0008070611511357129
Batch  11  loss:  0.0010127027053385973
Batch  21  loss:  0.0013262038119137287
Batch  31  loss:  0.0006573225255124271
Batch  41  loss:  0.0008880645036697388
Batch  51  loss:  0.0007107286946848035
Batch  61  loss:  0.0005651455139741302
Batch  71  loss:  0.0011802185326814651
Batch  81  loss:  0.000903846463188529
Batch  91  loss:  0.0008042810950428247
Validation on real data: 
LOSS supervised-train 0.0010061832203064115, valid 0.0013290680944919586
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0005269617540761828
Batch  11  loss:  0.0011450059246271849
Batch  21  loss:  0.001439107465557754
Batch  31  loss:  0.0006764595163986087
Batch  41  loss:  0.0012503117322921753
Batch  51  loss:  0.0007306256447918713
Batch  61  loss:  0.0008954934310168028
Batch  71  loss:  0.0010386079084128141
Batch  81  loss:  0.0008297730819322169
Batch  91  loss:  0.0009658922208473086
Validation on real data: 
LOSS supervised-train 0.0009949530474841594, valid 0.0008890717290341854
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.000576162536162883
Batch  11  loss:  0.0009497489081695676
Batch  21  loss:  0.0014898383524268866
Batch  31  loss:  0.000820385292172432
Batch  41  loss:  0.0009207555558532476
Batch  51  loss:  0.00079609255772084
Batch  61  loss:  0.0006137083400972188
Batch  71  loss:  0.0012816224480047822
Batch  81  loss:  0.0006071098032407463
Batch  91  loss:  0.0010001177433878183
Validation on real data: 
LOSS supervised-train 0.000976311873528175, valid 0.0009241324733011425
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0008161194273270667
Batch  11  loss:  0.0008695049909874797
Batch  21  loss:  0.0013867078814655542
Batch  31  loss:  0.0007981936214491725
Batch  41  loss:  0.0008901156252250075
Batch  51  loss:  0.000772574101574719
Batch  61  loss:  0.0008445493876934052
Batch  71  loss:  0.0011574082309380174
Batch  81  loss:  0.0006561761838383973
Batch  91  loss:  0.001168728806078434
Validation on real data: 
LOSS supervised-train 0.0009636186680290848, valid 0.0009964380878955126
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0006662853411398828
Batch  11  loss:  0.0006564968498423696
Batch  21  loss:  0.0013844120549038053
Batch  31  loss:  0.0007661950658075511
Batch  41  loss:  0.0008987314067780972
Batch  51  loss:  0.0007622945704497397
Batch  61  loss:  0.0006610483396798372
Batch  71  loss:  0.0012769256718456745
Batch  81  loss:  0.0007423072820529342
Batch  91  loss:  0.0007185337017290294
Validation on real data: 
LOSS supervised-train 0.0009486249118344859, valid 0.0007587879081256688
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0006098797312006354
Batch  11  loss:  0.0008418469806201756
Batch  21  loss:  0.0013820849126204848
Batch  31  loss:  0.0006729295710101724
Batch  41  loss:  0.0006380313425324857
Batch  51  loss:  0.0008408406283706427
Batch  61  loss:  0.0005127602489665151
Batch  71  loss:  0.0010591150494292378
Batch  81  loss:  0.0007705464959144592
Batch  91  loss:  0.0009093924309127033
Validation on real data: 
LOSS supervised-train 0.0009269618813414126, valid 0.0007347148493863642
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.000769702426623553
Batch  11  loss:  0.0007784691406413913
Batch  21  loss:  0.0013279281556606293
Batch  31  loss:  0.000738053466193378
Batch  41  loss:  0.0007588574080727994
Batch  51  loss:  0.0009758733212947845
Batch  61  loss:  0.0005522886640392244
Batch  71  loss:  0.0009293843759223819
Batch  81  loss:  0.0006733649643138051
Batch  91  loss:  0.0009071776876226068
Validation on real data: 
LOSS supervised-train 0.0009703271178295835, valid 0.000753256375901401
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0004598163068294525
Batch  11  loss:  0.0006971032707951963
Batch  21  loss:  0.0013899302575737238
Batch  31  loss:  0.0007460711640305817
Batch  41  loss:  0.0005964449956081808
Batch  51  loss:  0.0007691077771596611
Batch  61  loss:  0.0006815933156758547
Batch  71  loss:  0.0010009221732616425
Batch  81  loss:  0.0008714183932170272
Batch  91  loss:  0.0007235851953737438
Validation on real data: 
LOSS supervised-train 0.000942903553659562, valid 0.0009659300558269024
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0006267515709623694
Batch  11  loss:  0.0006424093153327703
Batch  21  loss:  0.001200331375002861
Batch  31  loss:  0.000763388117775321
Batch  41  loss:  0.001133643207140267
Batch  51  loss:  0.0008166080806404352
Batch  61  loss:  0.0005202794563956559
Batch  71  loss:  0.0014133674558252096
Batch  81  loss:  0.0009596524760127068
Batch  91  loss:  0.0007864146609790623
Validation on real data: 
LOSS supervised-train 0.0008975827792892233, valid 0.00060135533567518
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0006086972425691783
Batch  11  loss:  0.0006905065383762121
Batch  21  loss:  0.0012708716094493866
Batch  31  loss:  0.000790646590758115
Batch  41  loss:  0.0008729088585823774
Batch  51  loss:  0.0008138576522469521
Batch  61  loss:  0.0006610142299905419
Batch  71  loss:  0.0009217702900059521
Batch  81  loss:  0.0006659795762971044
Batch  91  loss:  0.000624030246399343
Validation on real data: 
LOSS supervised-train 0.0009391439144383185, valid 0.0007029070402495563
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0005728811374865472
Batch  11  loss:  0.0008771878783591092
Batch  21  loss:  0.0013936629984527826
Batch  31  loss:  0.0008377181366086006
Batch  41  loss:  0.0008972687646746635
Batch  51  loss:  0.0006678306963294744
Batch  61  loss:  0.0006019789143465459
Batch  71  loss:  0.0007913190638646483
Batch  81  loss:  0.0007661884883418679
Batch  91  loss:  0.0008902737172320485
Validation on real data: 
LOSS supervised-train 0.0009184405516134575, valid 0.000720794836524874
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0005471781478263438
Batch  11  loss:  0.0009022135054692626
Batch  21  loss:  0.001165161607787013
Batch  31  loss:  0.000690459564793855
Batch  41  loss:  0.0009176459861919284
Batch  51  loss:  0.0008248360245488584
Batch  61  loss:  0.0005084738950245082
Batch  71  loss:  0.0009656889014877379
Batch  81  loss:  0.0009744400158524513
Batch  91  loss:  0.0006759908865205944
Validation on real data: 
LOSS supervised-train 0.0009241081203799694, valid 0.001226364984177053
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0006030546501278877
Batch  11  loss:  0.0008022389956749976
Batch  21  loss:  0.0011701687471941113
Batch  31  loss:  0.0006059620645828545
Batch  41  loss:  0.0006780608673579991
Batch  51  loss:  0.0007456052117049694
Batch  61  loss:  0.0005823855753988028
Batch  71  loss:  0.0011985227465629578
Batch  81  loss:  0.0005586805054917932
Batch  91  loss:  0.000996647053398192
Validation on real data: 
LOSS supervised-train 0.0009223811593255959, valid 0.0006585025694221258
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0005199317238293588
Batch  11  loss:  0.0007108424324542284
Batch  21  loss:  0.0012429411290213466
Batch  31  loss:  0.0006803767755627632
Batch  41  loss:  0.0009416765533387661
Batch  51  loss:  0.0009023422026075423
Batch  61  loss:  0.0006707925931550562
Batch  71  loss:  0.0007913326262496412
Batch  81  loss:  0.0007510018185712397
Batch  91  loss:  0.0007334458059631288
Validation on real data: 
LOSS supervised-train 0.0008810662647010758, valid 0.0004920070641674101
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0006982695776969194
Batch  11  loss:  0.0009588211541995406
Batch  21  loss:  0.001189820934087038
Batch  31  loss:  0.0006086918874643743
Batch  41  loss:  0.0006745546706952155
Batch  51  loss:  0.0006382620777003467
Batch  61  loss:  0.0006609448464587331
Batch  71  loss:  0.001080801011994481
Batch  81  loss:  0.0005872404435649514
Batch  91  loss:  0.0007175250211730599
Validation on real data: 
LOSS supervised-train 0.0008740410624886863, valid 0.0008119675330817699
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0007879944168962538
Batch  11  loss:  0.0008175365510396659
Batch  21  loss:  0.001229920657351613
Batch  31  loss:  0.0006685183616355062
Batch  41  loss:  0.0005402371753007174
Batch  51  loss:  0.0007436073501594365
Batch  61  loss:  0.00047880259808152914
Batch  71  loss:  0.0006966686341911554
Batch  81  loss:  0.0008662556065246463
Batch  91  loss:  0.0006903365137986839
Validation on real data: 
LOSS supervised-train 0.000878266658692155, valid 0.0007140074740163982
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0006087433430366218
Batch  11  loss:  0.0006687096902169287
Batch  21  loss:  0.0009564777137711644
Batch  31  loss:  0.0006842913571745157
Batch  41  loss:  0.0010925800306722522
Batch  51  loss:  0.0008730481495149434
Batch  61  loss:  0.0005326322861947119
Batch  71  loss:  0.0009150619735009968
Batch  81  loss:  0.000855835503898561
Batch  91  loss:  0.0005907386657781899
Validation on real data: 
LOSS supervised-train 0.0008947456200257875, valid 0.0010280321585014462
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0004912456497550011
Batch  11  loss:  0.0008126223110593855
Batch  21  loss:  0.0013591519091278315
Batch  31  loss:  0.0005710801342502236
Batch  41  loss:  0.0007324019097723067
Batch  51  loss:  0.0007104945252649486
Batch  61  loss:  0.000482818199088797
Batch  71  loss:  0.00098962034098804
Batch  81  loss:  0.0007431446574628353
Batch  91  loss:  0.0008885422139428556
Validation on real data: 
LOSS supervised-train 0.0008769913853029721, valid 0.0006084775668568909
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0005335292662493885
Batch  11  loss:  0.0007389414822682738
Batch  21  loss:  0.00107674824539572
Batch  31  loss:  0.0006057930295355618
Batch  41  loss:  0.0006992790149524808
Batch  51  loss:  0.0008056108490563929
Batch  61  loss:  0.00047395675210282207
Batch  71  loss:  0.0018286406993865967
Batch  81  loss:  0.0007558733341284096
Batch  91  loss:  0.0006940884632058442
Validation on real data: 
LOSS supervised-train 0.0008576173751498572, valid 0.0008583519957028329
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0005897324881516397
Batch  11  loss:  0.0006859962013550103
Batch  21  loss:  0.001129694515839219
Batch  31  loss:  0.0006289809825830162
Batch  41  loss:  0.0007514485623687506
Batch  51  loss:  0.0007094204192981124
Batch  61  loss:  0.0006603197543881834
Batch  71  loss:  0.000578074948862195
Batch  81  loss:  0.0006375440279953182
Batch  91  loss:  0.000686937419231981
Validation on real data: 
LOSS supervised-train 0.0008509197953389957, valid 0.0006244820542633533
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0007217681268230081
Batch  11  loss:  0.0006682921084575355
Batch  21  loss:  0.0010574536863714457
Batch  31  loss:  0.0005686872755177319
Batch  41  loss:  0.0006750148604623973
Batch  51  loss:  0.0006781433476135135
Batch  61  loss:  0.000587940332479775
Batch  71  loss:  0.0007136720232665539
Batch  81  loss:  0.0008977008983492851
Batch  91  loss:  0.0008645691559650004
Validation on real data: 
LOSS supervised-train 0.0008395986692630686, valid 0.0006528170197270811
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0007574438350275159
Batch  11  loss:  0.0006977999000810087
Batch  21  loss:  0.0009771581972017884
Batch  31  loss:  0.0005806268309243023
Batch  41  loss:  0.0007837673183530569
Batch  51  loss:  0.0005759567720815539
Batch  61  loss:  0.0005146635812707245
Batch  71  loss:  0.0008982918225228786
Batch  81  loss:  0.0009173115831799805
Batch  91  loss:  0.0006505469791591167
Validation on real data: 
LOSS supervised-train 0.0008265182049944997, valid 0.0006135892472229898
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0006742235273122787
Batch  11  loss:  0.0006430059438571334
Batch  21  loss:  0.0013787683565169573
Batch  31  loss:  0.00046587816905230284
Batch  41  loss:  0.0006459853611886501
Batch  51  loss:  0.0006523149786517024
Batch  61  loss:  0.000534006510861218
Batch  71  loss:  0.0008788828272372484
Batch  81  loss:  0.0007237704121507704
Batch  91  loss:  0.000855419144500047
Validation on real data: 
LOSS supervised-train 0.0008438517930335365, valid 0.0006384566077031195
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0007523244712501764
Batch  11  loss:  0.0007027127430774271
Batch  21  loss:  0.0010462978389114141
Batch  31  loss:  0.0007560356752946973
Batch  41  loss:  0.0006569548277184367
Batch  51  loss:  0.0005823462852276862
Batch  61  loss:  0.0007387307705357671
Batch  71  loss:  0.0007965841796249151
Batch  81  loss:  0.000713453337084502
Batch  91  loss:  0.000985985971055925
Validation on real data: 
LOSS supervised-train 0.0008353472937596962, valid 0.0007303394377231598
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0005387341952882707
Batch  11  loss:  0.0007398047600872815
Batch  21  loss:  0.0014627394266426563
Batch  31  loss:  0.0007411317201331258
Batch  41  loss:  0.0005960988346487284
Batch  51  loss:  0.000712106644641608
Batch  61  loss:  0.0005505553563125432
Batch  71  loss:  0.0008031413308344781
Batch  81  loss:  0.0005545794847421348
Batch  91  loss:  0.000922420178540051
Validation on real data: 
LOSS supervised-train 0.0008490631511085667, valid 0.0007998496876098216
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0005523748113773763
Batch  11  loss:  0.0006916201673448086
Batch  21  loss:  0.0011961573036387563
Batch  31  loss:  0.0005861222743988037
Batch  41  loss:  0.0009918551659211516
Batch  51  loss:  0.0007354934350587428
Batch  61  loss:  0.0003868189814966172
Batch  71  loss:  0.001072019338607788
Batch  81  loss:  0.0006254373583942652
Batch  91  loss:  0.001010265783406794
Validation on real data: 
LOSS supervised-train 0.0008150964436936192, valid 0.0005759192863479257
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0007020357297733426
Batch  11  loss:  0.0009641906362958252
Batch  21  loss:  0.0010248352773487568
Batch  31  loss:  0.0006013874663040042
Batch  41  loss:  0.0007206242880783975
Batch  51  loss:  0.0007129486766643822
Batch  61  loss:  0.00044796234578825533
Batch  71  loss:  0.0009069742518477142
Batch  81  loss:  0.0008737153257243335
Batch  91  loss:  0.0008512914064340293
Validation on real data: 
LOSS supervised-train 0.0007905887271044776, valid 0.0006577617605216801
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0004655132652260363
Batch  11  loss:  0.0006708255386911333
Batch  21  loss:  0.0011120341951027513
Batch  31  loss:  0.0006054883124306798
Batch  41  loss:  0.0008111525094136596
Batch  51  loss:  0.0008342607179656625
Batch  61  loss:  0.0006291070603765547
Batch  71  loss:  0.0009471059893257916
Batch  81  loss:  0.0006346112350001931
Batch  91  loss:  0.000713046349119395
Validation on real data: 
LOSS supervised-train 0.0008139061872498132, valid 0.0007882286445237696
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0005917708622291684
Batch  11  loss:  0.0006649908027611673
Batch  21  loss:  0.0010211355984210968
Batch  31  loss:  0.0005410695448517799
Batch  41  loss:  0.000741012510843575
Batch  51  loss:  0.000686168554238975
Batch  61  loss:  0.0004412663693074137
Batch  71  loss:  0.00106857530772686
Batch  81  loss:  0.0005397305358201265
Batch  91  loss:  0.0007991415332071483
Validation on real data: 
LOSS supervised-train 0.0008115440321853385, valid 0.0009300959063693881
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.000441861804574728
Batch  11  loss:  0.000500306545291096
Batch  21  loss:  0.0011122754076495767
Batch  31  loss:  0.0006930005620233715
Batch  41  loss:  0.0011380832875147462
Batch  51  loss:  0.0008787524420768023
Batch  61  loss:  0.0005156620754860342
Batch  71  loss:  0.0006593599682673812
Batch  81  loss:  0.0006293949554674327
Batch  91  loss:  0.000596306927036494
Validation on real data: 
LOSS supervised-train 0.0008098258732934482, valid 0.0009723216062411666
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.000483696669107303
Batch  11  loss:  0.0006721173995174468
Batch  21  loss:  0.0011461058165878057
Batch  31  loss:  0.0006798875401727855
Batch  41  loss:  0.0006682196399196982
Batch  51  loss:  0.0006644120439887047
Batch  61  loss:  0.0006288594449870288
Batch  71  loss:  0.0008925849688239396
Batch  81  loss:  0.0006888793432153761
Batch  91  loss:  0.0008484410936944187
Validation on real data: 
LOSS supervised-train 0.0007953338502557017, valid 0.0005684265634045005
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0005854057963006198
Batch  11  loss:  0.0006154701113700867
Batch  21  loss:  0.0013390216045081615
Batch  31  loss:  0.0005440443055704236
Batch  41  loss:  0.0008254850399680436
Batch  51  loss:  0.000729381397832185
Batch  61  loss:  0.00045427767327055335
Batch  71  loss:  0.0009313118061982095
Batch  81  loss:  0.0005191673408262432
Batch  91  loss:  0.0007569876033812761
Validation on real data: 
LOSS supervised-train 0.0007842695305589586, valid 0.0006185872480273247
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0006059742299839854
Batch  11  loss:  0.0008554341620765626
Batch  21  loss:  0.0008920535910874605
Batch  31  loss:  0.0006457860581576824
Batch  41  loss:  0.0011373198358342052
Batch  51  loss:  0.00042274786392226815
Batch  61  loss:  0.0004653973737731576
Batch  71  loss:  0.0007963509415276349
Batch  81  loss:  0.0006125732907094061
Batch  91  loss:  0.0007966202101670206
Validation on real data: 
LOSS supervised-train 0.0007903000386431813, valid 0.0006417493568733335
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0004984388360753655
Batch  11  loss:  0.0006701182574033737
Batch  21  loss:  0.0010294371750205755
Batch  31  loss:  0.000657169905025512
Batch  41  loss:  0.0008517639944329858
Batch  51  loss:  0.000671312038321048
Batch  61  loss:  0.0004897009348496795
Batch  71  loss:  0.0009604878723621368
Batch  81  loss:  0.0005484828143380582
Batch  91  loss:  0.0009039758588187397
Validation on real data: 
LOSS supervised-train 0.000765965819300618, valid 0.0006879541906528175
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0005043601267971098
Batch  11  loss:  0.0006618577754124999
Batch  21  loss:  0.0009288608562201262
Batch  31  loss:  0.0005601986777037382
Batch  41  loss:  0.001041973358951509
Batch  51  loss:  0.0006862857262603939
Batch  61  loss:  0.0005010885070078075
Batch  71  loss:  0.0007409907993860543
Batch  81  loss:  0.0005134112434461713
Batch  91  loss:  0.0007790584932081401
Validation on real data: 
LOSS supervised-train 0.0007704977766843513, valid 0.000412049877922982
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0006690958980470896
Batch  11  loss:  0.0005134990788064897
Batch  21  loss:  0.0010686438763514161
Batch  31  loss:  0.0007852487615309656
Batch  41  loss:  0.0007276245160028338
Batch  51  loss:  0.0005895803915336728
Batch  61  loss:  0.00044335125130601227
Batch  71  loss:  0.0006957117584533989
Batch  81  loss:  0.00047401804476976395
Batch  91  loss:  0.0008321021450683475
Validation on real data: 
LOSS supervised-train 0.0007703718860284425, valid 0.0005751283606514335
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0007725111208856106
Batch  11  loss:  0.0009609386324882507
Batch  21  loss:  0.0009734175982885063
Batch  31  loss:  0.0005136938416399062
Batch  41  loss:  0.0005993740633130074
Batch  51  loss:  0.000738570699468255
Batch  61  loss:  0.0005734532605856657
Batch  71  loss:  0.0009815174853429198
Batch  81  loss:  0.0006529258098453283
Batch  91  loss:  0.0006595704471692443
Validation on real data: 
LOSS supervised-train 0.0007622856897069141, valid 0.0008104965090751648
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0006774874636903405
Batch  11  loss:  0.000569888623431325
Batch  21  loss:  0.0010345436166971922
Batch  31  loss:  0.0005749849951826036
Batch  41  loss:  0.0008114704978652298
Batch  51  loss:  0.00046682596439495683
Batch  61  loss:  0.00048349611461162567
Batch  71  loss:  0.0009631422581151128
Batch  81  loss:  0.000601095671299845
Batch  91  loss:  0.0007460700580850244
Validation on real data: 
LOSS supervised-train 0.0007615950217586942, valid 0.0006317211664281785
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0004987204447388649
Batch  11  loss:  0.0004957086639478803
Batch  21  loss:  0.0011306875385344028
Batch  31  loss:  0.000615282217040658
Batch  41  loss:  0.0009272489696741104
Batch  51  loss:  0.0008726365631446242
Batch  61  loss:  0.0005688557866960764
Batch  71  loss:  0.0010394545970484614
Batch  81  loss:  0.0006406392203643918
Batch  91  loss:  0.000739453244023025
Validation on real data: 
LOSS supervised-train 0.0007538291736273095, valid 0.0008717752643860877
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.000555308535695076
Batch  11  loss:  0.0007175212958827615
Batch  21  loss:  0.0008809590362943709
Batch  31  loss:  0.0005682320916093886
Batch  41  loss:  0.0007883444777689874
Batch  51  loss:  0.0008497343515045941
Batch  61  loss:  0.000370153778931126
Batch  71  loss:  0.0010289845522493124
Batch  81  loss:  0.0006267699645832181
Batch  91  loss:  0.00047737351269461215
Validation on real data: 
LOSS supervised-train 0.0007610585499787703, valid 0.0005830556619912386
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0004644719301722944
Batch  11  loss:  0.0006275869091041386
Batch  21  loss:  0.001008395804092288
Batch  31  loss:  0.0005144894239492714
Batch  41  loss:  0.000742458098102361
Batch  51  loss:  0.0007708240882493556
Batch  61  loss:  0.00044946878915652633
Batch  71  loss:  0.0008444592240266502
Batch  81  loss:  0.0007033538422547281
Batch  91  loss:  0.0007940768264234066
Validation on real data: 
LOSS supervised-train 0.0007685084146214649, valid 0.0005493167554959655
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0006581077468581498
Batch  11  loss:  0.000502300332300365
Batch  21  loss:  0.0009415954118594527
Batch  31  loss:  0.0005586454644799232
Batch  41  loss:  0.0007006218074820936
Batch  51  loss:  0.0006041837623342872
Batch  61  loss:  0.0005135734099894762
Batch  71  loss:  0.0010628129821270704
Batch  81  loss:  0.0005531703936867416
Batch  91  loss:  0.000858891464304179
Validation on real data: 
LOSS supervised-train 0.000760412305535283, valid 0.000498468813020736
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0005751447752118111
Batch  11  loss:  0.0007155349012464285
Batch  21  loss:  0.0009389699553139508
Batch  31  loss:  0.000601298117544502
Batch  41  loss:  0.0009974588174372911
Batch  51  loss:  0.0007057832553982735
Batch  61  loss:  0.0004220655537210405
Batch  71  loss:  0.0008306493400596082
Batch  81  loss:  0.0005504436558112502
Batch  91  loss:  0.0008162215817719698
Validation on real data: 
LOSS supervised-train 0.0007401743807713501, valid 0.0005810325383208692
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0005470982287079096
Batch  11  loss:  0.0006196544854901731
Batch  21  loss:  0.0008610388613305986
Batch  31  loss:  0.000592212425544858
Batch  41  loss:  0.0006438462878577411
Batch  51  loss:  0.0010737598640844226
Batch  61  loss:  0.0004243383591528982
Batch  71  loss:  0.0007873724098317325
Batch  81  loss:  0.0005637067370116711
Batch  91  loss:  0.000702088582329452
Validation on real data: 
LOSS supervised-train 0.000730861130869016, valid 0.00043410813668742776
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0005051429616287351
Batch  11  loss:  0.00046997552271932364
Batch  21  loss:  0.00109489809256047
Batch  31  loss:  0.0005319325719028711
Batch  41  loss:  0.0008678727899678051
Batch  51  loss:  0.0006725506391376257
Batch  61  loss:  0.0007719847490079701
Batch  71  loss:  0.0007720271241851151
Batch  81  loss:  0.00057236134307459
Batch  91  loss:  0.000723592471331358
Validation on real data: 
LOSS supervised-train 0.0007405472526443191, valid 0.00043637483031488955
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0007789148367010057
Batch  11  loss:  0.0006614533485844731
Batch  21  loss:  0.0010571854654699564
Batch  31  loss:  0.0005504900473169982
Batch  41  loss:  0.0013054542941972613
Batch  51  loss:  0.0007499082130379975
Batch  61  loss:  0.00038557819789275527
Batch  71  loss:  0.0007694281521253288
Batch  81  loss:  0.0006348203169181943
Batch  91  loss:  0.0007628458552062511
Validation on real data: 
LOSS supervised-train 0.000715649364865385, valid 0.0006894554244354367
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0006559832254424691
Batch  11  loss:  0.0007599908858537674
Batch  21  loss:  0.0009681934025138617
Batch  31  loss:  0.0005020794342271984
Batch  41  loss:  0.0006784264696761966
Batch  51  loss:  0.0007349866209551692
Batch  61  loss:  0.00045955347013659775
Batch  71  loss:  0.0008833472966216505
Batch  81  loss:  0.0005773502634838223
Batch  91  loss:  0.0007560541853308678
Validation on real data: 
LOSS supervised-train 0.0007122097120736726, valid 0.00041687319753691554
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  helmet ; Model ID: 3621cf047be0d1ae52fafb0cab311e6a
--------------------
Training baseline regression model:  2022-03-30 15:57:30.413678
Detector:  pointnet
Object:  helmet
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1610788
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.17954574525356293
Batch  11  loss:  0.10022764652967453
Batch  21  loss:  0.07533001899719238
Batch  31  loss:  0.060261715203523636
Batch  41  loss:  0.05546127259731293
Batch  51  loss:  0.05084528401494026
Batch  61  loss:  0.047916948795318604
Batch  71  loss:  0.03899529576301575
Batch  81  loss:  0.038752809166908264
Batch  91  loss:  0.03129209578037262
Validation on real data: 
LOSS supervised-train 0.06319044321775437, valid 0.030412083491683006
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.033359985798597336
Batch  11  loss:  0.025586523115634918
Batch  21  loss:  0.022226547822356224
Batch  31  loss:  0.01921907626092434
Batch  41  loss:  0.01879187487065792
Batch  51  loss:  0.01373116672039032
Batch  61  loss:  0.01782338134944439
Batch  71  loss:  0.015845738351345062
Batch  81  loss:  0.014456515200436115
Batch  91  loss:  0.01721884496510029
Validation on real data: 
LOSS supervised-train 0.019469222724437713, valid 0.009755486622452736
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.015839407220482826
Batch  11  loss:  0.010738978162407875
Batch  21  loss:  0.010703256353735924
Batch  31  loss:  0.012741154991090298
Batch  41  loss:  0.01442884560674429
Batch  51  loss:  0.008129785768687725
Batch  61  loss:  0.009644780308008194
Batch  71  loss:  0.009463522583246231
Batch  81  loss:  0.008645975962281227
Batch  91  loss:  0.013334913179278374
Validation on real data: 
LOSS supervised-train 0.011074858671054243, valid 0.004905791487544775
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.012377811595797539
Batch  11  loss:  0.009027696214616299
Batch  21  loss:  0.006476951763033867
Batch  31  loss:  0.012291290797293186
Batch  41  loss:  0.0112446965649724
Batch  51  loss:  0.006033522542566061
Batch  61  loss:  0.007030497770756483
Batch  71  loss:  0.006774379871785641
Batch  81  loss:  0.00682841707020998
Batch  91  loss:  0.007875618524849415
Validation on real data: 
LOSS supervised-train 0.008706916691735386, valid 0.005314282141625881
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.009769867174327374
Batch  11  loss:  0.009086634032428265
Batch  21  loss:  0.005693178623914719
Batch  31  loss:  0.009532947093248367
Batch  41  loss:  0.009523831307888031
Batch  51  loss:  0.0068342783488333225
Batch  61  loss:  0.007244512904435396
Batch  71  loss:  0.006191175431013107
Batch  81  loss:  0.006483843084424734
Batch  91  loss:  0.007300480268895626
Validation on real data: 
LOSS supervised-train 0.00734566523693502, valid 0.0034669404849410057
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.006079263519495726
Batch  11  loss:  0.008396072313189507
Batch  21  loss:  0.004541309550404549
Batch  31  loss:  0.0074592046439647675
Batch  41  loss:  0.009949482046067715
Batch  51  loss:  0.0051512932404875755
Batch  61  loss:  0.006956854369491339
Batch  71  loss:  0.005058501847088337
Batch  81  loss:  0.007253730669617653
Batch  91  loss:  0.006037294864654541
Validation on real data: 
LOSS supervised-train 0.00634694209555164, valid 0.002025541616603732
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.006453901529312134
Batch  11  loss:  0.008283010683953762
Batch  21  loss:  0.0029777782037854195
Batch  31  loss:  0.005725335795432329
Batch  41  loss:  0.007601470686495304
Batch  51  loss:  0.0037053220439702272
Batch  61  loss:  0.0056966086849570274
Batch  71  loss:  0.004133215639740229
Batch  81  loss:  0.006307661999017
Batch  91  loss:  0.006299004424363375
Validation on real data: 
LOSS supervised-train 0.005659047106746584, valid 0.002128366380929947
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.00723684299737215
Batch  11  loss:  0.006769778672605753
Batch  21  loss:  0.003472294192761183
Batch  31  loss:  0.004101170692592859
Batch  41  loss:  0.009775320999324322
Batch  51  loss:  0.003713404992595315
Batch  61  loss:  0.005040901713073254
Batch  71  loss:  0.004534985404461622
Batch  81  loss:  0.005264264065772295
Batch  91  loss:  0.005020583979785442
Validation on real data: 
LOSS supervised-train 0.005147144584916532, valid 0.0023404243402183056
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0058274357579648495
Batch  11  loss:  0.008053484372794628
Batch  21  loss:  0.003212039126083255
Batch  31  loss:  0.005477301776409149
Batch  41  loss:  0.006265510339289904
Batch  51  loss:  0.00327192060649395
Batch  61  loss:  0.005050845444202423
Batch  71  loss:  0.003402384463697672
Batch  81  loss:  0.006452468689531088
Batch  91  loss:  0.005252578295767307
Validation on real data: 
LOSS supervised-train 0.0047902368218638, valid 0.0022633180487900972
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.004431469831615686
Batch  11  loss:  0.006563148926943541
Batch  21  loss:  0.0036509435158222914
Batch  31  loss:  0.004911151248961687
Batch  41  loss:  0.0070948414504528046
Batch  51  loss:  0.003649480175226927
Batch  61  loss:  0.003892915090546012
Batch  71  loss:  0.00447904272004962
Batch  81  loss:  0.006000871304422617
Batch  91  loss:  0.006317140534520149
Validation on real data: 
LOSS supervised-train 0.004532561399973929, valid 0.0015513127436861396
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.004825213458389044
Batch  11  loss:  0.00709433713927865
Batch  21  loss:  0.0029757735319435596
Batch  31  loss:  0.004744680132716894
Batch  41  loss:  0.006639590486884117
Batch  51  loss:  0.0034793433733284473
Batch  61  loss:  0.0038577644154429436
Batch  71  loss:  0.004068527836352587
Batch  81  loss:  0.006132390350103378
Batch  91  loss:  0.004290794488042593
Validation on real data: 
LOSS supervised-train 0.004375675362534821, valid 0.0026168811600655317
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.004017345607280731
Batch  11  loss:  0.005351805593818426
Batch  21  loss:  0.0033565633930265903
Batch  31  loss:  0.005465842317789793
Batch  41  loss:  0.007398086134344339
Batch  51  loss:  0.0036586630158126354
Batch  61  loss:  0.0043550715781748295
Batch  71  loss:  0.0038127063307911158
Batch  81  loss:  0.006826478056609631
Batch  91  loss:  0.004485772922635078
Validation on real data: 
LOSS supervised-train 0.004091870281845331, valid 0.0013871790142729878
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.003606065409258008
Batch  11  loss:  0.005380774848163128
Batch  21  loss:  0.002230697777122259
Batch  31  loss:  0.00384023180231452
Batch  41  loss:  0.004616766236722469
Batch  51  loss:  0.0030323255341500044
Batch  61  loss:  0.0033189281821250916
Batch  71  loss:  0.0029748480301350355
Batch  81  loss:  0.004124599974602461
Batch  91  loss:  0.0033528723288327456
Validation on real data: 
LOSS supervised-train 0.00380445486982353, valid 0.0018119972664862871
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.003603642573580146
Batch  11  loss:  0.00591370789334178
Batch  21  loss:  0.002163500292226672
Batch  31  loss:  0.004577235784381628
Batch  41  loss:  0.005499609746038914
Batch  51  loss:  0.0024655030574649572
Batch  61  loss:  0.003746557980775833
Batch  71  loss:  0.002929853741079569
Batch  81  loss:  0.0037017427384853363
Batch  91  loss:  0.0033072573132812977
Validation on real data: 
LOSS supervised-train 0.0036984484642744063, valid 0.0010015928419306874
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.0045492690987885
Batch  11  loss:  0.004505702760070562
Batch  21  loss:  0.0025621771346777678
Batch  31  loss:  0.00495124189183116
Batch  41  loss:  0.004180424846708775
Batch  51  loss:  0.003638586960732937
Batch  61  loss:  0.0032133026979863644
Batch  71  loss:  0.003073503263294697
Batch  81  loss:  0.004363074898719788
Batch  91  loss:  0.004063860047608614
Validation on real data: 
LOSS supervised-train 0.0036768163763917982, valid 0.0011675318237394094
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.003980211913585663
Batch  11  loss:  0.006080165971070528
Batch  21  loss:  0.00293084722943604
Batch  31  loss:  0.004343729000538588
Batch  41  loss:  0.0051187146455049515
Batch  51  loss:  0.0023743577767163515
Batch  61  loss:  0.003056509653106332
Batch  71  loss:  0.002588642295449972
Batch  81  loss:  0.004234284162521362
Batch  91  loss:  0.004106162581592798
Validation on real data: 
LOSS supervised-train 0.0034649500879459084, valid 0.001420630724169314
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.004583705682307482
Batch  11  loss:  0.004509382415562868
Batch  21  loss:  0.0021916632540524006
Batch  31  loss:  0.005323455203324556
Batch  41  loss:  0.004691788461059332
Batch  51  loss:  0.0030980908777564764
Batch  61  loss:  0.0022483912762254477
Batch  71  loss:  0.0024319649673998356
Batch  81  loss:  0.004261868540197611
Batch  91  loss:  0.0036369001027196646
Validation on real data: 
LOSS supervised-train 0.003321884898468852, valid 0.001568527310155332
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.004077618010342121
Batch  11  loss:  0.005819512531161308
Batch  21  loss:  0.002280963584780693
Batch  31  loss:  0.004896977450698614
Batch  41  loss:  0.004133375361561775
Batch  51  loss:  0.0028213297482579947
Batch  61  loss:  0.0034408578649163246
Batch  71  loss:  0.00254903850145638
Batch  81  loss:  0.0034252693876624107
Batch  91  loss:  0.002961828839033842
Validation on real data: 
LOSS supervised-train 0.0032292735727969557, valid 0.001171177253127098
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0031287765596061945
Batch  11  loss:  0.005815751384943724
Batch  21  loss:  0.0024830964393913746
Batch  31  loss:  0.004992007277905941
Batch  41  loss:  0.005475344602018595
Batch  51  loss:  0.0028939859475940466
Batch  61  loss:  0.003186417045071721
Batch  71  loss:  0.0032715587876737118
Batch  81  loss:  0.003855555783957243
Batch  91  loss:  0.0036052647046744823
Validation on real data: 
LOSS supervised-train 0.0032669560390058906, valid 0.0013541339430958033
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0032897917553782463
Batch  11  loss:  0.004178149159997702
Batch  21  loss:  0.0026643509045243263
Batch  31  loss:  0.004251628648489714
Batch  41  loss:  0.0052187442779541016
Batch  51  loss:  0.0030024831648916006
Batch  61  loss:  0.0028726826421916485
Batch  71  loss:  0.003405582159757614
Batch  81  loss:  0.003721080254763365
Batch  91  loss:  0.0029634099919348955
Validation on real data: 
LOSS supervised-train 0.003119430185761303, valid 0.0011631293455138803
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0032235411927103996
Batch  11  loss:  0.0032546590082347393
Batch  21  loss:  0.0027613004203885794
Batch  31  loss:  0.003481213701888919
Batch  41  loss:  0.0035322096664458513
Batch  51  loss:  0.0026319988537579775
Batch  61  loss:  0.0025754827074706554
Batch  71  loss:  0.002304790774360299
Batch  81  loss:  0.0026617650873959064
Batch  91  loss:  0.0029606143943965435
Validation on real data: 
LOSS supervised-train 0.0030107789800968023, valid 0.0009432215592823923
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.002937415847554803
Batch  11  loss:  0.004728844854980707
Batch  21  loss:  0.0023255269043147564
Batch  31  loss:  0.004218821879476309
Batch  41  loss:  0.00394076481461525
Batch  51  loss:  0.002364236628636718
Batch  61  loss:  0.002582816407084465
Batch  71  loss:  0.0019055979792028666
Batch  81  loss:  0.004246490076184273
Batch  91  loss:  0.0031343307346105576
Validation on real data: 
LOSS supervised-train 0.002936131526948884, valid 0.0009576986776664853
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00360392895527184
Batch  11  loss:  0.0034219759982079268
Batch  21  loss:  0.0033415991347283125
Batch  31  loss:  0.0029914092738181353
Batch  41  loss:  0.0032202277798205614
Batch  51  loss:  0.0025624274276196957
Batch  61  loss:  0.003156604478135705
Batch  71  loss:  0.0026139330584555864
Batch  81  loss:  0.002982341917231679
Batch  91  loss:  0.002578013576567173
Validation on real data: 
LOSS supervised-train 0.00281347845797427, valid 0.0011115710949525237
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.00317592266947031
Batch  11  loss:  0.003743399865925312
Batch  21  loss:  0.002438342897221446
Batch  31  loss:  0.0036102558951824903
Batch  41  loss:  0.003369088750332594
Batch  51  loss:  0.0026214858517050743
Batch  61  loss:  0.002077119192108512
Batch  71  loss:  0.0023505419958382845
Batch  81  loss:  0.003222751198336482
Batch  91  loss:  0.0037920125760138035
Validation on real data: 
LOSS supervised-train 0.0027859455987345426, valid 0.00098367256578058
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0033233007416129112
Batch  11  loss:  0.00401313416659832
Batch  21  loss:  0.002170280320569873
Batch  31  loss:  0.0034538425970822573
Batch  41  loss:  0.003409032244235277
Batch  51  loss:  0.002698299242183566
Batch  61  loss:  0.0026363800279796124
Batch  71  loss:  0.002853872487321496
Batch  81  loss:  0.003449556417763233
Batch  91  loss:  0.002311383606866002
Validation on real data: 
LOSS supervised-train 0.002808644915930927, valid 0.0013685181038454175
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0028264978900551796
Batch  11  loss:  0.0031321877613663673
Batch  21  loss:  0.002254549413919449
Batch  31  loss:  0.0031496791634708643
Batch  41  loss:  0.0037542697973549366
Batch  51  loss:  0.002801222028210759
Batch  61  loss:  0.0025057753082364798
Batch  71  loss:  0.0021687496919184923
Batch  81  loss:  0.0027713736053556204
Batch  91  loss:  0.003430014243349433
Validation on real data: 
LOSS supervised-train 0.002636940265074372, valid 0.0010887912940233946
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0027161745820194483
Batch  11  loss:  0.003025858895853162
Batch  21  loss:  0.0018212365685030818
Batch  31  loss:  0.0047396947629749775
Batch  41  loss:  0.0033626246731728315
Batch  51  loss:  0.0014863526448607445
Batch  61  loss:  0.0027133142575621605
Batch  71  loss:  0.0020550424233078957
Batch  81  loss:  0.0025620083324611187
Batch  91  loss:  0.0027902191504836082
Validation on real data: 
LOSS supervised-train 0.0026389480789657683, valid 0.0012199063785374165
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0028967224061489105
Batch  11  loss:  0.0024851008784025908
Batch  21  loss:  0.002123202895745635
Batch  31  loss:  0.002890647854655981
Batch  41  loss:  0.0034611904993653297
Batch  51  loss:  0.001996742794290185
Batch  61  loss:  0.002499629510566592
Batch  71  loss:  0.002079644938930869
Batch  81  loss:  0.0031681768596172333
Batch  91  loss:  0.0025061587803065777
Validation on real data: 
LOSS supervised-train 0.0025721416750457138, valid 0.0008239737944677472
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.003012038068845868
Batch  11  loss:  0.002638468751683831
Batch  21  loss:  0.0017164793098345399
Batch  31  loss:  0.00316935358569026
Batch  41  loss:  0.0029227768536657095
Batch  51  loss:  0.0031450800597667694
Batch  61  loss:  0.0026983150746673346
Batch  71  loss:  0.002018552040681243
Batch  81  loss:  0.00295008416287601
Batch  91  loss:  0.0024556522257626057
Validation on real data: 
LOSS supervised-train 0.00250766244600527, valid 0.0010168775916099548
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0029628362972289324
Batch  11  loss:  0.0028825635090470314
Batch  21  loss:  0.0019570449367165565
Batch  31  loss:  0.003667751792818308
Batch  41  loss:  0.0038577772211283445
Batch  51  loss:  0.0017012380994856358
Batch  61  loss:  0.0022033757995814085
Batch  71  loss:  0.001455173478461802
Batch  81  loss:  0.002484896220266819
Batch  91  loss:  0.0030417616944760084
Validation on real data: 
LOSS supervised-train 0.002437192822108045, valid 0.0011438208166509867
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0027065807953476906
Batch  11  loss:  0.0032984453719109297
Batch  21  loss:  0.0024331563618034124
Batch  31  loss:  0.003949369303882122
Batch  41  loss:  0.0026844865642488003
Batch  51  loss:  0.002670272719115019
Batch  61  loss:  0.0020577663090080023
Batch  71  loss:  0.0024974634870886803
Batch  81  loss:  0.002563480054959655
Batch  91  loss:  0.0029896285850554705
Validation on real data: 
LOSS supervised-train 0.0024733015801757575, valid 0.0009871674701571465
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.002736645983532071
Batch  11  loss:  0.0054854522459208965
Batch  21  loss:  0.0021670195274055004
Batch  31  loss:  0.002355588600039482
Batch  41  loss:  0.0022987474221736193
Batch  51  loss:  0.0019970000721514225
Batch  61  loss:  0.002667759545147419
Batch  71  loss:  0.002188045997172594
Batch  81  loss:  0.00228095636703074
Batch  91  loss:  0.0024756607599556446
Validation on real data: 
LOSS supervised-train 0.0024454329791478813, valid 0.0009654324967414141
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0027073656674474478
Batch  11  loss:  0.0019805419724434614
Batch  21  loss:  0.0018605847144499421
Batch  31  loss:  0.0029783034697175026
Batch  41  loss:  0.003339075483381748
Batch  51  loss:  0.00175306701567024
Batch  61  loss:  0.0019891001284122467
Batch  71  loss:  0.00157199427485466
Batch  81  loss:  0.0031771683134138584
Batch  91  loss:  0.002204545307904482
Validation on real data: 
LOSS supervised-train 0.0023728452855721117, valid 0.001373317209072411
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0025677860248833895
Batch  11  loss:  0.0030726175755262375
Batch  21  loss:  0.002128909109160304
Batch  31  loss:  0.0029890667647123337
Batch  41  loss:  0.002315671183168888
Batch  51  loss:  0.0024312324821949005
Batch  61  loss:  0.0018781457329168916
Batch  71  loss:  0.002158916322514415
Batch  81  loss:  0.0026602332945913076
Batch  91  loss:  0.001983931753784418
Validation on real data: 
LOSS supervised-train 0.0023246108961757273, valid 0.0010870356345549226
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.002246020594611764
Batch  11  loss:  0.0026525999419391155
Batch  21  loss:  0.001759723643772304
Batch  31  loss:  0.003103130031377077
Batch  41  loss:  0.0024221299681812525
Batch  51  loss:  0.0021091506350785494
Batch  61  loss:  0.0021951457019895315
Batch  71  loss:  0.0020582794677466154
Batch  81  loss:  0.0025367713533341885
Batch  91  loss:  0.003707073861733079
Validation on real data: 
LOSS supervised-train 0.002304367223987356, valid 0.0010979017242789268
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.003001868724822998
Batch  11  loss:  0.002330420073121786
Batch  21  loss:  0.0020583851728588343
Batch  31  loss:  0.0026998291723430157
Batch  41  loss:  0.0018658506451174617
Batch  51  loss:  0.0017859882209450006
Batch  61  loss:  0.002638077363371849
Batch  71  loss:  0.0017742025665938854
Batch  81  loss:  0.0025159805081784725
Batch  91  loss:  0.002424264093860984
Validation on real data: 
LOSS supervised-train 0.0022786894033197313, valid 0.001349924597889185
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0020948683377355337
Batch  11  loss:  0.0024703270755708218
Batch  21  loss:  0.0018120085587725043
Batch  31  loss:  0.0022768641356378794
Batch  41  loss:  0.003076355205848813
Batch  51  loss:  0.0017209261422976851
Batch  61  loss:  0.002358050784096122
Batch  71  loss:  0.0016574948094785213
Batch  81  loss:  0.002505563199520111
Batch  91  loss:  0.0018080479931086302
Validation on real data: 
LOSS supervised-train 0.002184821462724358, valid 0.0011918582022190094
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0023789533879607916
Batch  11  loss:  0.0020649044308811426
Batch  21  loss:  0.0015147492522373796
Batch  31  loss:  0.0026161442510783672
Batch  41  loss:  0.0025349371135234833
Batch  51  loss:  0.002052966272458434
Batch  61  loss:  0.0023087800946086645
Batch  71  loss:  0.001836129929870367
Batch  81  loss:  0.0019655325450003147
Batch  91  loss:  0.0015852123033255339
Validation on real data: 
LOSS supervised-train 0.00213050348800607, valid 0.0010146598797291517
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0029719690792262554
Batch  11  loss:  0.004002118483185768
Batch  21  loss:  0.0016971018631011248
Batch  31  loss:  0.0018841199344024062
Batch  41  loss:  0.0022344121243804693
Batch  51  loss:  0.001856174087151885
Batch  61  loss:  0.0021402910351753235
Batch  71  loss:  0.002157212933525443
Batch  81  loss:  0.002462342381477356
Batch  91  loss:  0.0025071895215660334
Validation on real data: 
LOSS supervised-train 0.0021833942364901304, valid 0.0011231800308451056
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0021411122288554907
Batch  11  loss:  0.002660940634086728
Batch  21  loss:  0.0015593224670737982
Batch  31  loss:  0.002130164997652173
Batch  41  loss:  0.0023287576623260975
Batch  51  loss:  0.0012918878346681595
Batch  61  loss:  0.0020099713001400232
Batch  71  loss:  0.001559429569169879
Batch  81  loss:  0.002262041438370943
Batch  91  loss:  0.0019114980241283774
Validation on real data: 
LOSS supervised-train 0.002112658862024546, valid 0.0011951018823310733
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0019641381222754717
Batch  11  loss:  0.0022008237428963184
Batch  21  loss:  0.0018093687249347568
Batch  31  loss:  0.0019948051776736975
Batch  41  loss:  0.0024211318232119083
Batch  51  loss:  0.0021204473450779915
Batch  61  loss:  0.002286143833771348
Batch  71  loss:  0.001751028816215694
Batch  81  loss:  0.002104114508256316
Batch  91  loss:  0.002291756449267268
Validation on real data: 
LOSS supervised-train 0.002153447004966438, valid 0.0007700108108110726
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0018396019004285336
Batch  11  loss:  0.003400662215426564
Batch  21  loss:  0.0017736583249643445
Batch  31  loss:  0.0024147354997694492
Batch  41  loss:  0.0022904479410499334
Batch  51  loss:  0.00179428665433079
Batch  61  loss:  0.0020649356301873922
Batch  71  loss:  0.0015967738581821322
Batch  81  loss:  0.001875466201454401
Batch  91  loss:  0.0022084831725806
Validation on real data: 
LOSS supervised-train 0.0020344480220228435, valid 0.0007859061588533223
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0022989690769463778
Batch  11  loss:  0.0024655810557305813
Batch  21  loss:  0.0019089278066530824
Batch  31  loss:  0.001941609662026167
Batch  41  loss:  0.002445635851472616
Batch  51  loss:  0.0030856525991111994
Batch  61  loss:  0.0025893135461956263
Batch  71  loss:  0.0013810844393447042
Batch  81  loss:  0.002042656997218728
Batch  91  loss:  0.0018927972996607423
Validation on real data: 
LOSS supervised-train 0.002067447748268023, valid 0.0009921451564878225
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.002172953449189663
Batch  11  loss:  0.002188508166000247
Batch  21  loss:  0.0014689539093524218
Batch  31  loss:  0.0017481257673352957
Batch  41  loss:  0.0021399406250566244
Batch  51  loss:  0.0016291472129523754
Batch  61  loss:  0.0022425579372793436
Batch  71  loss:  0.0013716933317482471
Batch  81  loss:  0.002113804453983903
Batch  91  loss:  0.002190369414165616
Validation on real data: 
LOSS supervised-train 0.0019587160227820277, valid 0.0007550345035269856
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.002351900562644005
Batch  11  loss:  0.0028680211398750544
Batch  21  loss:  0.0015378030948340893
Batch  31  loss:  0.0023392359726130962
Batch  41  loss:  0.001992418197914958
Batch  51  loss:  0.00243739178404212
Batch  61  loss:  0.0021405427251011133
Batch  71  loss:  0.001668184413574636
Batch  81  loss:  0.001952046062797308
Batch  91  loss:  0.001947977696545422
Validation on real data: 
LOSS supervised-train 0.0019740010157693177, valid 0.0007681628922000527
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0029145919252187014
Batch  11  loss:  0.0031981286592781544
Batch  21  loss:  0.001458468148484826
Batch  31  loss:  0.002607273403555155
Batch  41  loss:  0.002236815867945552
Batch  51  loss:  0.001857758848927915
Batch  61  loss:  0.0022092291619628668
Batch  71  loss:  0.0018813034985214472
Batch  81  loss:  0.0026932808104902506
Batch  91  loss:  0.002605469198897481
Validation on real data: 
LOSS supervised-train 0.0020493208465632053, valid 0.000900853134226054
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0018962147878482938
Batch  11  loss:  0.001602925593033433
Batch  21  loss:  0.0011411596788093448
Batch  31  loss:  0.0018858104012906551
Batch  41  loss:  0.0025349301286041737
Batch  51  loss:  0.002661720383912325
Batch  61  loss:  0.0015881980070844293
Batch  71  loss:  0.0015909834764897823
Batch  81  loss:  0.001446734182536602
Batch  91  loss:  0.0018200684571638703
Validation on real data: 
LOSS supervised-train 0.0019077190826646984, valid 0.00108399149030447
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.002235486637800932
Batch  11  loss:  0.0026062410324811935
Batch  21  loss:  0.0017439654329791665
Batch  31  loss:  0.00142956105992198
Batch  41  loss:  0.001537361298687756
Batch  51  loss:  0.001543416641652584
Batch  61  loss:  0.0017469067825004458
Batch  71  loss:  0.0018623657524585724
Batch  81  loss:  0.002465864410623908
Batch  91  loss:  0.002311041811481118
Validation on real data: 
LOSS supervised-train 0.0019136791885830463, valid 0.0011192173697054386
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.002059434074908495
Batch  11  loss:  0.0023825562093406916
Batch  21  loss:  0.0010768770007416606
Batch  31  loss:  0.0015428196638822556
Batch  41  loss:  0.0020206195767968893
Batch  51  loss:  0.0019359238212928176
Batch  61  loss:  0.002153407083824277
Batch  71  loss:  0.001950478763319552
Batch  81  loss:  0.0019562840461730957
Batch  91  loss:  0.0015408099861815572
Validation on real data: 
LOSS supervised-train 0.0018869990552775563, valid 0.0010751793161034584
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0029349594842642546
Batch  11  loss:  0.0020148656331002712
Batch  21  loss:  0.0013773117680102587
Batch  31  loss:  0.0016825381899252534
Batch  41  loss:  0.0024149499367922544
Batch  51  loss:  0.0023759910836815834
Batch  61  loss:  0.0019165126141160727
Batch  71  loss:  0.0017548140604048967
Batch  81  loss:  0.0017504054121673107
Batch  91  loss:  0.002383624669164419
Validation on real data: 
LOSS supervised-train 0.0018971665215212852, valid 0.0008172251982614398
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0019341132137924433
Batch  11  loss:  0.002142910612747073
Batch  21  loss:  0.0016057780012488365
Batch  31  loss:  0.0019999321084469557
Batch  41  loss:  0.0020400604698807
Batch  51  loss:  0.001976619241759181
Batch  61  loss:  0.0013198608066886663
Batch  71  loss:  0.0015830310294404626
Batch  81  loss:  0.0016286424361169338
Batch  91  loss:  0.0013439665781334043
Validation on real data: 
LOSS supervised-train 0.0017976768233347684, valid 0.001161223859526217
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0015848594484850764
Batch  11  loss:  0.0020749708637595177
Batch  21  loss:  0.0018363326089456677
Batch  31  loss:  0.0020647591445595026
Batch  41  loss:  0.002535854931920767
Batch  51  loss:  0.0021250571589916945
Batch  61  loss:  0.0016530215507373214
Batch  71  loss:  0.0015636527677997947
Batch  81  loss:  0.0018943051109090447
Batch  91  loss:  0.00174125493504107
Validation on real data: 
LOSS supervised-train 0.0018745787191437558, valid 0.0008145747124217451
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.002285207621753216
Batch  11  loss:  0.0026700864546000957
Batch  21  loss:  0.0013588088331744075
Batch  31  loss:  0.00132960663177073
Batch  41  loss:  0.0018455088138580322
Batch  51  loss:  0.0016019371105358005
Batch  61  loss:  0.0015513105317950249
Batch  71  loss:  0.001923805451951921
Batch  81  loss:  0.001494004507549107
Batch  91  loss:  0.002089335350319743
Validation on real data: 
LOSS supervised-train 0.0017780064803082495, valid 0.0007534105679951608
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0019326445180922747
Batch  11  loss:  0.0018847122555598617
Batch  21  loss:  0.0014132271753624082
Batch  31  loss:  0.00183661631308496
Batch  41  loss:  0.001799090183340013
Batch  51  loss:  0.0014654010301455855
Batch  61  loss:  0.0013168425066396594
Batch  71  loss:  0.0016570501029491425
Batch  81  loss:  0.0021963980980217457
Batch  91  loss:  0.0017373322043567896
Validation on real data: 
LOSS supervised-train 0.001826186969410628, valid 0.0008474546484649181
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.002161620184779167
Batch  11  loss:  0.0017784703522920609
Batch  21  loss:  0.0017736166482791305
Batch  31  loss:  0.0017167883925139904
Batch  41  loss:  0.0017959222896024585
Batch  51  loss:  0.001459066872484982
Batch  61  loss:  0.0021771155297756195
Batch  71  loss:  0.0015176739543676376
Batch  81  loss:  0.0017227819189429283
Batch  91  loss:  0.0015712302410975099
Validation on real data: 
LOSS supervised-train 0.0018534243491012603, valid 0.0010940463980659842
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0017992063658311963
Batch  11  loss:  0.0018904923927038908
Batch  21  loss:  0.0017073878552764654
Batch  31  loss:  0.0018346349243074656
Batch  41  loss:  0.0019170641899108887
Batch  51  loss:  0.001251797890290618
Batch  61  loss:  0.001955856569111347
Batch  71  loss:  0.0013521185610443354
Batch  81  loss:  0.0022042919881641865
Batch  91  loss:  0.0017202560557052493
Validation on real data: 
LOSS supervised-train 0.0017466519330628216, valid 0.0009068390354514122
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.002098141936585307
Batch  11  loss:  0.0015141662443056703
Batch  21  loss:  0.001672848709858954
Batch  31  loss:  0.0015036424156278372
Batch  41  loss:  0.0017376558389514685
Batch  51  loss:  0.001735042897053063
Batch  61  loss:  0.0023212148807942867
Batch  71  loss:  0.001569052692502737
Batch  81  loss:  0.0022113074082881212
Batch  91  loss:  0.0017073171911761165
Validation on real data: 
LOSS supervised-train 0.0016884086548816413, valid 0.0008429284207522869
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0018153978744521737
Batch  11  loss:  0.00224499823525548
Batch  21  loss:  0.00171132932882756
Batch  31  loss:  0.0022240763064473867
Batch  41  loss:  0.0019888493698090315
Batch  51  loss:  0.0015021961880847812
Batch  61  loss:  0.002168637700378895
Batch  71  loss:  0.0015882208244875073
Batch  81  loss:  0.0017926754662767053
Batch  91  loss:  0.0017691063694655895
Validation on real data: 
LOSS supervised-train 0.0017746075202012435, valid 0.0009010813664644957
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.002169540151953697
Batch  11  loss:  0.0016290646744892001
Batch  21  loss:  0.001509654219262302
Batch  31  loss:  0.0019026289228349924
Batch  41  loss:  0.002152624074369669
Batch  51  loss:  0.0014222150202840567
Batch  61  loss:  0.001474139979109168
Batch  71  loss:  0.0017710882239043713
Batch  81  loss:  0.002085196552798152
Batch  91  loss:  0.0016598060028627515
Validation on real data: 
LOSS supervised-train 0.0016963150422088802, valid 0.0009256104240193963
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0018195031443610787
Batch  11  loss:  0.0013027378590777516
Batch  21  loss:  0.0015552439726889133
Batch  31  loss:  0.0017075437353923917
Batch  41  loss:  0.00164891651365906
Batch  51  loss:  0.0015524419723078609
Batch  61  loss:  0.0015915307449176908
Batch  71  loss:  0.0014933102065697312
Batch  81  loss:  0.0014473838964477181
Batch  91  loss:  0.0017122223507612944
Validation on real data: 
LOSS supervised-train 0.0017248309880960732, valid 0.0011010803282260895
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.001280666678212583
Batch  11  loss:  0.0018130317330360413
Batch  21  loss:  0.0014075621729716659
Batch  31  loss:  0.0022161041852086782
Batch  41  loss:  0.001968745142221451
Batch  51  loss:  0.0020532282069325447
Batch  61  loss:  0.0016932402504608035
Batch  71  loss:  0.0013354645343497396
Batch  81  loss:  0.0015768615994602442
Batch  91  loss:  0.0018660305067896843
Validation on real data: 
LOSS supervised-train 0.001694570352556184, valid 0.0007948889979161322
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0018967726500704885
Batch  11  loss:  0.0021922846790403128
Batch  21  loss:  0.0012980158207938075
Batch  31  loss:  0.0015746729914098978
Batch  41  loss:  0.0016072392463684082
Batch  51  loss:  0.0019515946041792631
Batch  61  loss:  0.0022765619214624166
Batch  71  loss:  0.0015102202305570245
Batch  81  loss:  0.0021354949567466974
Batch  91  loss:  0.0016457726014778018
Validation on real data: 
LOSS supervised-train 0.0017017757770372555, valid 0.0009097866714000702
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0020705056376755238
Batch  11  loss:  0.001536964438855648
Batch  21  loss:  0.001389943528920412
Batch  31  loss:  0.001948811230249703
Batch  41  loss:  0.001706408802419901
Batch  51  loss:  0.0017897372599691153
Batch  61  loss:  0.0018258100608363748
Batch  71  loss:  0.001340437331236899
Batch  81  loss:  0.0015526957577094436
Batch  91  loss:  0.0014046127907931805
Validation on real data: 
LOSS supervised-train 0.0016457488096784801, valid 0.00085826386930421
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0020429405849426985
Batch  11  loss:  0.0022493370342999697
Batch  21  loss:  0.001343602198176086
Batch  31  loss:  0.0013532815501093864
Batch  41  loss:  0.0018899065908044577
Batch  51  loss:  0.0019237978849560022
Batch  61  loss:  0.002031261334195733
Batch  71  loss:  0.001549766049720347
Batch  81  loss:  0.0016499764751642942
Batch  91  loss:  0.0014083698624745011
Validation on real data: 
LOSS supervised-train 0.0016301839647348971, valid 0.000799358997028321
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.002531439531594515
Batch  11  loss:  0.0023808274418115616
Batch  21  loss:  0.0013560694642364979
Batch  31  loss:  0.0014482492115348577
Batch  41  loss:  0.001686379313468933
Batch  51  loss:  0.0012304523261263967
Batch  61  loss:  0.0017262318870052695
Batch  71  loss:  0.0011673378758132458
Batch  81  loss:  0.0015147238736972213
Batch  91  loss:  0.0012558007147163153
Validation on real data: 
LOSS supervised-train 0.0016077254828996957, valid 0.001155943376943469
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0018564806086942554
Batch  11  loss:  0.0014619987923651934
Batch  21  loss:  0.0014674040721729398
Batch  31  loss:  0.001981814159080386
Batch  41  loss:  0.0019726341124624014
Batch  51  loss:  0.0011712254490703344
Batch  61  loss:  0.0013998451177030802
Batch  71  loss:  0.001207335270009935
Batch  81  loss:  0.0017109718173742294
Batch  91  loss:  0.0016698619583621621
Validation on real data: 
LOSS supervised-train 0.0015993554762098938, valid 0.0008516540401615202
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.001403355272486806
Batch  11  loss:  0.0014760022750124335
Batch  21  loss:  0.0012962957844138145
Batch  31  loss:  0.0016535893082618713
Batch  41  loss:  0.0015093903057277203
Batch  51  loss:  0.0018599812174215913
Batch  61  loss:  0.001672305166721344
Batch  71  loss:  0.0010072567965835333
Batch  81  loss:  0.0013141941744834185
Batch  91  loss:  0.0017853277968242764
Validation on real data: 
LOSS supervised-train 0.0016079758561681956, valid 0.0007589855813421309
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0024049736093729734
Batch  11  loss:  0.00145316356793046
Batch  21  loss:  0.0013521132059395313
Batch  31  loss:  0.0017030737362802029
Batch  41  loss:  0.0017885604174807668
Batch  51  loss:  0.0015864262823015451
Batch  61  loss:  0.001853380468674004
Batch  71  loss:  0.0015396473463624716
Batch  81  loss:  0.0012702604290097952
Batch  91  loss:  0.0015585693763568997
Validation on real data: 
LOSS supervised-train 0.0016051877767313272, valid 0.0009622987709008157
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0016180933453142643
Batch  11  loss:  0.0017622243613004684
Batch  21  loss:  0.0014678299194201827
Batch  31  loss:  0.001996104372665286
Batch  41  loss:  0.0016038429457694292
Batch  51  loss:  0.0015112743712961674
Batch  61  loss:  0.0014531186316162348
Batch  71  loss:  0.0012077927822247148
Batch  81  loss:  0.001904411823488772
Batch  91  loss:  0.0011414986802265048
Validation on real data: 
LOSS supervised-train 0.0015390052658040077, valid 0.0006772299529984593
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.002098614815622568
Batch  11  loss:  0.0010796301066875458
Batch  21  loss:  0.0018425052985548973
Batch  31  loss:  0.0019308367045596242
Batch  41  loss:  0.001669183955527842
Batch  51  loss:  0.0010908376425504684
Batch  61  loss:  0.001341882161796093
Batch  71  loss:  0.0012748853769153357
Batch  81  loss:  0.001666936557739973
Batch  91  loss:  0.0017600699793547392
Validation on real data: 
LOSS supervised-train 0.0016174633102491497, valid 0.000678073731251061
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0017860772786661983
Batch  11  loss:  0.0013667383464053273
Batch  21  loss:  0.0011007525026798248
Batch  31  loss:  0.0020282077603042126
Batch  41  loss:  0.0018574108835309744
Batch  51  loss:  0.001572971697896719
Batch  61  loss:  0.0014541303971782327
Batch  71  loss:  0.0010606159921735525
Batch  81  loss:  0.0014396064216271043
Batch  91  loss:  0.0017270942917093635
Validation on real data: 
LOSS supervised-train 0.0015367693244479597, valid 0.0008760757045820355
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0021592688281089067
Batch  11  loss:  0.0013474327279254794
Batch  21  loss:  0.001642922405153513
Batch  31  loss:  0.002329643350094557
Batch  41  loss:  0.0016620176611468196
Batch  51  loss:  0.001350357779301703
Batch  61  loss:  0.0019144341349601746
Batch  71  loss:  0.0014424518449231982
Batch  81  loss:  0.00171908107586205
Batch  91  loss:  0.0016723842127248645
Validation on real data: 
LOSS supervised-train 0.001613188348710537, valid 0.0010324314935132861
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0018043909221887589
Batch  11  loss:  0.0014159008860588074
Batch  21  loss:  0.0014343837974593043
Batch  31  loss:  0.0013995454646646976
Batch  41  loss:  0.0020229420624673367
Batch  51  loss:  0.0015427275793626904
Batch  61  loss:  0.002357413060963154
Batch  71  loss:  0.0013375726994127035
Batch  81  loss:  0.0012073480756953359
Batch  91  loss:  0.001062461407855153
Validation on real data: 
LOSS supervised-train 0.0015030745300464333, valid 0.000835500773973763
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0012772828340530396
Batch  11  loss:  0.002129779662936926
Batch  21  loss:  0.001459972350858152
Batch  31  loss:  0.0019809785299003124
Batch  41  loss:  0.001840669196099043
Batch  51  loss:  0.001201103557832539
Batch  61  loss:  0.001465183449909091
Batch  71  loss:  0.0023095919750630856
Batch  81  loss:  0.0014297431334853172
Batch  91  loss:  0.0012467186897993088
Validation on real data: 
LOSS supervised-train 0.0014881813502870501, valid 0.0009532012045383453
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0016106551047414541
Batch  11  loss:  0.0015833385987207294
Batch  21  loss:  0.001391108613461256
Batch  31  loss:  0.0019229071913287044
Batch  41  loss:  0.0014815123286098242
Batch  51  loss:  0.0014775206800550222
Batch  61  loss:  0.0021260525099933147
Batch  71  loss:  0.0010429591638967395
Batch  81  loss:  0.0016700683627277613
Batch  91  loss:  0.0014149420894682407
Validation on real data: 
LOSS supervised-train 0.0015104127372615038, valid 0.000937949400395155
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0016668530879542232
Batch  11  loss:  0.0015655953902751207
Batch  21  loss:  0.0018392784986644983
Batch  31  loss:  0.0012331458274275064
Batch  41  loss:  0.0015354874776676297
Batch  51  loss:  0.001835568342357874
Batch  61  loss:  0.0016049504047259688
Batch  71  loss:  0.001075949752703309
Batch  81  loss:  0.0012590870028361678
Batch  91  loss:  0.001001187483780086
Validation on real data: 
LOSS supervised-train 0.0014561983349267392, valid 0.000827491981908679
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.001817217911593616
Batch  11  loss:  0.0015726491110399365
Batch  21  loss:  0.0019953285809606314
Batch  31  loss:  0.0017140420386567712
Batch  41  loss:  0.0014314701547846198
Batch  51  loss:  0.0014737739693373442
Batch  61  loss:  0.002074137097224593
Batch  71  loss:  0.0014000997180119157
Batch  81  loss:  0.0017533388454467058
Batch  91  loss:  0.0018345423741266131
Validation on real data: 
LOSS supervised-train 0.0014934814168373123, valid 0.0007627464947290719
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0016405065543949604
Batch  11  loss:  0.002024663146585226
Batch  21  loss:  0.0011746417731046677
Batch  31  loss:  0.0017828228883445263
Batch  41  loss:  0.001706793555058539
Batch  51  loss:  0.0012427859473973513
Batch  61  loss:  0.0014544613659381866
Batch  71  loss:  0.0014434116892516613
Batch  81  loss:  0.0015147228259593248
Batch  91  loss:  0.0014123365981504321
Validation on real data: 
LOSS supervised-train 0.0014770173642318697, valid 0.0008229675586335361
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0018414374208077788
Batch  11  loss:  0.0018541690660640597
Batch  21  loss:  0.0014907229924574494
Batch  31  loss:  0.001359103829599917
Batch  41  loss:  0.0018539316952228546
Batch  51  loss:  0.0012515365378931165
Batch  61  loss:  0.0019511039135977626
Batch  71  loss:  0.001548536354675889
Batch  81  loss:  0.001514467061497271
Batch  91  loss:  0.0010713138617575169
Validation on real data: 
LOSS supervised-train 0.0014670777058927342, valid 0.0008923911955207586
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0014207479543983936
Batch  11  loss:  0.0011933832429349422
Batch  21  loss:  0.0013337397249415517
Batch  31  loss:  0.002199057722464204
Batch  41  loss:  0.0023972662165760994
Batch  51  loss:  0.0017923021223396063
Batch  61  loss:  0.0017210266087204218
Batch  71  loss:  0.001065057935193181
Batch  81  loss:  0.0010515087051317096
Batch  91  loss:  0.0015783210983499885
Validation on real data: 
LOSS supervised-train 0.001468219665111974, valid 0.0007467940449714661
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0014434739714488387
Batch  11  loss:  0.0016205263091251254
Batch  21  loss:  0.0017229809891432524
Batch  31  loss:  0.0018279437208548188
Batch  41  loss:  0.0015045433538034558
Batch  51  loss:  0.0010534829925745726
Batch  61  loss:  0.0012178373290225863
Batch  71  loss:  0.0011685368372127414
Batch  81  loss:  0.001724134897813201
Batch  91  loss:  0.0015923482133075595
Validation on real data: 
LOSS supervised-train 0.0014325406547868625, valid 0.00110107462387532
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.001378910499624908
Batch  11  loss:  0.0017022782703861594
Batch  21  loss:  0.0010213537607342005
Batch  31  loss:  0.0014318801695480943
Batch  41  loss:  0.0013439181493595243
Batch  51  loss:  0.002078092424198985
Batch  61  loss:  0.0015044388128444552
Batch  71  loss:  0.0013097540941089392
Batch  81  loss:  0.0012943586334586143
Batch  91  loss:  0.0011181241134181619
Validation on real data: 
LOSS supervised-train 0.0014557094097835943, valid 0.0008903816342353821
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0018182601779699326
Batch  11  loss:  0.0018865925958380103
Batch  21  loss:  0.0013464465737342834
Batch  31  loss:  0.0016047164099290967
Batch  41  loss:  0.001531454036012292
Batch  51  loss:  0.0011589032365009189
Batch  61  loss:  0.0013617303920909762
Batch  71  loss:  0.0014271849067881703
Batch  81  loss:  0.001651777303777635
Batch  91  loss:  0.0016805233899503946
Validation on real data: 
LOSS supervised-train 0.001444264582823962, valid 0.0007670128252357244
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0015031395014375448
Batch  11  loss:  0.0013193932827562094
Batch  21  loss:  0.0012027804041281343
Batch  31  loss:  0.0015184789663180709
Batch  41  loss:  0.0011631621746346354
Batch  51  loss:  0.0015222936635836959
Batch  61  loss:  0.0014116722159087658
Batch  71  loss:  0.0011696014553308487
Batch  81  loss:  0.0022329213097691536
Batch  91  loss:  0.0015129555249586701
Validation on real data: 
LOSS supervised-train 0.0014286176959285512, valid 0.0009177669417113066
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0018713156459853053
Batch  11  loss:  0.001000399817712605
Batch  21  loss:  0.0011335309827700257
Batch  31  loss:  0.0013206246076151729
Batch  41  loss:  0.001759293838404119
Batch  51  loss:  0.0016980920918285847
Batch  61  loss:  0.001535407966002822
Batch  71  loss:  0.0009704382391646504
Batch  81  loss:  0.0014632284874096513
Batch  91  loss:  0.0011963777942582965
Validation on real data: 
LOSS supervised-train 0.001381993842078373, valid 0.0008592223166488111
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0015122424811124802
Batch  11  loss:  0.001180507941171527
Batch  21  loss:  0.0012896622065454721
Batch  31  loss:  0.0011173223610967398
Batch  41  loss:  0.0012086395872756839
Batch  51  loss:  0.0012656248873099685
Batch  61  loss:  0.001521091558970511
Batch  71  loss:  0.001151221338659525
Batch  81  loss:  0.0016789210494607687
Batch  91  loss:  0.001113157719373703
Validation on real data: 
LOSS supervised-train 0.0013869401899864898, valid 0.001551366993226111
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0016938140615820885
Batch  11  loss:  0.001297028618864715
Batch  21  loss:  0.0008853430626913905
Batch  31  loss:  0.0021138563752174377
Batch  41  loss:  0.0013604057021439075
Batch  51  loss:  0.0016339508583769202
Batch  61  loss:  0.0016125678084790707
Batch  71  loss:  0.0012922650203108788
Batch  81  loss:  0.0013294126838445663
Batch  91  loss:  0.0012503220932558179
Validation on real data: 
LOSS supervised-train 0.001382540580816567, valid 0.0008910165634006262
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0016392540419474244
Batch  11  loss:  0.0012621214846149087
Batch  21  loss:  0.0009700554655864835
Batch  31  loss:  0.0012646904215216637
Batch  41  loss:  0.0010370338568463922
Batch  51  loss:  0.0011194071266800165
Batch  61  loss:  0.001215179800055921
Batch  71  loss:  0.0013239013496786356
Batch  81  loss:  0.0016551339067518711
Batch  91  loss:  0.0010192433837801218
Validation on real data: 
LOSS supervised-train 0.0013590639299945906, valid 0.0010155158815905452
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0014520654221996665
Batch  11  loss:  0.0011249157832935452
Batch  21  loss:  0.0013401746982708573
Batch  31  loss:  0.0010842799674719572
Batch  41  loss:  0.001198523328639567
Batch  51  loss:  0.0009385375306010246
Batch  61  loss:  0.0015290009323507547
Batch  71  loss:  0.0010335067054256797
Batch  81  loss:  0.0016120871296152472
Batch  91  loss:  0.0014079002430662513
Validation on real data: 
LOSS supervised-train 0.0014143494149902835, valid 0.0008080708794295788
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.001321242074482143
Batch  11  loss:  0.0012900930596515536
Batch  21  loss:  0.0016478901961818337
Batch  31  loss:  0.0017016909550875425
Batch  41  loss:  0.0015858408296480775
Batch  51  loss:  0.0020097545348107815
Batch  61  loss:  0.0013210609322413802
Batch  71  loss:  0.0010370875243097544
Batch  81  loss:  0.0016892340499907732
Batch  91  loss:  0.0009426908218301833
Validation on real data: 
LOSS supervised-train 0.0013428708649007603, valid 0.0011073549976572394
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0012119081802666187
Batch  11  loss:  0.0016960172215476632
Batch  21  loss:  0.0016494925366714597
Batch  31  loss:  0.0016461046179756522
Batch  41  loss:  0.0018555044662207365
Batch  51  loss:  0.0013118500355631113
Batch  61  loss:  0.0019289925694465637
Batch  71  loss:  0.0010505608515813947
Batch  81  loss:  0.0014995328383520246
Batch  91  loss:  0.0013882349012419581
Validation on real data: 
LOSS supervised-train 0.001342363299918361, valid 0.0011670845560729504
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0014687752118334174
Batch  11  loss:  0.0016104639507830143
Batch  21  loss:  0.0015610149130225182
Batch  31  loss:  0.0017114367801696062
Batch  41  loss:  0.0013976885238662362
Batch  51  loss:  0.0011970264604315162
Batch  61  loss:  0.0016183709958568215
Batch  71  loss:  0.001143240719102323
Batch  81  loss:  0.0011750197736546397
Batch  91  loss:  0.0016388067742809653
Validation on real data: 
LOSS supervised-train 0.0013385936559643596, valid 0.0009401830611750484
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0017101606354117393
Batch  11  loss:  0.0011243801563978195
Batch  21  loss:  0.0008332855650223792
Batch  31  loss:  0.001043434371240437
Batch  41  loss:  0.0016962564550340176
Batch  51  loss:  0.0008044947753660381
Batch  61  loss:  0.0015111025422811508
Batch  71  loss:  0.0010853996500372887
Batch  81  loss:  0.001419189851731062
Batch  91  loss:  0.001414655358530581
Validation on real data: 
LOSS supervised-train 0.0013158333126921207, valid 0.001066999277099967
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0015451160725206137
Batch  11  loss:  0.0012653019512072206
Batch  21  loss:  0.0013377710711210966
Batch  31  loss:  0.00136977166403085
Batch  41  loss:  0.0012673638993874192
Batch  51  loss:  0.0009863662999123335
Batch  61  loss:  0.0018528024666011333
Batch  71  loss:  0.0013082546647638083
Batch  81  loss:  0.0013466972159221768
Batch  91  loss:  0.0011300035985186696
Validation on real data: 
LOSS supervised-train 0.0013459935382707045, valid 0.0006236938061192632
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.001302766497246921
Batch  11  loss:  0.0018398507963865995
Batch  21  loss:  0.0012092243414372206
Batch  31  loss:  0.0014801421202719212
Batch  41  loss:  0.0011686828220263124
Batch  51  loss:  0.001214824616909027
Batch  61  loss:  0.0013292740331962705
Batch  71  loss:  0.0014627333730459213
Batch  81  loss:  0.0013760384172201157
Batch  91  loss:  0.0012240633368492126
Validation on real data: 
LOSS supervised-train 0.0013582096231402828, valid 0.0008561394643038511
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0016291147330775857
Batch  11  loss:  0.0012468027416616678
Batch  21  loss:  0.0010237901005893946
Batch  31  loss:  0.0013311717193573713
Batch  41  loss:  0.001484779641032219
Batch  51  loss:  0.0011823128443211317
Batch  61  loss:  0.0012771990150213242
Batch  71  loss:  0.0013997615315020084
Batch  81  loss:  0.0012871506623923779
Batch  91  loss:  0.0008783137891441584
Validation on real data: 
LOSS supervised-train 0.0013386081636417657, valid 0.000835308397654444
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0011153784580528736
Batch  11  loss:  0.0010730400681495667
Batch  21  loss:  0.0012040393194183707
Batch  31  loss:  0.0019475388107821345
Batch  41  loss:  0.001609264756552875
Batch  51  loss:  0.0014568291371688247
Batch  61  loss:  0.0015858614351600409
Batch  71  loss:  0.0009608381660655141
Batch  81  loss:  0.0018546211067587137
Batch  91  loss:  0.001262183766812086
Validation on real data: 
LOSS supervised-train 0.0013297036342555656, valid 0.0008627171628177166
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0012259583454579115
Batch  11  loss:  0.0009695301996544003
Batch  21  loss:  0.0010958138154819608
Batch  31  loss:  0.0018180968472734094
Batch  41  loss:  0.0015231778379529715
Batch  51  loss:  0.0015996539732441306
Batch  61  loss:  0.0018632604042068124
Batch  71  loss:  0.0011760168708860874
Batch  81  loss:  0.0008491180487908423
Batch  91  loss:  0.001074954285286367
Validation on real data: 
LOSS supervised-train 0.0012945546983974055, valid 0.0010003795614466071
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0013466719537973404
Batch  11  loss:  0.0021162366028875113
Batch  21  loss:  0.0008368152193725109
Batch  31  loss:  0.0011498969979584217
Batch  41  loss:  0.001437471597455442
Batch  51  loss:  0.0013569658622145653
Batch  61  loss:  0.0019978941418230534
Batch  71  loss:  0.001810202724300325
Batch  81  loss:  0.001254151575267315
Batch  91  loss:  0.001112916273996234
Validation on real data: 
LOSS supervised-train 0.0012906038982328028, valid 0.0009295324562117457
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0017554097576066852
Batch  11  loss:  0.0013283174484968185
Batch  21  loss:  0.0012030377984046936
Batch  31  loss:  0.00125958735588938
Batch  41  loss:  0.0016917631728574634
Batch  51  loss:  0.0011732139391824603
Batch  61  loss:  0.0010677591199055314
Batch  71  loss:  0.0010773533722385764
Batch  81  loss:  0.001566418563015759
Batch  91  loss:  0.0009055344271473587
Validation on real data: 
LOSS supervised-train 0.0012927670998033136, valid 0.0009587594540789723
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  knife ; Model ID: 819e16fd120732f4609e2d916fa0da27
--------------------
Training baseline regression model:  2022-03-30 16:20:35.108845
Detector:  pointnet
Object:  knife
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1608475
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.16019102931022644
Batch  11  loss:  0.08002721518278122
Batch  21  loss:  0.0361407995223999
Batch  31  loss:  0.01255325973033905
Batch  41  loss:  0.007464147172868252
Batch  51  loss:  0.01280093751847744
Batch  61  loss:  0.005826996639370918
Batch  71  loss:  0.0067606121301651
Batch  81  loss:  0.006848965771496296
Batch  91  loss:  0.004505575634539127
Validation on real data: 
LOSS supervised-train 0.021343052652664483, valid 0.004967013373970985
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.005581726785749197
Batch  11  loss:  0.004457088187336922
Batch  21  loss:  0.00499506713822484
Batch  31  loss:  0.005350963212549686
Batch  41  loss:  0.004401660058647394
Batch  51  loss:  0.004240711219608784
Batch  61  loss:  0.0046663121320307255
Batch  71  loss:  0.003636075183749199
Batch  81  loss:  0.0039264908991754055
Batch  91  loss:  0.0038508891593664885
Validation on real data: 
LOSS supervised-train 0.00466685900464654, valid 0.0036599510349333286
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.0039385780692100525
Batch  11  loss:  0.0038467305712401867
Batch  21  loss:  0.004241722635924816
Batch  31  loss:  0.0037436303682625294
Batch  41  loss:  0.003977389074862003
Batch  51  loss:  0.0038720127195119858
Batch  61  loss:  0.003896373324096203
Batch  71  loss:  0.00342629780061543
Batch  81  loss:  0.004160276614129543
Batch  91  loss:  0.0038516735658049583
Validation on real data: 
LOSS supervised-train 0.003979968694038689, valid 0.003932017367333174
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.003682934446260333
Batch  11  loss:  0.003547831205651164
Batch  21  loss:  0.00347505952231586
Batch  31  loss:  0.003926266450434923
Batch  41  loss:  0.0035782575141638517
Batch  51  loss:  0.003667921293526888
Batch  61  loss:  0.003675163025036454
Batch  71  loss:  0.003556582611054182
Batch  81  loss:  0.0033081290312111378
Batch  91  loss:  0.0037561601493507624
Validation on real data: 
LOSS supervised-train 0.0037144013377837836, valid 0.003435347694903612
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.0036261880304664373
Batch  11  loss:  0.0033614106941968203
Batch  21  loss:  0.0036773886531591415
Batch  31  loss:  0.0034151803702116013
Batch  41  loss:  0.003544313134625554
Batch  51  loss:  0.0035612767096608877
Batch  61  loss:  0.0037238600198179483
Batch  71  loss:  0.003364225849509239
Batch  81  loss:  0.003265744773671031
Batch  91  loss:  0.0034266416914761066
Validation on real data: 
LOSS supervised-train 0.003471425655297935, valid 0.0033933883532881737
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.003375785192474723
Batch  11  loss:  0.002963275881484151
Batch  21  loss:  0.003082485171034932
Batch  31  loss:  0.0031595502514392138
Batch  41  loss:  0.003097775625064969
Batch  51  loss:  0.0032464333344250917
Batch  61  loss:  0.0031521045602858067
Batch  71  loss:  0.0028037328738719225
Batch  81  loss:  0.0029505619313567877
Batch  91  loss:  0.0028973263688385487
Validation on real data: 
LOSS supervised-train 0.003173888586461544, valid 0.0033466839231550694
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.00318922265432775
Batch  11  loss:  0.0026240195147693157
Batch  21  loss:  0.0026889233849942684
Batch  31  loss:  0.0034625870175659657
Batch  41  loss:  0.002928257454186678
Batch  51  loss:  0.00298219732940197
Batch  61  loss:  0.0028992341831326485
Batch  71  loss:  0.0024920753203332424
Batch  81  loss:  0.0024848957546055317
Batch  91  loss:  0.0024606978986412287
Validation on real data: 
LOSS supervised-train 0.002811881904490292, valid 0.002957568969577551
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.002961270045489073
Batch  11  loss:  0.0022234704811125994
Batch  21  loss:  0.0021235437598079443
Batch  31  loss:  0.0022931313142180443
Batch  41  loss:  0.0025909983087331057
Batch  51  loss:  0.0024348353035748005
Batch  61  loss:  0.002296939492225647
Batch  71  loss:  0.002156806644052267
Batch  81  loss:  0.0018883339362218976
Batch  91  loss:  0.0022633131593465805
Validation on real data: 
LOSS supervised-train 0.0023250645655207335, valid 0.0021111308597028255
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0020352895371615887
Batch  11  loss:  0.0016551741864532232
Batch  21  loss:  0.0019267029128968716
Batch  31  loss:  0.0019589588046073914
Batch  41  loss:  0.0018717716448009014
Batch  51  loss:  0.0019799070432782173
Batch  61  loss:  0.0019791615195572376
Batch  71  loss:  0.0016009160317480564
Batch  81  loss:  0.0019330305512994528
Batch  91  loss:  0.0015689748106524348
Validation on real data: 
LOSS supervised-train 0.0018934921291656791, valid 0.0016188302543014288
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.001568127190694213
Batch  11  loss:  0.0013659116812050343
Batch  21  loss:  0.0015703398967161775
Batch  31  loss:  0.001643597730435431
Batch  41  loss:  0.0013270475901663303
Batch  51  loss:  0.001656981185078621
Batch  61  loss:  0.0016060678754001856
Batch  71  loss:  0.0011261862237006426
Batch  81  loss:  0.0014781143981963396
Batch  91  loss:  0.001177542028017342
Validation on real data: 
LOSS supervised-train 0.0015869720128830521, valid 0.001752045121975243
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0016157564241439104
Batch  11  loss:  0.0011562445433810353
Batch  21  loss:  0.0013726409524679184
Batch  31  loss:  0.0012316618813201785
Batch  41  loss:  0.0013543603708967566
Batch  51  loss:  0.0014391627628356218
Batch  61  loss:  0.001578357070684433
Batch  71  loss:  0.0014207939384505153
Batch  81  loss:  0.0012093598488718271
Batch  91  loss:  0.0013968822313472629
Validation on real data: 
LOSS supervised-train 0.0013894298550439998, valid 0.0011686213547363877
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0014169953064993024
Batch  11  loss:  0.0009725100826472044
Batch  21  loss:  0.0012417964171618223
Batch  31  loss:  0.0011565731838345528
Batch  41  loss:  0.0010972828604280949
Batch  51  loss:  0.0016070993151515722
Batch  61  loss:  0.0011259879684075713
Batch  71  loss:  0.0009811277268454432
Batch  81  loss:  0.0011221893364563584
Batch  91  loss:  0.0014077810337767005
Validation on real data: 
LOSS supervised-train 0.0012606879795202986, valid 0.0011223891051486135
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0015033897943794727
Batch  11  loss:  0.0009025578619912267
Batch  21  loss:  0.001105023780837655
Batch  31  loss:  0.0012020939029753208
Batch  41  loss:  0.0009582960628904402
Batch  51  loss:  0.0008744082879275084
Batch  61  loss:  0.0012273426400497556
Batch  71  loss:  0.0010961389634758234
Batch  81  loss:  0.0011526038870215416
Batch  91  loss:  0.0011132961371913552
Validation on real data: 
LOSS supervised-train 0.001169116476085037, valid 0.0008920849650166929
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.001304627163335681
Batch  11  loss:  0.0010178160155192018
Batch  21  loss:  0.0010000134352594614
Batch  31  loss:  0.0008556872489862144
Batch  41  loss:  0.0009377895039506257
Batch  51  loss:  0.0010402529733255506
Batch  61  loss:  0.0008949398179538548
Batch  71  loss:  0.0007846945081837475
Batch  81  loss:  0.00095028942450881
Batch  91  loss:  0.0008163197198882699
Validation on real data: 
LOSS supervised-train 0.001067604825948365, valid 0.0011503786081448197
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.003509293310344219
Batch  11  loss:  0.0007315959664992988
Batch  21  loss:  0.0009385183220729232
Batch  31  loss:  0.0009241330553777516
Batch  41  loss:  0.00082188262604177
Batch  51  loss:  0.000936530064791441
Batch  61  loss:  0.0008258151938207448
Batch  71  loss:  0.0010358416475355625
Batch  81  loss:  0.0009493189863860607
Batch  91  loss:  0.0008330133277922869
Validation on real data: 
LOSS supervised-train 0.001006744997575879, valid 0.001027818419970572
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.0011516772210597992
Batch  11  loss:  0.0007592971669510007
Batch  21  loss:  0.0009615423041395843
Batch  31  loss:  0.0007889854605309665
Batch  41  loss:  0.000994353205896914
Batch  51  loss:  0.0007379089365713298
Batch  61  loss:  0.0008588727214373648
Batch  71  loss:  0.0009386202436871827
Batch  81  loss:  0.00105087470728904
Batch  91  loss:  0.0008412570459768176
Validation on real data: 
LOSS supervised-train 0.0009386107872705906, valid 0.0006886163027957082
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0008586845942772925
Batch  11  loss:  0.001175795216113329
Batch  21  loss:  0.0009290080051869154
Batch  31  loss:  0.000732218730263412
Batch  41  loss:  0.0006203652010299265
Batch  51  loss:  0.0008145177853293717
Batch  61  loss:  0.000770114769693464
Batch  71  loss:  0.0008588593918830156
Batch  81  loss:  0.0009077517315745354
Batch  91  loss:  0.0006317525985650718
Validation on real data: 
LOSS supervised-train 0.0009232517023338005, valid 0.0006862650625407696
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0013001540210098028
Batch  11  loss:  0.0007846573716960847
Batch  21  loss:  0.0006740668904967606
Batch  31  loss:  0.001016095164231956
Batch  41  loss:  0.0007181891705840826
Batch  51  loss:  0.0007203416316770017
Batch  61  loss:  0.0007916040485724807
Batch  71  loss:  0.0005490960320457816
Batch  81  loss:  0.0007937492337077856
Batch  91  loss:  0.0007582264370284975
Validation on real data: 
LOSS supervised-train 0.0008730839559575543, valid 0.0006568141980096698
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.001068403827957809
Batch  11  loss:  0.000729824067093432
Batch  21  loss:  0.0007135533378459513
Batch  31  loss:  0.0007736310362815857
Batch  41  loss:  0.0007742212037555873
Batch  51  loss:  0.000982702593319118
Batch  61  loss:  0.0008036459912545979
Batch  71  loss:  0.00061980658210814
Batch  81  loss:  0.0010343806352466345
Batch  91  loss:  0.0007635108777321875
Validation on real data: 
LOSS supervised-train 0.0008477623359067365, valid 0.0006023291498422623
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0010297023691236973
Batch  11  loss:  0.0006122165941633284
Batch  21  loss:  0.0009483021567575634
Batch  31  loss:  0.0007776072598062456
Batch  41  loss:  0.0006497940048575401
Batch  51  loss:  0.0006425600149668753
Batch  61  loss:  0.0007091474253684282
Batch  71  loss:  0.000728509621694684
Batch  81  loss:  0.0008403628598898649
Batch  91  loss:  0.0007627024315297604
Validation on real data: 
LOSS supervised-train 0.0008363352774176747, valid 0.0005886927829124033
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0008474442292936146
Batch  11  loss:  0.0006539063178934157
Batch  21  loss:  0.0005782724474556744
Batch  31  loss:  0.0005700770416297019
Batch  41  loss:  0.0007717813132330775
Batch  51  loss:  0.000497607106808573
Batch  61  loss:  0.0007874221773818135
Batch  71  loss:  0.0008146899053826928
Batch  81  loss:  0.0007888920372352004
Batch  91  loss:  0.0006093612755648792
Validation on real data: 
LOSS supervised-train 0.0007806615368463099, valid 0.0005492744967341423
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.0010342743480578065
Batch  11  loss:  0.00048529572086408734
Batch  21  loss:  0.000800536188762635
Batch  31  loss:  0.0007036912138573825
Batch  41  loss:  0.0006577843450941145
Batch  51  loss:  0.000810619501862675
Batch  61  loss:  0.0007485075620934367
Batch  71  loss:  0.0011232671095058322
Batch  81  loss:  0.0005450286553241313
Batch  91  loss:  0.0007895727176219225
Validation on real data: 
LOSS supervised-train 0.0007609249293454922, valid 0.0007220752886496484
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.00105884182266891
Batch  11  loss:  0.0006766286096535623
Batch  21  loss:  0.0006613149307668209
Batch  31  loss:  0.0010153889888897538
Batch  41  loss:  0.0005802627420052886
Batch  51  loss:  0.0005957872490398586
Batch  61  loss:  0.0008155067916959524
Batch  71  loss:  0.0006951497052796185
Batch  81  loss:  0.0006338563980534673
Batch  91  loss:  0.0007836474105715752
Validation on real data: 
LOSS supervised-train 0.0007814353934372776, valid 0.0006613507866859436
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0007281722500920296
Batch  11  loss:  0.0005259852623566985
Batch  21  loss:  0.0006698141805827618
Batch  31  loss:  0.0005859367665834725
Batch  41  loss:  0.0007139244698919356
Batch  51  loss:  0.0006581760244444013
Batch  61  loss:  0.0007761148735880852
Batch  71  loss:  0.0008392356685362756
Batch  81  loss:  0.0005571491783484817
Batch  91  loss:  0.000567611597944051
Validation on real data: 
LOSS supervised-train 0.0007516789773944765, valid 0.0006898754509165883
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0007430489058606327
Batch  11  loss:  0.0007308594649657607
Batch  21  loss:  0.0006240630173124373
Batch  31  loss:  0.0004925978719256818
Batch  41  loss:  0.0005318016046658158
Batch  51  loss:  0.0009453931706957519
Batch  61  loss:  0.0006814998341724277
Batch  71  loss:  0.0005805198452435434
Batch  81  loss:  0.0006619594059884548
Batch  91  loss:  0.0007084262324497104
Validation on real data: 
LOSS supervised-train 0.0006955233775079251, valid 0.00044148342567496
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.001141293440014124
Batch  11  loss:  0.0005382572999224067
Batch  21  loss:  0.0005952477222308517
Batch  31  loss:  0.0004965785192325711
Batch  41  loss:  0.0006303457194007933
Batch  51  loss:  0.0006962431943975389
Batch  61  loss:  0.0006954955169931054
Batch  71  loss:  0.0007101055816747248
Batch  81  loss:  0.0006080148741602898
Batch  91  loss:  0.0005347169353626668
Validation on real data: 
LOSS supervised-train 0.0007341848008218222, valid 0.00041162679553963244
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0010740341385826468
Batch  11  loss:  0.0004983232938684523
Batch  21  loss:  0.0006595905870199203
Batch  31  loss:  0.0007448274991475046
Batch  41  loss:  0.0004482071672100574
Batch  51  loss:  0.0008452594047412276
Batch  61  loss:  0.0005585572216659784
Batch  71  loss:  0.0014629993820562959
Batch  81  loss:  0.0007600431563332677
Batch  91  loss:  0.0007215413497760892
Validation on real data: 
LOSS supervised-train 0.0007174495258368552, valid 0.0005219459417276084
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0005141227738931775
Batch  11  loss:  0.0006317325751297176
Batch  21  loss:  0.0007490516873076558
Batch  31  loss:  0.0004774957778863609
Batch  41  loss:  0.0004797216388396919
Batch  51  loss:  0.0007185728172771633
Batch  61  loss:  0.0006318004452623427
Batch  71  loss:  0.0005589817301370203
Batch  81  loss:  0.0006809548940509558
Batch  91  loss:  0.0007327431812882423
Validation on real data: 
LOSS supervised-train 0.0006718961652950384, valid 0.0006007675547152758
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0007128130528144538
Batch  11  loss:  0.00045934345689602196
Batch  21  loss:  0.0006399882258847356
Batch  31  loss:  0.0005002701072953641
Batch  41  loss:  0.00041776555008254945
Batch  51  loss:  0.0005631978274323046
Batch  61  loss:  0.0006727312575094402
Batch  71  loss:  0.000642794999293983
Batch  81  loss:  0.0007063563098199666
Batch  91  loss:  0.0005541123100556433
Validation on real data: 
LOSS supervised-train 0.0006837279189494439, valid 0.0004961519734933972
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0006274293409660459
Batch  11  loss:  0.0005340203060768545
Batch  21  loss:  0.0006447079358622432
Batch  31  loss:  0.0005637946305796504
Batch  41  loss:  0.0004983791150152683
Batch  51  loss:  0.0009210009593516588
Batch  61  loss:  0.0006141066551208496
Batch  71  loss:  0.0006031639641150832
Batch  81  loss:  0.0005128142074681818
Batch  91  loss:  0.0006626812391914427
Validation on real data: 
LOSS supervised-train 0.0006474224152043462, valid 0.0006058230646885931
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0007843673811294138
Batch  11  loss:  0.0005794668686576188
Batch  21  loss:  0.0006073871627449989
Batch  31  loss:  0.000578645383939147
Batch  41  loss:  0.000505527772475034
Batch  51  loss:  0.0006359745166264474
Batch  61  loss:  0.000546799274161458
Batch  71  loss:  0.0005953057552687824
Batch  81  loss:  0.0005448886658996344
Batch  91  loss:  0.0004924315144307911
Validation on real data: 
LOSS supervised-train 0.0006319125075242482, valid 0.0002653730334714055
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0008096121600829065
Batch  11  loss:  0.0004882440553046763
Batch  21  loss:  0.0006561594200320542
Batch  31  loss:  0.0004965850384905934
Batch  41  loss:  0.0005539994454011321
Batch  51  loss:  0.0006645002868026495
Batch  61  loss:  0.0006035225233063102
Batch  71  loss:  0.0006499169394373894
Batch  81  loss:  0.0005987785407342017
Batch  91  loss:  0.0004451387212611735
Validation on real data: 
LOSS supervised-train 0.0006586442654952407, valid 0.0004258318222127855
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0006625501555390656
Batch  11  loss:  0.00038048511487431824
Batch  21  loss:  0.0005658126901835203
Batch  31  loss:  0.00040342321153730154
Batch  41  loss:  0.0005143890739418566
Batch  51  loss:  0.0007958518690429628
Batch  61  loss:  0.00046856090193614364
Batch  71  loss:  0.0004382609622552991
Batch  81  loss:  0.0006434053066186607
Batch  91  loss:  0.0006561298505403101
Validation on real data: 
LOSS supervised-train 0.0006160381840891204, valid 0.0003494707925710827
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0008783004013821483
Batch  11  loss:  0.00048059565597213805
Batch  21  loss:  0.0005399109213612974
Batch  31  loss:  0.000611180264968425
Batch  41  loss:  0.0006830549100413918
Batch  51  loss:  0.0006094718701206148
Batch  61  loss:  0.00044968738802708685
Batch  71  loss:  0.00040742562850937247
Batch  81  loss:  0.0008525174926035106
Batch  91  loss:  0.0005080558476038277
Validation on real data: 
LOSS supervised-train 0.0006318450905382633, valid 0.0005693435668945312
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0005710893892683089
Batch  11  loss:  0.0004953863681294024
Batch  21  loss:  0.0005170342046767473
Batch  31  loss:  0.0005205761408433318
Batch  41  loss:  0.0003708649310283363
Batch  51  loss:  0.0005307153332978487
Batch  61  loss:  0.0006242467788979411
Batch  71  loss:  0.0004650866030715406
Batch  81  loss:  0.0004863297799602151
Batch  91  loss:  0.0005562810110859573
Validation on real data: 
LOSS supervised-train 0.0005990693037165329, valid 0.00033896122477017343
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.000664575956761837
Batch  11  loss:  0.00041722162859514356
Batch  21  loss:  0.00038331892574205995
Batch  31  loss:  0.0005916405934840441
Batch  41  loss:  0.00047895131865516305
Batch  51  loss:  0.0005633481196127832
Batch  61  loss:  0.00040325254667550325
Batch  71  loss:  0.0006349502364173532
Batch  81  loss:  0.0005605195183306932
Batch  91  loss:  0.0005932576023042202
Validation on real data: 
LOSS supervised-train 0.0006035684820380993, valid 0.0002997439296450466
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0006126629305072129
Batch  11  loss:  0.0006725277053192258
Batch  21  loss:  0.0005902312695980072
Batch  31  loss:  0.0005175265832804143
Batch  41  loss:  0.0004154932394158095
Batch  51  loss:  0.0005053741624578834
Batch  61  loss:  0.0004661657440010458
Batch  71  loss:  0.00045549130300059915
Batch  81  loss:  0.0005847961292602122
Batch  91  loss:  0.0004638071113731712
Validation on real data: 
LOSS supervised-train 0.0005895742878783495, valid 0.00029532023472711444
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.000852359866257757
Batch  11  loss:  0.0004847695236094296
Batch  21  loss:  0.0004137125797569752
Batch  31  loss:  0.0004591050965245813
Batch  41  loss:  0.0004068157868459821
Batch  51  loss:  0.0004297849372960627
Batch  61  loss:  0.0006030527874827385
Batch  71  loss:  0.0003809247864410281
Batch  81  loss:  0.0007489660056307912
Batch  91  loss:  0.000544919865205884
Validation on real data: 
LOSS supervised-train 0.0005871733464300632, valid 0.0003747487789951265
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.000719826202839613
Batch  11  loss:  0.0005125164170749485
Batch  21  loss:  0.0005623595206998289
Batch  31  loss:  0.0003580387565307319
Batch  41  loss:  0.00046492376714013517
Batch  51  loss:  0.0005413973121903837
Batch  61  loss:  0.0005991103826090693
Batch  71  loss:  0.0004358088772278279
Batch  81  loss:  0.0003658242349047214
Batch  91  loss:  0.0003720425011124462
Validation on real data: 
LOSS supervised-train 0.0005807519683730789, valid 0.00032813556026667356
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0007548819994553924
Batch  11  loss:  0.0005132531514391303
Batch  21  loss:  0.0005372200976125896
Batch  31  loss:  0.00043209356954321265
Batch  41  loss:  0.0005931135965511203
Batch  51  loss:  0.0005986563046462834
Batch  61  loss:  0.0006562053458765149
Batch  71  loss:  0.00045661476906389
Batch  81  loss:  0.0005287512321956456
Batch  91  loss:  0.00034528522519394755
Validation on real data: 
LOSS supervised-train 0.0005642463802359998, valid 0.0004254185769241303
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0006765125435777009
Batch  11  loss:  0.0004228163161315024
Batch  21  loss:  0.0005642212927341461
Batch  31  loss:  0.0005518813850358129
Batch  41  loss:  0.00042718343320302665
Batch  51  loss:  0.000412618595873937
Batch  61  loss:  0.0005679003661498427
Batch  71  loss:  0.0005766048561781645
Batch  81  loss:  0.000512199301738292
Batch  91  loss:  0.0004988744622096419
Validation on real data: 
LOSS supervised-train 0.0005772201149375178, valid 0.0003736152430064976
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.0007856118609197438
Batch  11  loss:  0.00038893151213414967
Batch  21  loss:  0.0004811860271729529
Batch  31  loss:  0.0005070886109024286
Batch  41  loss:  0.0005383575335144997
Batch  51  loss:  0.0004477943875826895
Batch  61  loss:  0.0006050702068023384
Batch  71  loss:  0.00039879223913885653
Batch  81  loss:  0.00046713504707440734
Batch  91  loss:  0.00043072120752185583
Validation on real data: 
LOSS supervised-train 0.0005830823720316403, valid 0.0003710739256348461
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0005983982118777931
Batch  11  loss:  0.00057198730064556
Batch  21  loss:  0.0005422954563982785
Batch  31  loss:  0.0004319752915762365
Batch  41  loss:  0.00043910613749176264
Batch  51  loss:  0.0004700984572991729
Batch  61  loss:  0.0004165002901572734
Batch  71  loss:  0.000442955904873088
Batch  81  loss:  0.000548595271538943
Batch  91  loss:  0.0005015810602344573
Validation on real data: 
LOSS supervised-train 0.0005474525317549706, valid 0.00029621797148138285
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.000733800872694701
Batch  11  loss:  0.00040320068364962935
Batch  21  loss:  0.00046610398567281663
Batch  31  loss:  0.00038200727431103587
Batch  41  loss:  0.0003823898732662201
Batch  51  loss:  0.0003521240723785013
Batch  61  loss:  0.00041386240627616644
Batch  71  loss:  0.0004363495681900531
Batch  81  loss:  0.00046527510858140886
Batch  91  loss:  0.0002780466165859252
Validation on real data: 
LOSS supervised-train 0.0005556101631373167, valid 0.000256378116318956
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0006009208736941218
Batch  11  loss:  0.0004968410357832909
Batch  21  loss:  0.00047013803850859404
Batch  31  loss:  0.0002908799797296524
Batch  41  loss:  0.00039557740092277527
Batch  51  loss:  0.0005673500709235668
Batch  61  loss:  0.0005331540596671402
Batch  71  loss:  0.0005384525284171104
Batch  81  loss:  0.00044911596341989934
Batch  91  loss:  0.0005435036146081984
Validation on real data: 
LOSS supervised-train 0.000562527397996746, valid 0.000382409431040287
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0003503742627799511
Batch  11  loss:  0.000533031823579222
Batch  21  loss:  0.0005019800737500191
Batch  31  loss:  0.00036933517549186945
Batch  41  loss:  0.0004104536783415824
Batch  51  loss:  0.0004350112285465002
Batch  61  loss:  0.00040997660835273564
Batch  71  loss:  0.0003998580505140126
Batch  81  loss:  0.0004016751190647483
Batch  91  loss:  0.00046832679072394967
Validation on real data: 
LOSS supervised-train 0.0005261473442078568, valid 0.0002836539060808718
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.000491398386657238
Batch  11  loss:  0.00039898656541481614
Batch  21  loss:  0.0004882769426330924
Batch  31  loss:  0.0002963244332931936
Batch  41  loss:  0.00032999913673847914
Batch  51  loss:  0.0004985457635484636
Batch  61  loss:  0.0005689971148967743
Batch  71  loss:  0.0005158901331014931
Batch  81  loss:  0.0005706565571017563
Batch  91  loss:  0.0005665274802595377
Validation on real data: 
LOSS supervised-train 0.0005392327907611616, valid 0.00036543316673487425
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0007089041173458099
Batch  11  loss:  0.0003123923670500517
Batch  21  loss:  0.0003891962696798146
Batch  31  loss:  0.00046796476817689836
Batch  41  loss:  0.00037678281660191715
Batch  51  loss:  0.0003825885069090873
Batch  61  loss:  0.0005101864226162434
Batch  71  loss:  0.000550981902051717
Batch  81  loss:  0.0004113915201742202
Batch  91  loss:  0.0005094747175462544
Validation on real data: 
LOSS supervised-train 0.0005391636057174764, valid 0.0004632567579392344
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0005105112213641405
Batch  11  loss:  0.0003578185860533267
Batch  21  loss:  0.0004599986132234335
Batch  31  loss:  0.0003780946717597544
Batch  41  loss:  0.0004437253810465336
Batch  51  loss:  0.0003125388757325709
Batch  61  loss:  0.00036942213773727417
Batch  71  loss:  0.00032385956728830934
Batch  81  loss:  0.0003211548028048128
Batch  91  loss:  0.0004024133668281138
Validation on real data: 
LOSS supervised-train 0.000495964368165005, valid 0.00030717323534190655
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.000835313112474978
Batch  11  loss:  0.00049335922813043
Batch  21  loss:  0.0004639183753170073
Batch  31  loss:  0.0003233809839002788
Batch  41  loss:  0.0003802768769674003
Batch  51  loss:  0.0004738695570267737
Batch  61  loss:  0.0005476945661939681
Batch  71  loss:  0.0006913112010806799
Batch  81  loss:  0.00047175606596283615
Batch  91  loss:  0.0004707275365944952
Validation on real data: 
LOSS supervised-train 0.0005169636130449362, valid 0.00030612011323682964
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0007106915581971407
Batch  11  loss:  0.0004921153886243701
Batch  21  loss:  0.00048404961125925183
Batch  31  loss:  0.0003585539816413075
Batch  41  loss:  0.00032284893677569926
Batch  51  loss:  0.0004585856222547591
Batch  61  loss:  0.00043222165550105274
Batch  71  loss:  0.0004430761910043657
Batch  81  loss:  0.0004205864679533988
Batch  91  loss:  0.0005089424666948617
Validation on real data: 
LOSS supervised-train 0.0005109525567968377, valid 0.00029391166754066944
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0005047473823651671
Batch  11  loss:  0.00041195368976332247
Batch  21  loss:  0.0004162288678344339
Batch  31  loss:  0.0003089965903200209
Batch  41  loss:  0.00040464376797899604
Batch  51  loss:  0.0005865865969099104
Batch  61  loss:  0.0005676117143593729
Batch  71  loss:  0.000427168735768646
Batch  81  loss:  0.00042127532651647925
Batch  91  loss:  0.00044186096056364477
Validation on real data: 
LOSS supervised-train 0.0005055908841313794, valid 0.0004109786241315305
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0006946543580852449
Batch  11  loss:  0.00046920430031605065
Batch  21  loss:  0.00045864065759815276
Batch  31  loss:  0.0004901190986856818
Batch  41  loss:  0.0003899238654412329
Batch  51  loss:  0.00036505862954072654
Batch  61  loss:  0.0003850477223750204
Batch  71  loss:  0.00048060878179967403
Batch  81  loss:  0.00046283568372018635
Batch  91  loss:  0.0005451418692246079
Validation on real data: 
LOSS supervised-train 0.0005000916181597859, valid 0.0003084652707912028
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0004806894576177001
Batch  11  loss:  0.00025651315809227526
Batch  21  loss:  0.0004931675503030419
Batch  31  loss:  0.00042056769598275423
Batch  41  loss:  0.00040505253127776086
Batch  51  loss:  0.0004970032605342567
Batch  61  loss:  0.0005128072807565331
Batch  71  loss:  0.0004853694699704647
Batch  81  loss:  0.000362233113264665
Batch  91  loss:  0.0003430770884733647
Validation on real data: 
LOSS supervised-train 0.0005269286455586552, valid 0.00037478082231245935
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.000531594967469573
Batch  11  loss:  0.0003684823459479958
Batch  21  loss:  0.00040559086482971907
Batch  31  loss:  0.0003670169971883297
Batch  41  loss:  0.00033437865204177797
Batch  51  loss:  0.0004946414846926928
Batch  61  loss:  0.0003783064312301576
Batch  71  loss:  0.0004935560864396393
Batch  81  loss:  0.0005684915813617408
Batch  91  loss:  0.0002894969657063484
Validation on real data: 
LOSS supervised-train 0.0005046696242061444, valid 0.00025081291096284986
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0005447572330012918
Batch  11  loss:  0.00047936689225025475
Batch  21  loss:  0.0004183906130492687
Batch  31  loss:  0.00041128284647129476
Batch  41  loss:  0.00045059213880449533
Batch  51  loss:  0.0004545989795587957
Batch  61  loss:  0.00039383143302984536
Batch  71  loss:  0.000376351730665192
Batch  81  loss:  0.0005557853728532791
Batch  91  loss:  0.0005191268865019083
Validation on real data: 
LOSS supervised-train 0.00048339796980144455, valid 0.0003428251075092703
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0004560684319585562
Batch  11  loss:  0.0004405316722113639
Batch  21  loss:  0.0005340816569514573
Batch  31  loss:  0.00033662281930446625
Batch  41  loss:  0.00028891267720609903
Batch  51  loss:  0.0005808359710499644
Batch  61  loss:  0.0005410526064224541
Batch  71  loss:  0.0003887777274940163
Batch  81  loss:  0.00034213950857520103
Batch  91  loss:  0.0004948822897858918
Validation on real data: 
LOSS supervised-train 0.0004954403490410186, valid 0.0003361123672220856
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0005042217671871185
Batch  11  loss:  0.0004918997292406857
Batch  21  loss:  0.0003801249258685857
Batch  31  loss:  0.0006705517880618572
Batch  41  loss:  0.0004114717012271285
Batch  51  loss:  0.0005031400942243636
Batch  61  loss:  0.00048649703967384994
Batch  71  loss:  0.00036476796958595514
Batch  81  loss:  0.0004300065338611603
Batch  91  loss:  0.000360835634637624
Validation on real data: 
LOSS supervised-train 0.0004931339097674936, valid 0.00028747820761054754
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0004780902527272701
Batch  11  loss:  0.00033889690530486405
Batch  21  loss:  0.0003099554742220789
Batch  31  loss:  0.0003968415257986635
Batch  41  loss:  0.0003671197628136724
Batch  51  loss:  0.00048756739124655724
Batch  61  loss:  0.0003673213650472462
Batch  71  loss:  0.0004263716982677579
Batch  81  loss:  0.00028970622224733233
Batch  91  loss:  0.00033947869087569416
Validation on real data: 
LOSS supervised-train 0.0004751014322391711, valid 0.0003870832151733339
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0005794083117507398
Batch  11  loss:  0.0004013035795651376
Batch  21  loss:  0.00033695888123475015
Batch  31  loss:  0.00038282619789242744
Batch  41  loss:  0.00037456434802152216
Batch  51  loss:  0.0004227474273648113
Batch  61  loss:  0.00045858777593821287
Batch  71  loss:  0.0005011434550397098
Batch  81  loss:  0.00037486868677660823
Batch  91  loss:  0.0004124463885091245
Validation on real data: 
LOSS supervised-train 0.0004919989468180574, valid 0.00024099386064335704
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0005805114633403718
Batch  11  loss:  0.00040066122892312706
Batch  21  loss:  0.0003522945917211473
Batch  31  loss:  0.00033715079189278185
Batch  41  loss:  0.0002659462334122509
Batch  51  loss:  0.00040009620715864
Batch  61  loss:  0.0006316427607089281
Batch  71  loss:  0.0012122959597036242
Batch  81  loss:  0.0003885813057422638
Batch  91  loss:  0.00037913655978627503
Validation on real data: 
LOSS supervised-train 0.000497314999520313, valid 0.0003154977457597852
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0004858337342739105
Batch  11  loss:  0.0002909578033722937
Batch  21  loss:  0.0004082634695805609
Batch  31  loss:  0.0002629231312312186
Batch  41  loss:  0.00033238582545891404
Batch  51  loss:  0.0005003064288757741
Batch  61  loss:  0.0004420321201905608
Batch  71  loss:  0.0004072782176081091
Batch  81  loss:  0.00045672673149965703
Batch  91  loss:  0.00033645625808276236
Validation on real data: 
LOSS supervised-train 0.00048537680951994845, valid 0.00027923661400564015
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0005389619036577642
Batch  11  loss:  0.0003668100980576128
Batch  21  loss:  0.00033593084663152695
Batch  31  loss:  0.0004417571472004056
Batch  41  loss:  0.00025854588602669537
Batch  51  loss:  0.0005104385199956596
Batch  61  loss:  0.00039120789733715355
Batch  71  loss:  0.00025496658054180443
Batch  81  loss:  0.00045606427011080086
Batch  91  loss:  0.00036664275103248656
Validation on real data: 
LOSS supervised-train 0.00046608977077994496, valid 0.00021953963732812554
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0006454468239098787
Batch  11  loss:  0.00040953472489491105
Batch  21  loss:  0.0003759510291274637
Batch  31  loss:  0.0002548388729337603
Batch  41  loss:  0.0004006026138085872
Batch  51  loss:  0.0003417476254981011
Batch  61  loss:  0.0004516562621574849
Batch  71  loss:  0.000373371149180457
Batch  81  loss:  0.0004994901246391237
Batch  91  loss:  0.00025734625523909926
Validation on real data: 
LOSS supervised-train 0.0004646265838528052, valid 0.0002450193860568106
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.00046563087380491197
Batch  11  loss:  0.0003287311992608011
Batch  21  loss:  0.0004895884194411337
Batch  31  loss:  0.0003589086118154228
Batch  41  loss:  0.00031634277547709644
Batch  51  loss:  0.0005480991094373167
Batch  61  loss:  0.00029882596572861075
Batch  71  loss:  0.0004277584666851908
Batch  81  loss:  0.0005362100200727582
Batch  91  loss:  0.0002880164538510144
Validation on real data: 
LOSS supervised-train 0.0004634550814807881, valid 0.0002608118229545653
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0005464139976538718
Batch  11  loss:  0.0003927346842829138
Batch  21  loss:  0.00042338648927398026
Batch  31  loss:  0.0002930625341832638
Batch  41  loss:  0.0002796668268274516
Batch  51  loss:  0.0003378322580829263
Batch  61  loss:  0.0005368507700040936
Batch  71  loss:  0.0005083271535113454
Batch  81  loss:  0.0003623285156209022
Batch  91  loss:  0.0003263101680204272
Validation on real data: 
LOSS supervised-train 0.0004616263826028444, valid 0.000298143393592909
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0005061940173618495
Batch  11  loss:  0.00027714495081454515
Batch  21  loss:  0.00047629771870560944
Batch  31  loss:  0.00023907929426059127
Batch  41  loss:  0.00036223625647835433
Batch  51  loss:  0.0006160395569168031
Batch  61  loss:  0.0003869586216751486
Batch  71  loss:  0.000467118457891047
Batch  81  loss:  0.0003455035912338644
Batch  91  loss:  0.000424199242843315
Validation on real data: 
LOSS supervised-train 0.000448388199438341, valid 0.0003888130886480212
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0004705421451944858
Batch  11  loss:  0.0004714590322691947
Batch  21  loss:  0.0003375059168320149
Batch  31  loss:  0.00025699997786432505
Batch  41  loss:  0.00031605263939127326
Batch  51  loss:  0.00042293971637263894
Batch  61  loss:  0.0005059009417891502
Batch  71  loss:  0.0003640175564214587
Batch  81  loss:  0.00031444060732610524
Batch  91  loss:  0.0004603663692250848
Validation on real data: 
LOSS supervised-train 0.00043876576353795826, valid 0.0002225423522759229
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.00047356195864267647
Batch  11  loss:  0.000477462395792827
Batch  21  loss:  0.0003802131686825305
Batch  31  loss:  0.00033008548780344427
Batch  41  loss:  0.00035500613739714026
Batch  51  loss:  0.0006168668624013662
Batch  61  loss:  0.0005290956469252706
Batch  71  loss:  0.00042950190254487097
Batch  81  loss:  0.0003270927700214088
Batch  91  loss:  0.0003440793079789728
Validation on real data: 
LOSS supervised-train 0.0004414987823111005, valid 0.00019397430878598243
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0005387509590946138
Batch  11  loss:  0.0004121874226257205
Batch  21  loss:  0.0004150837194174528
Batch  31  loss:  0.0004292161902412772
Batch  41  loss:  0.0003429182106629014
Batch  51  loss:  0.00046363286674022675
Batch  61  loss:  0.000316677731461823
Batch  71  loss:  0.00047162591363303363
Batch  81  loss:  0.00046909123193472624
Batch  91  loss:  0.00037393526872619987
Validation on real data: 
LOSS supervised-train 0.00046040082757826896, valid 0.0002670669637154788
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0003714552440214902
Batch  11  loss:  0.00045462630805559456
Batch  21  loss:  0.0002991254150401801
Batch  31  loss:  0.0003151112759951502
Batch  41  loss:  0.00028744045994244516
Batch  51  loss:  0.00044460760545916855
Batch  61  loss:  0.00043834670213982463
Batch  71  loss:  0.00040862872265279293
Batch  81  loss:  0.00042426979052834213
Batch  91  loss:  0.000504413153976202
Validation on real data: 
LOSS supervised-train 0.0004494253039592877, valid 0.00026386338868178427
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.00039225572254508734
Batch  11  loss:  0.0003618751361500472
Batch  21  loss:  0.0004537981585599482
Batch  31  loss:  0.000400910823373124
Batch  41  loss:  0.0003291904868092388
Batch  51  loss:  0.00039691690471954644
Batch  61  loss:  0.00045605102786794305
Batch  71  loss:  0.0003503915504552424
Batch  81  loss:  0.0002790219150483608
Batch  91  loss:  0.00029344376525841653
Validation on real data: 
LOSS supervised-train 0.0004294063540874049, valid 0.00030400650575757027
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0005009235464967787
Batch  11  loss:  0.00034625144326128066
Batch  21  loss:  0.000258914747973904
Batch  31  loss:  0.00031691030017100275
Batch  41  loss:  0.00030304069514386356
Batch  51  loss:  0.0004800081660505384
Batch  61  loss:  0.0003886065969709307
Batch  71  loss:  0.0004033569130115211
Batch  81  loss:  0.00034939555916935205
Batch  91  loss:  0.00034834034158848226
Validation on real data: 
LOSS supervised-train 0.00045530780611443335, valid 0.00021290310542099178
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0004114913463126868
Batch  11  loss:  0.00040470482781529427
Batch  21  loss:  0.00038289494113996625
Batch  31  loss:  0.0002900146064348519
Batch  41  loss:  0.0003256676718592644
Batch  51  loss:  0.0004820208123419434
Batch  61  loss:  0.00040090858237817883
Batch  71  loss:  0.0004340214654803276
Batch  81  loss:  0.00029997245292179286
Batch  91  loss:  0.0004502174269873649
Validation on real data: 
LOSS supervised-train 0.00046419961552601307, valid 0.00027569191297516227
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.00033415175857953727
Batch  11  loss:  0.0002774150634650141
Batch  21  loss:  0.0003407436888664961
Batch  31  loss:  0.0002985689206980169
Batch  41  loss:  0.00035546565777622163
Batch  51  loss:  0.0004383003688417375
Batch  61  loss:  0.0003249294531997293
Batch  71  loss:  0.0005871159955859184
Batch  81  loss:  0.0003814905066974461
Batch  91  loss:  0.00036682203062810004
Validation on real data: 
LOSS supervised-train 0.00043410826547187755, valid 0.0002705768565647304
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.000596974219661206
Batch  11  loss:  0.0003482472093310207
Batch  21  loss:  0.0002855713537428528
Batch  31  loss:  0.0003937069559469819
Batch  41  loss:  0.0003488682850729674
Batch  51  loss:  0.00030747204436920583
Batch  61  loss:  0.0004789889499079436
Batch  71  loss:  0.0003463352331891656
Batch  81  loss:  0.0003289787273388356
Batch  91  loss:  0.00035342315095476806
Validation on real data: 
LOSS supervised-train 0.00042736036877613517, valid 0.00025259522954002023
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.00040191857260651886
Batch  11  loss:  0.0004325306217651814
Batch  21  loss:  0.0003908007056452334
Batch  31  loss:  0.000378132303012535
Batch  41  loss:  0.00033096870174631476
Batch  51  loss:  0.00036541506415233016
Batch  61  loss:  0.00029256081325002015
Batch  71  loss:  0.00040108218672685325
Batch  81  loss:  0.00042047305032610893
Batch  91  loss:  0.00033974667894653976
Validation on real data: 
LOSS supervised-train 0.00041972007747972386, valid 0.00028480737819336355
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.00031116092577576637
Batch  11  loss:  0.0003549071552697569
Batch  21  loss:  0.0003221257065888494
Batch  31  loss:  0.0003755857760552317
Batch  41  loss:  0.0004492599400691688
Batch  51  loss:  0.0006691509624943137
Batch  61  loss:  0.0004044680099468678
Batch  71  loss:  0.00047825343790464103
Batch  81  loss:  0.00046438127174042165
Batch  91  loss:  0.0004339755978435278
Validation on real data: 
LOSS supervised-train 0.00043454829283291475, valid 0.00019964964303653687
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.00042180405580438673
Batch  11  loss:  0.00033089876524172723
Batch  21  loss:  0.00032227393239736557
Batch  31  loss:  0.00028943608049303293
Batch  41  loss:  0.00041862763464450836
Batch  51  loss:  0.00048276002053171396
Batch  61  loss:  0.00036168796941637993
Batch  71  loss:  0.0003539673052728176
Batch  81  loss:  0.0002536242827773094
Batch  91  loss:  0.0003196323523297906
Validation on real data: 
LOSS supervised-train 0.0004157652609865181, valid 0.0001958971406565979
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.000507887511048466
Batch  11  loss:  0.0003363857395015657
Batch  21  loss:  0.0003245710104238242
Batch  31  loss:  0.00030568442889489233
Batch  41  loss:  0.0002920089173130691
Batch  51  loss:  0.00028982170624658465
Batch  61  loss:  0.00044255226384848356
Batch  71  loss:  0.00029022598755545914
Batch  81  loss:  0.0003282950783614069
Batch  91  loss:  0.0005014042253606021
Validation on real data: 
LOSS supervised-train 0.0004249347947188653, valid 0.000247021991526708
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0004535958869382739
Batch  11  loss:  0.00034958377364091575
Batch  21  loss:  0.00034405881888233125
Batch  31  loss:  0.0003078131121583283
Batch  41  loss:  0.00030875232187099755
Batch  51  loss:  0.00038207165198400617
Batch  61  loss:  0.0003319743264000863
Batch  71  loss:  0.000419381947722286
Batch  81  loss:  0.0002463542914483696
Batch  91  loss:  0.0003854899841826409
Validation on real data: 
LOSS supervised-train 0.0004350489801436197, valid 0.00022893950517755002
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00038196094101294875
Batch  11  loss:  0.0003575356968212873
Batch  21  loss:  0.00041718571446835995
Batch  31  loss:  0.000385961786378175
Batch  41  loss:  0.0003716604842338711
Batch  51  loss:  0.0004041091597173363
Batch  61  loss:  0.00035741899046115577
Batch  71  loss:  0.00045065549784339964
Batch  81  loss:  0.0004744325706269592
Batch  91  loss:  0.0003808093606494367
Validation on real data: 
LOSS supervised-train 0.0004299268531030975, valid 0.0002899164683185518
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0002684890350792557
Batch  11  loss:  0.0003105720388703048
Batch  21  loss:  0.00023546548618469387
Batch  31  loss:  0.00027031596982851624
Batch  41  loss:  0.0003367392346262932
Batch  51  loss:  0.00042786062113009393
Batch  61  loss:  0.0003738792147487402
Batch  71  loss:  0.00033352323225699365
Batch  81  loss:  0.00034547955146990716
Batch  91  loss:  0.00037072747363708913
Validation on real data: 
LOSS supervised-train 0.00043193921286729166, valid 0.000236412015510723
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.00036807445576414466
Batch  11  loss:  0.0004002642526756972
Batch  21  loss:  0.0004052434815093875
Batch  31  loss:  0.0003358095127623528
Batch  41  loss:  0.0003951845283154398
Batch  51  loss:  0.0004982214886695147
Batch  61  loss:  0.0003924343909602612
Batch  71  loss:  0.0003196269681211561
Batch  81  loss:  0.0003038570866920054
Batch  91  loss:  0.0002825082337949425
Validation on real data: 
LOSS supervised-train 0.0003961505585175473, valid 0.0003237453638575971
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.000320902734529227
Batch  11  loss:  0.00032229654607363045
Batch  21  loss:  0.00037659337976947427
Batch  31  loss:  0.00023998229880817235
Batch  41  loss:  0.00029446097323670983
Batch  51  loss:  0.0004989933804608881
Batch  61  loss:  0.0004070027789566666
Batch  71  loss:  0.00033094154787249863
Batch  81  loss:  0.00035151722840964794
Batch  91  loss:  0.00032639040728099644
Validation on real data: 
LOSS supervised-train 0.0004221533153031487, valid 0.00023197289556264877
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.000497890985570848
Batch  11  loss:  0.0006950559909455478
Batch  21  loss:  0.0005395551561377943
Batch  31  loss:  0.0002701373887248337
Batch  41  loss:  0.0005003638798370957
Batch  51  loss:  0.00044880257337354124
Batch  61  loss:  0.00034709068131633103
Batch  71  loss:  0.0003035808040294796
Batch  81  loss:  0.0003639361239038408
Batch  91  loss:  0.0003332539345137775
Validation on real data: 
LOSS supervised-train 0.00041808314970694484, valid 0.00027923783636651933
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.00037591083673760295
Batch  11  loss:  0.0003383822622708976
Batch  21  loss:  0.00033026852179318666
Batch  31  loss:  0.0002914633078034967
Batch  41  loss:  0.00031431092065759003
Batch  51  loss:  0.0002838248619809747
Batch  61  loss:  0.0003128945827484131
Batch  71  loss:  0.0003134392900392413
Batch  81  loss:  0.00033757422352209687
Batch  91  loss:  0.00031388099887408316
Validation on real data: 
LOSS supervised-train 0.0004054509373963811, valid 0.00027253630105406046
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.00037481929757632315
Batch  11  loss:  0.00033832938061095774
Batch  21  loss:  0.00031181550002656877
Batch  31  loss:  0.000307168229483068
Batch  41  loss:  0.0003376085951458663
Batch  51  loss:  0.00035294637200422585
Batch  61  loss:  0.0005299305194057524
Batch  71  loss:  0.00033361639361828566
Batch  81  loss:  0.000326713256072253
Batch  91  loss:  0.000349728245055303
Validation on real data: 
LOSS supervised-train 0.00041668119229143485, valid 0.00024422575370408595
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.00032645976170897484
Batch  11  loss:  0.0002521643473301083
Batch  21  loss:  0.0005172676756046712
Batch  31  loss:  0.00022213176998775452
Batch  41  loss:  0.0003668168792501092
Batch  51  loss:  0.00039438484236598015
Batch  61  loss:  0.00029589206678792834
Batch  71  loss:  0.0005088493344374001
Batch  81  loss:  0.0003020092844963074
Batch  91  loss:  0.00034951124689541757
Validation on real data: 
LOSS supervised-train 0.00040701634410652334, valid 0.00020022454555146396
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0004001189663540572
Batch  11  loss:  0.0002519932750146836
Batch  21  loss:  0.0002931639610324055
Batch  31  loss:  0.0003162583743687719
Batch  41  loss:  0.0003366646415088326
Batch  51  loss:  0.00022080360213294625
Batch  61  loss:  0.00044108947622589767
Batch  71  loss:  0.0003032492531929165
Batch  81  loss:  0.0002205590280937031
Batch  91  loss:  0.00035053218016400933
Validation on real data: 
LOSS supervised-train 0.00042103152212803253, valid 0.00019284471636638045
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0005160503787919879
Batch  11  loss:  0.00040080511826090515
Batch  21  loss:  0.00030028526089154184
Batch  31  loss:  0.00029498731601051986
Batch  41  loss:  0.00031134835444390774
Batch  51  loss:  0.0003153681755065918
Batch  61  loss:  0.0002955872332677245
Batch  71  loss:  0.00024821722763590515
Batch  81  loss:  0.00034064563806168735
Batch  91  loss:  0.0002860829990822822
Validation on real data: 
LOSS supervised-train 0.000405543534870958, valid 0.0002477694652043283
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0004571212630253285
Batch  11  loss:  0.00028320998535491526
Batch  21  loss:  0.00032532733166590333
Batch  31  loss:  0.0003433095698710531
Batch  41  loss:  0.0002496383967809379
Batch  51  loss:  0.0004219629627186805
Batch  61  loss:  0.0002647204091772437
Batch  71  loss:  0.00038108855369500816
Batch  81  loss:  0.0003538533637765795
Batch  91  loss:  0.0003661764203570783
Validation on real data: 
LOSS supervised-train 0.0004019210704427678, valid 0.0002118893899023533
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0004892160068266094
Batch  11  loss:  0.0003465971676632762
Batch  21  loss:  0.00037725683068856597
Batch  31  loss:  0.00041941646486520767
Batch  41  loss:  0.00027174793649464846
Batch  51  loss:  0.0002675374271348119
Batch  61  loss:  0.0002455755020491779
Batch  71  loss:  0.0003063010808546096
Batch  81  loss:  0.0002955728559754789
Batch  91  loss:  0.0003298069932498038
Validation on real data: 
LOSS supervised-train 0.0004111244165687822, valid 0.00019255181541666389
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.000384285522159189
Batch  11  loss:  0.00029765587532892823
Batch  21  loss:  0.00047069540596567094
Batch  31  loss:  0.0003053117252420634
Batch  41  loss:  0.00028108453261666
Batch  51  loss:  0.0003962499904446304
Batch  61  loss:  0.00040091719711199403
Batch  71  loss:  0.00042962952284142375
Batch  81  loss:  0.00023520919785369188
Batch  91  loss:  0.00030515677644871175
Validation on real data: 
LOSS supervised-train 0.0003940604616946075, valid 0.00019275836530141532
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00040078736492432654
Batch  11  loss:  0.00032104243291541934
Batch  21  loss:  0.00044440513011068106
Batch  31  loss:  0.0002183256292482838
Batch  41  loss:  0.00030296584009192884
Batch  51  loss:  0.00038137679803185165
Batch  61  loss:  0.0004285990435164422
Batch  71  loss:  0.0003015502879861742
Batch  81  loss:  0.00027671080897562206
Batch  91  loss:  0.0003470238880254328
Validation on real data: 
LOSS supervised-train 0.00039979549052077346, valid 0.00023009878350421786
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0003669715079013258
Batch  11  loss:  0.00035429492709226906
Batch  21  loss:  0.0004713893576990813
Batch  31  loss:  0.00034203834366053343
Batch  41  loss:  0.0002560555876698345
Batch  51  loss:  0.00040158897172659636
Batch  61  loss:  0.00032321028993465006
Batch  71  loss:  0.0004739036376122385
Batch  81  loss:  0.0004317569255363196
Batch  91  loss:  0.00032816655584611
Validation on real data: 
LOSS supervised-train 0.0003934360865969211, valid 0.00018759544764179736
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.000306563510093838
Batch  11  loss:  0.00024632533313706517
Batch  21  loss:  0.0004320952284615487
Batch  31  loss:  0.0002114313538186252
Batch  41  loss:  0.000417903414927423
Batch  51  loss:  0.0002580067375674844
Batch  61  loss:  0.0003967774682678282
Batch  71  loss:  0.0002747501421254128
Batch  81  loss:  0.0003996929444838315
Batch  91  loss:  0.0003189442213624716
Validation on real data: 
LOSS supervised-train 0.0003935481492953841, valid 0.0002077496174024418
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00034585423418320715
Batch  11  loss:  0.00029484316473826766
Batch  21  loss:  0.00023078109370544553
Batch  31  loss:  0.0002521430724300444
Batch  41  loss:  0.0002808822027873248
Batch  51  loss:  0.0003966019139625132
Batch  61  loss:  0.0004239224945195019
Batch  71  loss:  0.00035599892726168036
Batch  81  loss:  0.0003085354983340949
Batch  91  loss:  0.0003678170032799244
Validation on real data: 
LOSS supervised-train 0.0003882983935181983, valid 0.00014728507085237652
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0005520880222320557
Batch  11  loss:  0.00025322253350168467
Batch  21  loss:  0.00033350553712807596
Batch  31  loss:  0.0003223253006581217
Batch  41  loss:  0.0002487695019226521
Batch  51  loss:  0.0004572178004309535
Batch  61  loss:  0.00021873234072700143
Batch  71  loss:  0.0005108854966238141
Batch  81  loss:  0.0002685829531401396
Batch  91  loss:  0.00024376354122068733
Validation on real data: 
LOSS supervised-train 0.0003989582302165218, valid 0.00016891032282728702
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.00028776831459254026
Batch  11  loss:  0.00030982887255959213
Batch  21  loss:  0.0004772493848577142
Batch  31  loss:  0.00021407182794064283
Batch  41  loss:  0.00025174758047796786
Batch  51  loss:  0.00034721664269454777
Batch  61  loss:  0.00031066822702996433
Batch  71  loss:  0.00023691098613198847
Batch  81  loss:  0.0003584013320505619
Batch  91  loss:  0.00038356430013664067
Validation on real data: 
LOSS supervised-train 0.0003758828503487166, valid 0.00020152247452642769
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  laptop ; Model ID: 519e98268bee56dddbb1de10c9529bf7
--------------------
Training baseline regression model:  2022-03-30 16:43:17.679878
Detector:  pointnet
Object:  laptop
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1608475
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.38325682282447815
Batch  11  loss:  0.21055276691913605
Batch  21  loss:  0.1440058797597885
Batch  31  loss:  0.08779396861791611
Batch  41  loss:  0.060515351593494415
Batch  51  loss:  0.04866382107138634
Batch  61  loss:  0.03775405138731003
Batch  71  loss:  0.030474914237856865
Batch  81  loss:  0.03764689713716507
Batch  91  loss:  0.02544538863003254
Validation on real data: 
LOSS supervised-train 0.08538623417727649, valid 0.011909758672118187
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.039646606892347336
Batch  11  loss:  0.014957845211029053
Batch  21  loss:  0.026084545999765396
Batch  31  loss:  0.02427462488412857
Batch  41  loss:  0.023264117538928986
Batch  51  loss:  0.01911219023168087
Batch  61  loss:  0.020256290212273598
Batch  71  loss:  0.012710400857031345
Batch  81  loss:  0.022049086168408394
Batch  91  loss:  0.015052630566060543
Validation on real data: 
LOSS supervised-train 0.020170399909839033, valid 0.008229045197367668
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.024084925651550293
Batch  11  loss:  0.012063287198543549
Batch  21  loss:  0.0236025620251894
Batch  31  loss:  0.012201671488583088
Batch  41  loss:  0.017473865300416946
Batch  51  loss:  0.01661105640232563
Batch  61  loss:  0.01719760335981846
Batch  71  loss:  0.015730958431959152
Batch  81  loss:  0.020532378926873207
Batch  91  loss:  0.013807660900056362
Validation on real data: 
LOSS supervised-train 0.016058261836878956, valid 0.004722889978438616
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.02761070616543293
Batch  11  loss:  0.012403230182826519
Batch  21  loss:  0.016632499173283577
Batch  31  loss:  0.012147140689194202
Batch  41  loss:  0.014173428528010845
Batch  51  loss:  0.017599696293473244
Batch  61  loss:  0.013215403072535992
Batch  71  loss:  0.012086734175682068
Batch  81  loss:  0.01643289439380169
Batch  91  loss:  0.013630686327815056
Validation on real data: 
LOSS supervised-train 0.014091049889102578, valid 0.00657415296882391
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.02229652740061283
Batch  11  loss:  0.008481807075440884
Batch  21  loss:  0.014790963381528854
Batch  31  loss:  0.009896804578602314
Batch  41  loss:  0.016067031770944595
Batch  51  loss:  0.016951384022831917
Batch  61  loss:  0.016097472980618477
Batch  71  loss:  0.011659771203994751
Batch  81  loss:  0.015104570426046848
Batch  91  loss:  0.010507234372198582
Validation on real data: 
LOSS supervised-train 0.0132025141781196, valid 0.004485713317990303
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.020097598433494568
Batch  11  loss:  0.0064655691385269165
Batch  21  loss:  0.013193135149776936
Batch  31  loss:  0.010308769531548023
Batch  41  loss:  0.012134735472500324
Batch  51  loss:  0.013988982886075974
Batch  61  loss:  0.014885450713336468
Batch  71  loss:  0.008544386364519596
Batch  81  loss:  0.01588962785899639
Batch  91  loss:  0.009434375911951065
Validation on real data: 
LOSS supervised-train 0.011830173837952316, valid 0.004360058810561895
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.02245066873729229
Batch  11  loss:  0.008418751880526543
Batch  21  loss:  0.017085112631320953
Batch  31  loss:  0.0078063891269266605
Batch  41  loss:  0.011737936176359653
Batch  51  loss:  0.012640612199902534
Batch  61  loss:  0.013749719597399235
Batch  71  loss:  0.008005484938621521
Batch  81  loss:  0.014218121767044067
Batch  91  loss:  0.011573812924325466
Validation on real data: 
LOSS supervised-train 0.011470249253325165, valid 0.0054225800558924675
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.018277650699019432
Batch  11  loss:  0.008336850441992283
Batch  21  loss:  0.012827884405851364
Batch  31  loss:  0.00922795943915844
Batch  41  loss:  0.010854117572307587
Batch  51  loss:  0.012198437936604023
Batch  61  loss:  0.013633279129862785
Batch  71  loss:  0.010024845600128174
Batch  81  loss:  0.01050189696252346
Batch  91  loss:  0.00950554944574833
Validation on real data: 
LOSS supervised-train 0.010578516060486437, valid 0.00561524135991931
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.016905706375837326
Batch  11  loss:  0.0070951757952570915
Batch  21  loss:  0.011592764407396317
Batch  31  loss:  0.007374834734946489
Batch  41  loss:  0.011482584290206432
Batch  51  loss:  0.015545772388577461
Batch  61  loss:  0.013914288952946663
Batch  71  loss:  0.010607612319290638
Batch  81  loss:  0.012065216898918152
Batch  91  loss:  0.010768632404506207
Validation on real data: 
LOSS supervised-train 0.01042925754096359, valid 0.005247118417173624
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0150865213945508
Batch  11  loss:  0.008610657416284084
Batch  21  loss:  0.010260773822665215
Batch  31  loss:  0.009291733615100384
Batch  41  loss:  0.009853865951299667
Batch  51  loss:  0.011319047771394253
Batch  61  loss:  0.01277336198836565
Batch  71  loss:  0.008616629987955093
Batch  81  loss:  0.013956286944448948
Batch  91  loss:  0.00908780936151743
Validation on real data: 
LOSS supervised-train 0.009625381152145564, valid 0.0037593510933220387
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.020054638385772705
Batch  11  loss:  0.0076722679659724236
Batch  21  loss:  0.009819199331104755
Batch  31  loss:  0.009552101604640484
Batch  41  loss:  0.007844286039471626
Batch  51  loss:  0.011142706498503685
Batch  61  loss:  0.010129783302545547
Batch  71  loss:  0.0073693254962563515
Batch  81  loss:  0.009468598291277885
Batch  91  loss:  0.009173554368317127
Validation on real data: 
LOSS supervised-train 0.00940817556809634, valid 0.005209573544561863
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.01186814159154892
Batch  11  loss:  0.007500784005969763
Batch  21  loss:  0.012095416896045208
Batch  31  loss:  0.006814395077526569
Batch  41  loss:  0.009106207638978958
Batch  51  loss:  0.010360185988247395
Batch  61  loss:  0.009033680893480778
Batch  71  loss:  0.006381556857377291
Batch  81  loss:  0.010693707503378391
Batch  91  loss:  0.010080964304506779
Validation on real data: 
LOSS supervised-train 0.009085402472410352, valid 0.0040691811591386795
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.01414678618311882
Batch  11  loss:  0.005471230484545231
Batch  21  loss:  0.010082668624818325
Batch  31  loss:  0.006887257564812899
Batch  41  loss:  0.009780625812709332
Batch  51  loss:  0.009505760855972767
Batch  61  loss:  0.010972369462251663
Batch  71  loss:  0.007054028566926718
Batch  81  loss:  0.009009896777570248
Batch  91  loss:  0.0071897124871611595
Validation on real data: 
LOSS supervised-train 0.008722869879566133, valid 0.004056579899042845
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.014168927446007729
Batch  11  loss:  0.005499298218637705
Batch  21  loss:  0.012413367629051208
Batch  31  loss:  0.00680692819878459
Batch  41  loss:  0.008078839629888535
Batch  51  loss:  0.00852045789361
Batch  61  loss:  0.010415594093501568
Batch  71  loss:  0.006525538861751556
Batch  81  loss:  0.012464893981814384
Batch  91  loss:  0.00890715979039669
Validation on real data: 
LOSS supervised-train 0.008402900274377316, valid 0.003794247517362237
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.014858188107609749
Batch  11  loss:  0.004753239452838898
Batch  21  loss:  0.009509123861789703
Batch  31  loss:  0.008625257760286331
Batch  41  loss:  0.008380281738936901
Batch  51  loss:  0.00833134539425373
Batch  61  loss:  0.009979493916034698
Batch  71  loss:  0.006389758549630642
Batch  81  loss:  0.010894942097365856
Batch  91  loss:  0.00801418162882328
Validation on real data: 
LOSS supervised-train 0.008229380056727678, valid 0.0033721369691193104
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.012747110798954964
Batch  11  loss:  0.007631251588463783
Batch  21  loss:  0.011975252069532871
Batch  31  loss:  0.00917741283774376
Batch  41  loss:  0.00761544331908226
Batch  51  loss:  0.008537936955690384
Batch  61  loss:  0.009330089204013348
Batch  71  loss:  0.007673223037272692
Batch  81  loss:  0.010039436630904675
Batch  91  loss:  0.007408755831420422
Validation on real data: 
LOSS supervised-train 0.008040721195284277, valid 0.003362866584211588
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.010647270828485489
Batch  11  loss:  0.005208643153309822
Batch  21  loss:  0.009470404125750065
Batch  31  loss:  0.006826875265687704
Batch  41  loss:  0.008811991661787033
Batch  51  loss:  0.011218526400625706
Batch  61  loss:  0.007249392569065094
Batch  71  loss:  0.006563956383615732
Batch  81  loss:  0.012039209716022015
Batch  91  loss:  0.008355086669325829
Validation on real data: 
LOSS supervised-train 0.007725990347098559, valid 0.0033918535336852074
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.011036922223865986
Batch  11  loss:  0.00642786780372262
Batch  21  loss:  0.009332382120192051
Batch  31  loss:  0.007360344752669334
Batch  41  loss:  0.008326760493218899
Batch  51  loss:  0.008089238777756691
Batch  61  loss:  0.008529242128133774
Batch  71  loss:  0.007259697653353214
Batch  81  loss:  0.012497417628765106
Batch  91  loss:  0.006297282874584198
Validation on real data: 
LOSS supervised-train 0.007373511074110866, valid 0.0022977155167609453
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.012810390442609787
Batch  11  loss:  0.00420208228752017
Batch  21  loss:  0.011098621413111687
Batch  31  loss:  0.007657194044440985
Batch  41  loss:  0.00653386116027832
Batch  51  loss:  0.005750058684498072
Batch  61  loss:  0.0072583649307489395
Batch  71  loss:  0.00688170175999403
Batch  81  loss:  0.00888499803841114
Batch  91  loss:  0.006977053824812174
Validation on real data: 
LOSS supervised-train 0.007175365942530334, valid 0.002998252399265766
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.012743213213980198
Batch  11  loss:  0.004831226542592049
Batch  21  loss:  0.011423083022236824
Batch  31  loss:  0.0069158184342086315
Batch  41  loss:  0.0078493133187294
Batch  51  loss:  0.008453662507236004
Batch  61  loss:  0.006322458852082491
Batch  71  loss:  0.006398940458893776
Batch  81  loss:  0.00792358722537756
Batch  91  loss:  0.006891312543302774
Validation on real data: 
LOSS supervised-train 0.00710831793025136, valid 0.0036769129801541567
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.01009378395974636
Batch  11  loss:  0.005404700059443712
Batch  21  loss:  0.012347359210252762
Batch  31  loss:  0.006047785747796297
Batch  41  loss:  0.006836150772869587
Batch  51  loss:  0.00803998950868845
Batch  61  loss:  0.006133201997727156
Batch  71  loss:  0.00819201860576868
Batch  81  loss:  0.008923211134970188
Batch  91  loss:  0.005857857409864664
Validation on real data: 
LOSS supervised-train 0.006817186358384788, valid 0.0031190591398626566
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.011068601161241531
Batch  11  loss:  0.005188112147152424
Batch  21  loss:  0.009897076524794102
Batch  31  loss:  0.006420415360480547
Batch  41  loss:  0.006137569434940815
Batch  51  loss:  0.007084005977958441
Batch  61  loss:  0.007412293925881386
Batch  71  loss:  0.007950338535010815
Batch  81  loss:  0.008089256472885609
Batch  91  loss:  0.006417902186512947
Validation on real data: 
LOSS supervised-train 0.006652221127878875, valid 0.003137304214760661
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.011185667477548122
Batch  11  loss:  0.005775333382189274
Batch  21  loss:  0.012599329464137554
Batch  31  loss:  0.008084296248853207
Batch  41  loss:  0.006769747473299503
Batch  51  loss:  0.00791234616190195
Batch  61  loss:  0.004518117755651474
Batch  71  loss:  0.0057846009731292725
Batch  81  loss:  0.006871485151350498
Batch  91  loss:  0.006548479665070772
Validation on real data: 
LOSS supervised-train 0.006494674473069608, valid 0.003156412160024047
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.008816166780889034
Batch  11  loss:  0.0035286236088722944
Batch  21  loss:  0.008585228584706783
Batch  31  loss:  0.007840369828045368
Batch  41  loss:  0.00531279481947422
Batch  51  loss:  0.005833883304148912
Batch  61  loss:  0.007303751073777676
Batch  71  loss:  0.005397005006670952
Batch  81  loss:  0.007436188403517008
Batch  91  loss:  0.006758163217455149
Validation on real data: 
LOSS supervised-train 0.006300263991579414, valid 0.00628213444724679
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.009984314441680908
Batch  11  loss:  0.004508106969296932
Batch  21  loss:  0.009803952649235725
Batch  31  loss:  0.005382194183766842
Batch  41  loss:  0.005646056961268187
Batch  51  loss:  0.005986771546304226
Batch  61  loss:  0.007319887634366751
Batch  71  loss:  0.005176981445401907
Batch  81  loss:  0.00846005231142044
Batch  91  loss:  0.0064047821797430515
Validation on real data: 
LOSS supervised-train 0.006192243916448206, valid 0.003189428010955453
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.010529930703341961
Batch  11  loss:  0.004481090698391199
Batch  21  loss:  0.008420339785516262
Batch  31  loss:  0.005857584998011589
Batch  41  loss:  0.006208496168255806
Batch  51  loss:  0.005998093169182539
Batch  61  loss:  0.005656485911458731
Batch  71  loss:  0.005051118787378073
Batch  81  loss:  0.007370638195425272
Batch  91  loss:  0.007271722890436649
Validation on real data: 
LOSS supervised-train 0.005937624915968626, valid 0.002869745483621955
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0098121901974082
Batch  11  loss:  0.004723327234387398
Batch  21  loss:  0.009544908069074154
Batch  31  loss:  0.005564436782151461
Batch  41  loss:  0.005730792880058289
Batch  51  loss:  0.007630525156855583
Batch  61  loss:  0.0055389306508004665
Batch  71  loss:  0.00532476045191288
Batch  81  loss:  0.00779532128944993
Batch  91  loss:  0.0070603941567242146
Validation on real data: 
LOSS supervised-train 0.0057622854853980245, valid 0.002991434885188937
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.01054864376783371
Batch  11  loss:  0.0037967022508382797
Batch  21  loss:  0.009064976125955582
Batch  31  loss:  0.006871657446026802
Batch  41  loss:  0.006435521878302097
Batch  51  loss:  0.004668037872761488
Batch  61  loss:  0.005840259604156017
Batch  71  loss:  0.005298777483403683
Batch  81  loss:  0.006093244533985853
Batch  91  loss:  0.007806540001183748
Validation on real data: 
LOSS supervised-train 0.005756496875546873, valid 0.003128297859802842
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.006956798490136862
Batch  11  loss:  0.004460819996893406
Batch  21  loss:  0.006514232140034437
Batch  31  loss:  0.005824477411806583
Batch  41  loss:  0.005077002570033073
Batch  51  loss:  0.004964129533618689
Batch  61  loss:  0.00668266462162137
Batch  71  loss:  0.0042100027203559875
Batch  81  loss:  0.00514526292681694
Batch  91  loss:  0.007512642070651054
Validation on real data: 
LOSS supervised-train 0.005470513007603586, valid 0.003135734237730503
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.00869941059499979
Batch  11  loss:  0.0036135753616690636
Batch  21  loss:  0.010600707493722439
Batch  31  loss:  0.005886525847017765
Batch  41  loss:  0.0041179112158715725
Batch  51  loss:  0.003302286844700575
Batch  61  loss:  0.005347211379557848
Batch  71  loss:  0.00521507766097784
Batch  81  loss:  0.005364335607737303
Batch  91  loss:  0.005529152229428291
Validation on real data: 
LOSS supervised-train 0.005358499123249203, valid 0.002707795239984989
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00868559256196022
Batch  11  loss:  0.0035984579008072615
Batch  21  loss:  0.008799985982477665
Batch  31  loss:  0.005242388229817152
Batch  41  loss:  0.004693332593888044
Batch  51  loss:  0.006989147514104843
Batch  61  loss:  0.005129114259034395
Batch  71  loss:  0.004109464585781097
Batch  81  loss:  0.005700271110981703
Batch  91  loss:  0.007260323967784643
Validation on real data: 
LOSS supervised-train 0.005260943081229925, valid 0.0032781576737761497
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.006467489060014486
Batch  11  loss:  0.004375080112367868
Batch  21  loss:  0.009795167483389378
Batch  31  loss:  0.00604203948751092
Batch  41  loss:  0.00482766842469573
Batch  51  loss:  0.005820473190397024
Batch  61  loss:  0.004714045207947493
Batch  71  loss:  0.003722347319126129
Batch  81  loss:  0.007601433899253607
Batch  91  loss:  0.005239400081336498
Validation on real data: 
LOSS supervised-train 0.005110428936313837, valid 0.002971094800159335
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.006213803309947252
Batch  11  loss:  0.003977200482040644
Batch  21  loss:  0.007341996766626835
Batch  31  loss:  0.0045547629706561565
Batch  41  loss:  0.004420693498104811
Batch  51  loss:  0.004166180267930031
Batch  61  loss:  0.0048788017593324184
Batch  71  loss:  0.005003639496862888
Batch  81  loss:  0.005812046583741903
Batch  91  loss:  0.007434257306158543
Validation on real data: 
LOSS supervised-train 0.004965925787109882, valid 0.0031806689221411943
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0077308546751737595
Batch  11  loss:  0.003684175433591008
Batch  21  loss:  0.011648107320070267
Batch  31  loss:  0.005357298534363508
Batch  41  loss:  0.005264465231448412
Batch  51  loss:  0.005111695267260075
Batch  61  loss:  0.005573222879320383
Batch  71  loss:  0.003929811995476484
Batch  81  loss:  0.005361361429095268
Batch  91  loss:  0.0055260732769966125
Validation on real data: 
LOSS supervised-train 0.005078452625311911, valid 0.002997427014634013
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.006604852620512247
Batch  11  loss:  0.004077355843037367
Batch  21  loss:  0.0094325365498662
Batch  31  loss:  0.006000957451760769
Batch  41  loss:  0.004150695633143187
Batch  51  loss:  0.005385742522776127
Batch  61  loss:  0.0039137969724833965
Batch  71  loss:  0.004601250868290663
Batch  81  loss:  0.004770686384290457
Batch  91  loss:  0.005476863123476505
Validation on real data: 
LOSS supervised-train 0.004969770053867251, valid 0.0029637219849973917
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.007437083404511213
Batch  11  loss:  0.0033865796867758036
Batch  21  loss:  0.006745216902345419
Batch  31  loss:  0.006388277746737003
Batch  41  loss:  0.004200862720608711
Batch  51  loss:  0.004157425370067358
Batch  61  loss:  0.004994732793420553
Batch  71  loss:  0.004115566145628691
Batch  81  loss:  0.00574900209903717
Batch  91  loss:  0.007675500120967627
Validation on real data: 
LOSS supervised-train 0.004787865201942623, valid 0.003004219848662615
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.006604589056223631
Batch  11  loss:  0.003787968773394823
Batch  21  loss:  0.007256915792822838
Batch  31  loss:  0.005794397555291653
Batch  41  loss:  0.004575592931360006
Batch  51  loss:  0.004707617219537497
Batch  61  loss:  0.005047414917498827
Batch  71  loss:  0.005358547903597355
Batch  81  loss:  0.0050277067348361015
Batch  91  loss:  0.005580979399383068
Validation on real data: 
LOSS supervised-train 0.004876579288393259, valid 0.0032886830158531666
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.006356050260365009
Batch  11  loss:  0.003906835801899433
Batch  21  loss:  0.007424048148095608
Batch  31  loss:  0.005523192696273327
Batch  41  loss:  0.004628885071724653
Batch  51  loss:  0.005230290349572897
Batch  61  loss:  0.00631413608789444
Batch  71  loss:  0.005216196645051241
Batch  81  loss:  0.0048920754343271255
Batch  91  loss:  0.0061459019780159
Validation on real data: 
LOSS supervised-train 0.0047845490579493345, valid 0.0040166317485272884
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.005183948669582605
Batch  11  loss:  0.0038771352265030146
Batch  21  loss:  0.007733149453997612
Batch  31  loss:  0.00621510436758399
Batch  41  loss:  0.004530219826847315
Batch  51  loss:  0.0049008470959961414
Batch  61  loss:  0.00699393218383193
Batch  71  loss:  0.003083622083067894
Batch  81  loss:  0.005494185723364353
Batch  91  loss:  0.00624789809808135
Validation on real data: 
LOSS supervised-train 0.004552256453316659, valid 0.002969098510220647
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.009244809858500957
Batch  11  loss:  0.0032645580358803272
Batch  21  loss:  0.007328859996050596
Batch  31  loss:  0.004785312805324793
Batch  41  loss:  0.004805798642337322
Batch  51  loss:  0.0036552976816892624
Batch  61  loss:  0.004295250866562128
Batch  71  loss:  0.0045469412580132484
Batch  81  loss:  0.005083919502794743
Batch  91  loss:  0.0042768302373588085
Validation on real data: 
LOSS supervised-train 0.0044631171505898235, valid 0.0027856857050210238
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.007142261136323214
Batch  11  loss:  0.003455233993008733
Batch  21  loss:  0.010338791646063328
Batch  31  loss:  0.006145849823951721
Batch  41  loss:  0.004426593892276287
Batch  51  loss:  0.0042464775033295155
Batch  61  loss:  0.004853053018450737
Batch  71  loss:  0.004342595115303993
Batch  81  loss:  0.004348026588559151
Batch  91  loss:  0.006683156359940767
Validation on real data: 
LOSS supervised-train 0.004573303768411279, valid 0.0038604228757321835
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.005071128718554974
Batch  11  loss:  0.004327135626226664
Batch  21  loss:  0.009887523017823696
Batch  31  loss:  0.0054364437237381935
Batch  41  loss:  0.004400504287332296
Batch  51  loss:  0.005464890971779823
Batch  61  loss:  0.0038308314979076385
Batch  71  loss:  0.004172436892986298
Batch  81  loss:  0.005185908637940884
Batch  91  loss:  0.004778389353305101
Validation on real data: 
LOSS supervised-train 0.004398944557178766, valid 0.002588577102869749
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.005276626907289028
Batch  11  loss:  0.003704969771206379
Batch  21  loss:  0.007175266742706299
Batch  31  loss:  0.0070338197983801365
Batch  41  loss:  0.004325615707784891
Batch  51  loss:  0.0033185577485710382
Batch  61  loss:  0.004377730656415224
Batch  71  loss:  0.004346802830696106
Batch  81  loss:  0.004980696830898523
Batch  91  loss:  0.004407272674143314
Validation on real data: 
LOSS supervised-train 0.004292611223645508, valid 0.0034768995828926563
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.004953313153237104
Batch  11  loss:  0.0040282635018229485
Batch  21  loss:  0.008987491019070148
Batch  31  loss:  0.005788295064121485
Batch  41  loss:  0.0037554767914116383
Batch  51  loss:  0.004415588919073343
Batch  61  loss:  0.005544779356569052
Batch  71  loss:  0.0032541160471737385
Batch  81  loss:  0.006855322979390621
Batch  91  loss:  0.00640595518052578
Validation on real data: 
LOSS supervised-train 0.004302141333464533, valid 0.003314427100121975
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.008013682439923286
Batch  11  loss:  0.00398347619920969
Batch  21  loss:  0.006207479629665613
Batch  31  loss:  0.005484497640281916
Batch  41  loss:  0.004798692185431719
Batch  51  loss:  0.0036393865011632442
Batch  61  loss:  0.005575274117290974
Batch  71  loss:  0.0036781090311706066
Batch  81  loss:  0.005403616465628147
Batch  91  loss:  0.0041239215061068535
Validation on real data: 
LOSS supervised-train 0.004274395648390055, valid 0.003094938350841403
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.00539919501170516
Batch  11  loss:  0.002953139366582036
Batch  21  loss:  0.009886762127280235
Batch  31  loss:  0.004348693881183863
Batch  41  loss:  0.003852180205285549
Batch  51  loss:  0.002889649709686637
Batch  61  loss:  0.004860575310885906
Batch  71  loss:  0.0026293860282748938
Batch  81  loss:  0.004213051404803991
Batch  91  loss:  0.006209434010088444
Validation on real data: 
LOSS supervised-train 0.004140346432104707, valid 0.0036175160203129053
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0052650184370577335
Batch  11  loss:  0.0028926581144332886
Batch  21  loss:  0.007298283278942108
Batch  31  loss:  0.004992129746824503
Batch  41  loss:  0.0036958931013941765
Batch  51  loss:  0.003868007566779852
Batch  61  loss:  0.003047580597922206
Batch  71  loss:  0.004246618133038282
Batch  81  loss:  0.00427445862442255
Batch  91  loss:  0.0058817313984036446
Validation on real data: 
LOSS supervised-train 0.004109814453404397, valid 0.0034359674900770187
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.00462225591763854
Batch  11  loss:  0.003037639195099473
Batch  21  loss:  0.0057480549439787865
Batch  31  loss:  0.004227571655064821
Batch  41  loss:  0.0034159785136580467
Batch  51  loss:  0.003286727238446474
Batch  61  loss:  0.0047920336946845055
Batch  71  loss:  0.002257188316434622
Batch  81  loss:  0.005759221035987139
Batch  91  loss:  0.005267676431685686
Validation on real data: 
LOSS supervised-train 0.00398455532034859, valid 0.00379652576521039
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.006990663707256317
Batch  11  loss:  0.004132247995585203
Batch  21  loss:  0.0065062991343438625
Batch  31  loss:  0.0037406806368380785
Batch  41  loss:  0.004967008717358112
Batch  51  loss:  0.004267222713679075
Batch  61  loss:  0.003990957047790289
Batch  71  loss:  0.0036057233810424805
Batch  81  loss:  0.005428166128695011
Batch  91  loss:  0.006561703979969025
Validation on real data: 
LOSS supervised-train 0.003992073123808951, valid 0.00322917103767395
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.005082346498966217
Batch  11  loss:  0.003468550741672516
Batch  21  loss:  0.0063892994076013565
Batch  31  loss:  0.00339036644436419
Batch  41  loss:  0.0037632447201758623
Batch  51  loss:  0.004247472621500492
Batch  61  loss:  0.0035967391449958086
Batch  71  loss:  0.003128914162516594
Batch  81  loss:  0.005232607014477253
Batch  91  loss:  0.004126861225813627
Validation on real data: 
LOSS supervised-train 0.003963833195157349, valid 0.0035666925832629204
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0051893675699830055
Batch  11  loss:  0.003453467972576618
Batch  21  loss:  0.006419672630727291
Batch  31  loss:  0.004406167194247246
Batch  41  loss:  0.003522191895172
Batch  51  loss:  0.003404434770345688
Batch  61  loss:  0.0038720315787941217
Batch  71  loss:  0.003251298563554883
Batch  81  loss:  0.003787233727052808
Batch  91  loss:  0.00530595425516367
Validation on real data: 
LOSS supervised-train 0.00381387876230292, valid 0.003917729947715998
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.005599434953182936
Batch  11  loss:  0.004677851218730211
Batch  21  loss:  0.005336787551641464
Batch  31  loss:  0.004486265126615763
Batch  41  loss:  0.004691006615757942
Batch  51  loss:  0.002686440944671631
Batch  61  loss:  0.004894204903393984
Batch  71  loss:  0.0034356408286839724
Batch  81  loss:  0.004682494793087244
Batch  91  loss:  0.0053322212770581245
Validation on real data: 
LOSS supervised-train 0.004037951945792884, valid 0.0033171053510159254
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.005801905412226915
Batch  11  loss:  0.003615838475525379
Batch  21  loss:  0.007381456904113293
Batch  31  loss:  0.004864128306508064
Batch  41  loss:  0.0037536602467298508
Batch  51  loss:  0.0030366014689207077
Batch  61  loss:  0.004988661035895348
Batch  71  loss:  0.003099476220086217
Batch  81  loss:  0.0032583987340331078
Batch  91  loss:  0.003491859883069992
Validation on real data: 
LOSS supervised-train 0.003796092341654003, valid 0.003394086379557848
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.005922841839492321
Batch  11  loss:  0.0030669178813695908
Batch  21  loss:  0.005070820916444063
Batch  31  loss:  0.005132297053933144
Batch  41  loss:  0.003614389104768634
Batch  51  loss:  0.0033667488023638725
Batch  61  loss:  0.005390297621488571
Batch  71  loss:  0.003803004277870059
Batch  81  loss:  0.004658318590372801
Batch  91  loss:  0.005955778062343597
Validation on real data: 
LOSS supervised-train 0.003866328133735806, valid 0.0032100151292979717
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.006449515465646982
Batch  11  loss:  0.0037114110309630632
Batch  21  loss:  0.0057422393001616
Batch  31  loss:  0.00361709576100111
Batch  41  loss:  0.0037631881423294544
Batch  51  loss:  0.002880039857700467
Batch  61  loss:  0.004724270198494196
Batch  71  loss:  0.0024249244015663862
Batch  81  loss:  0.0051226080395281315
Batch  91  loss:  0.0039800312370061874
Validation on real data: 
LOSS supervised-train 0.0037980629247613253, valid 0.004406045190989971
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.004817999433726072
Batch  11  loss:  0.003352199215441942
Batch  21  loss:  0.004235753323882818
Batch  31  loss:  0.004590851720422506
Batch  41  loss:  0.004033862147480249
Batch  51  loss:  0.0024361510295420885
Batch  61  loss:  0.005187507718801498
Batch  71  loss:  0.003495880402624607
Batch  81  loss:  0.004911018069833517
Batch  91  loss:  0.005810453090816736
Validation on real data: 
LOSS supervised-train 0.0038204266224056483, valid 0.0028985808603465557
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.008278505876660347
Batch  11  loss:  0.003761912463232875
Batch  21  loss:  0.003969467710703611
Batch  31  loss:  0.004328400827944279
Batch  41  loss:  0.003904558252543211
Batch  51  loss:  0.0025592620950192213
Batch  61  loss:  0.004004869144409895
Batch  71  loss:  0.0037555918097496033
Batch  81  loss:  0.005424148868769407
Batch  91  loss:  0.006440561730414629
Validation on real data: 
LOSS supervised-train 0.003962065591476858, valid 0.003671621670946479
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00462355837225914
Batch  11  loss:  0.005667918361723423
Batch  21  loss:  0.004367709159851074
Batch  31  loss:  0.005350193940103054
Batch  41  loss:  0.0041010393761098385
Batch  51  loss:  0.002694288967177272
Batch  61  loss:  0.003452399279922247
Batch  71  loss:  0.0038213178049772978
Batch  81  loss:  0.005419780034571886
Batch  91  loss:  0.005258212797343731
Validation on real data: 
LOSS supervised-train 0.003971548948902637, valid 0.004143671598285437
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.005948967300355434
Batch  11  loss:  0.0034966757521033287
Batch  21  loss:  0.008160438388586044
Batch  31  loss:  0.0034684203565120697
Batch  41  loss:  0.003967994824051857
Batch  51  loss:  0.0027249872218817472
Batch  61  loss:  0.0033855014480650425
Batch  71  loss:  0.003722008317708969
Batch  81  loss:  0.003659505397081375
Batch  91  loss:  0.006296496372669935
Validation on real data: 
LOSS supervised-train 0.0038816541293635962, valid 0.0034407624043524265
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0037151353899389505
Batch  11  loss:  0.004794166423380375
Batch  21  loss:  0.005212981719523668
Batch  31  loss:  0.003389135468751192
Batch  41  loss:  0.004909622948616743
Batch  51  loss:  0.0030077758710831404
Batch  61  loss:  0.00308779114857316
Batch  71  loss:  0.003441000124439597
Batch  81  loss:  0.003774486016482115
Batch  91  loss:  0.00614642258733511
Validation on real data: 
LOSS supervised-train 0.003834871827857569, valid 0.003074195235967636
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.005577186122536659
Batch  11  loss:  0.004087802954018116
Batch  21  loss:  0.00585283013060689
Batch  31  loss:  0.00307128531858325
Batch  41  loss:  0.004163031000643969
Batch  51  loss:  0.0023046310525387526
Batch  61  loss:  0.004069985821843147
Batch  71  loss:  0.0032910832669585943
Batch  81  loss:  0.004064900800585747
Batch  91  loss:  0.007201619446277618
Validation on real data: 
LOSS supervised-train 0.003923423413652927, valid 0.003914243541657925
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.004058048594743013
Batch  11  loss:  0.004431172739714384
Batch  21  loss:  0.004407602362334728
Batch  31  loss:  0.004137883894145489
Batch  41  loss:  0.003756236983463168
Batch  51  loss:  0.002393624046817422
Batch  61  loss:  0.003334402572363615
Batch  71  loss:  0.0023186306934803724
Batch  81  loss:  0.0029397630132734776
Batch  91  loss:  0.0054424055851995945
Validation on real data: 
LOSS supervised-train 0.003670697589404881, valid 0.0033143258187919855
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0074972412548959255
Batch  11  loss:  0.00434832600876689
Batch  21  loss:  0.006848612800240517
Batch  31  loss:  0.007433947175741196
Batch  41  loss:  0.00444318912923336
Batch  51  loss:  0.0026076710782945156
Batch  61  loss:  0.0032412081491202116
Batch  71  loss:  0.002710937289521098
Batch  81  loss:  0.0030027360189706087
Batch  91  loss:  0.006714733317494392
Validation on real data: 
LOSS supervised-train 0.003875217139720917, valid 0.0031033349223434925
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.00596250407397747
Batch  11  loss:  0.003123977454379201
Batch  21  loss:  0.00488606933504343
Batch  31  loss:  0.00733847264200449
Batch  41  loss:  0.0035354900173842907
Batch  51  loss:  0.002985713304951787
Batch  61  loss:  0.0035610198974609375
Batch  71  loss:  0.0030007758177816868
Batch  81  loss:  0.00472427299246192
Batch  91  loss:  0.004101952072232962
Validation on real data: 
LOSS supervised-train 0.003648837790824473, valid 0.0029551100451499224
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0054220110177993774
Batch  11  loss:  0.002661788370460272
Batch  21  loss:  0.006938217673450708
Batch  31  loss:  0.006091652903705835
Batch  41  loss:  0.0038648806512355804
Batch  51  loss:  0.003942635841667652
Batch  61  loss:  0.0028368968050926924
Batch  71  loss:  0.0029150566551834345
Batch  81  loss:  0.003807937493547797
Batch  91  loss:  0.004265381023287773
Validation on real data: 
LOSS supervised-train 0.0037722538644447924, valid 0.004998477641493082
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.007028714753687382
Batch  11  loss:  0.0034227638971060514
Batch  21  loss:  0.009331732988357544
Batch  31  loss:  0.006218045949935913
Batch  41  loss:  0.0029537018854171038
Batch  51  loss:  0.002108703600242734
Batch  61  loss:  0.0030931353103369474
Batch  71  loss:  0.0037947851233184338
Batch  81  loss:  0.004042118787765503
Batch  91  loss:  0.002665833570063114
Validation on real data: 
LOSS supervised-train 0.0038266853382810952, valid 0.0027550968807190657
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0044344086199998856
Batch  11  loss:  0.0026681360322982073
Batch  21  loss:  0.007057120092213154
Batch  31  loss:  0.0051405299454927444
Batch  41  loss:  0.003854346228763461
Batch  51  loss:  0.002988530555739999
Batch  61  loss:  0.003747015492990613
Batch  71  loss:  0.0064524817280471325
Batch  81  loss:  0.004212816711515188
Batch  91  loss:  0.003965532872825861
Validation on real data: 
LOSS supervised-train 0.004094064068049193, valid 0.0028396311681717634
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.005096657667309046
Batch  11  loss:  0.003296757349744439
Batch  21  loss:  0.01216463278979063
Batch  31  loss:  0.004206063691526651
Batch  41  loss:  0.002670670161023736
Batch  51  loss:  0.0025923147331923246
Batch  61  loss:  0.003990988712757826
Batch  71  loss:  0.0029787681996822357
Batch  81  loss:  0.004802823998034
Batch  91  loss:  0.0025725578889250755
Validation on real data: 
LOSS supervised-train 0.004064695206470788, valid 0.003112207865342498
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.006442169193178415
Batch  11  loss:  0.002675043186172843
Batch  21  loss:  0.006792000960558653
Batch  31  loss:  0.003913861233741045
Batch  41  loss:  0.005019675940275192
Batch  51  loss:  0.0022353266831487417
Batch  61  loss:  0.004462528973817825
Batch  71  loss:  0.0043969410471618176
Batch  81  loss:  0.005323146935552359
Batch  91  loss:  0.003170217154547572
Validation on real data: 
LOSS supervised-train 0.003885355363599956, valid 0.0024209863040596247
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0031620829831808805
Batch  11  loss:  0.002830908866599202
Batch  21  loss:  0.008863694034516811
Batch  31  loss:  0.0032348281238228083
Batch  41  loss:  0.004014298319816589
Batch  51  loss:  0.0022152531892061234
Batch  61  loss:  0.004880400374531746
Batch  71  loss:  0.003111725440248847
Batch  81  loss:  0.0035014229360967875
Batch  91  loss:  0.0029619650449603796
Validation on real data: 
LOSS supervised-train 0.0036334577715024354, valid 0.003277016803622246
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0065803504548966885
Batch  11  loss:  0.003080286318436265
Batch  21  loss:  0.005642001982778311
Batch  31  loss:  0.0035189385525882244
Batch  41  loss:  0.004855982959270477
Batch  51  loss:  0.003088661702349782
Batch  61  loss:  0.00617562048137188
Batch  71  loss:  0.0026846565306186676
Batch  81  loss:  0.004880123771727085
Batch  91  loss:  0.0032346020452678204
Validation on real data: 
LOSS supervised-train 0.0038169488753192126, valid 0.0045311227440834045
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0057118842378258705
Batch  11  loss:  0.004034081939607859
Batch  21  loss:  0.004609387367963791
Batch  31  loss:  0.0032009112183004618
Batch  41  loss:  0.003528091823682189
Batch  51  loss:  0.0029281347524374723
Batch  61  loss:  0.005363909061998129
Batch  71  loss:  0.002840084955096245
Batch  81  loss:  0.005001253914088011
Batch  91  loss:  0.0037280612159520388
Validation on real data: 
LOSS supervised-train 0.003658606824465096, valid 0.0042618392035365105
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.00527026504278183
Batch  11  loss:  0.0027750139124691486
Batch  21  loss:  0.004609772469848394
Batch  31  loss:  0.003758699167519808
Batch  41  loss:  0.0026799077168107033
Batch  51  loss:  0.002567295916378498
Batch  61  loss:  0.005182021297514439
Batch  71  loss:  0.003196512348949909
Batch  81  loss:  0.003961085341870785
Batch  91  loss:  0.004297832027077675
Validation on real data: 
LOSS supervised-train 0.003627959596924484, valid 0.004547894932329655
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0073174117133021355
Batch  11  loss:  0.0030434452928602695
Batch  21  loss:  0.005720060784369707
Batch  31  loss:  0.004208216443657875
Batch  41  loss:  0.0034242284018546343
Batch  51  loss:  0.0026935713831335306
Batch  61  loss:  0.0036761974915862083
Batch  71  loss:  0.003181801876053214
Batch  81  loss:  0.0036378377117216587
Batch  91  loss:  0.004028086084872484
Validation on real data: 
LOSS supervised-train 0.003687934739282355, valid 0.004029979929327965
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.004287940915673971
Batch  11  loss:  0.0030537296552211046
Batch  21  loss:  0.003839202458038926
Batch  31  loss:  0.0023911064490675926
Batch  41  loss:  0.003509021829813719
Batch  51  loss:  0.0015346085419878364
Batch  61  loss:  0.0040118759498000145
Batch  71  loss:  0.004316569771617651
Batch  81  loss:  0.00415049260482192
Batch  91  loss:  0.0065372479148209095
Validation on real data: 
LOSS supervised-train 0.003751899948110804, valid 0.004805050790309906
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.005972334183752537
Batch  11  loss:  0.004118871409446001
Batch  21  loss:  0.005416255909949541
Batch  31  loss:  0.0030700445640832186
Batch  41  loss:  0.004123218823224306
Batch  51  loss:  0.0035824780352413654
Batch  61  loss:  0.0038687591440975666
Batch  71  loss:  0.00412099901586771
Batch  81  loss:  0.0038888428825885057
Batch  91  loss:  0.00560191972181201
Validation on real data: 
LOSS supervised-train 0.003915258278138936, valid 0.0037950787227600813
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.004323873203247786
Batch  11  loss:  0.003217551624402404
Batch  21  loss:  0.004180031828582287
Batch  31  loss:  0.0033119081053882837
Batch  41  loss:  0.0033925932366400957
Batch  51  loss:  0.0021387627348303795
Batch  61  loss:  0.0029469369910657406
Batch  71  loss:  0.0034861837048083544
Batch  81  loss:  0.0035134186036884785
Batch  91  loss:  0.005726151168346405
Validation on real data: 
LOSS supervised-train 0.003870909062679857, valid 0.0035569306928664446
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.005058413837105036
Batch  11  loss:  0.003976747393608093
Batch  21  loss:  0.0036269822157919407
Batch  31  loss:  0.003869085805490613
Batch  41  loss:  0.0025988041888922453
Batch  51  loss:  0.002242498565465212
Batch  61  loss:  0.0035040760412812233
Batch  71  loss:  0.0023299832828342915
Batch  81  loss:  0.0029160981066524982
Batch  91  loss:  0.006700367666780949
Validation on real data: 
LOSS supervised-train 0.003879505596123636, valid 0.002962894970551133
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.004156151320785284
Batch  11  loss:  0.003343553515151143
Batch  21  loss:  0.004319664556533098
Batch  31  loss:  0.005700239911675453
Batch  41  loss:  0.0026678740978240967
Batch  51  loss:  0.00263812392950058
Batch  61  loss:  0.003041523974388838
Batch  71  loss:  0.0024361207615584135
Batch  81  loss:  0.003946549259126186
Batch  91  loss:  0.0050012278370559216
Validation on real data: 
LOSS supervised-train 0.003610985693521798, valid 0.002968046348541975
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0049932836554944515
Batch  11  loss:  0.003987504169344902
Batch  21  loss:  0.007397732697427273
Batch  31  loss:  0.0073981755413115025
Batch  41  loss:  0.002899997169151902
Batch  51  loss:  0.0026815375313162804
Batch  61  loss:  0.0027947258204221725
Batch  71  loss:  0.003179288934916258
Batch  81  loss:  0.004315807018429041
Batch  91  loss:  0.00462461868301034
Validation on real data: 
LOSS supervised-train 0.003782909600995481, valid 0.002900283318012953
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.006696315482258797
Batch  11  loss:  0.003947865217924118
Batch  21  loss:  0.007948224432766438
Batch  31  loss:  0.00527605228126049
Batch  41  loss:  0.002890191273763776
Batch  51  loss:  0.002600036794319749
Batch  61  loss:  0.0031609812285751104
Batch  71  loss:  0.0029927261639386415
Batch  81  loss:  0.003771074814721942
Batch  91  loss:  0.002491912106052041
Validation on real data: 
LOSS supervised-train 0.0037873967876657843, valid 0.002517544664442539
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.00773452315479517
Batch  11  loss:  0.0024241232313215733
Batch  21  loss:  0.00820856261998415
Batch  31  loss:  0.004147772211581469
Batch  41  loss:  0.003528045257553458
Batch  51  loss:  0.0041331094689667225
Batch  61  loss:  0.0022533568553626537
Batch  71  loss:  0.004783401265740395
Batch  81  loss:  0.0037006507627665997
Batch  91  loss:  0.0029939592350274324
Validation on real data: 
LOSS supervised-train 0.0038370226835832, valid 0.002560798078775406
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.005035205744206905
Batch  11  loss:  0.002833481878042221
Batch  21  loss:  0.008772820234298706
Batch  31  loss:  0.005440448876470327
Batch  41  loss:  0.004073349758982658
Batch  51  loss:  0.001600119168870151
Batch  61  loss:  0.0035469834692776203
Batch  71  loss:  0.0023086981382220984
Batch  81  loss:  0.0039977109991014
Batch  91  loss:  0.003378623863682151
Validation on real data: 
LOSS supervised-train 0.003850763050140813, valid 0.0023499950766563416
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.003927135374397039
Batch  11  loss:  0.0029626141767948866
Batch  21  loss:  0.013310113921761513
Batch  31  loss:  0.0037588237319141626
Batch  41  loss:  0.003935965709388256
Batch  51  loss:  0.0024893046356737614
Batch  61  loss:  0.005007550586014986
Batch  71  loss:  0.0038708888459950686
Batch  81  loss:  0.0037215016782283783
Batch  91  loss:  0.002591321710497141
Validation on real data: 
LOSS supervised-train 0.003819379999767989, valid 0.003191237337887287
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.003908245358616114
Batch  11  loss:  0.0027027667965739965
Batch  21  loss:  0.008766219951212406
Batch  31  loss:  0.004548570606857538
Batch  41  loss:  0.003655630862340331
Batch  51  loss:  0.0026396494358778
Batch  61  loss:  0.00440848246216774
Batch  71  loss:  0.003244622377678752
Batch  81  loss:  0.003441103268414736
Batch  91  loss:  0.0024071894586086273
Validation on real data: 
LOSS supervised-train 0.0035315805731806903, valid 0.0028902371414005756
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0042496793903410435
Batch  11  loss:  0.002915761200711131
Batch  21  loss:  0.004701751749962568
Batch  31  loss:  0.0027694690506905317
Batch  41  loss:  0.004506146069616079
Batch  51  loss:  0.0023162297438830137
Batch  61  loss:  0.005003871861845255
Batch  71  loss:  0.0032169946935027838
Batch  81  loss:  0.003049901919439435
Batch  91  loss:  0.0026246493216603994
Validation on real data: 
LOSS supervised-train 0.0033864799002185464, valid 0.004398448392748833
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0045928629115223885
Batch  11  loss:  0.0036404207348823547
Batch  21  loss:  0.006650274619460106
Batch  31  loss:  0.00393795408308506
Batch  41  loss:  0.0032609973568469286
Batch  51  loss:  0.0028108602855354548
Batch  61  loss:  0.00395546667277813
Batch  71  loss:  0.003967470023781061
Batch  81  loss:  0.004441241268068552
Batch  91  loss:  0.003994295373558998
Validation on real data: 
LOSS supervised-train 0.003305178276496008, valid 0.0038453631568700075
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.004582139663398266
Batch  11  loss:  0.002961635123938322
Batch  21  loss:  0.0061966958455741405
Batch  31  loss:  0.003140992484986782
Batch  41  loss:  0.004074674099683762
Batch  51  loss:  0.002803211798891425
Batch  61  loss:  0.0034316617529839277
Batch  71  loss:  0.0041280630975961685
Batch  81  loss:  0.005182575900107622
Batch  91  loss:  0.003292099107056856
Validation on real data: 
LOSS supervised-train 0.0032592796476092188, valid 0.004403895698487759
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.004274557810276747
Batch  11  loss:  0.003200170351192355
Batch  21  loss:  0.0033942677546292543
Batch  31  loss:  0.0028681724797934294
Batch  41  loss:  0.0035217772237956524
Batch  51  loss:  0.0024551921524107456
Batch  61  loss:  0.003392299637198448
Batch  71  loss:  0.0032154908403754234
Batch  81  loss:  0.004052288830280304
Batch  91  loss:  0.004054272547364235
Validation on real data: 
LOSS supervised-train 0.0033418328501284124, valid 0.003315430600196123
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.003942190203815699
Batch  11  loss:  0.004519167821854353
Batch  21  loss:  0.004322913940995932
Batch  31  loss:  0.002242552349343896
Batch  41  loss:  0.002923210384324193
Batch  51  loss:  0.002858938416466117
Batch  61  loss:  0.0028082740027457476
Batch  71  loss:  0.004071454983204603
Batch  81  loss:  0.0032594865188002586
Batch  91  loss:  0.006130187772214413
Validation on real data: 
LOSS supervised-train 0.0034448170743416997, valid 0.004901711829006672
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.004161292687058449
Batch  11  loss:  0.003642073832452297
Batch  21  loss:  0.004025793634355068
Batch  31  loss:  0.004087352193892002
Batch  41  loss:  0.0026854216121137142
Batch  51  loss:  0.0025254529900848866
Batch  61  loss:  0.002869072835892439
Batch  71  loss:  0.0035642373841255903
Batch  81  loss:  0.0031087058596313
Batch  91  loss:  0.0064598629251122475
Validation on real data: 
LOSS supervised-train 0.0034252229589037597, valid 0.003814471187070012
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0025106489192694426
Batch  11  loss:  0.004112879745662212
Batch  21  loss:  0.0038114809431135654
Batch  31  loss:  0.00351236155256629
Batch  41  loss:  0.0029482035897672176
Batch  51  loss:  0.0031943856738507748
Batch  61  loss:  0.002410836284980178
Batch  71  loss:  0.002668763278052211
Batch  81  loss:  0.0033446839079260826
Batch  91  loss:  0.006605172995477915
Validation on real data: 
LOSS supervised-train 0.0033105299668386577, valid 0.0031597469933331013
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.004221159033477306
Batch  11  loss:  0.0040607331320643425
Batch  21  loss:  0.0040638563223183155
Batch  31  loss:  0.004339457489550114
Batch  41  loss:  0.002869231626391411
Batch  51  loss:  0.001852040528319776
Batch  61  loss:  0.00264129345305264
Batch  71  loss:  0.0021117334254086018
Batch  81  loss:  0.0030053185764700174
Batch  91  loss:  0.0035713803954422474
Validation on real data: 
LOSS supervised-train 0.0031804805959109216, valid 0.0030298614874482155
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0048193917609751225
Batch  11  loss:  0.0035722535103559494
Batch  21  loss:  0.0065460787154734135
Batch  31  loss:  0.004987443797290325
Batch  41  loss:  0.003152629593387246
Batch  51  loss:  0.00232964800670743
Batch  61  loss:  0.0024614555295556784
Batch  71  loss:  0.0021777027286589146
Batch  81  loss:  0.0031261821277439594
Batch  91  loss:  0.002901220228523016
Validation on real data: 
LOSS supervised-train 0.003321084516355768, valid 0.0027849185280501842
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.003124237060546875
Batch  11  loss:  0.0022960107307881117
Batch  21  loss:  0.010252096690237522
Batch  31  loss:  0.0048435465432703495
Batch  41  loss:  0.005832891445606947
Batch  51  loss:  0.0032669943757355213
Batch  61  loss:  0.0028827758505940437
Batch  71  loss:  0.003344814060255885
Batch  81  loss:  0.002983551472425461
Batch  91  loss:  0.00295400433242321
Validation on real data: 
LOSS supervised-train 0.003690044446848333, valid 0.0024999992456287146
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.003542785532772541
Batch  11  loss:  0.0026767340023070574
Batch  21  loss:  0.011030802503228188
Batch  31  loss:  0.003147156210616231
Batch  41  loss:  0.0037833224050700665
Batch  51  loss:  0.0023625767789781094
Batch  61  loss:  0.004281438421458006
Batch  71  loss:  0.0024765469133853912
Batch  81  loss:  0.0028505551163107157
Batch  91  loss:  0.002957179443910718
Validation on real data: 
LOSS supervised-train 0.0036834219563752412, valid 0.0027351200114935637
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0033273433800786734
Batch  11  loss:  0.0028167697601020336
Batch  21  loss:  0.005911234300583601
Batch  31  loss:  0.0032536019571125507
Batch  41  loss:  0.0031953388825058937
Batch  51  loss:  0.002939168130978942
Batch  61  loss:  0.003864400554448366
Batch  71  loss:  0.0019759710412472486
Batch  81  loss:  0.0055693876929581165
Batch  91  loss:  0.0023259231820702553
Validation on real data: 
LOSS supervised-train 0.003416768538299948, valid 0.0034908829256892204
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.005332978907972574
Batch  11  loss:  0.002777307527139783
Batch  21  loss:  0.004707896150648594
Batch  31  loss:  0.0026536923833191395
Batch  41  loss:  0.002734922105446458
Batch  51  loss:  0.0021300422959029675
Batch  61  loss:  0.004479629453271627
Batch  71  loss:  0.0026517671067267656
Batch  81  loss:  0.0038175086956471205
Batch  91  loss:  0.004537676926702261
Validation on real data: 
LOSS supervised-train 0.00324617522303015, valid 0.004711131565272808
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0033185516949743032
Batch  11  loss:  0.002925426699221134
Batch  21  loss:  0.004398096818476915
Batch  31  loss:  0.0027885737363249063
Batch  41  loss:  0.003838299075141549
Batch  51  loss:  0.0026168853510171175
Batch  61  loss:  0.00430865865200758
Batch  71  loss:  0.0034883024636656046
Batch  81  loss:  0.0035904347896575928
Batch  91  loss:  0.003235585056245327
Validation on real data: 
LOSS supervised-train 0.0032315275515429676, valid 0.003601156175136566
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.003328553633764386
Batch  11  loss:  0.0031450858805328608
Batch  21  loss:  0.003933294210582972
Batch  31  loss:  0.0029447090346366167
Batch  41  loss:  0.002486241515725851
Batch  51  loss:  0.0028461292386054993
Batch  61  loss:  0.003522364189848304
Batch  71  loss:  0.003128087380900979
Batch  81  loss:  0.004167822189629078
Batch  91  loss:  0.005095741245895624
Validation on real data: 
LOSS supervised-train 0.0032146412367001177, valid 0.0031956445891410112
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  motorcycle ; Model ID: 481f7a57a12517e0fe1b9fad6c90c7bf
--------------------
Training baseline regression model:  2022-03-30 17:06:03.728117
Detector:  pointnet
Object:  motorcycle
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1614643
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.16094504296779633
Batch  11  loss:  0.1211681216955185
Batch  21  loss:  0.10117039084434509
Batch  31  loss:  0.08221503347158432
Batch  41  loss:  0.07462426275014877
Batch  51  loss:  0.06954999268054962
Batch  61  loss:  0.053126685321331024
Batch  71  loss:  0.04129276052117348
Batch  81  loss:  0.049658745527267456
Batch  91  loss:  0.03176172822713852
Validation on real data: 
LOSS supervised-train 0.07145340824499727, valid 0.021406235173344612
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.024013547226786613
Batch  11  loss:  0.024084007367491722
Batch  21  loss:  0.02635507844388485
Batch  31  loss:  0.017301378771662712
Batch  41  loss:  0.025309504941105843
Batch  51  loss:  0.025536881759762764
Batch  61  loss:  0.015632472932338715
Batch  71  loss:  0.013148561120033264
Batch  81  loss:  0.021498922258615494
Batch  91  loss:  0.014958539977669716
Validation on real data: 
LOSS supervised-train 0.019085383908823132, valid 0.01364284846931696
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.014566130004823208
Batch  11  loss:  0.01711345836520195
Batch  21  loss:  0.015339680947363377
Batch  31  loss:  0.014341496862471104
Batch  41  loss:  0.013835017569363117
Batch  51  loss:  0.01907866820693016
Batch  61  loss:  0.009618209674954414
Batch  71  loss:  0.008630897849798203
Batch  81  loss:  0.015176977962255478
Batch  91  loss:  0.009256796911358833
Validation on real data: 
LOSS supervised-train 0.011717841872014105, valid 0.010512461885809898
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.008226967416703701
Batch  11  loss:  0.012340955436229706
Batch  21  loss:  0.01179841160774231
Batch  31  loss:  0.00873087253421545
Batch  41  loss:  0.011148769408464432
Batch  51  loss:  0.012896036729216576
Batch  61  loss:  0.009806357324123383
Batch  71  loss:  0.0067541347816586494
Batch  81  loss:  0.012480318546295166
Batch  91  loss:  0.0064559015445411205
Validation on real data: 
LOSS supervised-train 0.009092244659550488, valid 0.009067309089004993
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.005423250142484903
Batch  11  loss:  0.011957076378166676
Batch  21  loss:  0.009010427631437778
Batch  31  loss:  0.006776674184948206
Batch  41  loss:  0.011557617224752903
Batch  51  loss:  0.014156496152281761
Batch  61  loss:  0.0075460514053702354
Batch  71  loss:  0.006204698700457811
Batch  81  loss:  0.012953339144587517
Batch  91  loss:  0.006545518524944782
Validation on real data: 
LOSS supervised-train 0.007976394724100827, valid 0.005919903516769409
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.004963076673448086
Batch  11  loss:  0.010892912745475769
Batch  21  loss:  0.010692359879612923
Batch  31  loss:  0.007128237280994654
Batch  41  loss:  0.008058356121182442
Batch  51  loss:  0.009355040267109871
Batch  61  loss:  0.007651050109416246
Batch  71  loss:  0.005536867305636406
Batch  81  loss:  0.008897485211491585
Batch  91  loss:  0.0055162059143185616
Validation on real data: 
LOSS supervised-train 0.006883849005680532, valid 0.004656927194446325
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.004906739108264446
Batch  11  loss:  0.011777873151004314
Batch  21  loss:  0.007463528774678707
Batch  31  loss:  0.007124211639165878
Batch  41  loss:  0.007699175737798214
Batch  51  loss:  0.007756443228572607
Batch  61  loss:  0.007301279343664646
Batch  71  loss:  0.005413001868873835
Batch  81  loss:  0.009767059236764908
Batch  91  loss:  0.007752302102744579
Validation on real data: 
LOSS supervised-train 0.0065752715896815065, valid 0.005569741129875183
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.006189162377268076
Batch  11  loss:  0.011808071285486221
Batch  21  loss:  0.006843349896371365
Batch  31  loss:  0.006018196698278189
Batch  41  loss:  0.006059302017092705
Batch  51  loss:  0.007103043608367443
Batch  61  loss:  0.004946868401020765
Batch  71  loss:  0.005526289343833923
Batch  81  loss:  0.006756282877177
Batch  91  loss:  0.005973095074295998
Validation on real data: 
LOSS supervised-train 0.006008808803744614, valid 0.007903157733380795
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.004900234751403332
Batch  11  loss:  0.011308101937174797
Batch  21  loss:  0.00505725247785449
Batch  31  loss:  0.004245798569172621
Batch  41  loss:  0.006280994974076748
Batch  51  loss:  0.00525257084518671
Batch  61  loss:  0.004713667090982199
Batch  71  loss:  0.005400142632424831
Batch  81  loss:  0.005444005597382784
Batch  91  loss:  0.004707506392151117
Validation on real data: 
LOSS supervised-train 0.005523638192098588, valid 0.005493191536515951
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.0035002727527171373
Batch  11  loss:  0.008378311060369015
Batch  21  loss:  0.005243699532002211
Batch  31  loss:  0.005265748593956232
Batch  41  loss:  0.00596849899739027
Batch  51  loss:  0.00664911512285471
Batch  61  loss:  0.005012365989387035
Batch  71  loss:  0.0056706760078668594
Batch  81  loss:  0.006667048670351505
Batch  91  loss:  0.004257673397660255
Validation on real data: 
LOSS supervised-train 0.005396875140722841, valid 0.006499109324067831
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.0041980016976594925
Batch  11  loss:  0.008996151387691498
Batch  21  loss:  0.007624363526701927
Batch  31  loss:  0.0051434701308608055
Batch  41  loss:  0.004283343441784382
Batch  51  loss:  0.005847384687513113
Batch  61  loss:  0.005570443347096443
Batch  71  loss:  0.005632058251649141
Batch  81  loss:  0.005027198698371649
Batch  91  loss:  0.0052551995031535625
Validation on real data: 
LOSS supervised-train 0.004987643752247095, valid 0.00427668821066618
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.0038587283343076706
Batch  11  loss:  0.00805264338850975
Batch  21  loss:  0.0063799298368394375
Batch  31  loss:  0.0047876532189548016
Batch  41  loss:  0.005298152565956116
Batch  51  loss:  0.0039230273105204105
Batch  61  loss:  0.0062254355289042
Batch  71  loss:  0.005556728690862656
Batch  81  loss:  0.00679568387567997
Batch  91  loss:  0.005593601148575544
Validation on real data: 
LOSS supervised-train 0.005029286746867001, valid 0.004082001280039549
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.003492033341899514
Batch  11  loss:  0.009042488411068916
Batch  21  loss:  0.005176651291549206
Batch  31  loss:  0.0042463550344109535
Batch  41  loss:  0.004000361077487469
Batch  51  loss:  0.0053911758586764336
Batch  61  loss:  0.005182108376175165
Batch  71  loss:  0.004104301333427429
Batch  81  loss:  0.0042542810551822186
Batch  91  loss:  0.003622959367930889
Validation on real data: 
LOSS supervised-train 0.004566176207736135, valid 0.004170575644820929
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00537860719487071
Batch  11  loss:  0.006156959570944309
Batch  21  loss:  0.0038782937917858362
Batch  31  loss:  0.0034607546404004097
Batch  41  loss:  0.004393800627440214
Batch  51  loss:  0.004969086963683367
Batch  61  loss:  0.005009782034903765
Batch  71  loss:  0.004496950190514326
Batch  81  loss:  0.005035259760916233
Batch  91  loss:  0.004101359751075506
Validation on real data: 
LOSS supervised-train 0.00446211508475244, valid 0.006434551440179348
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.004617160651832819
Batch  11  loss:  0.006626876536756754
Batch  21  loss:  0.004261518828570843
Batch  31  loss:  0.003842083038762212
Batch  41  loss:  0.003729680087417364
Batch  51  loss:  0.004125818610191345
Batch  61  loss:  0.0044796704314649105
Batch  71  loss:  0.003683861345052719
Batch  81  loss:  0.004342732019722462
Batch  91  loss:  0.0039321910589933395
Validation on real data: 
LOSS supervised-train 0.004237229349091649, valid 0.004101442638784647
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.002984170336276293
Batch  11  loss:  0.005508854053914547
Batch  21  loss:  0.005018341355025768
Batch  31  loss:  0.0034575380850583315
Batch  41  loss:  0.004455071873962879
Batch  51  loss:  0.005256017204374075
Batch  61  loss:  0.003965327981859446
Batch  71  loss:  0.004526019096374512
Batch  81  loss:  0.004820737987756729
Batch  91  loss:  0.00435732351616025
Validation on real data: 
LOSS supervised-train 0.004043657444417477, valid 0.004340831656008959
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.003270651213824749
Batch  11  loss:  0.006135144270956516
Batch  21  loss:  0.0044377935118973255
Batch  31  loss:  0.0027822607662528753
Batch  41  loss:  0.004106360021978617
Batch  51  loss:  0.005131888203322887
Batch  61  loss:  0.00444785738363862
Batch  71  loss:  0.004494781140238047
Batch  81  loss:  0.0044901189394295216
Batch  91  loss:  0.0038616040255874395
Validation on real data: 
LOSS supervised-train 0.004003604133613408, valid 0.0036361420061439276
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0044933101162314415
Batch  11  loss:  0.005245177075266838
Batch  21  loss:  0.004674588330090046
Batch  31  loss:  0.0027837075758725405
Batch  41  loss:  0.0035535013303160667
Batch  51  loss:  0.004309192765504122
Batch  61  loss:  0.003258492797613144
Batch  71  loss:  0.004306145943701267
Batch  81  loss:  0.003411014098674059
Batch  91  loss:  0.003605231177061796
Validation on real data: 
LOSS supervised-train 0.003859238214790821, valid 0.003112145233899355
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0030017325188964605
Batch  11  loss:  0.0060716662555933
Batch  21  loss:  0.002436690963804722
Batch  31  loss:  0.004524302203208208
Batch  41  loss:  0.004148110747337341
Batch  51  loss:  0.006643619854003191
Batch  61  loss:  0.00399851193651557
Batch  71  loss:  0.003657733788713813
Batch  81  loss:  0.0025096104945987463
Batch  91  loss:  0.0031550999265164137
Validation on real data: 
LOSS supervised-train 0.0037049624451901764, valid 0.004102512262761593
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.0027635805308818817
Batch  11  loss:  0.005176357924938202
Batch  21  loss:  0.002483722986653447
Batch  31  loss:  0.0028731892816722393
Batch  41  loss:  0.003764138789847493
Batch  51  loss:  0.0037186976987868547
Batch  61  loss:  0.003567463718354702
Batch  71  loss:  0.0029349070973694324
Batch  81  loss:  0.0037972889840602875
Batch  91  loss:  0.004018372390419245
Validation on real data: 
LOSS supervised-train 0.003664773779455572, valid 0.0039477054961025715
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.002363759558647871
Batch  11  loss:  0.005209522321820259
Batch  21  loss:  0.003915067296475172
Batch  31  loss:  0.0028557574842125177
Batch  41  loss:  0.0035647819750010967
Batch  51  loss:  0.004345524590462446
Batch  61  loss:  0.0033388822339475155
Batch  71  loss:  0.004745297133922577
Batch  81  loss:  0.0034709987230598927
Batch  91  loss:  0.003999244421720505
Validation on real data: 
LOSS supervised-train 0.003668856373988092, valid 0.0052376375533640385
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.002493364503607154
Batch  11  loss:  0.005721148569136858
Batch  21  loss:  0.004337966442108154
Batch  31  loss:  0.0038246738258749247
Batch  41  loss:  0.00322504760697484
Batch  51  loss:  0.0037287306040525436
Batch  61  loss:  0.0035976895596832037
Batch  71  loss:  0.0034916542936116457
Batch  81  loss:  0.00405766349285841
Batch  91  loss:  0.0035794370342046022
Validation on real data: 
LOSS supervised-train 0.0034523875545710327, valid 0.003376693930476904
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.003261895151808858
Batch  11  loss:  0.005311890505254269
Batch  21  loss:  0.003266196232289076
Batch  31  loss:  0.0026398675981909037
Batch  41  loss:  0.0036007086746394634
Batch  51  loss:  0.00404336815699935
Batch  61  loss:  0.002592140343040228
Batch  71  loss:  0.0034008168149739504
Batch  81  loss:  0.0039148395881056786
Batch  91  loss:  0.003711351891979575
Validation on real data: 
LOSS supervised-train 0.0034200620534829795, valid 0.0030388827435672283
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0027835064101964235
Batch  11  loss:  0.005112541373819113
Batch  21  loss:  0.00263404194265604
Batch  31  loss:  0.003184847766533494
Batch  41  loss:  0.0035492628812789917
Batch  51  loss:  0.003424132475629449
Batch  61  loss:  0.003645378164947033
Batch  71  loss:  0.0034553552977740765
Batch  81  loss:  0.003127153031527996
Batch  91  loss:  0.002683453494682908
Validation on real data: 
LOSS supervised-train 0.003202943856595084, valid 0.0029723276384174824
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0029083346016705036
Batch  11  loss:  0.004165689926594496
Batch  21  loss:  0.0033123381435871124
Batch  31  loss:  0.0026294218841940165
Batch  41  loss:  0.0030904137529432774
Batch  51  loss:  0.0028737829998135567
Batch  61  loss:  0.0028525551315397024
Batch  71  loss:  0.0034236933570355177
Batch  81  loss:  0.0037388557102531195
Batch  91  loss:  0.002916875761002302
Validation on real data: 
LOSS supervised-train 0.00307979773147963, valid 0.0031243956182152033
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.004417762625962496
Batch  11  loss:  0.005044387653470039
Batch  21  loss:  0.002554155420511961
Batch  31  loss:  0.0030469924677163363
Batch  41  loss:  0.0028889498207718134
Batch  51  loss:  0.0032212708611041307
Batch  61  loss:  0.0023697810247540474
Batch  71  loss:  0.0030053877271711826
Batch  81  loss:  0.0039015680085867643
Batch  91  loss:  0.0032165818847715855
Validation on real data: 
LOSS supervised-train 0.003070323037682101, valid 0.00315885990858078
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.002481506671756506
Batch  11  loss:  0.005657448433339596
Batch  21  loss:  0.0026194637175649405
Batch  31  loss:  0.002482455223798752
Batch  41  loss:  0.003179697087034583
Batch  51  loss:  0.004412365611642599
Batch  61  loss:  0.002967305015772581
Batch  71  loss:  0.0027686022222042084
Batch  81  loss:  0.00386217818595469
Batch  91  loss:  0.003171652788296342
Validation on real data: 
LOSS supervised-train 0.0030620149534661325, valid 0.004904878791421652
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.002984476275742054
Batch  11  loss:  0.004175173584371805
Batch  21  loss:  0.0034107130486518145
Batch  31  loss:  0.002434525638818741
Batch  41  loss:  0.0026482613757252693
Batch  51  loss:  0.0035365247167646885
Batch  61  loss:  0.00391591340303421
Batch  71  loss:  0.003967472352087498
Batch  81  loss:  0.0025519689079374075
Batch  91  loss:  0.0027443591970950365
Validation on real data: 
LOSS supervised-train 0.0029125323542393746, valid 0.0031121703796088696
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0023660152219235897
Batch  11  loss:  0.004592702258378267
Batch  21  loss:  0.0028933349531143904
Batch  31  loss:  0.003358816495165229
Batch  41  loss:  0.0030717519111931324
Batch  51  loss:  0.003199049737304449
Batch  61  loss:  0.002856035716831684
Batch  71  loss:  0.002705729566514492
Batch  81  loss:  0.0033247380051761866
Batch  91  loss:  0.0034389710053801537
Validation on real data: 
LOSS supervised-train 0.002847416547592729, valid 0.0026820015627890825
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.00260723615065217
Batch  11  loss:  0.005538338329643011
Batch  21  loss:  0.0029867233242839575
Batch  31  loss:  0.002362591912969947
Batch  41  loss:  0.0028843784239143133
Batch  51  loss:  0.0027947621420025826
Batch  61  loss:  0.003001381177455187
Batch  71  loss:  0.0029204371385276318
Batch  81  loss:  0.002614452736452222
Batch  91  loss:  0.002396911382675171
Validation on real data: 
LOSS supervised-train 0.0027636224403977394, valid 0.005646237172186375
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0032107317820191383
Batch  11  loss:  0.003662483999505639
Batch  21  loss:  0.003056911751627922
Batch  31  loss:  0.0023597790859639645
Batch  41  loss:  0.002931266324594617
Batch  51  loss:  0.003448179690167308
Batch  61  loss:  0.003179318504408002
Batch  71  loss:  0.002413651905953884
Batch  81  loss:  0.0023387924302369356
Batch  91  loss:  0.0023786097299307585
Validation on real data: 
LOSS supervised-train 0.002745790530461818, valid 0.0032061291858553886
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0023345942609012127
Batch  11  loss:  0.0036738954950124025
Batch  21  loss:  0.002198839094489813
Batch  31  loss:  0.0018826735904440284
Batch  41  loss:  0.0026485095731914043
Batch  51  loss:  0.00353822810575366
Batch  61  loss:  0.004468724597245455
Batch  71  loss:  0.002459272276610136
Batch  81  loss:  0.0025554709136486053
Batch  91  loss:  0.0021541251335293055
Validation on real data: 
LOSS supervised-train 0.0027304573624860494, valid 0.002895788988098502
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0024496957194060087
Batch  11  loss:  0.004409358836710453
Batch  21  loss:  0.002474317094311118
Batch  31  loss:  0.0020824437960982323
Batch  41  loss:  0.003276577452197671
Batch  51  loss:  0.002424876671284437
Batch  61  loss:  0.003020245349034667
Batch  71  loss:  0.002818913199007511
Batch  81  loss:  0.0031981896609067917
Batch  91  loss:  0.0033973203971982002
Validation on real data: 
LOSS supervised-train 0.002657456714659929, valid 0.0027590845711529255
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.003965084441006184
Batch  11  loss:  0.004042046144604683
Batch  21  loss:  0.002251366153359413
Batch  31  loss:  0.0024505225010216236
Batch  41  loss:  0.0033861733973026276
Batch  51  loss:  0.0030081693548709154
Batch  61  loss:  0.0025323256850242615
Batch  71  loss:  0.002853244077414274
Batch  81  loss:  0.002574098762124777
Batch  91  loss:  0.0028028765227645636
Validation on real data: 
LOSS supervised-train 0.0026421247492544354, valid 0.0021810727193951607
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.003341999603435397
Batch  11  loss:  0.00321781262755394
Batch  21  loss:  0.0028618169017136097
Batch  31  loss:  0.0025041878689080477
Batch  41  loss:  0.0026421721559017897
Batch  51  loss:  0.0025533740408718586
Batch  61  loss:  0.0028042281046509743
Batch  71  loss:  0.0024916434194892645
Batch  81  loss:  0.0026842779479920864
Batch  91  loss:  0.002662613522261381
Validation on real data: 
LOSS supervised-train 0.0026955772843211888, valid 0.00209612469188869
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0030154387932270765
Batch  11  loss:  0.0034178977366536856
Batch  21  loss:  0.002639881568029523
Batch  31  loss:  0.002293935976922512
Batch  41  loss:  0.002553889062255621
Batch  51  loss:  0.0028169869910925627
Batch  61  loss:  0.002252222504466772
Batch  71  loss:  0.002430815016850829
Batch  81  loss:  0.002303519519045949
Batch  91  loss:  0.0022519142366945744
Validation on real data: 
LOSS supervised-train 0.002540787891484797, valid 0.002747650956735015
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.0016883470816537738
Batch  11  loss:  0.0034183852840214968
Batch  21  loss:  0.00270551024004817
Batch  31  loss:  0.002119640354067087
Batch  41  loss:  0.0027026182506233454
Batch  51  loss:  0.0028775029350072145
Batch  61  loss:  0.003596703987568617
Batch  71  loss:  0.0021161839831620455
Batch  81  loss:  0.002708597807213664
Batch  91  loss:  0.002590646967291832
Validation on real data: 
LOSS supervised-train 0.002568042700877413, valid 0.0026848523411899805
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0022902244236320257
Batch  11  loss:  0.0036864876747131348
Batch  21  loss:  0.0018465903121978045
Batch  31  loss:  0.0022895648144185543
Batch  41  loss:  0.002954282332211733
Batch  51  loss:  0.003627493279054761
Batch  61  loss:  0.0019910114351660013
Batch  71  loss:  0.002263031667098403
Batch  81  loss:  0.002770356833934784
Batch  91  loss:  0.002494260435923934
Validation on real data: 
LOSS supervised-train 0.0025363378948532046, valid 0.0026360610499978065
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0026159840635955334
Batch  11  loss:  0.003324588993564248
Batch  21  loss:  0.0019562370143830776
Batch  31  loss:  0.0018401684937998652
Batch  41  loss:  0.002627647714689374
Batch  51  loss:  0.002687278436496854
Batch  61  loss:  0.0025741353165358305
Batch  71  loss:  0.002104640007019043
Batch  81  loss:  0.002259043511003256
Batch  91  loss:  0.0023033353500068188
Validation on real data: 
LOSS supervised-train 0.002465168071212247, valid 0.0017745817312970757
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0024404525756835938
Batch  11  loss:  0.002545588184148073
Batch  21  loss:  0.002730740001425147
Batch  31  loss:  0.0019751987420022488
Batch  41  loss:  0.0025197553914040327
Batch  51  loss:  0.0031051028054207563
Batch  61  loss:  0.0024855067022144794
Batch  71  loss:  0.0021312041208148003
Batch  81  loss:  0.0020739766769111156
Batch  91  loss:  0.002311168471351266
Validation on real data: 
LOSS supervised-train 0.0024590433144476265, valid 0.0023087586741894484
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0018952905666083097
Batch  11  loss:  0.0024012522771954536
Batch  21  loss:  0.002133026486262679
Batch  31  loss:  0.0018681708024814725
Batch  41  loss:  0.0030615534633398056
Batch  51  loss:  0.0027444122824817896
Batch  61  loss:  0.0020630559884011745
Batch  71  loss:  0.0020454174373298883
Batch  81  loss:  0.003351027611643076
Batch  91  loss:  0.0021592932753264904
Validation on real data: 
LOSS supervised-train 0.002414910738589242, valid 0.002308088820427656
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.003157296683639288
Batch  11  loss:  0.0031115661840885878
Batch  21  loss:  0.0024530403316020966
Batch  31  loss:  0.0018236307660117745
Batch  41  loss:  0.0025810778606683016
Batch  51  loss:  0.001943441340699792
Batch  61  loss:  0.0026322396006435156
Batch  71  loss:  0.0026856386102735996
Batch  81  loss:  0.002441209740936756
Batch  91  loss:  0.002095166826620698
Validation on real data: 
LOSS supervised-train 0.002329103535739705, valid 0.0024508140049874783
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.002489405917003751
Batch  11  loss:  0.002619759878143668
Batch  21  loss:  0.0022635089699178934
Batch  31  loss:  0.0021886355243623257
Batch  41  loss:  0.0023741002660244703
Batch  51  loss:  0.0024953477550297976
Batch  61  loss:  0.002251254627481103
Batch  71  loss:  0.0018801242113113403
Batch  81  loss:  0.0017696535214781761
Batch  91  loss:  0.0016514431918039918
Validation on real data: 
LOSS supervised-train 0.002312158283311874, valid 0.002141154371201992
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.001959934365004301
Batch  11  loss:  0.0031132264994084835
Batch  21  loss:  0.002318050479516387
Batch  31  loss:  0.002338101388886571
Batch  41  loss:  0.0024241178762167692
Batch  51  loss:  0.0025036355946213007
Batch  61  loss:  0.0024816812947392464
Batch  71  loss:  0.002504147822037339
Batch  81  loss:  0.002411747118458152
Batch  91  loss:  0.0018603319767862558
Validation on real data: 
LOSS supervised-train 0.002324993226211518, valid 0.0021925880573689938
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.002119926968589425
Batch  11  loss:  0.0029565314762294292
Batch  21  loss:  0.002181624062359333
Batch  31  loss:  0.002044394612312317
Batch  41  loss:  0.0026828318368643522
Batch  51  loss:  0.0024830710608512163
Batch  61  loss:  0.0032310406677424908
Batch  71  loss:  0.002179865026846528
Batch  81  loss:  0.0017644031904637814
Batch  91  loss:  0.00211184355430305
Validation on real data: 
LOSS supervised-train 0.002211699878098443, valid 0.0018365391297265887
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0025254033971577883
Batch  11  loss:  0.003438467625528574
Batch  21  loss:  0.0020636343397200108
Batch  31  loss:  0.002170185325667262
Batch  41  loss:  0.002378360368311405
Batch  51  loss:  0.0031560873612761497
Batch  61  loss:  0.0030367441941052675
Batch  71  loss:  0.0019129292340949178
Batch  81  loss:  0.0021529889199882746
Batch  91  loss:  0.002806643256917596
Validation on real data: 
LOSS supervised-train 0.0022669362579472363, valid 0.001991773722693324
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0019418150186538696
Batch  11  loss:  0.0021734735928475857
Batch  21  loss:  0.002260289154946804
Batch  31  loss:  0.002467856742441654
Batch  41  loss:  0.002641799161210656
Batch  51  loss:  0.0030959153082221746
Batch  61  loss:  0.002992746653035283
Batch  71  loss:  0.0019667930901050568
Batch  81  loss:  0.0028206370770931244
Batch  91  loss:  0.002322946675121784
Validation on real data: 
LOSS supervised-train 0.002197447211947292, valid 0.0020170228090137243
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0023781899362802505
Batch  11  loss:  0.0023051162716001272
Batch  21  loss:  0.0015947020146995783
Batch  31  loss:  0.0017943436978384852
Batch  41  loss:  0.003817542688921094
Batch  51  loss:  0.002085248241201043
Batch  61  loss:  0.0023781168274581432
Batch  71  loss:  0.0021482452284544706
Batch  81  loss:  0.002369075547903776
Batch  91  loss:  0.002382294274866581
Validation on real data: 
LOSS supervised-train 0.0022249727440066635, valid 0.0022353671956807375
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0017798037733882666
Batch  11  loss:  0.0023062345571815968
Batch  21  loss:  0.00224705901928246
Batch  31  loss:  0.001978101907297969
Batch  41  loss:  0.002952692098915577
Batch  51  loss:  0.002597473096102476
Batch  61  loss:  0.002273790305480361
Batch  71  loss:  0.0017041554674506187
Batch  81  loss:  0.0017874417826533318
Batch  91  loss:  0.0022664987482130527
Validation on real data: 
LOSS supervised-train 0.0021955299482215194, valid 0.002362726256251335
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0022227445151656866
Batch  11  loss:  0.0026398366317152977
Batch  21  loss:  0.002179007278755307
Batch  31  loss:  0.0019203355768695474
Batch  41  loss:  0.002891038078814745
Batch  51  loss:  0.0023905225098133087
Batch  61  loss:  0.002800353104248643
Batch  71  loss:  0.0020671526435762644
Batch  81  loss:  0.0023497752845287323
Batch  91  loss:  0.0023627581540495157
Validation on real data: 
LOSS supervised-train 0.0022349292307626456, valid 0.002008642302826047
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.002267654286697507
Batch  11  loss:  0.0025653832126408815
Batch  21  loss:  0.00211718725040555
Batch  31  loss:  0.001547162770293653
Batch  41  loss:  0.0029321229085326195
Batch  51  loss:  0.002631911775097251
Batch  61  loss:  0.002598363207653165
Batch  71  loss:  0.0017200678121298552
Batch  81  loss:  0.0019378801807761192
Batch  91  loss:  0.002411979017779231
Validation on real data: 
LOSS supervised-train 0.0021909196802880615, valid 0.0023352219723165035
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.002547116717323661
Batch  11  loss:  0.0025668456219136715
Batch  21  loss:  0.0023827534168958664
Batch  31  loss:  0.0019995335023850203
Batch  41  loss:  0.0028427871875464916
Batch  51  loss:  0.003093535779044032
Batch  61  loss:  0.002752722939476371
Batch  71  loss:  0.001967661315575242
Batch  81  loss:  0.0025054598227143288
Batch  91  loss:  0.0021678826306015253
Validation on real data: 
LOSS supervised-train 0.0022585694037843495, valid 0.0020418651401996613
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0025768594350665808
Batch  11  loss:  0.002255362691357732
Batch  21  loss:  0.001884525641798973
Batch  31  loss:  0.001952611142769456
Batch  41  loss:  0.0025899421889334917
Batch  51  loss:  0.002156075555831194
Batch  61  loss:  0.0017374117160215974
Batch  71  loss:  0.0020771147683262825
Batch  81  loss:  0.002074231393635273
Batch  91  loss:  0.0024796132929623127
Validation on real data: 
LOSS supervised-train 0.0022101791272871197, valid 0.0017891994211822748
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0021712484303861856
Batch  11  loss:  0.0024359887465834618
Batch  21  loss:  0.0017521202098578215
Batch  31  loss:  0.0016755037941038609
Batch  41  loss:  0.0029943713452667
Batch  51  loss:  0.0021289235446602106
Batch  61  loss:  0.001955074490979314
Batch  71  loss:  0.001881300937384367
Batch  81  loss:  0.0021170435938984156
Batch  91  loss:  0.0028434335254132748
Validation on real data: 
LOSS supervised-train 0.0022279068699572234, valid 0.002517620800063014
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0022455553989857435
Batch  11  loss:  0.0020979815162718296
Batch  21  loss:  0.001975167077034712
Batch  31  loss:  0.001920481096021831
Batch  41  loss:  0.0030233238358050585
Batch  51  loss:  0.0031148886773735285
Batch  61  loss:  0.0016681974520906806
Batch  71  loss:  0.001706077135168016
Batch  81  loss:  0.0017576869577169418
Batch  91  loss:  0.0020951563492417336
Validation on real data: 
LOSS supervised-train 0.0022109935735352337, valid 0.002252342412248254
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0028282657731324434
Batch  11  loss:  0.002636262681335211
Batch  21  loss:  0.0020404579117894173
Batch  31  loss:  0.003025149228051305
Batch  41  loss:  0.002641713246703148
Batch  51  loss:  0.003169274190440774
Batch  61  loss:  0.00222148303873837
Batch  71  loss:  0.002097507705911994
Batch  81  loss:  0.0015243780799210072
Batch  91  loss:  0.001731507247313857
Validation on real data: 
LOSS supervised-train 0.002263265906367451, valid 0.0016539943171665072
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0030406229197978973
Batch  11  loss:  0.0021790817845612764
Batch  21  loss:  0.002267103176563978
Batch  31  loss:  0.002321260515600443
Batch  41  loss:  0.002410164102911949
Batch  51  loss:  0.0029987464658915997
Batch  61  loss:  0.003515389747917652
Batch  71  loss:  0.002310446696355939
Batch  81  loss:  0.0020715435966849327
Batch  91  loss:  0.0027268053963780403
Validation on real data: 
LOSS supervised-train 0.0022453835722990334, valid 0.0015384546713903546
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0027001118287444115
Batch  11  loss:  0.0019629213493317366
Batch  21  loss:  0.0018302869284525514
Batch  31  loss:  0.003471647622063756
Batch  41  loss:  0.002650726120918989
Batch  51  loss:  0.002033933997154236
Batch  61  loss:  0.002141156466677785
Batch  71  loss:  0.002285031136125326
Batch  81  loss:  0.0025578599888831377
Batch  91  loss:  0.002509796293452382
Validation on real data: 
LOSS supervised-train 0.0021898618375416844, valid 0.0017704539932310581
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.002375885844230652
Batch  11  loss:  0.0031191748566925526
Batch  21  loss:  0.0016423030756413937
Batch  31  loss:  0.0019292188808321953
Batch  41  loss:  0.0025006216019392014
Batch  51  loss:  0.004568225704133511
Batch  61  loss:  0.0032682791352272034
Batch  71  loss:  0.0020449734292924404
Batch  81  loss:  0.0027463987935334444
Batch  91  loss:  0.0027658226899802685
Validation on real data: 
LOSS supervised-train 0.0023111926740966736, valid 0.0011967141181230545
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0031520086340606213
Batch  11  loss:  0.002948247827589512
Batch  21  loss:  0.001715707709081471
Batch  31  loss:  0.001829628017731011
Batch  41  loss:  0.0026609087362885475
Batch  51  loss:  0.0027926708571612835
Batch  61  loss:  0.0023877520579844713
Batch  71  loss:  0.0015329996822401881
Batch  81  loss:  0.0023763200733810663
Batch  91  loss:  0.002202615374699235
Validation on real data: 
LOSS supervised-train 0.0022387238359078765, valid 0.001998928375542164
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0035753219854086637
Batch  11  loss:  0.003915096167474985
Batch  21  loss:  0.0020342629868537188
Batch  31  loss:  0.0023787659592926502
Batch  41  loss:  0.0028679801616817713
Batch  51  loss:  0.0031666038557887077
Batch  61  loss:  0.002312256256118417
Batch  71  loss:  0.00221117096953094
Batch  81  loss:  0.0024346443824470043
Batch  91  loss:  0.0032832252327352762
Validation on real data: 
LOSS supervised-train 0.002334849387407303, valid 0.0021054137032479048
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0021963652689009905
Batch  11  loss:  0.0031830051448196173
Batch  21  loss:  0.0018072902457788587
Batch  31  loss:  0.002189950319007039
Batch  41  loss:  0.002657710574567318
Batch  51  loss:  0.0020422139205038548
Batch  61  loss:  0.0034207843709737062
Batch  71  loss:  0.0019881101325154305
Batch  81  loss:  0.0023073828779160976
Batch  91  loss:  0.0017685970524325967
Validation on real data: 
LOSS supervised-train 0.002205692244460806, valid 0.0020101869013160467
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00254626153036952
Batch  11  loss:  0.0039072055369615555
Batch  21  loss:  0.0015347370645031333
Batch  31  loss:  0.0032102512195706367
Batch  41  loss:  0.002718535019084811
Batch  51  loss:  0.0019321540603414178
Batch  61  loss:  0.004061059094965458
Batch  71  loss:  0.0015096778515726328
Batch  81  loss:  0.0024563351180404425
Batch  91  loss:  0.002564296592026949
Validation on real data: 
LOSS supervised-train 0.0022491399582941084, valid 0.001733656506985426
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0025976463221013546
Batch  11  loss:  0.0049437591806054115
Batch  21  loss:  0.0016640075482428074
Batch  31  loss:  0.002854021731764078
Batch  41  loss:  0.002328258939087391
Batch  51  loss:  0.0023418536875396967
Batch  61  loss:  0.002573250560089946
Batch  71  loss:  0.002292453311383724
Batch  81  loss:  0.002698213094845414
Batch  91  loss:  0.002460215939208865
Validation on real data: 
LOSS supervised-train 0.0022139896382577717, valid 0.002116631017997861
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.001878436654806137
Batch  11  loss:  0.006948526483029127
Batch  21  loss:  0.0014188438653945923
Batch  31  loss:  0.001829306478612125
Batch  41  loss:  0.002859096508473158
Batch  51  loss:  0.0025254434440284967
Batch  61  loss:  0.0036965433973819017
Batch  71  loss:  0.0019383187172934413
Batch  81  loss:  0.0027459191624075174
Batch  91  loss:  0.0016713255317881703
Validation on real data: 
LOSS supervised-train 0.002247199466219172, valid 0.0019344197353348136
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0018494640244171023
Batch  11  loss:  0.004751871805638075
Batch  21  loss:  0.0023938212543725967
Batch  31  loss:  0.0022583974059671164
Batch  41  loss:  0.003038235940039158
Batch  51  loss:  0.0020126071758568287
Batch  61  loss:  0.004377285949885845
Batch  71  loss:  0.0017319023609161377
Batch  81  loss:  0.0025949650444090366
Batch  91  loss:  0.001783087383955717
Validation on real data: 
LOSS supervised-train 0.002244857850018889, valid 0.0033613156992942095
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.00223625754006207
Batch  11  loss:  0.0046256836503744125
Batch  21  loss:  0.002374826930463314
Batch  31  loss:  0.0026603767182677984
Batch  41  loss:  0.0036292679142206907
Batch  51  loss:  0.0025261165574193
Batch  61  loss:  0.0037533931899815798
Batch  71  loss:  0.0037306006997823715
Batch  81  loss:  0.002835843712091446
Batch  91  loss:  0.00349754118360579
Validation on real data: 
LOSS supervised-train 0.002332240503747016, valid 0.0019931436982005835
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.002339133759960532
Batch  11  loss:  0.006291156634688377
Batch  21  loss:  0.0019114044262096286
Batch  31  loss:  0.0021634253207594156
Batch  41  loss:  0.0037968200631439686
Batch  51  loss:  0.0023928931914269924
Batch  61  loss:  0.0038071810267865658
Batch  71  loss:  0.003414550330489874
Batch  81  loss:  0.0025197165086865425
Batch  91  loss:  0.0028340935241431
Validation on real data: 
LOSS supervised-train 0.002446790406247601, valid 0.001627547200769186
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.001990614691749215
Batch  11  loss:  0.0057853818871080875
Batch  21  loss:  0.002177600283175707
Batch  31  loss:  0.0021032034419476986
Batch  41  loss:  0.0026796464808285236
Batch  51  loss:  0.002218651818111539
Batch  61  loss:  0.0032371000852435827
Batch  71  loss:  0.003192279255017638
Batch  81  loss:  0.0017380504868924618
Batch  91  loss:  0.0033466711174696684
Validation on real data: 
LOSS supervised-train 0.0022839829081203787, valid 0.002326586050912738
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.0013976573245599866
Batch  11  loss:  0.0042687635868787766
Batch  21  loss:  0.0026523740962147713
Batch  31  loss:  0.0019181343959644437
Batch  41  loss:  0.0033644759096205235
Batch  51  loss:  0.00276904902420938
Batch  61  loss:  0.0032700239680707455
Batch  71  loss:  0.0038830661214888096
Batch  81  loss:  0.003356117755174637
Batch  91  loss:  0.0022762827575206757
Validation on real data: 
LOSS supervised-train 0.002383652941789478, valid 0.0023051712196320295
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.002495155204087496
Batch  11  loss:  0.004508568439632654
Batch  21  loss:  0.0021868629846721888
Batch  31  loss:  0.002255880506709218
Batch  41  loss:  0.00237925979308784
Batch  51  loss:  0.0020484118722379208
Batch  61  loss:  0.0030002216808497906
Batch  71  loss:  0.003312406362965703
Batch  81  loss:  0.002120583551004529
Batch  91  loss:  0.0020768865942955017
Validation on real data: 
LOSS supervised-train 0.002330457573989406, valid 0.0029597401153296232
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.002711695386096835
Batch  11  loss:  0.004195314832031727
Batch  21  loss:  0.0021567039657384157
Batch  31  loss:  0.001534742652438581
Batch  41  loss:  0.002204272197559476
Batch  51  loss:  0.002161362674087286
Batch  61  loss:  0.004088531713932753
Batch  71  loss:  0.0033324998803436756
Batch  81  loss:  0.002502989722415805
Batch  91  loss:  0.0023946643341332674
Validation on real data: 
LOSS supervised-train 0.0023422055086120964, valid 0.0024182009510695934
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0032260529696941376
Batch  11  loss:  0.0054506477899849415
Batch  21  loss:  0.0021954989060759544
Batch  31  loss:  0.0015040745493024588
Batch  41  loss:  0.0020321679767221212
Batch  51  loss:  0.002910116221755743
Batch  61  loss:  0.0020849702414125204
Batch  71  loss:  0.003244296880438924
Batch  81  loss:  0.0020157084800302982
Batch  91  loss:  0.0016848525265231729
Validation on real data: 
LOSS supervised-train 0.002260230890242383, valid 0.0021161509212106466
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0023354922886937857
Batch  11  loss:  0.003577816067263484
Batch  21  loss:  0.0021755911875516176
Batch  31  loss:  0.0017178580164909363
Batch  41  loss:  0.002027764916419983
Batch  51  loss:  0.0028645966667681932
Batch  61  loss:  0.0024149357341229916
Batch  71  loss:  0.002799874171614647
Batch  81  loss:  0.0025093532167375088
Batch  91  loss:  0.0015954984119161963
Validation on real data: 
LOSS supervised-train 0.0022163777565583588, valid 0.002274958183988929
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0025417369324713945
Batch  11  loss:  0.003321445779874921
Batch  21  loss:  0.0025418433360755444
Batch  31  loss:  0.001884020515717566
Batch  41  loss:  0.0022252905182540417
Batch  51  loss:  0.001772850751876831
Batch  61  loss:  0.0026972058694809675
Batch  71  loss:  0.0027077815029770136
Batch  81  loss:  0.0025161292869597673
Batch  91  loss:  0.001687595504336059
Validation on real data: 
LOSS supervised-train 0.002136084915837273, valid 0.0017239873996004462
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.003052137792110443
Batch  11  loss:  0.003911295440047979
Batch  21  loss:  0.001887482008896768
Batch  31  loss:  0.002515119966119528
Batch  41  loss:  0.0021238268818706274
Batch  51  loss:  0.0023627649061381817
Batch  61  loss:  0.0025217889342457056
Batch  71  loss:  0.0020913383923470974
Batch  81  loss:  0.002331720432266593
Batch  91  loss:  0.0012654472375288606
Validation on real data: 
LOSS supervised-train 0.0020900916436221452, valid 0.0022080375347286463
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0019707155879586935
Batch  11  loss:  0.0025598800275474787
Batch  21  loss:  0.0014546316815540195
Batch  31  loss:  0.0013348226202651858
Batch  41  loss:  0.0018026065081357956
Batch  51  loss:  0.0019729582127183676
Batch  61  loss:  0.00191215961240232
Batch  71  loss:  0.002082815393805504
Batch  81  loss:  0.0023098543751984835
Batch  91  loss:  0.001388290198519826
Validation on real data: 
LOSS supervised-train 0.002023946089902893, valid 0.0016345578478649259
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0029562776908278465
Batch  11  loss:  0.0016444569919258356
Batch  21  loss:  0.0020462251268327236
Batch  31  loss:  0.0018548532389104366
Batch  41  loss:  0.0018696957267820835
Batch  51  loss:  0.0030031567439436913
Batch  61  loss:  0.002125462284311652
Batch  71  loss:  0.0020165625028312206
Batch  81  loss:  0.003767419373616576
Batch  91  loss:  0.0019117242190986872
Validation on real data: 
LOSS supervised-train 0.002070648460648954, valid 0.001643113442696631
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.002133044647052884
Batch  11  loss:  0.0017585803288966417
Batch  21  loss:  0.0021713972091674805
Batch  31  loss:  0.0018119955202564597
Batch  41  loss:  0.0026292859110981226
Batch  51  loss:  0.0024704341776669025
Batch  61  loss:  0.0020925982389599085
Batch  71  loss:  0.0016169250011444092
Batch  81  loss:  0.0020362036302685738
Batch  91  loss:  0.0018132621189579368
Validation on real data: 
LOSS supervised-train 0.0020436660759150984, valid 0.0016366969794034958
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.002284833462908864
Batch  11  loss:  0.0020968944299966097
Batch  21  loss:  0.0014708020025864244
Batch  31  loss:  0.0016430704854428768
Batch  41  loss:  0.002085098996758461
Batch  51  loss:  0.0016661452827975154
Batch  61  loss:  0.0022412098478525877
Batch  71  loss:  0.001480091828852892
Batch  81  loss:  0.0022323147859424353
Batch  91  loss:  0.0013785811606794596
Validation on real data: 
LOSS supervised-train 0.0020107481023296713, valid 0.0024767969734966755
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.001447513815946877
Batch  11  loss:  0.0017472991021350026
Batch  21  loss:  0.0014594444073736668
Batch  31  loss:  0.0019643213599920273
Batch  41  loss:  0.002296264050528407
Batch  51  loss:  0.0025189053267240524
Batch  61  loss:  0.0021201064810156822
Batch  71  loss:  0.0021524771582335234
Batch  81  loss:  0.002288101939484477
Batch  91  loss:  0.0017338823527097702
Validation on real data: 
LOSS supervised-train 0.001992688082391396, valid 0.0013538534985855222
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0020258184522390366
Batch  11  loss:  0.0018032141961157322
Batch  21  loss:  0.0015043442836031318
Batch  31  loss:  0.0015844074077904224
Batch  41  loss:  0.0027320003136992455
Batch  51  loss:  0.0022797035053372383
Batch  61  loss:  0.0018970660166814923
Batch  71  loss:  0.001590678351931274
Batch  81  loss:  0.0019010433461517096
Batch  91  loss:  0.0019259541295468807
Validation on real data: 
LOSS supervised-train 0.0019224113156087698, valid 0.0012998614693060517
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0021311845630407333
Batch  11  loss:  0.002149880165234208
Batch  21  loss:  0.0015325738349929452
Batch  31  loss:  0.001496999873779714
Batch  41  loss:  0.0023486237041652203
Batch  51  loss:  0.0018688517156988382
Batch  61  loss:  0.0023641400039196014
Batch  71  loss:  0.0017964075086638331
Batch  81  loss:  0.002605061512440443
Batch  91  loss:  0.0018338876543566585
Validation on real data: 
LOSS supervised-train 0.0020350563165266065, valid 0.001096357824280858
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0017247775103896856
Batch  11  loss:  0.0016599888913333416
Batch  21  loss:  0.0017862474778667092
Batch  31  loss:  0.001654073246754706
Batch  41  loss:  0.0025524746160954237
Batch  51  loss:  0.00195452687330544
Batch  61  loss:  0.0018862405559048057
Batch  71  loss:  0.001782915205694735
Batch  81  loss:  0.0019948119297623634
Batch  91  loss:  0.0028284701984375715
Validation on real data: 
LOSS supervised-train 0.001935215701814741, valid 0.0012060597073286772
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0018786629661917686
Batch  11  loss:  0.001700033899396658
Batch  21  loss:  0.0014897820074111223
Batch  31  loss:  0.0016732602380216122
Batch  41  loss:  0.002463262528181076
Batch  51  loss:  0.002083836356177926
Batch  61  loss:  0.002388406079262495
Batch  71  loss:  0.0022711206693202257
Batch  81  loss:  0.0013286744942888618
Batch  91  loss:  0.002080060075968504
Validation on real data: 
LOSS supervised-train 0.001935482150875032, valid 0.0014410557923838496
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0031509536784142256
Batch  11  loss:  0.0015965269412845373
Batch  21  loss:  0.0017906897701323032
Batch  31  loss:  0.0019602731335908175
Batch  41  loss:  0.002337023848667741
Batch  51  loss:  0.0026945124845951796
Batch  61  loss:  0.0022015755530446768
Batch  71  loss:  0.0024036297108978033
Batch  81  loss:  0.0026141086127609015
Batch  91  loss:  0.0018701015505939722
Validation on real data: 
LOSS supervised-train 0.0019735347770620136, valid 0.0012430037604644895
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.002066614804789424
Batch  11  loss:  0.00204442604444921
Batch  21  loss:  0.002717621624469757
Batch  31  loss:  0.002166472375392914
Batch  41  loss:  0.0023957740049809217
Batch  51  loss:  0.0029385515954345465
Batch  61  loss:  0.0019975611940026283
Batch  71  loss:  0.0017986911116167903
Batch  81  loss:  0.0018277311464771628
Batch  91  loss:  0.002846578834578395
Validation on real data: 
LOSS supervised-train 0.0019653429090976714, valid 0.0018028325866907835
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0017514529172331095
Batch  11  loss:  0.001644217874854803
Batch  21  loss:  0.0021158617455512285
Batch  31  loss:  0.0017677434952929616
Batch  41  loss:  0.0020695901475846767
Batch  51  loss:  0.002060635481029749
Batch  61  loss:  0.002846537157893181
Batch  71  loss:  0.0018378134118393064
Batch  81  loss:  0.0031196586787700653
Batch  91  loss:  0.002132019028067589
Validation on real data: 
LOSS supervised-train 0.0018579224066343158, valid 0.0017212730599567294
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.003666864475235343
Batch  11  loss:  0.002956298878416419
Batch  21  loss:  0.0017248325748369098
Batch  31  loss:  0.001923036528751254
Batch  41  loss:  0.0019903057254850864
Batch  51  loss:  0.0021806282456964254
Batch  61  loss:  0.0020975584629923105
Batch  71  loss:  0.0013479250483214855
Batch  81  loss:  0.002128488617017865
Batch  91  loss:  0.0016706702299416065
Validation on real data: 
LOSS supervised-train 0.001870384142966941, valid 0.0018732263706624508
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.003158787963911891
Batch  11  loss:  0.002522084629163146
Batch  21  loss:  0.0016967087285593152
Batch  31  loss:  0.0017555414233356714
Batch  41  loss:  0.0022043364588171244
Batch  51  loss:  0.0017695225542411208
Batch  61  loss:  0.0025531204883009195
Batch  71  loss:  0.002193721244111657
Batch  81  loss:  0.0029634651727974415
Batch  91  loss:  0.0019557997584342957
Validation on real data: 
LOSS supervised-train 0.0017878306948114187, valid 0.0016704981680959463
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.002057162579149008
Batch  11  loss:  0.003912358079105616
Batch  21  loss:  0.0013596124481409788
Batch  31  loss:  0.002014860510826111
Batch  41  loss:  0.0021583768539130688
Batch  51  loss:  0.0024918692652136087
Batch  61  loss:  0.0030188129749149084
Batch  71  loss:  0.0017660766607150435
Batch  81  loss:  0.003200684441253543
Batch  91  loss:  0.002776287728920579
Validation on real data: 
LOSS supervised-train 0.002022643358213827, valid 0.00167793536093086
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.003432902041822672
Batch  11  loss:  0.004889797419309616
Batch  21  loss:  0.0019940268248319626
Batch  31  loss:  0.0023689933586865664
Batch  41  loss:  0.0026509505696594715
Batch  51  loss:  0.0022751442156732082
Batch  61  loss:  0.0021508601494133472
Batch  71  loss:  0.002426929771900177
Batch  81  loss:  0.0020657547283917665
Batch  91  loss:  0.001610900741070509
Validation on real data: 
LOSS supervised-train 0.0019335337565280497, valid 0.0016344285104423761
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0018159153405576944
Batch  11  loss:  0.0034922552295029163
Batch  21  loss:  0.001226632040925324
Batch  31  loss:  0.0018902828451246023
Batch  41  loss:  0.0020141282584518194
Batch  51  loss:  0.0017906127031892538
Batch  61  loss:  0.001996025675907731
Batch  71  loss:  0.002166758757084608
Batch  81  loss:  0.0017953424248844385
Batch  91  loss:  0.0018746060086414218
Validation on real data: 
LOSS supervised-train 0.001761857494711876, valid 0.0016339959111064672
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0014034530613571405
Batch  11  loss:  0.0032334851566702127
Batch  21  loss:  0.0017409726278856397
Batch  31  loss:  0.0016649685567244887
Batch  41  loss:  0.0023913278710097075
Batch  51  loss:  0.002090617548674345
Batch  61  loss:  0.003726110327988863
Batch  71  loss:  0.0019123251549899578
Batch  81  loss:  0.0021023377776145935
Batch  91  loss:  0.0015250591095536947
Validation on real data: 
LOSS supervised-train 0.0019226825260557235, valid 0.0014426048146560788
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.002582734217867255
Batch  11  loss:  0.004360237158834934
Batch  21  loss:  0.0016393137630075216
Batch  31  loss:  0.0013981788652017713
Batch  41  loss:  0.0021748258732259274
Batch  51  loss:  0.0016021215124055743
Batch  61  loss:  0.0020080835092812777
Batch  71  loss:  0.003409589407965541
Batch  81  loss:  0.0018702176166698337
Batch  91  loss:  0.0015581868356093764
Validation on real data: 
LOSS supervised-train 0.0018463173671625554, valid 0.0014785394305363297
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0014348089462146163
Batch  11  loss:  0.003023191587999463
Batch  21  loss:  0.0018809878965839744
Batch  31  loss:  0.0017114272341132164
Batch  41  loss:  0.002262511057779193
Batch  51  loss:  0.0015187978278845549
Batch  61  loss:  0.001606777892448008
Batch  71  loss:  0.002316976198926568
Batch  81  loss:  0.0016802330501377583
Batch  91  loss:  0.001384307979606092
Validation on real data: 
LOSS supervised-train 0.0018308874079957604, valid 0.0014182485174387693
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0011821043444797397
Batch  11  loss:  0.003058923175558448
Batch  21  loss:  0.0020068867597728968
Batch  31  loss:  0.0017047168221324682
Batch  41  loss:  0.0017251097597181797
Batch  51  loss:  0.00243526604026556
Batch  61  loss:  0.0019777731504291296
Batch  71  loss:  0.002987793879583478
Batch  81  loss:  0.0021172387059777975
Batch  91  loss:  0.0016471169656142592
Validation on real data: 
LOSS supervised-train 0.0019297567685134709, valid 0.001774016534909606
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.00206962414085865
Batch  11  loss:  0.0029877307824790478
Batch  21  loss:  0.0017541073029860854
Batch  31  loss:  0.00155104068107903
Batch  41  loss:  0.00188316754065454
Batch  51  loss:  0.0022746550384908915
Batch  61  loss:  0.0024244245141744614
Batch  71  loss:  0.0023007001727819443
Batch  81  loss:  0.001820686156861484
Batch  91  loss:  0.0013982702512294054
Validation on real data: 
LOSS supervised-train 0.0018464010587194934, valid 0.0011664245976135135
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0021863935980945826
Batch  11  loss:  0.003021195763722062
Batch  21  loss:  0.002022105734795332
Batch  31  loss:  0.0017733353888615966
Batch  41  loss:  0.001912927022203803
Batch  51  loss:  0.0016351673984900117
Batch  61  loss:  0.002178662456572056
Batch  71  loss:  0.002296516904607415
Batch  81  loss:  0.002253020415082574
Batch  91  loss:  0.0013190280878916383
Validation on real data: 
LOSS supervised-train 0.0018993553100153805, valid 0.0012804442085325718
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0018093070248141885
Batch  11  loss:  0.0023701670579612255
Batch  21  loss:  0.0015717080095782876
Batch  31  loss:  0.0015248722629621625
Batch  41  loss:  0.0015559252351522446
Batch  51  loss:  0.003251510439440608
Batch  61  loss:  0.0017205147305503488
Batch  71  loss:  0.0011843537213280797
Batch  81  loss:  0.002023825654760003
Batch  91  loss:  0.0019073779694736004
Validation on real data: 
LOSS supervised-train 0.0018343667284352705, valid 0.001219266327098012
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  mug ; Model ID: f3a7f8198cc50c225f5e789acd4d1122
--------------------
Training baseline regression model:  2022-03-30 17:28:52.519750
Detector:  pointnet
Object:  mug
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1612330
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.30587244033813477
Batch  11  loss:  0.10940452665090561
Batch  21  loss:  0.07408023625612259
Batch  31  loss:  0.04512827843427658
Batch  41  loss:  0.04061494767665863
Batch  51  loss:  0.040589962154626846
Batch  61  loss:  0.04334653913974762
Batch  71  loss:  0.028868526220321655
Batch  81  loss:  0.042112842202186584
Batch  91  loss:  0.027118634432554245
Validation on real data: 
LOSS supervised-train 0.061241614669561385, valid 0.026902083307504654
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.021214419975876808
Batch  11  loss:  0.020941268652677536
Batch  21  loss:  0.017855431884527206
Batch  31  loss:  0.01792212389409542
Batch  41  loss:  0.01914224401116371
Batch  51  loss:  0.012331890873610973
Batch  61  loss:  0.015522995963692665
Batch  71  loss:  0.017029639333486557
Batch  81  loss:  0.012684266082942486
Batch  91  loss:  0.019299227744340897
Validation on real data: 
LOSS supervised-train 0.015989589486271143, valid 0.011195101775228977
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.011027876287698746
Batch  11  loss:  0.008858103305101395
Batch  21  loss:  0.011176417581737041
Batch  31  loss:  0.012200061231851578
Batch  41  loss:  0.01170914713293314
Batch  51  loss:  0.006820530630648136
Batch  61  loss:  0.011490581557154655
Batch  71  loss:  0.009935830719769001
Batch  81  loss:  0.009931698441505432
Batch  91  loss:  0.010767673142254353
Validation on real data: 
LOSS supervised-train 0.010235909493640066, valid 0.010724800638854504
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.00799840409308672
Batch  11  loss:  0.007214919198304415
Batch  21  loss:  0.00642546359449625
Batch  31  loss:  0.008938701823353767
Batch  41  loss:  0.010591576807200909
Batch  51  loss:  0.006144849117845297
Batch  61  loss:  0.008147499524056911
Batch  71  loss:  0.008011044934391975
Batch  81  loss:  0.007571720983833075
Batch  91  loss:  0.008348978124558926
Validation on real data: 
LOSS supervised-train 0.008254166822880507, valid 0.007683084346354008
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.006955164950340986
Batch  11  loss:  0.006718132179230452
Batch  21  loss:  0.00726819084957242
Batch  31  loss:  0.008669075556099415
Batch  41  loss:  0.007998248562216759
Batch  51  loss:  0.005032935179769993
Batch  61  loss:  0.008215675130486488
Batch  71  loss:  0.006496004294604063
Batch  81  loss:  0.005845196545124054
Batch  91  loss:  0.006640933454036713
Validation on real data: 
LOSS supervised-train 0.007192317969165743, valid 0.006786523852497339
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.006191291846334934
Batch  11  loss:  0.004414011258631945
Batch  21  loss:  0.005502418149262667
Batch  31  loss:  0.008366611786186695
Batch  41  loss:  0.0052959732711315155
Batch  51  loss:  0.005192628595978022
Batch  61  loss:  0.008000044152140617
Batch  71  loss:  0.00521100265905261
Batch  81  loss:  0.00532948225736618
Batch  91  loss:  0.00518728606402874
Validation on real data: 
LOSS supervised-train 0.006343530339654535, valid 0.005801839288324118
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.005991070065647364
Batch  11  loss:  0.0056176865473389626
Batch  21  loss:  0.005035298876464367
Batch  31  loss:  0.006997120101004839
Batch  41  loss:  0.006005661562085152
Batch  51  loss:  0.004195961635559797
Batch  61  loss:  0.007438624743372202
Batch  71  loss:  0.004834786057472229
Batch  81  loss:  0.005057430826127529
Batch  91  loss:  0.005641718860715628
Validation on real data: 
LOSS supervised-train 0.0057652589585632085, valid 0.003459846368059516
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.006058214232325554
Batch  11  loss:  0.003956731408834457
Batch  21  loss:  0.004728592932224274
Batch  31  loss:  0.005789489485323429
Batch  41  loss:  0.004378424491733313
Batch  51  loss:  0.003460766514763236
Batch  61  loss:  0.0063189235515892506
Batch  71  loss:  0.004812782164663076
Batch  81  loss:  0.0048909918405115604
Batch  91  loss:  0.004565063398331404
Validation on real data: 
LOSS supervised-train 0.00523222389863804, valid 0.0035512885078787804
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.0053063854575157166
Batch  11  loss:  0.0038663982413709164
Batch  21  loss:  0.004319615662097931
Batch  31  loss:  0.006103539373725653
Batch  41  loss:  0.0042973970994353294
Batch  51  loss:  0.002953033894300461
Batch  61  loss:  0.005483557935804129
Batch  71  loss:  0.004558594431728125
Batch  81  loss:  0.005530945491045713
Batch  91  loss:  0.003454058663919568
Validation on real data: 
LOSS supervised-train 0.004905245113186538, valid 0.00288885785266757
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.005350194405764341
Batch  11  loss:  0.0037566295359283686
Batch  21  loss:  0.004586582072079182
Batch  31  loss:  0.005167336203157902
Batch  41  loss:  0.004824190400540829
Batch  51  loss:  0.0034129272680729628
Batch  61  loss:  0.0052515738643705845
Batch  71  loss:  0.003650518599897623
Batch  81  loss:  0.005079176276922226
Batch  91  loss:  0.003277723677456379
Validation on real data: 
LOSS supervised-train 0.004488835653755813, valid 0.0018402507994323969
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.004889157135039568
Batch  11  loss:  0.0032532315235584974
Batch  21  loss:  0.0034446774516254663
Batch  31  loss:  0.004764339420944452
Batch  41  loss:  0.003884395817294717
Batch  51  loss:  0.0031980990897864103
Batch  61  loss:  0.005029906518757343
Batch  71  loss:  0.0032681385055184364
Batch  81  loss:  0.0046299356035888195
Batch  91  loss:  0.0033416813239455223
Validation on real data: 
LOSS supervised-train 0.004360550483688712, valid 0.002585445297881961
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.00412346888333559
Batch  11  loss:  0.0035213609226047993
Batch  21  loss:  0.003001148346811533
Batch  31  loss:  0.0050106896087527275
Batch  41  loss:  0.004365678410977125
Batch  51  loss:  0.003017386654391885
Batch  61  loss:  0.005882591940462589
Batch  71  loss:  0.0034182032104581594
Batch  81  loss:  0.004351313225924969
Batch  91  loss:  0.002717898227274418
Validation on real data: 
LOSS supervised-train 0.004231395157985389, valid 0.0018604376818984747
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.0034124283120036125
Batch  11  loss:  0.0028709513135254383
Batch  21  loss:  0.0044740596786141396
Batch  31  loss:  0.004838933236896992
Batch  41  loss:  0.0037421195302158594
Batch  51  loss:  0.0025670616887509823
Batch  61  loss:  0.0044018663465976715
Batch  71  loss:  0.0032862508669495583
Batch  81  loss:  0.00431076530367136
Batch  91  loss:  0.0044731139205396175
Validation on real data: 
LOSS supervised-train 0.00397545394487679, valid 0.0023297357838600874
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.0037529198452830315
Batch  11  loss:  0.0033387010917067528
Batch  21  loss:  0.002561661647632718
Batch  31  loss:  0.005081172101199627
Batch  41  loss:  0.003770367708057165
Batch  51  loss:  0.003156821010634303
Batch  61  loss:  0.0029708652291446924
Batch  71  loss:  0.002925327280536294
Batch  81  loss:  0.002817472442984581
Batch  91  loss:  0.0031673116609454155
Validation on real data: 
LOSS supervised-train 0.003841443946585059, valid 0.0024299556389451027
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.003665611147880554
Batch  11  loss:  0.0022831035312265158
Batch  21  loss:  0.0034366357140243053
Batch  31  loss:  0.005462602246552706
Batch  41  loss:  0.0038327393122017384
Batch  51  loss:  0.0023257071152329445
Batch  61  loss:  0.004801085218787193
Batch  71  loss:  0.0020612413063645363
Batch  81  loss:  0.003596964292228222
Batch  91  loss:  0.002806760836392641
Validation on real data: 
LOSS supervised-train 0.0036601831344887613, valid 0.002455974230542779
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.003715035505592823
Batch  11  loss:  0.0026774578727781773
Batch  21  loss:  0.0033153940457850695
Batch  31  loss:  0.004240676295012236
Batch  41  loss:  0.0033909003250300884
Batch  51  loss:  0.0023792076390236616
Batch  61  loss:  0.004070924129337072
Batch  71  loss:  0.002508814912289381
Batch  81  loss:  0.0038646000903099775
Batch  91  loss:  0.0029176657553762197
Validation on real data: 
LOSS supervised-train 0.0035975573235191407, valid 0.0019284602021798491
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.0025948595721274614
Batch  11  loss:  0.0026179866399616003
Batch  21  loss:  0.003322963137179613
Batch  31  loss:  0.003511646296828985
Batch  41  loss:  0.0032624006271362305
Batch  51  loss:  0.002370735164731741
Batch  61  loss:  0.005136355757713318
Batch  71  loss:  0.00324239837937057
Batch  81  loss:  0.0035114181227982044
Batch  91  loss:  0.0029821288771927357
Validation on real data: 
LOSS supervised-train 0.0034770168317481876, valid 0.001471928902901709
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.004110198002308607
Batch  11  loss:  0.003665484022349119
Batch  21  loss:  0.0033890530467033386
Batch  31  loss:  0.003703139955177903
Batch  41  loss:  0.00361441308632493
Batch  51  loss:  0.0018110856181010604
Batch  61  loss:  0.0040809414349496365
Batch  71  loss:  0.0023331227712333202
Batch  81  loss:  0.0043103997595608234
Batch  91  loss:  0.0026132208295166492
Validation on real data: 
LOSS supervised-train 0.0034123308351263404, valid 0.002426453400403261
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.0031724050641059875
Batch  11  loss:  0.0025045108050107956
Batch  21  loss:  0.0022964547388255596
Batch  31  loss:  0.003770188894122839
Batch  41  loss:  0.0028586930129677057
Batch  51  loss:  0.002540052868425846
Batch  61  loss:  0.0037194478791207075
Batch  71  loss:  0.0038235976826399565
Batch  81  loss:  0.0025467269588261843
Batch  91  loss:  0.0026395688764750957
Validation on real data: 
LOSS supervised-train 0.0033037400129251183, valid 0.002003717003390193
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.003667752258479595
Batch  11  loss:  0.002305980771780014
Batch  21  loss:  0.0020007702987641096
Batch  31  loss:  0.004440185613930225
Batch  41  loss:  0.002730398206040263
Batch  51  loss:  0.002235805382952094
Batch  61  loss:  0.004002040717750788
Batch  71  loss:  0.0030179517343640327
Batch  81  loss:  0.0030264698434621096
Batch  91  loss:  0.0026273929979652166
Validation on real data: 
LOSS supervised-train 0.003220851180376485, valid 0.0018176304874941707
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0038581830449402332
Batch  11  loss:  0.002610067604109645
Batch  21  loss:  0.0026306926738470793
Batch  31  loss:  0.0037334959488362074
Batch  41  loss:  0.0027011355850845575
Batch  51  loss:  0.0021226361859589815
Batch  61  loss:  0.002967097330838442
Batch  71  loss:  0.0024158647283911705
Batch  81  loss:  0.0021918676793575287
Batch  91  loss:  0.0023413952440023422
Validation on real data: 
LOSS supervised-train 0.0030821132194250823, valid 0.0015463106101378798
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.003289599437266588
Batch  11  loss:  0.0025414428673684597
Batch  21  loss:  0.0020671843085438013
Batch  31  loss:  0.0034442704636603594
Batch  41  loss:  0.0025229123421013355
Batch  51  loss:  0.002188280690461397
Batch  61  loss:  0.002866047667339444
Batch  71  loss:  0.0029702335596084595
Batch  81  loss:  0.0029070472810417414
Batch  91  loss:  0.002082139952108264
Validation on real data: 
LOSS supervised-train 0.003028052120935172, valid 0.0019199588568881154
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.002872155047953129
Batch  11  loss:  0.0026710121892392635
Batch  21  loss:  0.002067321678623557
Batch  31  loss:  0.003844916122034192
Batch  41  loss:  0.0030772138852626085
Batch  51  loss:  0.0022610605228692293
Batch  61  loss:  0.0021920849103480577
Batch  71  loss:  0.002194064436480403
Batch  81  loss:  0.0038337844889611006
Batch  91  loss:  0.0019513790030032396
Validation on real data: 
LOSS supervised-train 0.003012887437362224, valid 0.0017498154193162918
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.003351791063323617
Batch  11  loss:  0.0021595112048089504
Batch  21  loss:  0.0019578500650823116
Batch  31  loss:  0.003819165751338005
Batch  41  loss:  0.0025598686188459396
Batch  51  loss:  0.001726976945064962
Batch  61  loss:  0.003546158317476511
Batch  71  loss:  0.0024116900749504566
Batch  81  loss:  0.0025197542272508144
Batch  91  loss:  0.001979897264391184
Validation on real data: 
LOSS supervised-train 0.0028574493096675726, valid 0.0014357444597408175
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.0036403315607458353
Batch  11  loss:  0.002164237666875124
Batch  21  loss:  0.002655599731951952
Batch  31  loss:  0.0024888734333217144
Batch  41  loss:  0.003123430535197258
Batch  51  loss:  0.0022101919166743755
Batch  61  loss:  0.002863087924197316
Batch  71  loss:  0.0024105426855385303
Batch  81  loss:  0.0022773658856749535
Batch  91  loss:  0.0023243501782417297
Validation on real data: 
LOSS supervised-train 0.002919822013936937, valid 0.00165271433070302
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.003228332381695509
Batch  11  loss:  0.0028476363513618708
Batch  21  loss:  0.002115420764312148
Batch  31  loss:  0.00300403474830091
Batch  41  loss:  0.002835524035617709
Batch  51  loss:  0.0024222072679549456
Batch  61  loss:  0.0025575803592801094
Batch  71  loss:  0.0025435229763388634
Batch  81  loss:  0.0028427422512322664
Batch  91  loss:  0.0021740791853517294
Validation on real data: 
LOSS supervised-train 0.0028190173592884094, valid 0.0016474954318255186
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.003129776567220688
Batch  11  loss:  0.0024990278761833906
Batch  21  loss:  0.0030073451343923807
Batch  31  loss:  0.002900313353165984
Batch  41  loss:  0.0025312777142971754
Batch  51  loss:  0.0019845401402562857
Batch  61  loss:  0.0026810213457792997
Batch  71  loss:  0.002646412467584014
Batch  81  loss:  0.0026823675725609064
Batch  91  loss:  0.002730867825448513
Validation on real data: 
LOSS supervised-train 0.0028177027392666787, valid 0.0012769487220793962
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.0026990887708961964
Batch  11  loss:  0.0021870259661227465
Batch  21  loss:  0.0023245110642164946
Batch  31  loss:  0.003022416727617383
Batch  41  loss:  0.0027804654091596603
Batch  51  loss:  0.0019868623930960894
Batch  61  loss:  0.0023565636947751045
Batch  71  loss:  0.0019009554525837302
Batch  81  loss:  0.002177928574383259
Batch  91  loss:  0.0024622331839054823
Validation on real data: 
LOSS supervised-train 0.0026220954849850387, valid 0.0022061222698539495
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.0027263264637440443
Batch  11  loss:  0.00221923366189003
Batch  21  loss:  0.0024056141264736652
Batch  31  loss:  0.003346505342051387
Batch  41  loss:  0.002686928492039442
Batch  51  loss:  0.0021126666106283665
Batch  61  loss:  0.0026894807815551758
Batch  71  loss:  0.002659660531207919
Batch  81  loss:  0.0027598922606557608
Batch  91  loss:  0.0015799602260813117
Validation on real data: 
LOSS supervised-train 0.002696111361728981, valid 0.001629089703783393
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.0029105194844305515
Batch  11  loss:  0.002349474001675844
Batch  21  loss:  0.00206302129663527
Batch  31  loss:  0.002571152290329337
Batch  41  loss:  0.0029097087681293488
Batch  51  loss:  0.0018458465347066522
Batch  61  loss:  0.00446345517411828
Batch  71  loss:  0.002142125042155385
Batch  81  loss:  0.0023963761050254107
Batch  91  loss:  0.002315575024113059
Validation on real data: 
LOSS supervised-train 0.0025727913645096122, valid 0.0009550198446959257
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0029065257403999567
Batch  11  loss:  0.0021381976548582315
Batch  21  loss:  0.0017483929404988885
Batch  31  loss:  0.0029587382450699806
Batch  41  loss:  0.002142079174518585
Batch  51  loss:  0.0017207240452989936
Batch  61  loss:  0.0029598891269415617
Batch  71  loss:  0.002404911210760474
Batch  81  loss:  0.0027298300992697477
Batch  91  loss:  0.0025260017719119787
Validation on real data: 
LOSS supervised-train 0.0025160709139890967, valid 0.0014353443402796984
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.0032965189311653376
Batch  11  loss:  0.0021828203462064266
Batch  21  loss:  0.001784785999916494
Batch  31  loss:  0.0028728812467306852
Batch  41  loss:  0.0026819792110472918
Batch  51  loss:  0.00186050811316818
Batch  61  loss:  0.0029941685497760773
Batch  71  loss:  0.0021116212010383606
Batch  81  loss:  0.0033042680006474257
Batch  91  loss:  0.0017500626854598522
Validation on real data: 
LOSS supervised-train 0.0025523453240748496, valid 0.001495914999395609
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0029423516243696213
Batch  11  loss:  0.0020374474115669727
Batch  21  loss:  0.0023975179065018892
Batch  31  loss:  0.0030533478129655123
Batch  41  loss:  0.0023644992616027594
Batch  51  loss:  0.0019210929749533534
Batch  61  loss:  0.003376434091478586
Batch  71  loss:  0.0017096903175115585
Batch  81  loss:  0.002160783624276519
Batch  91  loss:  0.0013688355684280396
Validation on real data: 
LOSS supervised-train 0.002460942978505045, valid 0.0013644249411299825
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.002799248555675149
Batch  11  loss:  0.0029159586410969496
Batch  21  loss:  0.002508982317522168
Batch  31  loss:  0.0024941819719970226
Batch  41  loss:  0.0019511174177750945
Batch  51  loss:  0.0020983784925192595
Batch  61  loss:  0.002333251293748617
Batch  71  loss:  0.001582390978001058
Batch  81  loss:  0.00208867690525949
Batch  91  loss:  0.002339687431231141
Validation on real data: 
LOSS supervised-train 0.0024272481282241643, valid 0.0010055257007479668
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0024033458903431892
Batch  11  loss:  0.0021811602637171745
Batch  21  loss:  0.0019585920963436365
Batch  31  loss:  0.0027379069942981005
Batch  41  loss:  0.0026552421040832996
Batch  51  loss:  0.0017900809179991484
Batch  61  loss:  0.0029187940526753664
Batch  71  loss:  0.0027419989928603172
Batch  81  loss:  0.001802602200768888
Batch  91  loss:  0.002027207752689719
Validation on real data: 
LOSS supervised-train 0.0024495760712306947, valid 0.0012262093368917704
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.0031814835965633392
Batch  11  loss:  0.002180321142077446
Batch  21  loss:  0.0016487499233335257
Batch  31  loss:  0.002597321756184101
Batch  41  loss:  0.0020606881007552147
Batch  51  loss:  0.001880742609500885
Batch  61  loss:  0.003392390441149473
Batch  71  loss:  0.002426475752145052
Batch  81  loss:  0.0022095078602433205
Batch  91  loss:  0.0017273795092478395
Validation on real data: 
LOSS supervised-train 0.0024573094921652226, valid 0.0015046518528833985
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.002766125835478306
Batch  11  loss:  0.0028100397903472185
Batch  21  loss:  0.00252488418482244
Batch  31  loss:  0.002679808298125863
Batch  41  loss:  0.0021102793980389833
Batch  51  loss:  0.001943653798662126
Batch  61  loss:  0.0025291561614722013
Batch  71  loss:  0.0018054390093311667
Batch  81  loss:  0.002078792080283165
Batch  91  loss:  0.0020476030185818672
Validation on real data: 
LOSS supervised-train 0.0023853122373111548, valid 0.0014933913480490446
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0027871327474713326
Batch  11  loss:  0.0022893480490893126
Batch  21  loss:  0.0017195739783346653
Batch  31  loss:  0.0025298302061855793
Batch  41  loss:  0.002025151625275612
Batch  51  loss:  0.0014773864531889558
Batch  61  loss:  0.002482529729604721
Batch  71  loss:  0.0019612477626651525
Batch  81  loss:  0.0021697923075407743
Batch  91  loss:  0.002113144379109144
Validation on real data: 
LOSS supervised-train 0.0023407542519271376, valid 0.0014392680022865534
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0024316920898854733
Batch  11  loss:  0.0022085546515882015
Batch  21  loss:  0.0018717190250754356
Batch  31  loss:  0.002849035197868943
Batch  41  loss:  0.0024371864274144173
Batch  51  loss:  0.002066998044028878
Batch  61  loss:  0.002213283209130168
Batch  71  loss:  0.002179129282012582
Batch  81  loss:  0.0016411634860560298
Batch  91  loss:  0.0018474470125511289
Validation on real data: 
LOSS supervised-train 0.0023310671432409434, valid 0.0012664112728089094
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0021613070275634527
Batch  11  loss:  0.0018783234991133213
Batch  21  loss:  0.001823035185225308
Batch  31  loss:  0.0022136420011520386
Batch  41  loss:  0.0021011957433074713
Batch  51  loss:  0.0016578325303271413
Batch  61  loss:  0.0029184941668063402
Batch  71  loss:  0.0016724461456760764
Batch  81  loss:  0.001981747103855014
Batch  91  loss:  0.001642846968024969
Validation on real data: 
LOSS supervised-train 0.0022122132661752405, valid 0.0013699831906706095
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0018229987472295761
Batch  11  loss:  0.0019552200101315975
Batch  21  loss:  0.0016446602530777454
Batch  31  loss:  0.0032070172019302845
Batch  41  loss:  0.0017965063452720642
Batch  51  loss:  0.0019093304872512817
Batch  61  loss:  0.002295164158567786
Batch  71  loss:  0.0019795692060142756
Batch  81  loss:  0.002546727191656828
Batch  91  loss:  0.001975755440071225
Validation on real data: 
LOSS supervised-train 0.0022478030214551836, valid 0.0015686661936342716
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00244846660643816
Batch  11  loss:  0.0021362334955483675
Batch  21  loss:  0.0015501483576372266
Batch  31  loss:  0.002480381168425083
Batch  41  loss:  0.0014382522786036134
Batch  51  loss:  0.0019176536006852984
Batch  61  loss:  0.0025452012196183205
Batch  71  loss:  0.0020169455092400312
Batch  81  loss:  0.001801868318580091
Batch  91  loss:  0.0019046006491407752
Validation on real data: 
LOSS supervised-train 0.002231091500725597, valid 0.0012867695186287165
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.002325237262994051
Batch  11  loss:  0.0016877364832907915
Batch  21  loss:  0.002240630565211177
Batch  31  loss:  0.002305706962943077
Batch  41  loss:  0.002205578377470374
Batch  51  loss:  0.0022938500624150038
Batch  61  loss:  0.0024213981814682484
Batch  71  loss:  0.0018820909317582846
Batch  81  loss:  0.002190330997109413
Batch  91  loss:  0.0023891881573945284
Validation on real data: 
LOSS supervised-train 0.002178778874222189, valid 0.0012003656011074781
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.002833544509485364
Batch  11  loss:  0.0024268552660942078
Batch  21  loss:  0.0013935896568000317
Batch  31  loss:  0.002542435424402356
Batch  41  loss:  0.0023117533419281244
Batch  51  loss:  0.0014229690423235297
Batch  61  loss:  0.0021382556296885014
Batch  71  loss:  0.0017813392914831638
Batch  81  loss:  0.0017685904167592525
Batch  91  loss:  0.001501964288763702
Validation on real data: 
LOSS supervised-train 0.002148640180239454, valid 0.0013984183315187693
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0030279895290732384
Batch  11  loss:  0.0018769648158922791
Batch  21  loss:  0.0017716869479045272
Batch  31  loss:  0.002678962890058756
Batch  41  loss:  0.0016139056533575058
Batch  51  loss:  0.001587146194651723
Batch  61  loss:  0.002523815492168069
Batch  71  loss:  0.002369728870689869
Batch  81  loss:  0.0016559305367991328
Batch  91  loss:  0.002192480256780982
Validation on real data: 
LOSS supervised-train 0.0021533437515608967, valid 0.0010971789015457034
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0025546990800648928
Batch  11  loss:  0.0019260966219007969
Batch  21  loss:  0.0016648488817736506
Batch  31  loss:  0.0025082826614379883
Batch  41  loss:  0.0018728108843788505
Batch  51  loss:  0.0017688170773908496
Batch  61  loss:  0.0024357426445931196
Batch  71  loss:  0.0024225753732025623
Batch  81  loss:  0.002267845906317234
Batch  91  loss:  0.0015560808824375272
Validation on real data: 
LOSS supervised-train 0.0021360699937213214, valid 0.0012803067220374942
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0021870972122997046
Batch  11  loss:  0.0017956625670194626
Batch  21  loss:  0.0018433412769809365
Batch  31  loss:  0.002282717963680625
Batch  41  loss:  0.0022128382697701454
Batch  51  loss:  0.001594318076968193
Batch  61  loss:  0.0022384768817573786
Batch  71  loss:  0.001890043611638248
Batch  81  loss:  0.0017004390247166157
Batch  91  loss:  0.001404321170412004
Validation on real data: 
LOSS supervised-train 0.0020879059238359332, valid 0.001272908179089427
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.0018830427434295416
Batch  11  loss:  0.0013684364967048168
Batch  21  loss:  0.0019340553553774953
Batch  31  loss:  0.0023575308732688427
Batch  41  loss:  0.0017492431215941906
Batch  51  loss:  0.002259795553982258
Batch  61  loss:  0.0037989141419529915
Batch  71  loss:  0.0012485175393521786
Batch  81  loss:  0.0015100481687113643
Batch  91  loss:  0.0013525709509849548
Validation on real data: 
LOSS supervised-train 0.0019987844012212008, valid 0.0008372223237529397
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0021529807709157467
Batch  11  loss:  0.002371600130572915
Batch  21  loss:  0.002081996761262417
Batch  31  loss:  0.0019730201456695795
Batch  41  loss:  0.0018962915055453777
Batch  51  loss:  0.002066470915451646
Batch  61  loss:  0.0025408894289284945
Batch  71  loss:  0.0018873715307563543
Batch  81  loss:  0.0015266353730112314
Batch  91  loss:  0.0022034908179193735
Validation on real data: 
LOSS supervised-train 0.0019875087344553323, valid 0.0011925201397389174
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0020908068399876356
Batch  11  loss:  0.0016862987540662289
Batch  21  loss:  0.0018527876818552613
Batch  31  loss:  0.001688926829956472
Batch  41  loss:  0.0021470915526151657
Batch  51  loss:  0.0015913500683382154
Batch  61  loss:  0.0024832722265273333
Batch  71  loss:  0.0015698274364694953
Batch  81  loss:  0.001803117454983294
Batch  91  loss:  0.00150793488137424
Validation on real data: 
LOSS supervised-train 0.0019753366394434126, valid 0.0012162536149844527
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.002964982995763421
Batch  11  loss:  0.0015592043055221438
Batch  21  loss:  0.00238874857313931
Batch  31  loss:  0.002445667516440153
Batch  41  loss:  0.0015746456338092685
Batch  51  loss:  0.0014528330648317933
Batch  61  loss:  0.0022082054056227207
Batch  71  loss:  0.0017002379754558206
Batch  81  loss:  0.0015539154410362244
Batch  91  loss:  0.001411212026141584
Validation on real data: 
LOSS supervised-train 0.001918533038115129, valid 0.0008927381713874638
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.002159545896574855
Batch  11  loss:  0.0014921278925612569
Batch  21  loss:  0.0019269746262580156
Batch  31  loss:  0.0026563326828181744
Batch  41  loss:  0.002084585605189204
Batch  51  loss:  0.0012903615133836865
Batch  61  loss:  0.0021294821053743362
Batch  71  loss:  0.001956941792741418
Batch  81  loss:  0.0022176471538841724
Batch  91  loss:  0.001451068208552897
Validation on real data: 
LOSS supervised-train 0.0019469798123463989, valid 0.001728893956169486
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0024406290613114834
Batch  11  loss:  0.0015732556348666549
Batch  21  loss:  0.001955607905983925
Batch  31  loss:  0.0019795717671513557
Batch  41  loss:  0.0015831912169232965
Batch  51  loss:  0.0010027083335444331
Batch  61  loss:  0.002558940090239048
Batch  71  loss:  0.0014687696238979697
Batch  81  loss:  0.0017632490489631891
Batch  91  loss:  0.0018951969686895609
Validation on real data: 
LOSS supervised-train 0.0019273719354532658, valid 0.0011507024755701423
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0019438370363786817
Batch  11  loss:  0.0019262550631538033
Batch  21  loss:  0.0016475522425025702
Batch  31  loss:  0.0024703005328774452
Batch  41  loss:  0.0019347940105944872
Batch  51  loss:  0.0015436172252520919
Batch  61  loss:  0.0019657392986118793
Batch  71  loss:  0.001644306117668748
Batch  81  loss:  0.0016012288397178054
Batch  91  loss:  0.001326369121670723
Validation on real data: 
LOSS supervised-train 0.0019231754750944673, valid 0.0014843823155388236
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0019150286680087447
Batch  11  loss:  0.0019260322442278266
Batch  21  loss:  0.0018515739357098937
Batch  31  loss:  0.0021291652228683233
Batch  41  loss:  0.0019446571823209524
Batch  51  loss:  0.0014381735818460584
Batch  61  loss:  0.002044783439487219
Batch  71  loss:  0.0014837512280791998
Batch  81  loss:  0.0016763568855822086
Batch  91  loss:  0.0015915597323328257
Validation on real data: 
LOSS supervised-train 0.0018545566988177598, valid 0.001412225654348731
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.002022786298766732
Batch  11  loss:  0.001596484100446105
Batch  21  loss:  0.0016785901971161366
Batch  31  loss:  0.0018732234602794051
Batch  41  loss:  0.001697916304692626
Batch  51  loss:  0.0017420905642211437
Batch  61  loss:  0.001863172510638833
Batch  71  loss:  0.0016636577202007174
Batch  81  loss:  0.0014056070940569043
Batch  91  loss:  0.0014972162898629904
Validation on real data: 
LOSS supervised-train 0.0018091520457528532, valid 0.0011601807782426476
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.00233911769464612
Batch  11  loss:  0.001638422254472971
Batch  21  loss:  0.0017794573213905096
Batch  31  loss:  0.0026053274050354958
Batch  41  loss:  0.0016654086066409945
Batch  51  loss:  0.0017788502154871821
Batch  61  loss:  0.00199188943952322
Batch  71  loss:  0.0016163476975634694
Batch  81  loss:  0.0017595210811123252
Batch  91  loss:  0.0013547221897169948
Validation on real data: 
LOSS supervised-train 0.0019064368377439678, valid 0.001114286482334137
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0021582418121397495
Batch  11  loss:  0.0014768128748983145
Batch  21  loss:  0.0016445247456431389
Batch  31  loss:  0.0026846756227314472
Batch  41  loss:  0.0019004321657121181
Batch  51  loss:  0.0012285488191992044
Batch  61  loss:  0.001876005087979138
Batch  71  loss:  0.0014695471618324518
Batch  81  loss:  0.0014545619487762451
Batch  91  loss:  0.0017556966049596667
Validation on real data: 
LOSS supervised-train 0.001842695139348507, valid 0.0015121459728106856
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0023391833528876305
Batch  11  loss:  0.0017664146143943071
Batch  21  loss:  0.0012824821751564741
Batch  31  loss:  0.0024432134814560413
Batch  41  loss:  0.0018624508520588279
Batch  51  loss:  0.0013969013234600425
Batch  61  loss:  0.0019344999454915524
Batch  71  loss:  0.00133861496578902
Batch  81  loss:  0.0013630392495542765
Batch  91  loss:  0.0015757068758830428
Validation on real data: 
LOSS supervised-train 0.0017960072070127354, valid 0.0014902196126058698
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.0023603509180247784
Batch  11  loss:  0.001555331633426249
Batch  21  loss:  0.0014350598212331533
Batch  31  loss:  0.002364600310102105
Batch  41  loss:  0.0013925594976171851
Batch  51  loss:  0.0014518584357574582
Batch  61  loss:  0.001655859756283462
Batch  71  loss:  0.002096284180879593
Batch  81  loss:  0.0018263390520587564
Batch  91  loss:  0.001647304161451757
Validation on real data: 
LOSS supervised-train 0.0017337702657096088, valid 0.0014327217359095812
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.002366510685533285
Batch  11  loss:  0.001516234944574535
Batch  21  loss:  0.0013862490886822343
Batch  31  loss:  0.0019992750603705645
Batch  41  loss:  0.0016238216776400805
Batch  51  loss:  0.0017100563272833824
Batch  61  loss:  0.0018347672885283828
Batch  71  loss:  0.0019393007969483733
Batch  81  loss:  0.0013964950339868665
Batch  91  loss:  0.0015604662476107478
Validation on real data: 
LOSS supervised-train 0.0017863627546466888, valid 0.0011276495642960072
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0018258866621181369
Batch  11  loss:  0.0013971020234748721
Batch  21  loss:  0.0020899989176541567
Batch  31  loss:  0.002606673166155815
Batch  41  loss:  0.001894588116556406
Batch  51  loss:  0.0015252613229677081
Batch  61  loss:  0.0017825703835114837
Batch  71  loss:  0.0016362869646400213
Batch  81  loss:  0.001595759647898376
Batch  91  loss:  0.0013180365785956383
Validation on real data: 
LOSS supervised-train 0.0017174576286925003, valid 0.001697725965641439
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.00208366010338068
Batch  11  loss:  0.0013233415083959699
Batch  21  loss:  0.0011524762958288193
Batch  31  loss:  0.0019486744422465563
Batch  41  loss:  0.001393863931298256
Batch  51  loss:  0.001293013570830226
Batch  61  loss:  0.002179862232878804
Batch  71  loss:  0.00127918622456491
Batch  81  loss:  0.0013605660060420632
Batch  91  loss:  0.0020638287533074617
Validation on real data: 
LOSS supervised-train 0.0016830434545408934, valid 0.0009133348939940333
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.001729034585878253
Batch  11  loss:  0.001822383957915008
Batch  21  loss:  0.0013883828651160002
Batch  31  loss:  0.0023459941148757935
Batch  41  loss:  0.0014388259733095765
Batch  51  loss:  0.0014160078717395663
Batch  61  loss:  0.0017517987871542573
Batch  71  loss:  0.001136257080361247
Batch  81  loss:  0.0014640635345131159
Batch  91  loss:  0.0014107045717537403
Validation on real data: 
LOSS supervised-train 0.0017316893691895529, valid 0.0012236691545695066
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.001862766221165657
Batch  11  loss:  0.0014521997654810548
Batch  21  loss:  0.00109436409547925
Batch  31  loss:  0.0018620193004608154
Batch  41  loss:  0.0011921973200514913
Batch  51  loss:  0.0012556265573948622
Batch  61  loss:  0.0024720532819628716
Batch  71  loss:  0.001293019624426961
Batch  81  loss:  0.0015004586894065142
Batch  91  loss:  0.0014045735588297248
Validation on real data: 
LOSS supervised-train 0.0016625106730498373, valid 0.0013041144702583551
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.002306816866621375
Batch  11  loss:  0.0013879233738407493
Batch  21  loss:  0.0016771076479926705
Batch  31  loss:  0.001258345553651452
Batch  41  loss:  0.001661928603425622
Batch  51  loss:  0.0012892205268144608
Batch  61  loss:  0.0016825924394652247
Batch  71  loss:  0.001020675990730524
Batch  81  loss:  0.0016690210904926062
Batch  91  loss:  0.00163888034876436
Validation on real data: 
LOSS supervised-train 0.0016331045411061495, valid 0.0011200924636796117
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.002059772377833724
Batch  11  loss:  0.0016591785242781043
Batch  21  loss:  0.001546700601466
Batch  31  loss:  0.002255020895972848
Batch  41  loss:  0.0015202458016574383
Batch  51  loss:  0.001238362048752606
Batch  61  loss:  0.0016953333979472518
Batch  71  loss:  0.0013547625858336687
Batch  81  loss:  0.001650505349971354
Batch  91  loss:  0.001238362048752606
Validation on real data: 
LOSS supervised-train 0.0016932728281244635, valid 0.001279581105336547
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0018940712325274944
Batch  11  loss:  0.001826692372560501
Batch  21  loss:  0.0014892505714669824
Batch  31  loss:  0.00238613854162395
Batch  41  loss:  0.0014856336638331413
Batch  51  loss:  0.0012364067370072007
Batch  61  loss:  0.0019581206142902374
Batch  71  loss:  0.001376239350065589
Batch  81  loss:  0.0013883629580959678
Batch  91  loss:  0.0019929418340325356
Validation on real data: 
LOSS supervised-train 0.0016338414198253303, valid 0.001158444443717599
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0020896627102047205
Batch  11  loss:  0.001819461933337152
Batch  21  loss:  0.001686382805928588
Batch  31  loss:  0.0018577403388917446
Batch  41  loss:  0.001572545152157545
Batch  51  loss:  0.0012691232841461897
Batch  61  loss:  0.0012291657039895654
Batch  71  loss:  0.001629090285860002
Batch  81  loss:  0.001409190590493381
Batch  91  loss:  0.0010725470492616296
Validation on real data: 
LOSS supervised-train 0.0015990908606909216, valid 0.0012199864722788334
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.00208463822491467
Batch  11  loss:  0.0011081466218456626
Batch  21  loss:  0.001365902367979288
Batch  31  loss:  0.0015328257577493787
Batch  41  loss:  0.001228894921950996
Batch  51  loss:  0.0013865451328456402
Batch  61  loss:  0.001625047530978918
Batch  71  loss:  0.0015562623739242554
Batch  81  loss:  0.0017420455114915967
Batch  91  loss:  0.0011759038316085935
Validation on real data: 
LOSS supervised-train 0.001551819696323946, valid 0.000666584528516978
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.001726674847304821
Batch  11  loss:  0.0016014993889257312
Batch  21  loss:  0.001316914684139192
Batch  31  loss:  0.0017002254026010633
Batch  41  loss:  0.0008940876577980816
Batch  51  loss:  0.0010382189648225904
Batch  61  loss:  0.0018247170373797417
Batch  71  loss:  0.0012882726732641459
Batch  81  loss:  0.0013606762513518333
Batch  91  loss:  0.001362000359222293
Validation on real data: 
LOSS supervised-train 0.0015473881061188876, valid 0.0007102263625711203
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0022586886771023273
Batch  11  loss:  0.0012590375263243914
Batch  21  loss:  0.0016487541142851114
Batch  31  loss:  0.002200179733335972
Batch  41  loss:  0.0015548706287518144
Batch  51  loss:  0.0010153934126719832
Batch  61  loss:  0.0017543365247547626
Batch  71  loss:  0.0014156918041408062
Batch  81  loss:  0.0010407960508018732
Batch  91  loss:  0.001814618124626577
Validation on real data: 
LOSS supervised-train 0.00157461374765262, valid 0.0011913427151739597
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0021020241547375917
Batch  11  loss:  0.001189325237646699
Batch  21  loss:  0.00164923130068928
Batch  31  loss:  0.0018325895071029663
Batch  41  loss:  0.0014440709492191672
Batch  51  loss:  0.0013814480043947697
Batch  61  loss:  0.0024938436690717936
Batch  71  loss:  0.0019923353102058172
Batch  81  loss:  0.0015956074930727482
Batch  91  loss:  0.0017063107807189226
Validation on real data: 
LOSS supervised-train 0.0015721666760509833, valid 0.0012283935211598873
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0017234646948054433
Batch  11  loss:  0.0015977361472323537
Batch  21  loss:  0.0011485774302855134
Batch  31  loss:  0.001718230196274817
Batch  41  loss:  0.0012511643581092358
Batch  51  loss:  0.001331993262283504
Batch  61  loss:  0.002531246980652213
Batch  71  loss:  0.002019384177401662
Batch  81  loss:  0.0013945403043180704
Batch  91  loss:  0.003092728089541197
Validation on real data: 
LOSS supervised-train 0.0015235991909867152, valid 0.0011916260700672865
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0016837335424497724
Batch  11  loss:  0.0011841957457363605
Batch  21  loss:  0.0017182595329359174
Batch  31  loss:  0.0014951118500903249
Batch  41  loss:  0.0011450644815340638
Batch  51  loss:  0.0011087720049545169
Batch  61  loss:  0.0016832827823236585
Batch  71  loss:  0.001844400423578918
Batch  81  loss:  0.0013986786361783743
Batch  91  loss:  0.001421942375600338
Validation on real data: 
LOSS supervised-train 0.001510195427108556, valid 0.0011362320510670543
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.001205191481858492
Batch  11  loss:  0.0009975911816582084
Batch  21  loss:  0.001537560485303402
Batch  31  loss:  0.0014780849451199174
Batch  41  loss:  0.0013144182739779353
Batch  51  loss:  0.0011759557528421283
Batch  61  loss:  0.0019774860702455044
Batch  71  loss:  0.0013215953949838877
Batch  81  loss:  0.0011676371796056628
Batch  91  loss:  0.0014148708432912827
Validation on real data: 
LOSS supervised-train 0.0014991604938404636, valid 0.0011082760756835341
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0013836986618116498
Batch  11  loss:  0.0016129782889038324
Batch  21  loss:  0.0013307499466463923
Batch  31  loss:  0.0018723224056884646
Batch  41  loss:  0.0013645931612700224
Batch  51  loss:  0.0013272261712700129
Batch  61  loss:  0.001771337352693081
Batch  71  loss:  0.0009799930267035961
Batch  81  loss:  0.001528542721644044
Batch  91  loss:  0.0017009960720315576
Validation on real data: 
LOSS supervised-train 0.0015200615877984092, valid 0.0015385870356112719
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0013956775655969977
Batch  11  loss:  0.001142812892794609
Batch  21  loss:  0.0011562904110178351
Batch  31  loss:  0.0019935164600610733
Batch  41  loss:  0.0012000828282907605
Batch  51  loss:  0.0012064955662935972
Batch  61  loss:  0.0018029281636700034
Batch  71  loss:  0.0016195797361433506
Batch  81  loss:  0.001319267088547349
Batch  91  loss:  0.0012731228489428759
Validation on real data: 
LOSS supervised-train 0.0014760732045397162, valid 0.0009962880285456777
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0018871267093345523
Batch  11  loss:  0.0009747674921527505
Batch  21  loss:  0.00102960632648319
Batch  31  loss:  0.0013197677908465266
Batch  41  loss:  0.0011740130139514804
Batch  51  loss:  0.0009636175236664712
Batch  61  loss:  0.0023180185817182064
Batch  71  loss:  0.0011893936898559332
Batch  81  loss:  0.001616662135347724
Batch  91  loss:  0.0015129095409065485
Validation on real data: 
LOSS supervised-train 0.0014555424853460862, valid 0.0010305014438927174
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.001909559010528028
Batch  11  loss:  0.0015394174261018634
Batch  21  loss:  0.0013656010851264
Batch  31  loss:  0.0015261709922924638
Batch  41  loss:  0.0011432141764089465
Batch  51  loss:  0.0012006544275209308
Batch  61  loss:  0.0020727207884192467
Batch  71  loss:  0.0015229585114866495
Batch  81  loss:  0.0011084239231422544
Batch  91  loss:  0.0014292910927906632
Validation on real data: 
LOSS supervised-train 0.0014374469261383637, valid 0.0009563143248669803
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.0015009825583547354
Batch  11  loss:  0.0015260243089869618
Batch  21  loss:  0.0012538228183984756
Batch  31  loss:  0.0016494470182806253
Batch  41  loss:  0.0013685580343008041
Batch  51  loss:  0.0012763198465108871
Batch  61  loss:  0.0019948002882301807
Batch  71  loss:  0.0008883859263733029
Batch  81  loss:  0.0011575301177799702
Batch  91  loss:  0.0008779483032412827
Validation on real data: 
LOSS supervised-train 0.001463725125649944, valid 0.0010637291707098484
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0014416060876101255
Batch  11  loss:  0.0012899532448500395
Batch  21  loss:  0.0016142764361575246
Batch  31  loss:  0.0014741799095645547
Batch  41  loss:  0.001455273013561964
Batch  51  loss:  0.001112744677811861
Batch  61  loss:  0.0033555522095412016
Batch  71  loss:  0.0012354778591543436
Batch  81  loss:  0.0011725444346666336
Batch  91  loss:  0.0011592083610594273
Validation on real data: 
LOSS supervised-train 0.0014189601386897265, valid 0.0009890838991850615
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0020911660976707935
Batch  11  loss:  0.0011028898879885674
Batch  21  loss:  0.0013191422913223505
Batch  31  loss:  0.0024980688467621803
Batch  41  loss:  0.0010455783922225237
Batch  51  loss:  0.001207050634548068
Batch  61  loss:  0.0015262990491464734
Batch  71  loss:  0.001271539949811995
Batch  81  loss:  0.0012666864786297083
Batch  91  loss:  0.0011585577158257365
Validation on real data: 
LOSS supervised-train 0.0014091634732903912, valid 0.0010955213801935315
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0015009674243628979
Batch  11  loss:  0.0009551206603646278
Batch  21  loss:  0.0012777443043887615
Batch  31  loss:  0.001433331985026598
Batch  41  loss:  0.0009697417845018208
Batch  51  loss:  0.0009602675563655794
Batch  61  loss:  0.0013440543552860618
Batch  71  loss:  0.0012572633568197489
Batch  81  loss:  0.0018042812589555979
Batch  91  loss:  0.0014020117232576013
Validation on real data: 
LOSS supervised-train 0.0013741151260910555, valid 0.0009665975230745971
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0022356740664690733
Batch  11  loss:  0.0010629206662997603
Batch  21  loss:  0.0014627826167270541
Batch  31  loss:  0.0020871772430837154
Batch  41  loss:  0.0011704936623573303
Batch  51  loss:  0.001071982434950769
Batch  61  loss:  0.0020343416836112738
Batch  71  loss:  0.0013580394443124533
Batch  81  loss:  0.0014968105824664235
Batch  91  loss:  0.0011896942742168903
Validation on real data: 
LOSS supervised-train 0.0014209336997009813, valid 0.0009644904057495296
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0014838003553450108
Batch  11  loss:  0.0014808406122028828
Batch  21  loss:  0.0012973234988749027
Batch  31  loss:  0.0017058493103832006
Batch  41  loss:  0.001034284825436771
Batch  51  loss:  0.0010921721113845706
Batch  61  loss:  0.0021370972972363234
Batch  71  loss:  0.0013360172742977738
Batch  81  loss:  0.0011764999944716692
Batch  91  loss:  0.00120175676420331
Validation on real data: 
LOSS supervised-train 0.0014108751277672126, valid 0.000870475429110229
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.0015668205451220274
Batch  11  loss:  0.0014476883225142956
Batch  21  loss:  0.0011305018560960889
Batch  31  loss:  0.0015248379204422235
Batch  41  loss:  0.001197780016809702
Batch  51  loss:  0.0010947649134323
Batch  61  loss:  0.0015114940470084548
Batch  71  loss:  0.0016203024424612522
Batch  81  loss:  0.0012745916610583663
Batch  91  loss:  0.0009171796846203506
Validation on real data: 
LOSS supervised-train 0.0013740560592850671, valid 0.0013121612137183547
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.001761903753504157
Batch  11  loss:  0.0015181645285338163
Batch  21  loss:  0.0010665406007319689
Batch  31  loss:  0.001766110653989017
Batch  41  loss:  0.0006282564718276262
Batch  51  loss:  0.0010310070356354117
Batch  61  loss:  0.0017357965698465705
Batch  71  loss:  0.00164431007578969
Batch  81  loss:  0.0011554686352610588
Batch  91  loss:  0.001465087989345193
Validation on real data: 
LOSS supervised-train 0.0013467940979171545, valid 0.0014115881640464067
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.001818522228859365
Batch  11  loss:  0.0015713712200522423
Batch  21  loss:  0.0012773653725162148
Batch  31  loss:  0.0016185903223231435
Batch  41  loss:  0.0010763707105070353
Batch  51  loss:  0.0012803090503439307
Batch  61  loss:  0.001632538391277194
Batch  71  loss:  0.0010142073733732104
Batch  81  loss:  0.0012372845085337758
Batch  91  loss:  0.001704714260995388
Validation on real data: 
LOSS supervised-train 0.0013973165914649144, valid 0.0008666227804496884
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0012302958639338613
Batch  11  loss:  0.0012027042685076594
Batch  21  loss:  0.0011101834243163466
Batch  31  loss:  0.0019271326018497348
Batch  41  loss:  0.0010837435256689787
Batch  51  loss:  0.0011430919403210282
Batch  61  loss:  0.0018664939561858773
Batch  71  loss:  0.0010735666146501899
Batch  81  loss:  0.0015763824339956045
Batch  91  loss:  0.0009246845147572458
Validation on real data: 
LOSS supervised-train 0.0013733650563517586, valid 0.0013015535660088062
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.001412073615938425
Batch  11  loss:  0.0011378847993910313
Batch  21  loss:  0.0011148395715281367
Batch  31  loss:  0.0016904049552977085
Batch  41  loss:  0.0016734771197661757
Batch  51  loss:  0.0011397970374673605
Batch  61  loss:  0.0018232462462037802
Batch  71  loss:  0.0012362716952338815
Batch  81  loss:  0.001304943929426372
Batch  91  loss:  0.0009299476514570415
Validation on real data: 
LOSS supervised-train 0.0013518725137691946, valid 0.0012398300459608436
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.0015089907683432102
Batch  11  loss:  0.0013845015782862902
Batch  21  loss:  0.0009486897033639252
Batch  31  loss:  0.0016120246145874262
Batch  41  loss:  0.001113920588977635
Batch  51  loss:  0.0010673131328076124
Batch  61  loss:  0.0011450329329818487
Batch  71  loss:  0.0013415273278951645
Batch  81  loss:  0.0010231437627226114
Batch  91  loss:  0.0007011127308942378
Validation on real data: 
LOSS supervised-train 0.0013303393247770146, valid 0.0009931717067956924
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0018039982533082366
Batch  11  loss:  0.0017388274427503347
Batch  21  loss:  0.001807236229069531
Batch  31  loss:  0.001147485338151455
Batch  41  loss:  0.001051494269631803
Batch  51  loss:  0.0011359049240127206
Batch  61  loss:  0.0013874222058802843
Batch  71  loss:  0.0009067382197827101
Batch  81  loss:  0.0011989753693342209
Batch  91  loss:  0.0014578010886907578
Validation on real data: 
LOSS supervised-train 0.0013722537254216148, valid 0.0011337046744301915
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.001909637707285583
Batch  11  loss:  0.0015601909253746271
Batch  21  loss:  0.001318067777901888
Batch  31  loss:  0.0015943690668791533
Batch  41  loss:  0.0014844168908894062
Batch  51  loss:  0.0014626517659053206
Batch  61  loss:  0.0016750555951148272
Batch  71  loss:  0.0011837220517918468
Batch  81  loss:  0.0018614473519846797
Batch  91  loss:  0.001311883912421763
Validation on real data: 
LOSS supervised-train 0.0013477596431039273, valid 0.00119575927965343
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00148356007412076
Batch  11  loss:  0.001306645106524229
Batch  21  loss:  0.0012225777609273791
Batch  31  loss:  0.0014135322999209166
Batch  41  loss:  0.0008484756690450013
Batch  51  loss:  0.0009262850508093834
Batch  61  loss:  0.0017162950243800879
Batch  71  loss:  0.0014023081166669726
Batch  81  loss:  0.0012825776357203722
Batch  91  loss:  0.0015019287820905447
Validation on real data: 
LOSS supervised-train 0.001287328710895963, valid 0.0012176258023828268
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0012828130275011063
Batch  11  loss:  0.0011647341307252645
Batch  21  loss:  0.001214431831613183
Batch  31  loss:  0.0013124794932082295
Batch  41  loss:  0.0015893458621576428
Batch  51  loss:  0.0013578656362369657
Batch  61  loss:  0.0023184993769973516
Batch  71  loss:  0.0010596208740025759
Batch  81  loss:  0.0010019158944487572
Batch  91  loss:  0.0014737799065187573
Validation on real data: 
LOSS supervised-train 0.0013031443837098777, valid 0.0010694736847653985
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0013804254122078419
Batch  11  loss:  0.0013203367125242949
Batch  21  loss:  0.0013466597301885486
Batch  31  loss:  0.0019094455055892467
Batch  41  loss:  0.0014155054232105613
Batch  51  loss:  0.0013149769511073828
Batch  61  loss:  0.0013680097181349993
Batch  71  loss:  0.0012300967937335372
Batch  81  loss:  0.0009730804595164955
Batch  91  loss:  0.001482936437241733
Validation on real data: 
LOSS supervised-train 0.0013124577945563942, valid 0.0015527174109593034
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0012902772286906838
Batch  11  loss:  0.0015365397557616234
Batch  21  loss:  0.0010851966217160225
Batch  31  loss:  0.001590071595273912
Batch  41  loss:  0.001240248209796846
Batch  51  loss:  0.0007069927523843944
Batch  61  loss:  0.001140349661000073
Batch  71  loss:  0.0008610881632193923
Batch  81  loss:  0.0012756276410073042
Batch  91  loss:  0.0014474826166406274
Validation on real data: 
LOSS supervised-train 0.0012298648676369339, valid 0.0009355443762615323
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.0017090648179873824
Batch  11  loss:  0.0018496665870770812
Batch  21  loss:  0.0008896174840629101
Batch  31  loss:  0.0016081943176686764
Batch  41  loss:  0.0014079391257837415
Batch  51  loss:  0.0011098308023065329
Batch  61  loss:  0.0014882168034091592
Batch  71  loss:  0.0011560089187696576
Batch  81  loss:  0.0012183416401967406
Batch  91  loss:  0.001619040733203292
Validation on real data: 
LOSS supervised-train 0.0012874040787573904, valid 0.0012993559939786792
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0012101244647055864
Batch  11  loss:  0.0011443012626841664
Batch  21  loss:  0.0012086894130334258
Batch  31  loss:  0.001683270325884223
Batch  41  loss:  0.0010708713671192527
Batch  51  loss:  0.0009776725200936198
Batch  61  loss:  0.0015987022779881954
Batch  71  loss:  0.0011893273331224918
Batch  81  loss:  0.0012473193928599358
Batch  91  loss:  0.0011864069383591413
Validation on real data: 
LOSS supervised-train 0.0012486711813835428, valid 0.0009021184523589909
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  skateboard ; Model ID: 98222a1e5f59f2098745e78dbc45802e
--------------------
Training baseline regression model:  2022-03-30 17:51:37.949580
Detector:  pointnet
Object:  skateboard
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1611559
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.27161887288093567
Batch  11  loss:  0.22728440165519714
Batch  21  loss:  0.17389743030071259
Batch  31  loss:  0.1639184206724167
Batch  41  loss:  0.1723942905664444
Batch  51  loss:  0.14955371618270874
Batch  61  loss:  0.13087932765483856
Batch  71  loss:  0.12263480573892593
Batch  81  loss:  0.13592691719532013
Batch  91  loss:  0.1042708158493042
Validation on real data: 
LOSS supervised-train 0.15273699209094047, valid 0.07190578430891037
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.08778423815965652
Batch  11  loss:  0.06630218029022217
Batch  21  loss:  0.07650619745254517
Batch  31  loss:  0.05627482384443283
Batch  41  loss:  0.04502161964774132
Batch  51  loss:  0.037365544587373734
Batch  61  loss:  0.02693955786526203
Batch  71  loss:  0.0371052585542202
Batch  81  loss:  0.06559249758720398
Batch  91  loss:  0.02095569111406803
Validation on real data: 
LOSS supervised-train 0.044503355994820595, valid 0.02094860188663006
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.01780054345726967
Batch  11  loss:  0.021009650081396103
Batch  21  loss:  0.019368739798665047
Batch  31  loss:  0.01692274585366249
Batch  41  loss:  0.021904801949858665
Batch  51  loss:  0.016322610899806023
Batch  61  loss:  0.012818536721169949
Batch  71  loss:  0.024257004261016846
Batch  81  loss:  0.045937128365039825
Batch  91  loss:  0.012622136622667313
Validation on real data: 
LOSS supervised-train 0.017547793793492018, valid 0.008282903581857681
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.011257614940404892
Batch  11  loss:  0.009813905693590641
Batch  21  loss:  0.013169629499316216
Batch  31  loss:  0.008695394732058048
Batch  41  loss:  0.013184376992285252
Batch  51  loss:  0.012392384000122547
Batch  61  loss:  0.01178050972521305
Batch  71  loss:  0.015921128913760185
Batch  81  loss:  0.03564666584134102
Batch  91  loss:  0.00907168723642826
Validation on real data: 
LOSS supervised-train 0.012400901587679982, valid 0.006878308951854706
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.010074900463223457
Batch  11  loss:  0.007846551947295666
Batch  21  loss:  0.012222853489220142
Batch  31  loss:  0.006952551659196615
Batch  41  loss:  0.010133787989616394
Batch  51  loss:  0.01270315982401371
Batch  61  loss:  0.008439847268164158
Batch  71  loss:  0.017922403290867805
Batch  81  loss:  0.030734635889530182
Batch  91  loss:  0.00831717811524868
Validation on real data: 
LOSS supervised-train 0.01105675392318517, valid 0.0075347148813307285
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.010056786239147186
Batch  11  loss:  0.005884986836463213
Batch  21  loss:  0.008983227424323559
Batch  31  loss:  0.007852372713387012
Batch  41  loss:  0.009986430406570435
Batch  51  loss:  0.013306654058396816
Batch  61  loss:  0.006549829617142677
Batch  71  loss:  0.01269849669188261
Batch  81  loss:  0.027065949514508247
Batch  91  loss:  0.007550717331469059
Validation on real data: 
LOSS supervised-train 0.009939923784695565, valid 0.006552607752382755
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.008209919556975365
Batch  11  loss:  0.006456463597714901
Batch  21  loss:  0.009024272672832012
Batch  31  loss:  0.008670452982187271
Batch  41  loss:  0.010352451354265213
Batch  51  loss:  0.009941616095602512
Batch  61  loss:  0.00902910903096199
Batch  71  loss:  0.013430207967758179
Batch  81  loss:  0.02453957498073578
Batch  91  loss:  0.007047141436487436
Validation on real data: 
LOSS supervised-train 0.009236585935577751, valid 0.006331429351121187
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.008219609968364239
Batch  11  loss:  0.006033692043274641
Batch  21  loss:  0.00819617509841919
Batch  31  loss:  0.007105942815542221
Batch  41  loss:  0.008754036389291286
Batch  51  loss:  0.010621463879942894
Batch  61  loss:  0.008617062121629715
Batch  71  loss:  0.014222563244402409
Batch  81  loss:  0.026764173060655594
Batch  91  loss:  0.008412249386310577
Validation on real data: 
LOSS supervised-train 0.008966364725492894, valid 0.007890557870268822
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.007433731574565172
Batch  11  loss:  0.0063022589311003685
Batch  21  loss:  0.007990405894815922
Batch  31  loss:  0.005900755524635315
Batch  41  loss:  0.007743531372398138
Batch  51  loss:  0.011640239506959915
Batch  61  loss:  0.006666443310678005
Batch  71  loss:  0.010846991091966629
Batch  81  loss:  0.023924900218844414
Batch  91  loss:  0.006696352269500494
Validation on real data: 
LOSS supervised-train 0.008345159846358, valid 0.007580935955047607
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.008982349187135696
Batch  11  loss:  0.006093745585530996
Batch  21  loss:  0.009256662800908089
Batch  31  loss:  0.0062005287036299706
Batch  41  loss:  0.007728771772235632
Batch  51  loss:  0.010046379640698433
Batch  61  loss:  0.007607009261846542
Batch  71  loss:  0.011587611399590969
Batch  81  loss:  0.024805469438433647
Batch  91  loss:  0.005913153290748596
Validation on real data: 
LOSS supervised-train 0.008292265781201423, valid 0.007675267290323973
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.007657837588340044
Batch  11  loss:  0.005744230467826128
Batch  21  loss:  0.008574061095714569
Batch  31  loss:  0.006662829779088497
Batch  41  loss:  0.007600582204759121
Batch  51  loss:  0.008139501325786114
Batch  61  loss:  0.0064737843349576
Batch  71  loss:  0.010619592852890491
Batch  81  loss:  0.021577386185526848
Batch  91  loss:  0.005519546568393707
Validation on real data: 
LOSS supervised-train 0.007783506154082715, valid 0.006399168167263269
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.007686281111091375
Batch  11  loss:  0.005598241928964853
Batch  21  loss:  0.009115931577980518
Batch  31  loss:  0.005719451699405909
Batch  41  loss:  0.007845437154173851
Batch  51  loss:  0.010355422273278236
Batch  61  loss:  0.0048112450167536736
Batch  71  loss:  0.011438214220106602
Batch  81  loss:  0.019970577210187912
Batch  91  loss:  0.005696346517652273
Validation on real data: 
LOSS supervised-train 0.007607117197476327, valid 0.006391631904989481
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.006570093333721161
Batch  11  loss:  0.005682728718966246
Batch  21  loss:  0.007062351331114769
Batch  31  loss:  0.00628902530297637
Batch  41  loss:  0.007139234337955713
Batch  51  loss:  0.007295331917703152
Batch  61  loss:  0.006147398613393307
Batch  71  loss:  0.010106359608471394
Batch  81  loss:  0.020323781296610832
Batch  91  loss:  0.005046035163104534
Validation on real data: 
LOSS supervised-train 0.007357160076498986, valid 0.006846056319773197
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.00538866501301527
Batch  11  loss:  0.005048340652137995
Batch  21  loss:  0.007593606598675251
Batch  31  loss:  0.00571480393409729
Batch  41  loss:  0.00723562715575099
Batch  51  loss:  0.008401546627283096
Batch  61  loss:  0.004896846134215593
Batch  71  loss:  0.012265171855688095
Batch  81  loss:  0.02237415313720703
Batch  91  loss:  0.005897143855690956
Validation on real data: 
LOSS supervised-train 0.0071636394457891585, valid 0.006924868561327457
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.007227028254419565
Batch  11  loss:  0.0047499011270701885
Batch  21  loss:  0.00732681667432189
Batch  31  loss:  0.0045768157579004765
Batch  41  loss:  0.0061527094803750515
Batch  51  loss:  0.006951817311346531
Batch  61  loss:  0.006175828631967306
Batch  71  loss:  0.009316707961261272
Batch  81  loss:  0.025881251320242882
Batch  91  loss:  0.005635846871882677
Validation on real data: 
LOSS supervised-train 0.007103778077289462, valid 0.006604344584047794
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.007154406048357487
Batch  11  loss:  0.004527939483523369
Batch  21  loss:  0.006618264596909285
Batch  31  loss:  0.005483883433043957
Batch  41  loss:  0.008467820473015308
Batch  51  loss:  0.008905052207410336
Batch  61  loss:  0.0057785664685070515
Batch  71  loss:  0.00852123275399208
Batch  81  loss:  0.017713116481900215
Batch  91  loss:  0.006024520378559828
Validation on real data: 
LOSS supervised-train 0.006961693088524044, valid 0.004956357181072235
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.006366671063005924
Batch  11  loss:  0.004877317696809769
Batch  21  loss:  0.006134195253252983
Batch  31  loss:  0.004371164832264185
Batch  41  loss:  0.007236170582473278
Batch  51  loss:  0.00630085589364171
Batch  61  loss:  0.00567897642031312
Batch  71  loss:  0.011481516063213348
Batch  81  loss:  0.020763486623764038
Batch  91  loss:  0.005854376591742039
Validation on real data: 
LOSS supervised-train 0.006708645997568965, valid 0.006170671433210373
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.005498773884028196
Batch  11  loss:  0.004803025629371405
Batch  21  loss:  0.005807033274322748
Batch  31  loss:  0.005993435624986887
Batch  41  loss:  0.006148593500256538
Batch  51  loss:  0.007220177445560694
Batch  61  loss:  0.007150625344365835
Batch  71  loss:  0.008697385899722576
Batch  81  loss:  0.020972514525055885
Batch  91  loss:  0.005649839527904987
Validation on real data: 
LOSS supervised-train 0.006486289380118251, valid 0.005866801831871271
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.005507367663085461
Batch  11  loss:  0.004791489336639643
Batch  21  loss:  0.006188259460031986
Batch  31  loss:  0.004723296035081148
Batch  41  loss:  0.006007367745041847
Batch  51  loss:  0.0069321333430707455
Batch  61  loss:  0.004822025075554848
Batch  71  loss:  0.008779391646385193
Batch  81  loss:  0.02058185264468193
Batch  91  loss:  0.005480232182890177
Validation on real data: 
LOSS supervised-train 0.006519723497331142, valid 0.005090274848043919
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.004755629692226648
Batch  11  loss:  0.004868632182478905
Batch  21  loss:  0.007378161884844303
Batch  31  loss:  0.0057215336710214615
Batch  41  loss:  0.006485464982688427
Batch  51  loss:  0.006881273351609707
Batch  61  loss:  0.005005690734833479
Batch  71  loss:  0.00783043634146452
Batch  81  loss:  0.017886238172650337
Batch  91  loss:  0.0057966578751802444
Validation on real data: 
LOSS supervised-train 0.006255152884405106, valid 0.005712809972465038
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0051155309192836285
Batch  11  loss:  0.003303583711385727
Batch  21  loss:  0.005859339144080877
Batch  31  loss:  0.005199051927775145
Batch  41  loss:  0.006604206282645464
Batch  51  loss:  0.00688767759129405
Batch  61  loss:  0.004321167711168528
Batch  71  loss:  0.009264775551855564
Batch  81  loss:  0.01552893128246069
Batch  91  loss:  0.005327455699443817
Validation on real data: 
LOSS supervised-train 0.006118331293109804, valid 0.004317254293709993
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.00715884892269969
Batch  11  loss:  0.004347929731011391
Batch  21  loss:  0.005835420917719603
Batch  31  loss:  0.0048921918496489525
Batch  41  loss:  0.005651670508086681
Batch  51  loss:  0.006344974506646395
Batch  61  loss:  0.006160985212773085
Batch  71  loss:  0.008388440124690533
Batch  81  loss:  0.01624956540763378
Batch  91  loss:  0.004890048410743475
Validation on real data: 
LOSS supervised-train 0.006054985148366541, valid 0.005055570974946022
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.006652628071606159
Batch  11  loss:  0.004822388291358948
Batch  21  loss:  0.0060564689338207245
Batch  31  loss:  0.005428747273981571
Batch  41  loss:  0.005573559086769819
Batch  51  loss:  0.006820825394243002
Batch  61  loss:  0.005529625341296196
Batch  71  loss:  0.007900873199105263
Batch  81  loss:  0.01766018569469452
Batch  91  loss:  0.004471487365663052
Validation on real data: 
LOSS supervised-train 0.005953073611017316, valid 0.004771813750267029
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.0063358792103827
Batch  11  loss:  0.005029040388762951
Batch  21  loss:  0.004843957722187042
Batch  31  loss:  0.005937614943832159
Batch  41  loss:  0.0063502369448542595
Batch  51  loss:  0.0062210713513195515
Batch  61  loss:  0.004556132014840841
Batch  71  loss:  0.008765697479248047
Batch  81  loss:  0.01667938008904457
Batch  91  loss:  0.0053400336764752865
Validation on real data: 
LOSS supervised-train 0.005863831026945263, valid 0.005270092748105526
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.004999153316020966
Batch  11  loss:  0.004986929707229137
Batch  21  loss:  0.005644042976200581
Batch  31  loss:  0.005022476892918348
Batch  41  loss:  0.005409799050539732
Batch  51  loss:  0.006958489306271076
Batch  61  loss:  0.0049118585884571075
Batch  71  loss:  0.008205916732549667
Batch  81  loss:  0.015376710332930088
Batch  91  loss:  0.005266314838081598
Validation on real data: 
LOSS supervised-train 0.00566941402386874, valid 0.0054354118183255196
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.006531520746648312
Batch  11  loss:  0.004235861822962761
Batch  21  loss:  0.006088878493756056
Batch  31  loss:  0.004602766130119562
Batch  41  loss:  0.0056053209118545055
Batch  51  loss:  0.006406289059668779
Batch  61  loss:  0.003951020538806915
Batch  71  loss:  0.008065365254878998
Batch  81  loss:  0.01617967151105404
Batch  91  loss:  0.004482305608689785
Validation on real data: 
LOSS supervised-train 0.005719978339038789, valid 0.005516569595783949
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.005747352726757526
Batch  11  loss:  0.003799760015681386
Batch  21  loss:  0.00610392214730382
Batch  31  loss:  0.005112285725772381
Batch  41  loss:  0.005456604063510895
Batch  51  loss:  0.005858874414116144
Batch  61  loss:  0.00432620570063591
Batch  71  loss:  0.006549556739628315
Batch  81  loss:  0.014569729566574097
Batch  91  loss:  0.004149619955569506
Validation on real data: 
LOSS supervised-train 0.005610013320110738, valid 0.005238230340182781
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.005353325977921486
Batch  11  loss:  0.0041738031432032585
Batch  21  loss:  0.00565142510458827
Batch  31  loss:  0.004725399427115917
Batch  41  loss:  0.004864747170358896
Batch  51  loss:  0.00580707797780633
Batch  61  loss:  0.004549889825284481
Batch  71  loss:  0.007206600159406662
Batch  81  loss:  0.012754417024552822
Batch  91  loss:  0.004320103209465742
Validation on real data: 
LOSS supervised-train 0.0055997208529151975, valid 0.004981665406376123
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.00536805484443903
Batch  11  loss:  0.003536302363499999
Batch  21  loss:  0.005340062081813812
Batch  31  loss:  0.004875616170465946
Batch  41  loss:  0.005287004169076681
Batch  51  loss:  0.006178880576044321
Batch  61  loss:  0.0034556919708848
Batch  71  loss:  0.007163172587752342
Batch  81  loss:  0.012496532872319221
Batch  91  loss:  0.005307833664119244
Validation on real data: 
LOSS supervised-train 0.005439699382986873, valid 0.005279303062707186
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.004394869785755873
Batch  11  loss:  0.003962052054703236
Batch  21  loss:  0.005196401849389076
Batch  31  loss:  0.004337806720286608
Batch  41  loss:  0.005580474156886339
Batch  51  loss:  0.005934969522058964
Batch  61  loss:  0.0038686010520905256
Batch  71  loss:  0.007035843562334776
Batch  81  loss:  0.009234379976987839
Batch  91  loss:  0.0044394610449671745
Validation on real data: 
LOSS supervised-train 0.00532988301012665, valid 0.0042151957750320435
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.00543419923633337
Batch  11  loss:  0.004439221229404211
Batch  21  loss:  0.005703792441636324
Batch  31  loss:  0.00479871267452836
Batch  41  loss:  0.004816368222236633
Batch  51  loss:  0.007241907529532909
Batch  61  loss:  0.004478132352232933
Batch  71  loss:  0.007169955875724554
Batch  81  loss:  0.010850375518202782
Batch  91  loss:  0.005059144459664822
Validation on real data: 
LOSS supervised-train 0.005287162538152188, valid 0.004306814167648554
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.004751428961753845
Batch  11  loss:  0.0042268382385373116
Batch  21  loss:  0.0064717731438577175
Batch  31  loss:  0.0036580434534698725
Batch  41  loss:  0.005753544624894857
Batch  51  loss:  0.004671269096434116
Batch  61  loss:  0.004072460811585188
Batch  71  loss:  0.0072968993335962296
Batch  81  loss:  0.010972950607538223
Batch  91  loss:  0.004965298809111118
Validation on real data: 
LOSS supervised-train 0.0052936647157184775, valid 0.0050030481070280075
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.005461882799863815
Batch  11  loss:  0.00334607413969934
Batch  21  loss:  0.005403610412031412
Batch  31  loss:  0.004532592371106148
Batch  41  loss:  0.00516882399097085
Batch  51  loss:  0.00494963675737381
Batch  61  loss:  0.0031605642288923264
Batch  71  loss:  0.00644677085801959
Batch  81  loss:  0.012240657582879066
Batch  91  loss:  0.005185280926525593
Validation on real data: 
LOSS supervised-train 0.005193112476263195, valid 0.004005677532404661
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.00482547702267766
Batch  11  loss:  0.003676262218505144
Batch  21  loss:  0.004135526716709137
Batch  31  loss:  0.0038775152061134577
Batch  41  loss:  0.005513889715075493
Batch  51  loss:  0.006785050965845585
Batch  61  loss:  0.004156885202974081
Batch  71  loss:  0.007418270688503981
Batch  81  loss:  0.008690751157701015
Batch  91  loss:  0.00433376943692565
Validation on real data: 
LOSS supervised-train 0.0051624263729900125, valid 0.003655466251075268
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0045358422212302685
Batch  11  loss:  0.0038780218455940485
Batch  21  loss:  0.005366247612982988
Batch  31  loss:  0.004761280491948128
Batch  41  loss:  0.005724803544580936
Batch  51  loss:  0.006941610481590033
Batch  61  loss:  0.00447229715064168
Batch  71  loss:  0.006486463826149702
Batch  81  loss:  0.010227903723716736
Batch  91  loss:  0.004730803892016411
Validation on real data: 
LOSS supervised-train 0.005128445869777351, valid 0.004984323866665363
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.003609144827350974
Batch  11  loss:  0.00412260927259922
Batch  21  loss:  0.004900529980659485
Batch  31  loss:  0.005006961524486542
Batch  41  loss:  0.004755469970405102
Batch  51  loss:  0.005507923197001219
Batch  61  loss:  0.0035450952127575874
Batch  71  loss:  0.006475318688899279
Batch  81  loss:  0.007998847402632236
Batch  91  loss:  0.005616862792521715
Validation on real data: 
LOSS supervised-train 0.004907254385761917, valid 0.004754777066409588
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.005194924771785736
Batch  11  loss:  0.0036053794901818037
Batch  21  loss:  0.0059996116906404495
Batch  31  loss:  0.003719317726790905
Batch  41  loss:  0.005626971367746592
Batch  51  loss:  0.004510132595896721
Batch  61  loss:  0.004195737652480602
Batch  71  loss:  0.004716277122497559
Batch  81  loss:  0.010497486218810081
Batch  91  loss:  0.005242538172751665
Validation on real data: 
LOSS supervised-train 0.0048715546284802255, valid 0.004268880467861891
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0054818796925246716
Batch  11  loss:  0.004183735232800245
Batch  21  loss:  0.006038537714630365
Batch  31  loss:  0.0038323013577610254
Batch  41  loss:  0.0052032326348125935
Batch  51  loss:  0.004422715399414301
Batch  61  loss:  0.0039703333750367165
Batch  71  loss:  0.005366316996514797
Batch  81  loss:  0.011447959579527378
Batch  91  loss:  0.004694794304668903
Validation on real data: 
LOSS supervised-train 0.004912957448977977, valid 0.003570434870198369
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.005129355471581221
Batch  11  loss:  0.0038958375807851553
Batch  21  loss:  0.005063517019152641
Batch  31  loss:  0.004840807523578405
Batch  41  loss:  0.004738024435937405
Batch  51  loss:  0.005494887009263039
Batch  61  loss:  0.004083476960659027
Batch  71  loss:  0.005581575445830822
Batch  81  loss:  0.010977908968925476
Batch  91  loss:  0.004739474505186081
Validation on real data: 
LOSS supervised-train 0.004874915101099759, valid 0.0035573726054280996
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0035969018936157227
Batch  11  loss:  0.004608613904565573
Batch  21  loss:  0.00427947798743844
Batch  31  loss:  0.0032508692238479853
Batch  41  loss:  0.005229278467595577
Batch  51  loss:  0.004285992123186588
Batch  61  loss:  0.0039559477008879185
Batch  71  loss:  0.005222893785685301
Batch  81  loss:  0.010753955692052841
Batch  91  loss:  0.005295553710311651
Validation on real data: 
LOSS supervised-train 0.004771776283159852, valid 0.00434531457722187
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.003947774413973093
Batch  11  loss:  0.0042024231515824795
Batch  21  loss:  0.005420721601694822
Batch  31  loss:  0.0046745226718485355
Batch  41  loss:  0.005514802876859903
Batch  51  loss:  0.004539421293884516
Batch  61  loss:  0.004745981190353632
Batch  71  loss:  0.006156388204544783
Batch  81  loss:  0.009482070803642273
Batch  91  loss:  0.0037349092308431864
Validation on real data: 
LOSS supervised-train 0.00475833780830726, valid 0.0038903681561350822
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.00447787344455719
Batch  11  loss:  0.004284794442355633
Batch  21  loss:  0.004727421794086695
Batch  31  loss:  0.0036026393063366413
Batch  41  loss:  0.004677172284573317
Batch  51  loss:  0.005266136489808559
Batch  61  loss:  0.0032647012267261744
Batch  71  loss:  0.004810682497918606
Batch  81  loss:  0.00806072261184454
Batch  91  loss:  0.0035489825531840324
Validation on real data: 
LOSS supervised-train 0.00460067177657038, valid 0.004453223664313555
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.004387911409139633
Batch  11  loss:  0.003714298829436302
Batch  21  loss:  0.004191940184682608
Batch  31  loss:  0.004640956874936819
Batch  41  loss:  0.005518596153706312
Batch  51  loss:  0.004754778929054737
Batch  61  loss:  0.0035280894953757524
Batch  71  loss:  0.005271256901323795
Batch  81  loss:  0.008918868377804756
Batch  91  loss:  0.0045873927883803844
Validation on real data: 
LOSS supervised-train 0.004630978563800454, valid 0.004386284854263067
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.004028410650789738
Batch  11  loss:  0.0034781675785779953
Batch  21  loss:  0.003715032245963812
Batch  31  loss:  0.003900541691109538
Batch  41  loss:  0.005118201952427626
Batch  51  loss:  0.004669082351028919
Batch  61  loss:  0.004144052509218454
Batch  71  loss:  0.006204422563314438
Batch  81  loss:  0.008607557974755764
Batch  91  loss:  0.0053057074546813965
Validation on real data: 
LOSS supervised-train 0.004593606549315154, valid 0.004417102783918381
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.004540550522506237
Batch  11  loss:  0.004386808257550001
Batch  21  loss:  0.0038199513219296932
Batch  31  loss:  0.0039463890716433525
Batch  41  loss:  0.005274638533592224
Batch  51  loss:  0.004303951282054186
Batch  61  loss:  0.004321507178246975
Batch  71  loss:  0.005376156885176897
Batch  81  loss:  0.007948690094053745
Batch  91  loss:  0.0031558512710034847
Validation on real data: 
LOSS supervised-train 0.004549649036489427, valid 0.00405572634190321
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.0045295292511582375
Batch  11  loss:  0.0037817375268787146
Batch  21  loss:  0.005172404460608959
Batch  31  loss:  0.00419591274112463
Batch  41  loss:  0.004966813139617443
Batch  51  loss:  0.004780761431902647
Batch  61  loss:  0.004443074576556683
Batch  71  loss:  0.005449718330055475
Batch  81  loss:  0.009582728147506714
Batch  91  loss:  0.0040873256511986256
Validation on real data: 
LOSS supervised-train 0.004447638937272132, valid 0.003767232410609722
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.004007728770375252
Batch  11  loss:  0.0032131848856806755
Batch  21  loss:  0.005377535708248615
Batch  31  loss:  0.0041352068074047565
Batch  41  loss:  0.0044274586252868176
Batch  51  loss:  0.004170610103756189
Batch  61  loss:  0.0037110468838363886
Batch  71  loss:  0.005178301129490137
Batch  81  loss:  0.009675605222582817
Batch  91  loss:  0.004366438835859299
Validation on real data: 
LOSS supervised-train 0.004426331950817257, valid 0.00409448379650712
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.005081075243651867
Batch  11  loss:  0.0034665807615965605
Batch  21  loss:  0.005158533342182636
Batch  31  loss:  0.003720515174791217
Batch  41  loss:  0.004500194452702999
Batch  51  loss:  0.004490460269153118
Batch  61  loss:  0.003193718148395419
Batch  71  loss:  0.0050895861349999905
Batch  81  loss:  0.010279698297381401
Batch  91  loss:  0.0038802889175713062
Validation on real data: 
LOSS supervised-train 0.004408702447544783, valid 0.004241416696459055
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.004481078125536442
Batch  11  loss:  0.0034132718574255705
Batch  21  loss:  0.003917451482266188
Batch  31  loss:  0.0035487376153469086
Batch  41  loss:  0.005332114640623331
Batch  51  loss:  0.005111888982355595
Batch  61  loss:  0.0030311159789562225
Batch  71  loss:  0.005743661895394325
Batch  81  loss:  0.008135269396007061
Batch  91  loss:  0.004161651246249676
Validation on real data: 
LOSS supervised-train 0.004393969131633639, valid 0.0037991919089108706
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.004713624250143766
Batch  11  loss:  0.003987674135714769
Batch  21  loss:  0.0046783750876784325
Batch  31  loss:  0.0041694785468280315
Batch  41  loss:  0.00561771634966135
Batch  51  loss:  0.0049599348567426205
Batch  61  loss:  0.0036379811353981495
Batch  71  loss:  0.005846720654517412
Batch  81  loss:  0.008895199745893478
Batch  91  loss:  0.0049794926308095455
Validation on real data: 
LOSS supervised-train 0.00439922081772238, valid 0.0034580135252326727
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0032834529411047697
Batch  11  loss:  0.003989106509834528
Batch  21  loss:  0.005122474394738674
Batch  31  loss:  0.004420258104801178
Batch  41  loss:  0.0048546018078923225
Batch  51  loss:  0.004498555790632963
Batch  61  loss:  0.004277243744581938
Batch  71  loss:  0.004641402047127485
Batch  81  loss:  0.00920860841870308
Batch  91  loss:  0.002695250092074275
Validation on real data: 
LOSS supervised-train 0.004384606475941837, valid 0.003990361467003822
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.003587977262213826
Batch  11  loss:  0.0036458331160247326
Batch  21  loss:  0.0032459180802106857
Batch  31  loss:  0.0037169791758060455
Batch  41  loss:  0.0047851502895355225
Batch  51  loss:  0.0040125008672475815
Batch  61  loss:  0.003483374137431383
Batch  71  loss:  0.005769667681306601
Batch  81  loss:  0.007694970350712538
Batch  91  loss:  0.003959381487220526
Validation on real data: 
LOSS supervised-train 0.004201971963047981, valid 0.00402251910418272
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.003952426370233297
Batch  11  loss:  0.002980147022753954
Batch  21  loss:  0.004339203238487244
Batch  31  loss:  0.004834366962313652
Batch  41  loss:  0.0048860833048820496
Batch  51  loss:  0.004970913287252188
Batch  61  loss:  0.004206796642392874
Batch  71  loss:  0.005177061539143324
Batch  81  loss:  0.007066120859235525
Batch  91  loss:  0.0041173468343913555
Validation on real data: 
LOSS supervised-train 0.004189069934654981, valid 0.003203794825822115
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.00395544059574604
Batch  11  loss:  0.004502605181187391
Batch  21  loss:  0.00460844999179244
Batch  31  loss:  0.004045311827212572
Batch  41  loss:  0.004347631707787514
Batch  51  loss:  0.004585147835314274
Batch  61  loss:  0.004166137892752886
Batch  71  loss:  0.005220159888267517
Batch  81  loss:  0.006660835817456245
Batch  91  loss:  0.003560358425602317
Validation on real data: 
LOSS supervised-train 0.004174697294365615, valid 0.004232864361256361
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0037371963262557983
Batch  11  loss:  0.002740404335781932
Batch  21  loss:  0.0048222970217466354
Batch  31  loss:  0.0035854296293109655
Batch  41  loss:  0.004020835738629103
Batch  51  loss:  0.004905960988253355
Batch  61  loss:  0.003761839121580124
Batch  71  loss:  0.00537258991971612
Batch  81  loss:  0.007594676222652197
Batch  91  loss:  0.0038076203782111406
Validation on real data: 
LOSS supervised-train 0.0041076157800853256, valid 0.0033671902492642403
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.003931842278689146
Batch  11  loss:  0.0037499265745282173
Batch  21  loss:  0.004883021581918001
Batch  31  loss:  0.005028603132814169
Batch  41  loss:  0.004045151174068451
Batch  51  loss:  0.004167727194726467
Batch  61  loss:  0.003121359273791313
Batch  71  loss:  0.004162335768342018
Batch  81  loss:  0.007076272275298834
Batch  91  loss:  0.003623397322371602
Validation on real data: 
LOSS supervised-train 0.004072201794479043, valid 0.002927022520452738
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.004290618933737278
Batch  11  loss:  0.003714852500706911
Batch  21  loss:  0.004825568292289972
Batch  31  loss:  0.003746036207303405
Batch  41  loss:  0.004634130280464888
Batch  51  loss:  0.005207756534218788
Batch  61  loss:  0.0034425323829054832
Batch  71  loss:  0.004472534637898207
Batch  81  loss:  0.006211361847817898
Batch  91  loss:  0.003453451907262206
Validation on real data: 
LOSS supervised-train 0.004005435109138489, valid 0.003568494925275445
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.0038017320912331343
Batch  11  loss:  0.0034035788848996162
Batch  21  loss:  0.0033666868694126606
Batch  31  loss:  0.003977873362600803
Batch  41  loss:  0.004600293468683958
Batch  51  loss:  0.004764355719089508
Batch  61  loss:  0.003307834267616272
Batch  71  loss:  0.004413538612425327
Batch  81  loss:  0.006801333278417587
Batch  91  loss:  0.003177291015163064
Validation on real data: 
LOSS supervised-train 0.0039996533188968895, valid 0.003486461006104946
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.0033048309851437807
Batch  11  loss:  0.003279885044321418
Batch  21  loss:  0.003974378574639559
Batch  31  loss:  0.003917560912668705
Batch  41  loss:  0.0053482819348573685
Batch  51  loss:  0.004417842719703913
Batch  61  loss:  0.004173066932708025
Batch  71  loss:  0.005034878849983215
Batch  81  loss:  0.008181503042578697
Batch  91  loss:  0.0038759426679462194
Validation on real data: 
LOSS supervised-train 0.004066931032575667, valid 0.004397721961140633
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.003956336062401533
Batch  11  loss:  0.003618488786742091
Batch  21  loss:  0.004079677164554596
Batch  31  loss:  0.003954763524234295
Batch  41  loss:  0.004262729547917843
Batch  51  loss:  0.0038656943943351507
Batch  61  loss:  0.0029582385905086994
Batch  71  loss:  0.004878590814769268
Batch  81  loss:  0.006958710495382547
Batch  91  loss:  0.0029011969454586506
Validation on real data: 
LOSS supervised-train 0.003931216702330858, valid 0.004598086234182119
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.003934826701879501
Batch  11  loss:  0.003048895625397563
Batch  21  loss:  0.003616881789639592
Batch  31  loss:  0.003773733275011182
Batch  41  loss:  0.00480624008923769
Batch  51  loss:  0.005006350576877594
Batch  61  loss:  0.003375069936737418
Batch  71  loss:  0.004373444244265556
Batch  81  loss:  0.007024326361715794
Batch  91  loss:  0.004631799180060625
Validation on real data: 
LOSS supervised-train 0.003903606228996068, valid 0.0030372978653758764
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.004307304974645376
Batch  11  loss:  0.003159116953611374
Batch  21  loss:  0.003839232726022601
Batch  31  loss:  0.0031953821890056133
Batch  41  loss:  0.004659484606236219
Batch  51  loss:  0.00463267182931304
Batch  61  loss:  0.003098191926255822
Batch  71  loss:  0.004480145405977964
Batch  81  loss:  0.007248072884976864
Batch  91  loss:  0.0034201533999294043
Validation on real data: 
LOSS supervised-train 0.003901425884105265, valid 0.0041373977437615395
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.003884154139086604
Batch  11  loss:  0.004086985252797604
Batch  21  loss:  0.004593334626406431
Batch  31  loss:  0.003475298173725605
Batch  41  loss:  0.0044469828717410564
Batch  51  loss:  0.004170198459178209
Batch  61  loss:  0.0035698204301297665
Batch  71  loss:  0.004086680710315704
Batch  81  loss:  0.00848318636417389
Batch  91  loss:  0.005556843709200621
Validation on real data: 
LOSS supervised-train 0.0039481438836082815, valid 0.003300959011539817
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0042741307988762856
Batch  11  loss:  0.003927361685782671
Batch  21  loss:  0.0031259297393262386
Batch  31  loss:  0.003562445752322674
Batch  41  loss:  0.004464090336114168
Batch  51  loss:  0.0037829470820724964
Batch  61  loss:  0.004060834646224976
Batch  71  loss:  0.004305636044591665
Batch  81  loss:  0.007255604490637779
Batch  91  loss:  0.0033035448286682367
Validation on real data: 
LOSS supervised-train 0.0038804834824986757, valid 0.00282745948061347
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.004186438396573067
Batch  11  loss:  0.0035263400059193373
Batch  21  loss:  0.00439426489174366
Batch  31  loss:  0.004041570704430342
Batch  41  loss:  0.004936581943184137
Batch  51  loss:  0.0050862496718764305
Batch  61  loss:  0.0037550858687609434
Batch  71  loss:  0.0035268913488835096
Batch  81  loss:  0.005888496991246939
Batch  91  loss:  0.0035366783849895
Validation on real data: 
LOSS supervised-train 0.0037543392833322285, valid 0.00412008399143815
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0042123072780668736
Batch  11  loss:  0.003471008501946926
Batch  21  loss:  0.0036349021829664707
Batch  31  loss:  0.0030313909519463778
Batch  41  loss:  0.004076714627444744
Batch  51  loss:  0.003923607524484396
Batch  61  loss:  0.00450539356097579
Batch  71  loss:  0.004822912160307169
Batch  81  loss:  0.006406147964298725
Batch  91  loss:  0.004058651626110077
Validation on real data: 
LOSS supervised-train 0.003841198701411486, valid 0.0032257132697850466
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.0031928569078445435
Batch  11  loss:  0.003504527034237981
Batch  21  loss:  0.004498505499213934
Batch  31  loss:  0.0032832613214850426
Batch  41  loss:  0.0037282216362655163
Batch  51  loss:  0.003656309563666582
Batch  61  loss:  0.002874525962397456
Batch  71  loss:  0.0037054079584777355
Batch  81  loss:  0.006238054018467665
Batch  91  loss:  0.003923100885003805
Validation on real data: 
LOSS supervised-train 0.003697443481069058, valid 0.003107914002612233
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.0027375053614377975
Batch  11  loss:  0.0029914884362369776
Batch  21  loss:  0.0029400703497231007
Batch  31  loss:  0.003942330367863178
Batch  41  loss:  0.0045458185486495495
Batch  51  loss:  0.0034585099201649427
Batch  61  loss:  0.002963129198178649
Batch  71  loss:  0.0037203431129455566
Batch  81  loss:  0.006223541218787432
Batch  91  loss:  0.00426231836900115
Validation on real data: 
LOSS supervised-train 0.003743417568039149, valid 0.003709502751007676
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.0036415320355445147
Batch  11  loss:  0.002840028377249837
Batch  21  loss:  0.0038815210573375225
Batch  31  loss:  0.003530024318024516
Batch  41  loss:  0.003598232753574848
Batch  51  loss:  0.004284263122826815
Batch  61  loss:  0.0026409891434013844
Batch  71  loss:  0.004627516493201256
Batch  81  loss:  0.005924469791352749
Batch  91  loss:  0.003268223023042083
Validation on real data: 
LOSS supervised-train 0.0037284028925932944, valid 0.0036804925184696913
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.004200490657240152
Batch  11  loss:  0.003022771328687668
Batch  21  loss:  0.0034949996042996645
Batch  31  loss:  0.0031764667946845293
Batch  41  loss:  0.005181311164051294
Batch  51  loss:  0.0035978376399725676
Batch  61  loss:  0.0027141831815242767
Batch  71  loss:  0.004350088536739349
Batch  81  loss:  0.005488536320626736
Batch  91  loss:  0.003689114935696125
Validation on real data: 
LOSS supervised-train 0.0037612469820305704, valid 0.0029686125926673412
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.0034664615523070097
Batch  11  loss:  0.0027142083272337914
Batch  21  loss:  0.003324943594634533
Batch  31  loss:  0.003397180000320077
Batch  41  loss:  0.0037389013450592756
Batch  51  loss:  0.003792158327996731
Batch  61  loss:  0.0036588588263839483
Batch  71  loss:  0.00426488695666194
Batch  81  loss:  0.007059317082166672
Batch  91  loss:  0.00469647953286767
Validation on real data: 
LOSS supervised-train 0.0037561796815134586, valid 0.002963913604617119
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.003347728867083788
Batch  11  loss:  0.0028671915642917156
Batch  21  loss:  0.0033322898671031
Batch  31  loss:  0.003496079705655575
Batch  41  loss:  0.004797683097422123
Batch  51  loss:  0.005014635156840086
Batch  61  loss:  0.002876765327528119
Batch  71  loss:  0.004748459905385971
Batch  81  loss:  0.0074053192511200905
Batch  91  loss:  0.003447737079113722
Validation on real data: 
LOSS supervised-train 0.0036780128022655843, valid 0.0032966509461402893
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.004320358391851187
Batch  11  loss:  0.0033038060646504164
Batch  21  loss:  0.0038403396029025316
Batch  31  loss:  0.0038590584881603718
Batch  41  loss:  0.004499868489801884
Batch  51  loss:  0.0031842337921261787
Batch  61  loss:  0.0028465958312153816
Batch  71  loss:  0.003995025530457497
Batch  81  loss:  0.0045435307547450066
Batch  91  loss:  0.004335872363299131
Validation on real data: 
LOSS supervised-train 0.0035751270991750063, valid 0.003052003914490342
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0026583801954984665
Batch  11  loss:  0.003321523778140545
Batch  21  loss:  0.0035665039904415607
Batch  31  loss:  0.003972474951297045
Batch  41  loss:  0.003487188834697008
Batch  51  loss:  0.003314119763672352
Batch  61  loss:  0.002408190630376339
Batch  71  loss:  0.004359564743936062
Batch  81  loss:  0.005042547360062599
Batch  91  loss:  0.002950633643195033
Validation on real data: 
LOSS supervised-train 0.0035760602448135616, valid 0.0024763173423707485
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.003044192446395755
Batch  11  loss:  0.0033334975596517324
Batch  21  loss:  0.0042023188434541225
Batch  31  loss:  0.0031813557725399733
Batch  41  loss:  0.004828250966966152
Batch  51  loss:  0.003755831392481923
Batch  61  loss:  0.0032523933332413435
Batch  71  loss:  0.004172341898083687
Batch  81  loss:  0.008519257418811321
Batch  91  loss:  0.003315277863293886
Validation on real data: 
LOSS supervised-train 0.003648457897361368, valid 0.0031201529782265425
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0031040310859680176
Batch  11  loss:  0.0031565416138619184
Batch  21  loss:  0.0031975230667740107
Batch  31  loss:  0.0036093455273658037
Batch  41  loss:  0.004255712032318115
Batch  51  loss:  0.0043003614991903305
Batch  61  loss:  0.0034337155520915985
Batch  71  loss:  0.0037590067368000746
Batch  81  loss:  0.005315013229846954
Batch  91  loss:  0.003328113816678524
Validation on real data: 
LOSS supervised-train 0.0036410171375609933, valid 0.003505405969917774
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.0034635497722774744
Batch  11  loss:  0.0032547758892178535
Batch  21  loss:  0.0033977734856307507
Batch  31  loss:  0.0039019512478262186
Batch  41  loss:  0.003960017580538988
Batch  51  loss:  0.003958959598094225
Batch  61  loss:  0.0029203956946730614
Batch  71  loss:  0.003465915797278285
Batch  81  loss:  0.00835450179874897
Batch  91  loss:  0.0034823354799300432
Validation on real data: 
LOSS supervised-train 0.0035605920408852396, valid 0.003642755327746272
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.003408098127692938
Batch  11  loss:  0.003106550546362996
Batch  21  loss:  0.0031068359967321157
Batch  31  loss:  0.0031121105421334505
Batch  41  loss:  0.004198430106043816
Batch  51  loss:  0.0035118316300213337
Batch  61  loss:  0.0031797566916793585
Batch  71  loss:  0.004839623346924782
Batch  81  loss:  0.005620678421109915
Batch  91  loss:  0.003505897941067815
Validation on real data: 
LOSS supervised-train 0.0035505794174969197, valid 0.003399304114282131
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.003954638727009296
Batch  11  loss:  0.0032195185776799917
Batch  21  loss:  0.004484062548726797
Batch  31  loss:  0.003732790006324649
Batch  41  loss:  0.004070369992405176
Batch  51  loss:  0.0037567883264273405
Batch  61  loss:  0.0033566830679774284
Batch  71  loss:  0.004190310835838318
Batch  81  loss:  0.00573684461414814
Batch  91  loss:  0.0027405102737247944
Validation on real data: 
LOSS supervised-train 0.003521060268394649, valid 0.0033253435976803303
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0037987621035426855
Batch  11  loss:  0.0029290951788425446
Batch  21  loss:  0.003939212765544653
Batch  31  loss:  0.0033157472498714924
Batch  41  loss:  0.004420825280249119
Batch  51  loss:  0.0033030216582119465
Batch  61  loss:  0.002875030506402254
Batch  71  loss:  0.003857919480651617
Batch  81  loss:  0.00747081870213151
Batch  91  loss:  0.003896987298503518
Validation on real data: 
LOSS supervised-train 0.0035645122756250204, valid 0.0034828202333301306
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.002578479703515768
Batch  11  loss:  0.002678403165191412
Batch  21  loss:  0.00309139396995306
Batch  31  loss:  0.0030618500895798206
Batch  41  loss:  0.00382481561973691
Batch  51  loss:  0.004212080966681242
Batch  61  loss:  0.002898881444707513
Batch  71  loss:  0.004183625336736441
Batch  81  loss:  0.00516316294670105
Batch  91  loss:  0.00375570310279727
Validation on real data: 
LOSS supervised-train 0.003541485297027975, valid 0.00293659302406013
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0034441882744431496
Batch  11  loss:  0.0035675906110554934
Batch  21  loss:  0.0039605856873095036
Batch  31  loss:  0.0032994491048157215
Batch  41  loss:  0.0039052814245224
Batch  51  loss:  0.00361773488111794
Batch  61  loss:  0.0036231016274541616
Batch  71  loss:  0.003938258159905672
Batch  81  loss:  0.006233668886125088
Batch  91  loss:  0.004162577912211418
Validation on real data: 
LOSS supervised-train 0.0035287689836695792, valid 0.003333166241645813
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.003971924539655447
Batch  11  loss:  0.0034170590806752443
Batch  21  loss:  0.0035925020929425955
Batch  31  loss:  0.0035317789297550917
Batch  41  loss:  0.0033167898654937744
Batch  51  loss:  0.004325455520302057
Batch  61  loss:  0.0025391040835529566
Batch  71  loss:  0.005449214950203896
Batch  81  loss:  0.005191880278289318
Batch  91  loss:  0.003069543279707432
Validation on real data: 
LOSS supervised-train 0.003464456049259752, valid 0.0032703839242458344
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0025870141107589006
Batch  11  loss:  0.002776823006570339
Batch  21  loss:  0.0035977817606180906
Batch  31  loss:  0.0034921811893582344
Batch  41  loss:  0.003826717147603631
Batch  51  loss:  0.005147636868059635
Batch  61  loss:  0.003165941219776869
Batch  71  loss:  0.0030364382546395063
Batch  81  loss:  0.006157338619232178
Batch  91  loss:  0.002811712445691228
Validation on real data: 
LOSS supervised-train 0.003494385019876063, valid 0.0029623943846672773
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.00329795409925282
Batch  11  loss:  0.003819452365860343
Batch  21  loss:  0.00328528368845582
Batch  31  loss:  0.0032736225984990597
Batch  41  loss:  0.004107354208827019
Batch  51  loss:  0.00491148978471756
Batch  61  loss:  0.0024788840673863888
Batch  71  loss:  0.004343525972217321
Batch  81  loss:  0.005253315903246403
Batch  91  loss:  0.0031791427172720432
Validation on real data: 
LOSS supervised-train 0.003415637796279043, valid 0.003011466236785054
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.003177978564053774
Batch  11  loss:  0.0029734415002167225
Batch  21  loss:  0.0038808200042694807
Batch  31  loss:  0.0034547888208180666
Batch  41  loss:  0.0031349877826869488
Batch  51  loss:  0.005160510540008545
Batch  61  loss:  0.0030196879524737597
Batch  71  loss:  0.0035379540640860796
Batch  81  loss:  0.004990630783140659
Batch  91  loss:  0.0029014067258685827
Validation on real data: 
LOSS supervised-train 0.0034150447230786086, valid 0.0029555391520261765
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.002671684604138136
Batch  11  loss:  0.0030939264688640833
Batch  21  loss:  0.003314757952466607
Batch  31  loss:  0.002523028291761875
Batch  41  loss:  0.0037864921614527702
Batch  51  loss:  0.00391772948205471
Batch  61  loss:  0.0025627887807786465
Batch  71  loss:  0.003665641415864229
Batch  81  loss:  0.005317796487361193
Batch  91  loss:  0.0034737740643322468
Validation on real data: 
LOSS supervised-train 0.0033956084749661385, valid 0.0025906788650900126
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0026693178806453943
Batch  11  loss:  0.0031031735707074404
Batch  21  loss:  0.002831954974681139
Batch  31  loss:  0.0029691557865589857
Batch  41  loss:  0.0032451068982481956
Batch  51  loss:  0.004583950620144606
Batch  61  loss:  0.003040555166080594
Batch  71  loss:  0.004025157541036606
Batch  81  loss:  0.0069532692432403564
Batch  91  loss:  0.00353541923686862
Validation on real data: 
LOSS supervised-train 0.003355210805311799, valid 0.003060047747567296
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.002873422345146537
Batch  11  loss:  0.002860036911442876
Batch  21  loss:  0.003284940728917718
Batch  31  loss:  0.0031516647431999445
Batch  41  loss:  0.004157720133662224
Batch  51  loss:  0.004093710798770189
Batch  61  loss:  0.00334047875367105
Batch  71  loss:  0.0032823847141116858
Batch  81  loss:  0.004801361821591854
Batch  91  loss:  0.00357585190795362
Validation on real data: 
LOSS supervised-train 0.0033076695608906448, valid 0.002688966691493988
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0024852235801517963
Batch  11  loss:  0.0028301519341766834
Batch  21  loss:  0.0032986635342240334
Batch  31  loss:  0.0035652166698127985
Batch  41  loss:  0.003449089825153351
Batch  51  loss:  0.003935891203582287
Batch  61  loss:  0.003071956569328904
Batch  71  loss:  0.003451300784945488
Batch  81  loss:  0.005630327854305506
Batch  91  loss:  0.0032088779844343662
Validation on real data: 
LOSS supervised-train 0.003389470267575234, valid 0.0024275388568639755
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0035914822947233915
Batch  11  loss:  0.003408817807212472
Batch  21  loss:  0.003518870333209634
Batch  31  loss:  0.0029128927271813154
Batch  41  loss:  0.0032647650223225355
Batch  51  loss:  0.0037239736411720514
Batch  61  loss:  0.003246129723265767
Batch  71  loss:  0.0037398040294647217
Batch  81  loss:  0.004612893797457218
Batch  91  loss:  0.0032674449030309916
Validation on real data: 
LOSS supervised-train 0.0033783757989294828, valid 0.0034537522587925196
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.00271641300059855
Batch  11  loss:  0.0034615371841937304
Batch  21  loss:  0.0037537056487053633
Batch  31  loss:  0.003009916516020894
Batch  41  loss:  0.004075799603015184
Batch  51  loss:  0.003774317679926753
Batch  61  loss:  0.0027747871354222298
Batch  71  loss:  0.0031190733425319195
Batch  81  loss:  0.004489809274673462
Batch  91  loss:  0.0028865295462310314
Validation on real data: 
LOSS supervised-train 0.003337654094211757, valid 0.0024046716280281544
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.003027833066880703
Batch  11  loss:  0.002793053863570094
Batch  21  loss:  0.00320739415474236
Batch  31  loss:  0.00284443493001163
Batch  41  loss:  0.0044488548301160336
Batch  51  loss:  0.003712498350068927
Batch  61  loss:  0.003081801114603877
Batch  71  loss:  0.0030157784931361675
Batch  81  loss:  0.005660615861415863
Batch  91  loss:  0.003572256537154317
Validation on real data: 
LOSS supervised-train 0.003385787436272949, valid 0.0026244306936860085
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.00295152235776186
Batch  11  loss:  0.0025024067144840956
Batch  21  loss:  0.003784246975556016
Batch  31  loss:  0.0036166859790682793
Batch  41  loss:  0.003855963470414281
Batch  51  loss:  0.0042307376861572266
Batch  61  loss:  0.0023912375327199697
Batch  71  loss:  0.003542169462889433
Batch  81  loss:  0.004864952061325312
Batch  91  loss:  0.0031951372511684895
Validation on real data: 
LOSS supervised-train 0.003316205779556185, valid 0.0027533487882465124
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.00305468263104558
Batch  11  loss:  0.0023756250739097595
Batch  21  loss:  0.0025913098361343145
Batch  31  loss:  0.0030934088863432407
Batch  41  loss:  0.00355534628033638
Batch  51  loss:  0.0035887707490473986
Batch  61  loss:  0.0031327756587415934
Batch  71  loss:  0.003705075941979885
Batch  81  loss:  0.0061370194889605045
Batch  91  loss:  0.0033213754650205374
Validation on real data: 
LOSS supervised-train 0.0033350894204340876, valid 0.0025041853077709675
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.0026164324954152107
Batch  11  loss:  0.0031811147928237915
Batch  21  loss:  0.004012520425021648
Batch  31  loss:  0.0027295665349811316
Batch  41  loss:  0.003787216730415821
Batch  51  loss:  0.003436036640778184
Batch  61  loss:  0.002709103748202324
Batch  71  loss:  0.0035152118653059006
Batch  81  loss:  0.0043584792874753475
Batch  91  loss:  0.0032797413878142834
Validation on real data: 
LOSS supervised-train 0.0032150911702774464, valid 0.0025479106698185205
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.003490261733531952
Batch  11  loss:  0.0026195687241852283
Batch  21  loss:  0.003110710997134447
Batch  31  loss:  0.003423781832680106
Batch  41  loss:  0.003601276781409979
Batch  51  loss:  0.0037939634639769793
Batch  61  loss:  0.004132804926484823
Batch  71  loss:  0.004034745506942272
Batch  81  loss:  0.005459625739604235
Batch  91  loss:  0.004377512726932764
Validation on real data: 
LOSS supervised-train 0.0033789729466661813, valid 0.0028953051660209894
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.004076366312801838
Batch  11  loss:  0.002838856540620327
Batch  21  loss:  0.003917790483683348
Batch  31  loss:  0.0032545842695981264
Batch  41  loss:  0.004383699502795935
Batch  51  loss:  0.0034455223940312862
Batch  61  loss:  0.0029688982758671045
Batch  71  loss:  0.0036585801281034946
Batch  81  loss:  0.004388686269521713
Batch  91  loss:  0.003811708651483059
Validation on real data: 
LOSS supervised-train 0.003287582374177873, valid 0.0026336880400776863
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.002469863975420594
Batch  11  loss:  0.003406792413443327
Batch  21  loss:  0.0035276056732982397
Batch  31  loss:  0.0028659331146627665
Batch  41  loss:  0.0035896843764930964
Batch  51  loss:  0.0033201156184077263
Batch  61  loss:  0.003081102389842272
Batch  71  loss:  0.0033597892615944147
Batch  81  loss:  0.005429100710898638
Batch  91  loss:  0.0030789768788963556
Validation on real data: 
LOSS supervised-train 0.0032282529980875553, valid 0.003546286839991808
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0026833927258849144
Batch  11  loss:  0.0031319207046180964
Batch  21  loss:  0.002822580048814416
Batch  31  loss:  0.00258573517203331
Batch  41  loss:  0.0033764541149139404
Batch  51  loss:  0.0033328800927847624
Batch  61  loss:  0.0035931102465838194
Batch  71  loss:  0.003567529609426856
Batch  81  loss:  0.005900779739022255
Batch  91  loss:  0.0033175796270370483
Validation on real data: 
LOSS supervised-train 0.0033289172709919513, valid 0.002610572148114443
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  table ; Model ID: 3f5daa8fe93b68fa87e2d08958d6900c
--------------------
Training baseline regression model:  2022-03-30 18:15:10.570052
Detector:  pointnet
Object:  table
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1610017
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.2643146812915802
Batch  11  loss:  0.2473471760749817
Batch  21  loss:  0.2271261215209961
Batch  31  loss:  0.22814618051052094
Batch  41  loss:  0.2301839143037796
Batch  51  loss:  0.2257409244775772
Batch  61  loss:  0.2230483889579773
Batch  71  loss:  0.22287029027938843
Batch  81  loss:  0.22470012307167053
Batch  91  loss:  0.22393134236335754
Validation on real data: 
LOSS supervised-train 0.23088843047618865, valid 0.22547844052314758
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.2235751748085022
Batch  11  loss:  0.22495657205581665
Batch  21  loss:  0.22216707468032837
Batch  31  loss:  0.21820051968097687
Batch  41  loss:  0.22880561649799347
Batch  51  loss:  0.22314514219760895
Batch  61  loss:  0.22127322852611542
Batch  71  loss:  0.2232653796672821
Batch  81  loss:  0.22249361872673035
Batch  91  loss:  0.22343501448631287
Validation on real data: 
LOSS supervised-train 0.2239419463276863, valid 0.22269855439662933
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.2238299697637558
Batch  11  loss:  0.22397774457931519
Batch  21  loss:  0.22158576548099518
Batch  31  loss:  0.21782752871513367
Batch  41  loss:  0.2289223074913025
Batch  51  loss:  0.22183765470981598
Batch  61  loss:  0.22138194739818573
Batch  71  loss:  0.2225852608680725
Batch  81  loss:  0.22156113386154175
Batch  91  loss:  0.22274959087371826
Validation on real data: 
LOSS supervised-train 0.22370871305465698, valid 0.22323615849018097
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.22252324223518372
Batch  11  loss:  0.22280769050121307
Batch  21  loss:  0.22211810946464539
Batch  31  loss:  0.21927937865257263
Batch  41  loss:  0.2284175455570221
Batch  51  loss:  0.2217928022146225
Batch  61  loss:  0.22249023616313934
Batch  71  loss:  0.22199183702468872
Batch  81  loss:  0.22153183817863464
Batch  91  loss:  0.22401168942451477
Validation on real data: 
LOSS supervised-train 0.2234603127837181, valid 0.22294072806835175
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.2210613191127777
Batch  11  loss:  0.22339223325252533
Batch  21  loss:  0.22212548553943634
Batch  31  loss:  0.21676935255527496
Batch  41  loss:  0.22889861464500427
Batch  51  loss:  0.22194045782089233
Batch  61  loss:  0.22084498405456543
Batch  71  loss:  0.22163881361484528
Batch  81  loss:  0.22126303613185883
Batch  91  loss:  0.22388233244419098
Validation on real data: 
LOSS supervised-train 0.22309467509388925, valid 0.2219458520412445
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.22100546956062317
Batch  11  loss:  0.22387449443340302
Batch  21  loss:  0.21967583894729614
Batch  31  loss:  0.21730726957321167
Batch  41  loss:  0.22825679183006287
Batch  51  loss:  0.21911700069904327
Batch  61  loss:  0.2221558541059494
Batch  71  loss:  0.22127196192741394
Batch  81  loss:  0.22130803763866425
Batch  91  loss:  0.2229052484035492
Validation on real data: 
LOSS supervised-train 0.22286730036139488, valid 0.21991409361362457
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.22038359940052032
Batch  11  loss:  0.22314055263996124
Batch  21  loss:  0.22101566195487976
Batch  31  loss:  0.2160976678133011
Batch  41  loss:  0.22607295215129852
Batch  51  loss:  0.21995936334133148
Batch  61  loss:  0.22033938765525818
Batch  71  loss:  0.2198500633239746
Batch  81  loss:  0.2216569483280182
Batch  91  loss:  0.22254551947116852
Validation on real data: 
LOSS supervised-train 0.2226019537448883, valid 0.22513355314731598
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.2211616337299347
Batch  11  loss:  0.22357286512851715
Batch  21  loss:  0.21920858323574066
Batch  31  loss:  0.21575148403644562
Batch  41  loss:  0.22862748801708221
Batch  51  loss:  0.21968181431293488
Batch  61  loss:  0.22122424840927124
Batch  71  loss:  0.221365287899971
Batch  81  loss:  0.22043025493621826
Batch  91  loss:  0.22415076196193695
Validation on real data: 
LOSS supervised-train 0.22247274607419967, valid 0.22227971255779266
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.22285239398479462
Batch  11  loss:  0.2230842560529709
Batch  21  loss:  0.2177056074142456
Batch  31  loss:  0.21637439727783203
Batch  41  loss:  0.23154757916927338
Batch  51  loss:  0.2209775149822235
Batch  61  loss:  0.22061973810195923
Batch  71  loss:  0.22181756794452667
Batch  81  loss:  0.21960598230361938
Batch  91  loss:  0.22639918327331543
Validation on real data: 
LOSS supervised-train 0.2223253908753395, valid 0.22249087691307068
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.21818780899047852
Batch  11  loss:  0.22265572845935822
Batch  21  loss:  0.21857617795467377
Batch  31  loss:  0.2113010585308075
Batch  41  loss:  0.22644563019275665
Batch  51  loss:  0.21764124929904938
Batch  61  loss:  0.22079676389694214
Batch  71  loss:  0.21779389679431915
Batch  81  loss:  0.2197020798921585
Batch  91  loss:  0.22391454875469208
Validation on real data: 
LOSS supervised-train 0.2216463293135166, valid 0.22838428616523743
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.22136838734149933
Batch  11  loss:  0.22185976803302765
Batch  21  loss:  0.21930177509784698
Batch  31  loss:  0.21189594268798828
Batch  41  loss:  0.22714963555335999
Batch  51  loss:  0.2172510027885437
Batch  61  loss:  0.2169908732175827
Batch  71  loss:  0.21897286176681519
Batch  81  loss:  0.22082622349262238
Batch  91  loss:  0.22494305670261383
Validation on real data: 
LOSS supervised-train 0.22118395701050758, valid 0.21908219158649445
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.2170579731464386
Batch  11  loss:  0.22207726538181305
Batch  21  loss:  0.21489009261131287
Batch  31  loss:  0.21009254455566406
Batch  41  loss:  0.22969651222229004
Batch  51  loss:  0.21446190774440765
Batch  61  loss:  0.21561358869075775
Batch  71  loss:  0.21780912578105927
Batch  81  loss:  0.2181258201599121
Batch  91  loss:  0.22672103345394135
Validation on real data: 
LOSS supervised-train 0.22047747537493706, valid 0.22419388592243195
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.21726243197917938
Batch  11  loss:  0.21942248940467834
Batch  21  loss:  0.21681448817253113
Batch  31  loss:  0.20969946682453156
Batch  41  loss:  0.22395311295986176
Batch  51  loss:  0.21905578672885895
Batch  61  loss:  0.21281801164150238
Batch  71  loss:  0.21312199532985687
Batch  81  loss:  0.21492865681648254
Batch  91  loss:  0.22436325252056122
Validation on real data: 
LOSS supervised-train 0.21958581000566482, valid 0.22005121409893036
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.21651893854141235
Batch  11  loss:  0.2188277244567871
Batch  21  loss:  0.2097247689962387
Batch  31  loss:  0.20448823273181915
Batch  41  loss:  0.22430317103862762
Batch  51  loss:  0.2185816913843155
Batch  61  loss:  0.21186977624893188
Batch  71  loss:  0.20735830068588257
Batch  81  loss:  0.2154410183429718
Batch  91  loss:  0.22476652264595032
Validation on real data: 
LOSS supervised-train 0.21835757315158844, valid 0.2252054512500763
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.21296530961990356
Batch  11  loss:  0.21583659946918488
Batch  21  loss:  0.20577464997768402
Batch  31  loss:  0.21369215846061707
Batch  41  loss:  0.2299359142780304
Batch  51  loss:  0.21743369102478027
Batch  61  loss:  0.20714589953422546
Batch  71  loss:  0.20019549131393433
Batch  81  loss:  0.2088637501001358
Batch  91  loss:  0.22400031983852386
Validation on real data: 
LOSS supervised-train 0.21607750326395034, valid 0.21802596747875214
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.21292588114738464
Batch  11  loss:  0.2098046988248825
Batch  21  loss:  0.20050369203090668
Batch  31  loss:  0.21381354331970215
Batch  41  loss:  0.2302420735359192
Batch  51  loss:  0.21766896545886993
Batch  61  loss:  0.20756176114082336
Batch  71  loss:  0.19786874949932098
Batch  81  loss:  0.2040296494960785
Batch  91  loss:  0.22498705983161926
Validation on real data: 
LOSS supervised-train 0.21503303363919257, valid 0.21496745944023132
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.21075333654880524
Batch  11  loss:  0.2011847048997879
Batch  21  loss:  0.18969647586345673
Batch  31  loss:  0.2033734917640686
Batch  41  loss:  0.22653281688690186
Batch  51  loss:  0.2112671583890915
Batch  61  loss:  0.2102639377117157
Batch  71  loss:  0.1961069405078888
Batch  81  loss:  0.19987571239471436
Batch  91  loss:  0.229848250746727
Validation on real data: 
LOSS supervised-train 0.21069726049900056, valid 0.22122512757778168
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.2087889462709427
Batch  11  loss:  0.20361731946468353
Batch  21  loss:  0.1956445276737213
Batch  31  loss:  0.2396416813135147
Batch  41  loss:  0.22448915243148804
Batch  51  loss:  0.21103255450725555
Batch  61  loss:  0.2193211317062378
Batch  71  loss:  0.1851990818977356
Batch  81  loss:  0.1864604502916336
Batch  91  loss:  0.24579577147960663
Validation on real data: 
LOSS supervised-train 0.21074164509773255, valid 0.21469677984714508
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.21029037237167358
Batch  11  loss:  0.20349164307117462
Batch  21  loss:  0.19991181790828705
Batch  31  loss:  0.21852025389671326
Batch  41  loss:  0.23585976660251617
Batch  51  loss:  0.20343463122844696
Batch  61  loss:  0.20571206510066986
Batch  71  loss:  0.1736120730638504
Batch  81  loss:  0.1893738955259323
Batch  91  loss:  0.23370760679244995
Validation on real data: 
LOSS supervised-train 0.20837580278515816, valid 0.19099068641662598
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.21732169389724731
Batch  11  loss:  0.19842952489852905
Batch  21  loss:  0.17518733441829681
Batch  31  loss:  0.19736187160015106
Batch  41  loss:  0.22343726456165314
Batch  51  loss:  0.2090226262807846
Batch  61  loss:  0.21587000787258148
Batch  71  loss:  0.1712000072002411
Batch  81  loss:  0.17807330191135406
Batch  91  loss:  0.24848270416259766
Validation on real data: 
LOSS supervised-train 0.20570872262120246, valid 0.20733365416526794
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.22827407717704773
Batch  11  loss:  0.20237594842910767
Batch  21  loss:  0.19205842912197113
Batch  31  loss:  0.20338180661201477
Batch  41  loss:  0.21441268920898438
Batch  51  loss:  0.20248092710971832
Batch  61  loss:  0.20618626475334167
Batch  71  loss:  0.1602526754140854
Batch  81  loss:  0.17956353724002838
Batch  91  loss:  0.23945823311805725
Validation on real data: 
LOSS supervised-train 0.20327206298708916, valid 0.20013202726840973
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.2063296139240265
Batch  11  loss:  0.19513092935085297
Batch  21  loss:  0.185240238904953
Batch  31  loss:  0.22327500581741333
Batch  41  loss:  0.21691830456256866
Batch  51  loss:  0.21782276034355164
Batch  61  loss:  0.19936101138591766
Batch  71  loss:  0.1918189376592636
Batch  81  loss:  0.18267250061035156
Batch  91  loss:  0.24403156340122223
Validation on real data: 
LOSS supervised-train 0.206198593378067, valid 0.1974906623363495
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.20169581472873688
Batch  11  loss:  0.19330066442489624
Batch  21  loss:  0.1782849133014679
Batch  31  loss:  0.20896762609481812
Batch  41  loss:  0.2101268172264099
Batch  51  loss:  0.18999743461608887
Batch  61  loss:  0.1958564966917038
Batch  71  loss:  0.18161262571811676
Batch  81  loss:  0.17786403000354767
Batch  91  loss:  0.2431482970714569
Validation on real data: 
LOSS supervised-train 0.19964719370007514, valid 0.23073218762874603
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.18938224017620087
Batch  11  loss:  0.1949423998594284
Batch  21  loss:  0.18809419870376587
Batch  31  loss:  0.18900996446609497
Batch  41  loss:  0.20833046734333038
Batch  51  loss:  0.20365096628665924
Batch  61  loss:  0.18585674464702606
Batch  71  loss:  0.17920814454555511
Batch  81  loss:  0.16727420687675476
Batch  91  loss:  0.2335338145494461
Validation on real data: 
LOSS supervised-train 0.19652601450681687, valid 0.23391574621200562
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.1789660006761551
Batch  11  loss:  0.19974826276302338
Batch  21  loss:  0.17596259713172913
Batch  31  loss:  0.18955545127391815
Batch  41  loss:  0.1878298968076706
Batch  51  loss:  0.18950581550598145
Batch  61  loss:  0.1867363303899765
Batch  71  loss:  0.18002934753894806
Batch  81  loss:  0.16301539540290833
Batch  91  loss:  0.231375589966774
Validation on real data: 
LOSS supervised-train 0.19629248067736627, valid 0.22773002088069916
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.1804170161485672
Batch  11  loss:  0.17226916551589966
Batch  21  loss:  0.20411944389343262
Batch  31  loss:  0.16783376038074493
Batch  41  loss:  0.1853034794330597
Batch  51  loss:  0.18382743000984192
Batch  61  loss:  0.19455787539482117
Batch  71  loss:  0.17376112937927246
Batch  81  loss:  0.16090072691440582
Batch  91  loss:  0.2182641625404358
Validation on real data: 
LOSS supervised-train 0.19166741788387298, valid 0.24835588037967682
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.16982926428318024
Batch  11  loss:  0.16978591680526733
Batch  21  loss:  0.1543419063091278
Batch  31  loss:  0.18144536018371582
Batch  41  loss:  0.18569819629192352
Batch  51  loss:  0.193495512008667
Batch  61  loss:  0.20395156741142273
Batch  71  loss:  0.1672755777835846
Batch  81  loss:  0.16816705465316772
Batch  91  loss:  0.22670844197273254
Validation on real data: 
LOSS supervised-train 0.18870551034808158, valid 0.2419636845588684
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.17635321617126465
Batch  11  loss:  0.1724473536014557
Batch  21  loss:  0.16814616322517395
Batch  31  loss:  0.16878516972064972
Batch  41  loss:  0.1793593317270279
Batch  51  loss:  0.19538429379463196
Batch  61  loss:  0.18012595176696777
Batch  71  loss:  0.1663043200969696
Batch  81  loss:  0.17829887568950653
Batch  91  loss:  0.21572209894657135
Validation on real data: 
LOSS supervised-train 0.1855102777481079, valid 0.2013293355703354
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.14650923013687134
Batch  11  loss:  0.158645361661911
Batch  21  loss:  0.1645885854959488
Batch  31  loss:  0.19012773036956787
Batch  41  loss:  0.1807461529970169
Batch  51  loss:  0.19372504949569702
Batch  61  loss:  0.18797050416469574
Batch  71  loss:  0.14996439218521118
Batch  81  loss:  0.1684006154537201
Batch  91  loss:  0.22928176820278168
Validation on real data: 
LOSS supervised-train 0.1833215108513832, valid 0.22266170382499695
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.15680061280727386
Batch  11  loss:  0.18984182178974152
Batch  21  loss:  0.16275419294834137
Batch  31  loss:  0.17492260038852692
Batch  41  loss:  0.17849181592464447
Batch  51  loss:  0.17878369987010956
Batch  61  loss:  0.17828315496444702
Batch  71  loss:  0.1932879537343979
Batch  81  loss:  0.13796018064022064
Batch  91  loss:  0.22606448829174042
Validation on real data: 
LOSS supervised-train 0.18142018765211104, valid 0.21127226948738098
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.14389538764953613
Batch  11  loss:  0.1764248162508011
Batch  21  loss:  0.17473575472831726
Batch  31  loss:  0.15760332345962524
Batch  41  loss:  0.17477501928806305
Batch  51  loss:  0.19062818586826324
Batch  61  loss:  0.1727731078863144
Batch  71  loss:  0.19125618040561676
Batch  81  loss:  0.13750071823596954
Batch  91  loss:  0.21950680017471313
Validation on real data: 
LOSS supervised-train 0.17837518200278282, valid 0.220847025513649
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.17773063480854034
Batch  11  loss:  0.1691979169845581
Batch  21  loss:  0.18125107884407043
Batch  31  loss:  0.16439712047576904
Batch  41  loss:  0.1667346954345703
Batch  51  loss:  0.20323550701141357
Batch  61  loss:  0.2022426873445511
Batch  71  loss:  0.15915873646736145
Batch  81  loss:  0.17805583775043488
Batch  91  loss:  0.21854941546916962
Validation on real data: 
LOSS supervised-train 0.17988399162888527, valid 0.26564085483551025
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.16678690910339355
Batch  11  loss:  0.16905206441879272
Batch  21  loss:  0.16039645671844482
Batch  31  loss:  0.17079101502895355
Batch  41  loss:  0.16661350429058075
Batch  51  loss:  0.17014144361019135
Batch  61  loss:  0.15379609167575836
Batch  71  loss:  0.15730206668376923
Batch  81  loss:  0.1559225618839264
Batch  91  loss:  0.22591695189476013
Validation on real data: 
LOSS supervised-train 0.17301248781383038, valid 0.22924643754959106
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.16800948977470398
Batch  11  loss:  0.1612555980682373
Batch  21  loss:  0.17379578948020935
Batch  31  loss:  0.17585566639900208
Batch  41  loss:  0.1449851542711258
Batch  51  loss:  0.16031847894191742
Batch  61  loss:  0.16533811390399933
Batch  71  loss:  0.14484485983848572
Batch  81  loss:  0.1467820405960083
Batch  91  loss:  0.19267131388187408
Validation on real data: 
LOSS supervised-train 0.17074480563402175, valid 0.251913458108902
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.1568930447101593
Batch  11  loss:  0.17185592651367188
Batch  21  loss:  0.13821470737457275
Batch  31  loss:  0.1683518886566162
Batch  41  loss:  0.128061905503273
Batch  51  loss:  0.15964357554912567
Batch  61  loss:  0.16931813955307007
Batch  71  loss:  0.12616683542728424
Batch  81  loss:  0.1398375928401947
Batch  91  loss:  0.16275329887866974
Validation on real data: 
LOSS supervised-train 0.1626374913007021, valid 0.23730750381946564
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.1699230968952179
Batch  11  loss:  0.15943507850170135
Batch  21  loss:  0.15333573520183563
Batch  31  loss:  0.16957060992717743
Batch  41  loss:  0.12935484945774078
Batch  51  loss:  0.18064309656620026
Batch  61  loss:  0.16192016005516052
Batch  71  loss:  0.14564183354377747
Batch  81  loss:  0.14893245697021484
Batch  91  loss:  0.17516610026359558
Validation on real data: 
LOSS supervised-train 0.16006090126931669, valid 0.2021896243095398
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.18157631158828735
Batch  11  loss:  0.14139430224895477
Batch  21  loss:  0.16239608824253082
Batch  31  loss:  0.16623732447624207
Batch  41  loss:  0.13054737448692322
Batch  51  loss:  0.18184466660022736
Batch  61  loss:  0.16417478024959564
Batch  71  loss:  0.12041399627923965
Batch  81  loss:  0.15084916353225708
Batch  91  loss:  0.17402473092079163
Validation on real data: 
LOSS supervised-train 0.16089742690324782, valid 0.20669668912887573
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.13060681521892548
Batch  11  loss:  0.15609504282474518
Batch  21  loss:  0.15236946940422058
Batch  31  loss:  0.12532171607017517
Batch  41  loss:  0.14541512727737427
Batch  51  loss:  0.16198860108852386
Batch  61  loss:  0.16351310908794403
Batch  71  loss:  0.11802882701158524
Batch  81  loss:  0.14098991453647614
Batch  91  loss:  0.17963947355747223
Validation on real data: 
LOSS supervised-train 0.16012616634368895, valid 0.24333599209785461
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.12367406487464905
Batch  11  loss:  0.13323195278644562
Batch  21  loss:  0.17299999296665192
Batch  31  loss:  0.17108093202114105
Batch  41  loss:  0.1633896380662918
Batch  51  loss:  0.16813218593597412
Batch  61  loss:  0.18130305409431458
Batch  71  loss:  0.13204757869243622
Batch  81  loss:  0.13256332278251648
Batch  91  loss:  0.18632332980632782
Validation on real data: 
LOSS supervised-train 0.15711586199700833, valid 0.22140803933143616
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.11020582169294357
Batch  11  loss:  0.13570073246955872
Batch  21  loss:  0.13628806173801422
Batch  31  loss:  0.1405494213104248
Batch  41  loss:  0.12961961328983307
Batch  51  loss:  0.17722506821155548
Batch  61  loss:  0.15240898728370667
Batch  71  loss:  0.1197042316198349
Batch  81  loss:  0.15137383341789246
Batch  91  loss:  0.18013034760951996
Validation on real data: 
LOSS supervised-train 0.14837445363402366, valid 0.2518584430217743
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.12360162287950516
Batch  11  loss:  0.17923511564731598
Batch  21  loss:  0.1319625973701477
Batch  31  loss:  0.1311289370059967
Batch  41  loss:  0.1102144792675972
Batch  51  loss:  0.1640009582042694
Batch  61  loss:  0.15506672859191895
Batch  71  loss:  0.15940402448177338
Batch  81  loss:  0.14151276648044586
Batch  91  loss:  0.1821945160627365
Validation on real data: 
LOSS supervised-train 0.1434076176583767, valid 0.2874155640602112
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.12142389267683029
Batch  11  loss:  0.13158102333545685
Batch  21  loss:  0.13371241092681885
Batch  31  loss:  0.1148192286491394
Batch  41  loss:  0.13339971005916595
Batch  51  loss:  0.16980502009391785
Batch  61  loss:  0.1491144746541977
Batch  71  loss:  0.09926574677228928
Batch  81  loss:  0.13889816403388977
Batch  91  loss:  0.17929521203041077
Validation on real data: 
LOSS supervised-train 0.13954223573207855, valid 0.3100919723510742
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.09106869250535965
Batch  11  loss:  0.13304577767848969
Batch  21  loss:  0.12956270575523376
Batch  31  loss:  0.12584583461284637
Batch  41  loss:  0.19068127870559692
Batch  51  loss:  0.14901715517044067
Batch  61  loss:  0.11603612452745438
Batch  71  loss:  0.15451498329639435
Batch  81  loss:  0.13529466092586517
Batch  91  loss:  0.1704866886138916
Validation on real data: 
LOSS supervised-train 0.1434729965031147, valid 0.24250496923923492
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0893581286072731
Batch  11  loss:  0.13610424101352692
Batch  21  loss:  0.13887356221675873
Batch  31  loss:  0.134263277053833
Batch  41  loss:  0.14078594744205475
Batch  51  loss:  0.14384779334068298
Batch  61  loss:  0.13489766418933868
Batch  71  loss:  0.13327094912528992
Batch  81  loss:  0.14586935937404633
Batch  91  loss:  0.1902017593383789
Validation on real data: 
LOSS supervised-train 0.14410507217049598, valid 0.22908973693847656
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.11388653516769409
Batch  11  loss:  0.15100201964378357
Batch  21  loss:  0.13573408126831055
Batch  31  loss:  0.1032317504286766
Batch  41  loss:  0.1190994456410408
Batch  51  loss:  0.18195666372776031
Batch  61  loss:  0.1264008730649948
Batch  71  loss:  0.11675798147916794
Batch  81  loss:  0.13460592925548553
Batch  91  loss:  0.15556837618350983
Validation on real data: 
LOSS supervised-train 0.13184826612472533, valid 0.3339710235595703
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.08480849862098694
Batch  11  loss:  0.14545226097106934
Batch  21  loss:  0.14057116210460663
Batch  31  loss:  0.105495385825634
Batch  41  loss:  0.1059935912489891
Batch  51  loss:  0.1484309434890747
Batch  61  loss:  0.1159881204366684
Batch  71  loss:  0.0934164822101593
Batch  81  loss:  0.15567848086357117
Batch  91  loss:  0.16025592386722565
Validation on real data: 
LOSS supervised-train 0.12396011412143708, valid 0.2756364941596985
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.09434423595666885
Batch  11  loss:  0.0965413823723793
Batch  21  loss:  0.11296669393777847
Batch  31  loss:  0.14296771585941315
Batch  41  loss:  0.1467280387878418
Batch  51  loss:  0.15572427213191986
Batch  61  loss:  0.13158144056797028
Batch  71  loss:  0.12075507640838623
Batch  81  loss:  0.13228562474250793
Batch  91  loss:  0.14569585025310516
Validation on real data: 
LOSS supervised-train 0.13426595263183116, valid 0.26771605014801025
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.10514051467180252
Batch  11  loss:  0.11269716918468475
Batch  21  loss:  0.12428363412618637
Batch  31  loss:  0.11941350996494293
Batch  41  loss:  0.11668111383914948
Batch  51  loss:  0.16357852518558502
Batch  61  loss:  0.1368349939584732
Batch  71  loss:  0.12193746119737625
Batch  81  loss:  0.12008018046617508
Batch  91  loss:  0.12978266179561615
Validation on real data: 
LOSS supervised-train 0.12720671951770782, valid 0.30270305275917053
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.08660191297531128
Batch  11  loss:  0.09077794849872589
Batch  21  loss:  0.11611690372228622
Batch  31  loss:  0.12434987723827362
Batch  41  loss:  0.11869999021291733
Batch  51  loss:  0.16401657462120056
Batch  61  loss:  0.1259925812482834
Batch  71  loss:  0.09042438119649887
Batch  81  loss:  0.12975861132144928
Batch  91  loss:  0.14823928475379944
Validation on real data: 
LOSS supervised-train 0.11862176716327667, valid 0.21306884288787842
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.07997120171785355
Batch  11  loss:  0.09202457964420319
Batch  21  loss:  0.10378125309944153
Batch  31  loss:  0.11257791519165039
Batch  41  loss:  0.1324767917394638
Batch  51  loss:  0.12472590804100037
Batch  61  loss:  0.11044105887413025
Batch  71  loss:  0.10272937268018723
Batch  81  loss:  0.12085142731666565
Batch  91  loss:  0.13577528297901154
Validation on real data: 
LOSS supervised-train 0.11555802889168262, valid 0.27151229977607727
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.07281308621168137
Batch  11  loss:  0.13646537065505981
Batch  21  loss:  0.08993459492921829
Batch  31  loss:  0.07910189777612686
Batch  41  loss:  0.0827324390411377
Batch  51  loss:  0.14361798763275146
Batch  61  loss:  0.11763307452201843
Batch  71  loss:  0.05545822158455849
Batch  81  loss:  0.11526644229888916
Batch  91  loss:  0.11343637108802795
Validation on real data: 
LOSS supervised-train 0.10633102733641862, valid 0.2643526792526245
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.07266969978809357
Batch  11  loss:  0.0742650255560875
Batch  21  loss:  0.11384119838476181
Batch  31  loss:  0.10363075137138367
Batch  41  loss:  0.08896418660879135
Batch  51  loss:  0.10874919593334198
Batch  61  loss:  0.10591614246368408
Batch  71  loss:  0.05397333577275276
Batch  81  loss:  0.10202889889478683
Batch  91  loss:  0.09805095195770264
Validation on real data: 
LOSS supervised-train 0.10446287836879492, valid 0.21539153158664703
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.09873752295970917
Batch  11  loss:  0.07268822193145752
Batch  21  loss:  0.1196078285574913
Batch  31  loss:  0.13107258081436157
Batch  41  loss:  0.12674303352832794
Batch  51  loss:  0.14074452221393585
Batch  61  loss:  0.07603784650564194
Batch  71  loss:  0.06269913166761398
Batch  81  loss:  0.09492402523756027
Batch  91  loss:  0.12230178713798523
Validation on real data: 
LOSS supervised-train 0.10231133926659823, valid 0.2031072974205017
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.05615333467721939
Batch  11  loss:  0.102889783680439
Batch  21  loss:  0.10019917041063309
Batch  31  loss:  0.11878897994756699
Batch  41  loss:  0.08955211192369461
Batch  51  loss:  0.12681294977664948
Batch  61  loss:  0.0981815904378891
Batch  71  loss:  0.07351996004581451
Batch  81  loss:  0.09861156344413757
Batch  91  loss:  0.09787123650312424
Validation on real data: 
LOSS supervised-train 0.10438436694443226, valid 0.22427015006542206
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.055206917226314545
Batch  11  loss:  0.0711105540394783
Batch  21  loss:  0.07498034834861755
Batch  31  loss:  0.09279334545135498
Batch  41  loss:  0.08235989511013031
Batch  51  loss:  0.11821040511131287
Batch  61  loss:  0.0842776671051979
Batch  71  loss:  0.08636949956417084
Batch  81  loss:  0.09499717503786087
Batch  91  loss:  0.1258641928434372
Validation on real data: 
LOSS supervised-train 0.10118639253079892, valid 0.24220909178256989
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.06051674857735634
Batch  11  loss:  0.10117126256227493
Batch  21  loss:  0.10435949265956879
Batch  31  loss:  0.10813352465629578
Batch  41  loss:  0.1102815568447113
Batch  51  loss:  0.1443099081516266
Batch  61  loss:  0.1365099400281906
Batch  71  loss:  0.07656694948673248
Batch  81  loss:  0.0940878689289093
Batch  91  loss:  0.11460806429386139
Validation on real data: 
LOSS supervised-train 0.10953132905066014, valid 0.3337618112564087
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.12217087298631668
Batch  11  loss:  0.1157076433300972
Batch  21  loss:  0.09242884069681168
Batch  31  loss:  0.12676027417182922
Batch  41  loss:  0.14065033197402954
Batch  51  loss:  0.11216333508491516
Batch  61  loss:  0.1090574786067009
Batch  71  loss:  0.07818446308374405
Batch  81  loss:  0.0960644856095314
Batch  91  loss:  0.12984907627105713
Validation on real data: 
LOSS supervised-train 0.10561652015894651, valid 0.27515968680381775
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.05034702271223068
Batch  11  loss:  0.07562290132045746
Batch  21  loss:  0.08049437403678894
Batch  31  loss:  0.11814349889755249
Batch  41  loss:  0.1320420503616333
Batch  51  loss:  0.10675045102834702
Batch  61  loss:  0.11684497445821762
Batch  71  loss:  0.10693445056676865
Batch  81  loss:  0.1082497388124466
Batch  91  loss:  0.11038725823163986
Validation on real data: 
LOSS supervised-train 0.09807615265250207, valid 0.23750609159469604
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.05180585756897926
Batch  11  loss:  0.06699160486459732
Batch  21  loss:  0.08817096799612045
Batch  31  loss:  0.1094178557395935
Batch  41  loss:  0.1464066058397293
Batch  51  loss:  0.09742461889982224
Batch  61  loss:  0.11113603413105011
Batch  71  loss:  0.09968654066324234
Batch  81  loss:  0.11178034543991089
Batch  91  loss:  0.13679489493370056
Validation on real data: 
LOSS supervised-train 0.09973070610314608, valid 0.2512114346027374
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.06793253123760223
Batch  11  loss:  0.07373218238353729
Batch  21  loss:  0.10246976464986801
Batch  31  loss:  0.13335385918617249
Batch  41  loss:  0.12422296404838562
Batch  51  loss:  0.08787865191698074
Batch  61  loss:  0.09051167219877243
Batch  71  loss:  0.06047392636537552
Batch  81  loss:  0.06893309205770493
Batch  91  loss:  0.10848594456911087
Validation on real data: 
LOSS supervised-train 0.09209821470081807, valid 0.268047034740448
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.07024740427732468
Batch  11  loss:  0.10298071801662445
Batch  21  loss:  0.08929640799760818
Batch  31  loss:  0.1272997111082077
Batch  41  loss:  0.11371913552284241
Batch  51  loss:  0.09951427578926086
Batch  61  loss:  0.06060992553830147
Batch  71  loss:  0.08045133203268051
Batch  81  loss:  0.1139509379863739
Batch  91  loss:  0.08769643306732178
Validation on real data: 
LOSS supervised-train 0.09079220414161682, valid 0.3714523911476135
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.11207389831542969
Batch  11  loss:  0.06967844814062119
Batch  21  loss:  0.07809240370988846
Batch  31  loss:  0.07619404047727585
Batch  41  loss:  0.08336269855499268
Batch  51  loss:  0.1154201477766037
Batch  61  loss:  0.07225687801837921
Batch  71  loss:  0.06250917911529541
Batch  81  loss:  0.08000104874372482
Batch  91  loss:  0.08098926395177841
Validation on real data: 
LOSS supervised-train 0.08502706356346607, valid 0.3545551598072052
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.07333465665578842
Batch  11  loss:  0.07561971247196198
Batch  21  loss:  0.07483639568090439
Batch  31  loss:  0.07850894331932068
Batch  41  loss:  0.08009322732686996
Batch  51  loss:  0.08958223462104797
Batch  61  loss:  0.05468195676803589
Batch  71  loss:  0.04916907101869583
Batch  81  loss:  0.06721530109643936
Batch  91  loss:  0.09792736917734146
Validation on real data: 
LOSS supervised-train 0.07837582375854253, valid 0.18275175988674164
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.07179082185029984
Batch  11  loss:  0.03724958747625351
Batch  21  loss:  0.08475415408611298
Batch  31  loss:  0.08175162225961685
Batch  41  loss:  0.07113370299339294
Batch  51  loss:  0.07631998509168625
Batch  61  loss:  0.07363028079271317
Batch  71  loss:  0.05647439509630203
Batch  81  loss:  0.07078742235898972
Batch  91  loss:  0.08263161778450012
Validation on real data: 
LOSS supervised-train 0.07722601097077131, valid 0.36008304357528687
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.05318174138665199
Batch  11  loss:  0.0585857555270195
Batch  21  loss:  0.04641129449009895
Batch  31  loss:  0.06872408837080002
Batch  41  loss:  0.09653919190168381
Batch  51  loss:  0.09353696554899216
Batch  61  loss:  0.07327743619680405
Batch  71  loss:  0.08701697736978531
Batch  81  loss:  0.07413264364004135
Batch  91  loss:  0.08111103624105453
Validation on real data: 
LOSS supervised-train 0.0841321786120534, valid 0.2532770335674286
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.08013762533664703
Batch  11  loss:  0.07665083557367325
Batch  21  loss:  0.054861173033714294
Batch  31  loss:  0.07626921683549881
Batch  41  loss:  0.07678323984146118
Batch  51  loss:  0.09432952851057053
Batch  61  loss:  0.07269518822431564
Batch  71  loss:  0.07992811501026154
Batch  81  loss:  0.06317738443613052
Batch  91  loss:  0.10286334156990051
Validation on real data: 
LOSS supervised-train 0.0854051698744297, valid 0.29081377387046814
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.057805269956588745
Batch  11  loss:  0.07834641635417938
Batch  21  loss:  0.04683132469654083
Batch  31  loss:  0.10437260568141937
Batch  41  loss:  0.09238415211439133
Batch  51  loss:  0.08635188639163971
Batch  61  loss:  0.07224318385124207
Batch  71  loss:  0.06252143532037735
Batch  81  loss:  0.11098285764455795
Batch  91  loss:  0.11001598089933395
Validation on real data: 
LOSS supervised-train 0.08721945289522409, valid 0.2888946235179901
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.06522803753614426
Batch  11  loss:  0.08697376400232315
Batch  21  loss:  0.06520497053861618
Batch  31  loss:  0.07255063205957413
Batch  41  loss:  0.10334973782300949
Batch  51  loss:  0.07533258199691772
Batch  61  loss:  0.07134842872619629
Batch  71  loss:  0.05272413790225983
Batch  81  loss:  0.09550821036100388
Batch  91  loss:  0.08815698325634003
Validation on real data: 
LOSS supervised-train 0.08272408921271562, valid 0.37107527256011963
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.06454482674598694
Batch  11  loss:  0.04363473877310753
Batch  21  loss:  0.059369392693042755
Batch  31  loss:  0.07831721752882004
Batch  41  loss:  0.05780801177024841
Batch  51  loss:  0.08119874447584152
Batch  61  loss:  0.07183883339166641
Batch  71  loss:  0.06344954669475555
Batch  81  loss:  0.06910889595746994
Batch  91  loss:  0.07126550376415253
Validation on real data: 
LOSS supervised-train 0.074058520719409, valid 0.2526828646659851
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.05213681235909462
Batch  11  loss:  0.07526597380638123
Batch  21  loss:  0.07079112529754639
Batch  31  loss:  0.06891759485006332
Batch  41  loss:  0.05571909248828888
Batch  51  loss:  0.05940999835729599
Batch  61  loss:  0.07570885866880417
Batch  71  loss:  0.05078379064798355
Batch  81  loss:  0.12366078794002533
Batch  91  loss:  0.06466032564640045
Validation on real data: 
LOSS supervised-train 0.07683114051818847, valid 0.2105213850736618
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.08353469520807266
Batch  11  loss:  0.0828866958618164
Batch  21  loss:  0.057543106377124786
Batch  31  loss:  0.10608189553022385
Batch  41  loss:  0.06925206631422043
Batch  51  loss:  0.089251808822155
Batch  61  loss:  0.08877156674861908
Batch  71  loss:  0.07542318850755692
Batch  81  loss:  0.10499097406864166
Batch  91  loss:  0.10224021971225739
Validation on real data: 
LOSS supervised-train 0.08510532699525357, valid 0.2120952159166336
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.06391298770904541
Batch  11  loss:  0.07078444212675095
Batch  21  loss:  0.05788279324769974
Batch  31  loss:  0.08478529006242752
Batch  41  loss:  0.056937143206596375
Batch  51  loss:  0.06725020706653595
Batch  61  loss:  0.0713917538523674
Batch  71  loss:  0.05285409837961197
Batch  81  loss:  0.06591571122407913
Batch  91  loss:  0.08483649790287018
Validation on real data: 
LOSS supervised-train 0.0696721913293004, valid 0.22911719977855682
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.058007847517728806
Batch  11  loss:  0.08973952382802963
Batch  21  loss:  0.05480576306581497
Batch  31  loss:  0.06782922893762589
Batch  41  loss:  0.06149567291140556
Batch  51  loss:  0.06839173287153244
Batch  61  loss:  0.05207252502441406
Batch  71  loss:  0.04279821738600731
Batch  81  loss:  0.06113269552588463
Batch  91  loss:  0.03832099214196205
Validation on real data: 
LOSS supervised-train 0.0666309467703104, valid 0.32078802585601807
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.036121368408203125
Batch  11  loss:  0.05619407445192337
Batch  21  loss:  0.06225888058543205
Batch  31  loss:  0.07512357831001282
Batch  41  loss:  0.08402590453624725
Batch  51  loss:  0.08017159253358841
Batch  61  loss:  0.07026012986898422
Batch  71  loss:  0.05726322904229164
Batch  81  loss:  0.09562952071428299
Batch  91  loss:  0.06397432833909988
Validation on real data: 
LOSS supervised-train 0.06939702317118644, valid 0.21949626505374908
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.07182621955871582
Batch  11  loss:  0.08569880574941635
Batch  21  loss:  0.07405944913625717
Batch  31  loss:  0.08132804930210114
Batch  41  loss:  0.0824315994977951
Batch  51  loss:  0.06682392209768295
Batch  61  loss:  0.09972928464412689
Batch  71  loss:  0.035768069326877594
Batch  81  loss:  0.05132588371634483
Batch  91  loss:  0.07013523578643799
Validation on real data: 
LOSS supervised-train 0.07145566057413816, valid 0.1285482496023178
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.050845205783843994
Batch  11  loss:  0.04535842686891556
Batch  21  loss:  0.057431094348430634
Batch  31  loss:  0.07333289831876755
Batch  41  loss:  0.07885021716356277
Batch  51  loss:  0.054789502173662186
Batch  61  loss:  0.06125493347644806
Batch  71  loss:  0.052117008715867996
Batch  81  loss:  0.11206086724996567
Batch  91  loss:  0.059991538524627686
Validation on real data: 
LOSS supervised-train 0.0689748907275498, valid 0.2913063168525696
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.058561041951179504
Batch  11  loss:  0.09775875508785248
Batch  21  loss:  0.08826052397489548
Batch  31  loss:  0.12065520882606506
Batch  41  loss:  0.06696886569261551
Batch  51  loss:  0.04963827133178711
Batch  61  loss:  0.06291227787733078
Batch  71  loss:  0.06113269552588463
Batch  81  loss:  0.05222761631011963
Batch  91  loss:  0.06979815661907196
Validation on real data: 
LOSS supervised-train 0.07814491409808397, valid 0.3067258894443512
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.06475387513637543
Batch  11  loss:  0.07305219024419785
Batch  21  loss:  0.0608135387301445
Batch  31  loss:  0.07362314313650131
Batch  41  loss:  0.06801819056272507
Batch  51  loss:  0.06591936200857162
Batch  61  loss:  0.05991921201348305
Batch  71  loss:  0.04646468907594681
Batch  81  loss:  0.06131942570209503
Batch  91  loss:  0.04718315601348877
Validation on real data: 
LOSS supervised-train 0.06359557457268238, valid 0.42027708888053894
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.060088150203228
Batch  11  loss:  0.047444477677345276
Batch  21  loss:  0.02565380558371544
Batch  31  loss:  0.06351833790540695
Batch  41  loss:  0.06541402637958527
Batch  51  loss:  0.04731336608529091
Batch  61  loss:  0.04896415024995804
Batch  71  loss:  0.03505159914493561
Batch  81  loss:  0.05034129321575165
Batch  91  loss:  0.05167819559574127
Validation on real data: 
LOSS supervised-train 0.05262306312099099, valid 0.27432775497436523
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0693143978714943
Batch  11  loss:  0.0387689583003521
Batch  21  loss:  0.04790619760751724
Batch  31  loss:  0.05417580157518387
Batch  41  loss:  0.07472825050354004
Batch  51  loss:  0.056610845029354095
Batch  61  loss:  0.06931788474321365
Batch  71  loss:  0.06707395613193512
Batch  81  loss:  0.05063464865088463
Batch  91  loss:  0.0644005537033081
Validation on real data: 
LOSS supervised-train 0.05802801033481955, valid 0.22327035665512085
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.04812673479318619
Batch  11  loss:  0.03314560279250145
Batch  21  loss:  0.03836045041680336
Batch  31  loss:  0.07955723255872726
Batch  41  loss:  0.0466170571744442
Batch  51  loss:  0.05115616321563721
Batch  61  loss:  0.05621280148625374
Batch  71  loss:  0.031316712498664856
Batch  81  loss:  0.05434917286038399
Batch  91  loss:  0.07112204283475876
Validation on real data: 
LOSS supervised-train 0.055162456780672074, valid 0.27595025300979614
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.032657966017723083
Batch  11  loss:  0.07178244739770889
Batch  21  loss:  0.033737652003765106
Batch  31  loss:  0.10089622437953949
Batch  41  loss:  0.06362497806549072
Batch  51  loss:  0.046658605337142944
Batch  61  loss:  0.049618594348430634
Batch  71  loss:  0.05149377882480621
Batch  81  loss:  0.03565983846783638
Batch  91  loss:  0.0432317778468132
Validation on real data: 
LOSS supervised-train 0.05332156455144286, valid 0.2286020666360855
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.049174223095178604
Batch  11  loss:  0.061305951327085495
Batch  21  loss:  0.03455250710248947
Batch  31  loss:  0.06124431639909744
Batch  41  loss:  0.0667911022901535
Batch  51  loss:  0.047600965946912766
Batch  61  loss:  0.05292978137731552
Batch  71  loss:  0.042463067919015884
Batch  81  loss:  0.07355283945798874
Batch  91  loss:  0.07509569823741913
Validation on real data: 
LOSS supervised-train 0.0518578477203846, valid 0.3769051730632782
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.031061625108122826
Batch  11  loss:  0.04398929327726364
Batch  21  loss:  0.04078465327620506
Batch  31  loss:  0.05989310145378113
Batch  41  loss:  0.037613797932863235
Batch  51  loss:  0.07458475232124329
Batch  61  loss:  0.05284278839826584
Batch  71  loss:  0.03520713746547699
Batch  81  loss:  0.058277249336242676
Batch  91  loss:  0.05000302568078041
Validation on real data: 
LOSS supervised-train 0.04777650952339172, valid 0.3215291500091553
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0468817874789238
Batch  11  loss:  0.030082115903496742
Batch  21  loss:  0.025568708777427673
Batch  31  loss:  0.04564962163567543
Batch  41  loss:  0.05838630720973015
Batch  51  loss:  0.06508103013038635
Batch  61  loss:  0.06330142170190811
Batch  71  loss:  0.04051075503230095
Batch  81  loss:  0.052054643630981445
Batch  91  loss:  0.05477839335799217
Validation on real data: 
LOSS supervised-train 0.047182074636220935, valid 0.3485807776451111
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.03828465938568115
Batch  11  loss:  0.0321049839258194
Batch  21  loss:  0.023757942020893097
Batch  31  loss:  0.12551915645599365
Batch  41  loss:  0.07868478447198868
Batch  51  loss:  0.07371684163808823
Batch  61  loss:  0.044508323073387146
Batch  71  loss:  0.030564311891794205
Batch  81  loss:  0.04964060336351395
Batch  91  loss:  0.05981873348355293
Validation on real data: 
LOSS supervised-train 0.05102185327559709, valid 0.31222954392433167
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.046759460121393204
Batch  11  loss:  0.044741202145814896
Batch  21  loss:  0.052586574107408524
Batch  31  loss:  0.046203721314668655
Batch  41  loss:  0.0383840836584568
Batch  51  loss:  0.054286450147628784
Batch  61  loss:  0.04240792989730835
Batch  71  loss:  0.02908443659543991
Batch  81  loss:  0.04219280928373337
Batch  91  loss:  0.06270650029182434
Validation on real data: 
LOSS supervised-train 0.048469952493906024, valid 0.3150668740272522
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.049938272684812546
Batch  11  loss:  0.056536417454481125
Batch  21  loss:  0.040371980518102646
Batch  31  loss:  0.06345006078481674
Batch  41  loss:  0.06806620210409164
Batch  51  loss:  0.0602237842977047
Batch  61  loss:  0.04668469354510307
Batch  71  loss:  0.052604712545871735
Batch  81  loss:  0.05100594833493233
Batch  91  loss:  0.045101746916770935
Validation on real data: 
LOSS supervised-train 0.051762590371072295, valid 0.27070334553718567
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.04208619147539139
Batch  11  loss:  0.05891890078783035
Batch  21  loss:  0.040694884955883026
Batch  31  loss:  0.04795868694782257
Batch  41  loss:  0.04616070166230202
Batch  51  loss:  0.05405197665095329
Batch  61  loss:  0.047810353338718414
Batch  71  loss:  0.03937394917011261
Batch  81  loss:  0.03244074806571007
Batch  91  loss:  0.04909973219037056
Validation on real data: 
LOSS supervised-train 0.04390066539868712, valid 0.3171289265155792
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0291504617780447
Batch  11  loss:  0.0396258570253849
Batch  21  loss:  0.038226425647735596
Batch  31  loss:  0.04748612269759178
Batch  41  loss:  0.04471099376678467
Batch  51  loss:  0.04603241756558418
Batch  61  loss:  0.048319995403289795
Batch  71  loss:  0.020910508930683136
Batch  81  loss:  0.029161909595131874
Batch  91  loss:  0.0526496060192585
Validation on real data: 
LOSS supervised-train 0.04530246358364821, valid 0.3340110182762146
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.044877078384160995
Batch  11  loss:  0.04172381013631821
Batch  21  loss:  0.028257476165890694
Batch  31  loss:  0.047607675194740295
Batch  41  loss:  0.034250929951667786
Batch  51  loss:  0.04709609970450401
Batch  61  loss:  0.037040580064058304
Batch  71  loss:  0.03093944489955902
Batch  81  loss:  0.04135707765817642
Batch  91  loss:  0.0421348512172699
Validation on real data: 
LOSS supervised-train 0.04546366702765226, valid 0.2155112773180008
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.04101419448852539
Batch  11  loss:  0.05312865972518921
Batch  21  loss:  0.04953720048069954
Batch  31  loss:  0.04837808012962341
Batch  41  loss:  0.05346421152353287
Batch  51  loss:  0.026676330715417862
Batch  61  loss:  0.03486468642950058
Batch  71  loss:  0.02847953327000141
Batch  81  loss:  0.030067291110754013
Batch  91  loss:  0.03250016272068024
Validation on real data: 
LOSS supervised-train 0.04387328708544373, valid 0.15364864468574524
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.028715357184410095
Batch  11  loss:  0.04491449147462845
Batch  21  loss:  0.038986656814813614
Batch  31  loss:  0.042571570724248886
Batch  41  loss:  0.023153914138674736
Batch  51  loss:  0.04768751561641693
Batch  61  loss:  0.027043839916586876
Batch  71  loss:  0.03197006508708
Batch  81  loss:  0.05071363225579262
Batch  91  loss:  0.037078794091939926
Validation on real data: 
LOSS supervised-train 0.043653149902820584, valid 0.23950634896755219
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.06186453998088837
Batch  11  loss:  0.027961917221546173
Batch  21  loss:  0.026275409385561943
Batch  31  loss:  0.049583472311496735
Batch  41  loss:  0.02752266265451908
Batch  51  loss:  0.07332618534564972
Batch  61  loss:  0.047808606177568436
Batch  71  loss:  0.04984474182128906
Batch  81  loss:  0.05626287683844566
Batch  91  loss:  0.032370295375585556
Validation on real data: 
LOSS supervised-train 0.045983813591301444, valid 0.3110079765319824
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.06902892887592316
Batch  11  loss:  0.05291289836168289
Batch  21  loss:  0.023919833824038506
Batch  31  loss:  0.041715968400239944
Batch  41  loss:  0.02937447279691696
Batch  51  loss:  0.048963937908411026
Batch  61  loss:  0.033071745187044144
Batch  71  loss:  0.03226836025714874
Batch  81  loss:  0.025595754384994507
Batch  91  loss:  0.024881349876523018
Validation on real data: 
LOSS supervised-train 0.039902751464396716, valid 0.33508917689323425
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.026349563151597977
Batch  11  loss:  0.03938383236527443
Batch  21  loss:  0.05463513359427452
Batch  31  loss:  0.025458315387368202
Batch  41  loss:  0.0465828962624073
Batch  51  loss:  0.03549114987254143
Batch  61  loss:  0.055475443601608276
Batch  71  loss:  0.026916977018117905
Batch  81  loss:  0.03764926642179489
Batch  91  loss:  0.044990260154008865
Validation on real data: 
LOSS supervised-train 0.036182066770270464, valid 0.227507084608078
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.02818758599460125
Batch  11  loss:  0.028027279302477837
Batch  21  loss:  0.01794341951608658
Batch  31  loss:  0.040355194360017776
Batch  41  loss:  0.036077000200748444
Batch  51  loss:  0.029870256781578064
Batch  61  loss:  0.03273134306073189
Batch  71  loss:  0.02228257991373539
Batch  81  loss:  0.03325813636183739
Batch  91  loss:  0.05822717398405075
Validation on real data: 
LOSS supervised-train 0.035462647108361126, valid 0.2672552466392517
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.025257987901568413
Batch  11  loss:  0.0371243841946125
Batch  21  loss:  0.016855323687195778
Batch  31  loss:  0.022487256675958633
Batch  41  loss:  0.023251943290233612
Batch  51  loss:  0.035224247723817825
Batch  61  loss:  0.027445772662758827
Batch  71  loss:  0.023455679416656494
Batch  81  loss:  0.02669377066195011
Batch  91  loss:  0.050711262971162796
Validation on real data: 
LOSS supervised-train 0.02905557463876903, valid 0.2945431172847748
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.02393774501979351
Batch  11  loss:  0.03129491209983826
Batch  21  loss:  0.0229613296687603
Batch  31  loss:  0.02222529426217079
Batch  41  loss:  0.02155892178416252
Batch  51  loss:  0.027383603155612946
Batch  61  loss:  0.022254707291722298
Batch  71  loss:  0.017997831106185913
Batch  81  loss:  0.054326131939888
Batch  91  loss:  0.020580792799592018
Validation on real data: 
LOSS supervised-train 0.026375381676480174, valid 0.3154903054237366
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0344027616083622
Batch  11  loss:  0.02579495869576931
Batch  21  loss:  0.020068062469363213
Batch  31  loss:  0.045311931520700455
Batch  41  loss:  0.019536210224032402
Batch  51  loss:  0.03586333617568016
Batch  61  loss:  0.025267433375120163
Batch  71  loss:  0.02616766467690468
Batch  81  loss:  0.04508977755904198
Batch  91  loss:  0.023661427199840546
Validation on real data: 
LOSS supervised-train 0.02857879128307104, valid 0.2178245335817337
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  vessel ; Model ID: 5c54100c798dd681bfeb646a8eadb57
--------------------
Training baseline regression model:  2022-03-30 18:38:44.963097
Detector:  pointnet
Object:  vessel
--------------------
device is  cuda
--------------------
Number of trainable parameters:  1616185
EPOCH 1:
Training on simulated data with supervision:
Batch  1  loss:  0.2042953372001648
Batch  11  loss:  0.1314658373594284
Batch  21  loss:  0.10484856367111206
Batch  31  loss:  0.0882221981883049
Batch  41  loss:  0.08019723743200302
Batch  51  loss:  0.08155544102191925
Batch  61  loss:  0.08211714774370193
Batch  71  loss:  0.06757836043834686
Batch  81  loss:  0.051996782422065735
Batch  91  loss:  0.056863125413656235
Validation on real data: 
LOSS supervised-train 0.08900116663426161, valid 0.05221375450491905
EPOCH 2:
Training on simulated data with supervision:
Batch  1  loss:  0.04573635011911392
Batch  11  loss:  0.03303416073322296
Batch  21  loss:  0.041062984615564346
Batch  31  loss:  0.03370488062500954
Batch  41  loss:  0.028103498741984367
Batch  51  loss:  0.02666868083178997
Batch  61  loss:  0.02768820710480213
Batch  71  loss:  0.01843443512916565
Batch  81  loss:  0.00861735362559557
Batch  91  loss:  0.01153761800378561
Validation on real data: 
LOSS supervised-train 0.0273203959222883, valid 0.014216274954378605
EPOCH 3:
Training on simulated data with supervision:
Batch  1  loss:  0.015392022207379341
Batch  11  loss:  0.013242943212389946
Batch  21  loss:  0.016601139679551125
Batch  31  loss:  0.01281642634421587
Batch  41  loss:  0.008970413357019424
Batch  51  loss:  0.010266527533531189
Batch  61  loss:  0.00714048370718956
Batch  71  loss:  0.011146540753543377
Batch  81  loss:  0.006445930339396
Batch  91  loss:  0.005936968140304089
Validation on real data: 
LOSS supervised-train 0.010922458493150771, valid 0.004490968771278858
EPOCH 4:
Training on simulated data with supervision:
Batch  1  loss:  0.010866458527743816
Batch  11  loss:  0.009364689700305462
Batch  21  loss:  0.007412320002913475
Batch  31  loss:  0.007854556664824486
Batch  41  loss:  0.006104358937591314
Batch  51  loss:  0.009364006109535694
Batch  61  loss:  0.005545050371438265
Batch  71  loss:  0.00848858617246151
Batch  81  loss:  0.005846028216183186
Batch  91  loss:  0.005677680019289255
Validation on real data: 
LOSS supervised-train 0.007543314539361745, valid 0.003094080602750182
EPOCH 5:
Training on simulated data with supervision:
Batch  1  loss:  0.010077605955302715
Batch  11  loss:  0.007121718488633633
Batch  21  loss:  0.006149394437670708
Batch  31  loss:  0.005632586777210236
Batch  41  loss:  0.00516190379858017
Batch  51  loss:  0.007224896922707558
Batch  61  loss:  0.004855699371546507
Batch  71  loss:  0.007946892641484737
Batch  81  loss:  0.0056497263722121716
Batch  91  loss:  0.004237921442836523
Validation on real data: 
LOSS supervised-train 0.006396672744303941, valid 0.002619850216433406
EPOCH 6:
Training on simulated data with supervision:
Batch  1  loss:  0.009092147462069988
Batch  11  loss:  0.007356965448707342
Batch  21  loss:  0.0053197783417999744
Batch  31  loss:  0.005742635577917099
Batch  41  loss:  0.003927512560039759
Batch  51  loss:  0.006958125624805689
Batch  61  loss:  0.004352818708866835
Batch  71  loss:  0.006258451845496893
Batch  81  loss:  0.004436846356838942
Batch  91  loss:  0.003472900716587901
Validation on real data: 
LOSS supervised-train 0.005496869103517384, valid 0.002037285827100277
EPOCH 7:
Training on simulated data with supervision:
Batch  1  loss:  0.008009378798305988
Batch  11  loss:  0.005668060854077339
Batch  21  loss:  0.004863160662353039
Batch  31  loss:  0.004980210680514574
Batch  41  loss:  0.004212636966258287
Batch  51  loss:  0.005626254715025425
Batch  61  loss:  0.003793960204347968
Batch  71  loss:  0.0070085045881569386
Batch  81  loss:  0.00521844020113349
Batch  91  loss:  0.0029705106280744076
Validation on real data: 
LOSS supervised-train 0.005013905020896345, valid 0.0021019824780523777
EPOCH 8:
Training on simulated data with supervision:
Batch  1  loss:  0.005828235764056444
Batch  11  loss:  0.005300692282617092
Batch  21  loss:  0.004077824763953686
Batch  31  loss:  0.004988476634025574
Batch  41  loss:  0.0036770428996533155
Batch  51  loss:  0.003546664258465171
Batch  61  loss:  0.004270727746188641
Batch  71  loss:  0.005131879821419716
Batch  81  loss:  0.004784046206623316
Batch  91  loss:  0.002954155672341585
Validation on real data: 
LOSS supervised-train 0.004571277052164078, valid 0.0020622878801077604
EPOCH 9:
Training on simulated data with supervision:
Batch  1  loss:  0.006495206151157618
Batch  11  loss:  0.006138834170997143
Batch  21  loss:  0.004399349447339773
Batch  31  loss:  0.004623922519385815
Batch  41  loss:  0.002882886677980423
Batch  51  loss:  0.005515305791050196
Batch  61  loss:  0.002905413508415222
Batch  71  loss:  0.00610008230432868
Batch  81  loss:  0.003547428175806999
Batch  91  loss:  0.0034184169489890337
Validation on real data: 
LOSS supervised-train 0.004460399602539838, valid 0.002472164575010538
EPOCH 10:
Training on simulated data with supervision:
Batch  1  loss:  0.004445056430995464
Batch  11  loss:  0.005245434585958719
Batch  21  loss:  0.00447311345487833
Batch  31  loss:  0.004559580702334642
Batch  41  loss:  0.0027177375741302967
Batch  51  loss:  0.0037090633995831013
Batch  61  loss:  0.003307306906208396
Batch  71  loss:  0.003925948869436979
Batch  81  loss:  0.0030143323820084333
Batch  91  loss:  0.00294747157022357
Validation on real data: 
LOSS supervised-train 0.00408297152724117, valid 0.0023701097816228867
EPOCH 11:
Training on simulated data with supervision:
Batch  1  loss:  0.004716213792562485
Batch  11  loss:  0.004566833842545748
Batch  21  loss:  0.003808610839769244
Batch  31  loss:  0.004446697887033224
Batch  41  loss:  0.0037360908463597298
Batch  51  loss:  0.003750673495233059
Batch  61  loss:  0.002646912820637226
Batch  71  loss:  0.004469124134629965
Batch  81  loss:  0.0030309876892715693
Batch  91  loss:  0.003069539787247777
Validation on real data: 
LOSS supervised-train 0.0039435532107017935, valid 0.0018253540620207787
EPOCH 12:
Training on simulated data with supervision:
Batch  1  loss:  0.005931026767939329
Batch  11  loss:  0.004460046999156475
Batch  21  loss:  0.0035422053188085556
Batch  31  loss:  0.003790087066590786
Batch  41  loss:  0.0025528280530124903
Batch  51  loss:  0.003393400926142931
Batch  61  loss:  0.002967988373711705
Batch  71  loss:  0.003922842442989349
Batch  81  loss:  0.0033360032830387354
Batch  91  loss:  0.0031916690059006214
Validation on real data: 
LOSS supervised-train 0.003775640863459557, valid 0.0019925653468817472
EPOCH 13:
Training on simulated data with supervision:
Batch  1  loss:  0.004199221730232239
Batch  11  loss:  0.005188468378037214
Batch  21  loss:  0.004052582196891308
Batch  31  loss:  0.003768235445022583
Batch  41  loss:  0.00226684776134789
Batch  51  loss:  0.0036684018559753895
Batch  61  loss:  0.0035321402829140425
Batch  71  loss:  0.003354914952069521
Batch  81  loss:  0.002902338048443198
Batch  91  loss:  0.0031197627540677786
Validation on real data: 
LOSS supervised-train 0.003662680205889046, valid 0.002074092859402299
EPOCH 14:
Training on simulated data with supervision:
Batch  1  loss:  0.005939987953752279
Batch  11  loss:  0.003843568963930011
Batch  21  loss:  0.0035270543303340673
Batch  31  loss:  0.0038526940625160933
Batch  41  loss:  0.002307342132553458
Batch  51  loss:  0.0038619718980044127
Batch  61  loss:  0.0029207945335656404
Batch  71  loss:  0.004262533038854599
Batch  81  loss:  0.003003385616466403
Batch  91  loss:  0.0027762490790337324
Validation on real data: 
LOSS supervised-train 0.003455053378129378, valid 0.0019190044840797782
EPOCH 15:
Training on simulated data with supervision:
Batch  1  loss:  0.003501158207654953
Batch  11  loss:  0.0038209091871976852
Batch  21  loss:  0.002955340314656496
Batch  31  loss:  0.003671025624498725
Batch  41  loss:  0.00239102216437459
Batch  51  loss:  0.0041146352887153625
Batch  61  loss:  0.002516935346648097
Batch  71  loss:  0.0035710977390408516
Batch  81  loss:  0.002316397614777088
Batch  91  loss:  0.0021804948337376118
Validation on real data: 
LOSS supervised-train 0.0033711240615230055, valid 0.0018886306788772345
EPOCH 16:
Training on simulated data with supervision:
Batch  1  loss:  0.004084339365363121
Batch  11  loss:  0.0033656740561127663
Batch  21  loss:  0.003194838762283325
Batch  31  loss:  0.004030706826597452
Batch  41  loss:  0.0024052539374679327
Batch  51  loss:  0.003781909355893731
Batch  61  loss:  0.002241765148937702
Batch  71  loss:  0.004358823876827955
Batch  81  loss:  0.002760059665888548
Batch  91  loss:  0.0025000330060720444
Validation on real data: 
LOSS supervised-train 0.0033395786385517568, valid 0.0020289423409849405
EPOCH 17:
Training on simulated data with supervision:
Batch  1  loss:  0.003445868380367756
Batch  11  loss:  0.0030963048338890076
Batch  21  loss:  0.003094639629125595
Batch  31  loss:  0.0035599987022578716
Batch  41  loss:  0.001513637020252645
Batch  51  loss:  0.003635670989751816
Batch  61  loss:  0.0026711118407547474
Batch  71  loss:  0.0027197375893592834
Batch  81  loss:  0.0028901633340865374
Batch  91  loss:  0.0021112647373229265
Validation on real data: 
LOSS supervised-train 0.0031325063691474498, valid 0.0017311654519289732
EPOCH 18:
Training on simulated data with supervision:
Batch  1  loss:  0.0030862134881317616
Batch  11  loss:  0.003487605368718505
Batch  21  loss:  0.003241495694965124
Batch  31  loss:  0.003704539267346263
Batch  41  loss:  0.0023767428938299417
Batch  51  loss:  0.003853448433801532
Batch  61  loss:  0.0024059407878667116
Batch  71  loss:  0.0032780568581074476
Batch  81  loss:  0.00290365400724113
Batch  91  loss:  0.002522373106330633
Validation on real data: 
LOSS supervised-train 0.00310723323142156, valid 0.0017732393462210894
EPOCH 19:
Training on simulated data with supervision:
Batch  1  loss:  0.003395548090338707
Batch  11  loss:  0.0028767751064151525
Batch  21  loss:  0.0033248760737478733
Batch  31  loss:  0.0034326647873967886
Batch  41  loss:  0.0016053076833486557
Batch  51  loss:  0.0023356846068054438
Batch  61  loss:  0.0038168542087078094
Batch  71  loss:  0.00312577816657722
Batch  81  loss:  0.0020653544925153255
Batch  91  loss:  0.0018525230698287487
Validation on real data: 
LOSS supervised-train 0.0029247741668950765, valid 0.0017490256577730179
EPOCH 20:
Training on simulated data with supervision:
Batch  1  loss:  0.003606930375099182
Batch  11  loss:  0.002841164590790868
Batch  21  loss:  0.003567255102097988
Batch  31  loss:  0.003465709276497364
Batch  41  loss:  0.001800617203116417
Batch  51  loss:  0.0030716490000486374
Batch  61  loss:  0.0021520638838410378
Batch  71  loss:  0.004254893399775028
Batch  81  loss:  0.0037382631562650204
Batch  91  loss:  0.0026741274632513523
Validation on real data: 
LOSS supervised-train 0.0029357503878418355, valid 0.0017276594880968332
EPOCH 21:
Training on simulated data with supervision:
Batch  1  loss:  0.0034573094453662634
Batch  11  loss:  0.003677837084978819
Batch  21  loss:  0.0028046900406479836
Batch  31  loss:  0.0030328333377838135
Batch  41  loss:  0.0015085948398336768
Batch  51  loss:  0.003680313704535365
Batch  61  loss:  0.0023022107779979706
Batch  71  loss:  0.0026431202422827482
Batch  81  loss:  0.0025191670283675194
Batch  91  loss:  0.0024558049626648426
Validation on real data: 
LOSS supervised-train 0.0027862090163398535, valid 0.0017529195174574852
EPOCH 22:
Training on simulated data with supervision:
Batch  1  loss:  0.004174040164798498
Batch  11  loss:  0.003566520754247904
Batch  21  loss:  0.0031669216696172953
Batch  31  loss:  0.0032360313925892115
Batch  41  loss:  0.0020909993909299374
Batch  51  loss:  0.0031916259322315454
Batch  61  loss:  0.0026964209973812103
Batch  71  loss:  0.002707218285650015
Batch  81  loss:  0.0020617868285626173
Batch  91  loss:  0.0023086671717464924
Validation on real data: 
LOSS supervised-train 0.002718313925433904, valid 0.0016075832536444068
EPOCH 23:
Training on simulated data with supervision:
Batch  1  loss:  0.002623089589178562
Batch  11  loss:  0.002177797257900238
Batch  21  loss:  0.003460008418187499
Batch  31  loss:  0.0032133401837199926
Batch  41  loss:  0.001803195453248918
Batch  51  loss:  0.003041991265490651
Batch  61  loss:  0.002441813936457038
Batch  71  loss:  0.002337827580049634
Batch  81  loss:  0.002743514021858573
Batch  91  loss:  0.0028158400673419237
Validation on real data: 
LOSS supervised-train 0.0026716305466834454, valid 0.0016698228428140283
EPOCH 24:
Training on simulated data with supervision:
Batch  1  loss:  0.004774367902427912
Batch  11  loss:  0.0026782474014908075
Batch  21  loss:  0.003364902688190341
Batch  31  loss:  0.0035264536272734404
Batch  41  loss:  0.0021745823323726654
Batch  51  loss:  0.0025494014844298363
Batch  61  loss:  0.0026386284735053778
Batch  71  loss:  0.002613973803818226
Batch  81  loss:  0.002084939042106271
Batch  91  loss:  0.00194266508333385
Validation on real data: 
LOSS supervised-train 0.002582716742763296, valid 0.001979226479306817
EPOCH 25:
Training on simulated data with supervision:
Batch  1  loss:  0.002844457048922777
Batch  11  loss:  0.0026754969730973244
Batch  21  loss:  0.0028337889816612005
Batch  31  loss:  0.002518058754503727
Batch  41  loss:  0.002339286031201482
Batch  51  loss:  0.002553087193518877
Batch  61  loss:  0.0025702393613755703
Batch  71  loss:  0.0023992108181118965
Batch  81  loss:  0.0027596247382462025
Batch  91  loss:  0.0020437852945178747
Validation on real data: 
LOSS supervised-train 0.00261120074079372, valid 0.002020102459937334
EPOCH 26:
Training on simulated data with supervision:
Batch  1  loss:  0.0027764143887907267
Batch  11  loss:  0.0021620269399136305
Batch  21  loss:  0.0029177330434322357
Batch  31  loss:  0.0018808257300406694
Batch  41  loss:  0.0014440391678363085
Batch  51  loss:  0.002734314650297165
Batch  61  loss:  0.0020550291519612074
Batch  71  loss:  0.0023926803842186928
Batch  81  loss:  0.0021918814163655043
Batch  91  loss:  0.002428971463814378
Validation on real data: 
LOSS supervised-train 0.0025104643520899115, valid 0.0017657498829066753
EPOCH 27:
Training on simulated data with supervision:
Batch  1  loss:  0.0026765167713165283
Batch  11  loss:  0.0019758539274334908
Batch  21  loss:  0.00252136355265975
Batch  31  loss:  0.003001944161951542
Batch  41  loss:  0.0015399872791022062
Batch  51  loss:  0.003383765695616603
Batch  61  loss:  0.0019246885785833001
Batch  71  loss:  0.002776727080345154
Batch  81  loss:  0.002490412909537554
Batch  91  loss:  0.0025050342082977295
Validation on real data: 
LOSS supervised-train 0.002472881781868637, valid 0.001320150913670659
EPOCH 28:
Training on simulated data with supervision:
Batch  1  loss:  0.004137798678129911
Batch  11  loss:  0.0023250197991728783
Batch  21  loss:  0.001956593943759799
Batch  31  loss:  0.0027561229653656483
Batch  41  loss:  0.0020125173032283783
Batch  51  loss:  0.002635141834616661
Batch  61  loss:  0.0025298106484115124
Batch  71  loss:  0.0022785363253206015
Batch  81  loss:  0.002462096279487014
Batch  91  loss:  0.001830371911637485
Validation on real data: 
LOSS supervised-train 0.002349284589290619, valid 0.00147110759280622
EPOCH 29:
Training on simulated data with supervision:
Batch  1  loss:  0.002299634739756584
Batch  11  loss:  0.002671822439879179
Batch  21  loss:  0.003014340065419674
Batch  31  loss:  0.0027658608742058277
Batch  41  loss:  0.001800617203116417
Batch  51  loss:  0.0027669917326420546
Batch  61  loss:  0.0032862303778529167
Batch  71  loss:  0.002580540021881461
Batch  81  loss:  0.0019538698252290487
Batch  91  loss:  0.002186899073421955
Validation on real data: 
LOSS supervised-train 0.002381540936185047, valid 0.0014740662882104516
EPOCH 30:
Training on simulated data with supervision:
Batch  1  loss:  0.002198070054873824
Batch  11  loss:  0.0019069050904363394
Batch  21  loss:  0.0021791185718029737
Batch  31  loss:  0.002687247935682535
Batch  41  loss:  0.001956642372533679
Batch  51  loss:  0.002447754144668579
Batch  61  loss:  0.002458032686263323
Batch  71  loss:  0.0018181937048211694
Batch  81  loss:  0.001838008756749332
Batch  91  loss:  0.001962633104994893
Validation on real data: 
LOSS supervised-train 0.0023078447650186715, valid 0.001966704847291112
EPOCH 31:
Training on simulated data with supervision:
Batch  1  loss:  0.0022598356008529663
Batch  11  loss:  0.0022558248601853848
Batch  21  loss:  0.0028188792057335377
Batch  31  loss:  0.002725867787376046
Batch  41  loss:  0.001303312717936933
Batch  51  loss:  0.0030196320731192827
Batch  61  loss:  0.0027632713317871094
Batch  71  loss:  0.0023151985369622707
Batch  81  loss:  0.0022411176469177008
Batch  91  loss:  0.002144094090908766
Validation on real data: 
LOSS supervised-train 0.0022883102379273623, valid 0.001797187840566039
EPOCH 32:
Training on simulated data with supervision:
Batch  1  loss:  0.002509060548618436
Batch  11  loss:  0.0023018864449113607
Batch  21  loss:  0.0027122357860207558
Batch  31  loss:  0.0028103338554501534
Batch  41  loss:  0.0018166151130571961
Batch  51  loss:  0.0035182551946491003
Batch  61  loss:  0.0018507729982957244
Batch  71  loss:  0.002152397995814681
Batch  81  loss:  0.0022970768623054028
Batch  91  loss:  0.0019071418792009354
Validation on real data: 
LOSS supervised-train 0.0022205035935621707, valid 0.00185440166387707
EPOCH 33:
Training on simulated data with supervision:
Batch  1  loss:  0.0026769558899104595
Batch  11  loss:  0.002304613124579191
Batch  21  loss:  0.0023408355191349983
Batch  31  loss:  0.003014457644894719
Batch  41  loss:  0.0018657156033441424
Batch  51  loss:  0.002566712675616145
Batch  61  loss:  0.0020403440576046705
Batch  71  loss:  0.002490502316504717
Batch  81  loss:  0.0018298083450645208
Batch  91  loss:  0.002179443836212158
Validation on real data: 
LOSS supervised-train 0.002225392737891525, valid 0.0014326946111395955
EPOCH 34:
Training on simulated data with supervision:
Batch  1  loss:  0.0025107557885348797
Batch  11  loss:  0.0019259101245552301
Batch  21  loss:  0.0022053606808185577
Batch  31  loss:  0.0023109493777155876
Batch  41  loss:  0.001477702520787716
Batch  51  loss:  0.0026983581483364105
Batch  61  loss:  0.0021851668134331703
Batch  71  loss:  0.0018647757824510336
Batch  81  loss:  0.0021567093208432198
Batch  91  loss:  0.0021248147822916508
Validation on real data: 
LOSS supervised-train 0.002157868412323296, valid 0.001530983718112111
EPOCH 35:
Training on simulated data with supervision:
Batch  1  loss:  0.0027586095966398716
Batch  11  loss:  0.002213725121691823
Batch  21  loss:  0.002210296457633376
Batch  31  loss:  0.0028337629046291113
Batch  41  loss:  0.0013143608812242746
Batch  51  loss:  0.003521253354847431
Batch  61  loss:  0.0023480302188545465
Batch  71  loss:  0.002076215809211135
Batch  81  loss:  0.0036458992399275303
Batch  91  loss:  0.0018646392272785306
Validation on real data: 
LOSS supervised-train 0.0022178120946045963, valid 0.0021300853695720434
EPOCH 36:
Training on simulated data with supervision:
Batch  1  loss:  0.003624146804213524
Batch  11  loss:  0.002115418203175068
Batch  21  loss:  0.0026226788759231567
Batch  31  loss:  0.0022493551950901747
Batch  41  loss:  0.0014041733229532838
Batch  51  loss:  0.0020956683438271284
Batch  61  loss:  0.0018937819404527545
Batch  71  loss:  0.0019285307498648763
Batch  81  loss:  0.0015543558401986957
Batch  91  loss:  0.0019916438031941652
Validation on real data: 
LOSS supervised-train 0.002154227396240458, valid 0.0015157019952312112
EPOCH 37:
Training on simulated data with supervision:
Batch  1  loss:  0.002430035499855876
Batch  11  loss:  0.0025086121167987585
Batch  21  loss:  0.0024807387962937355
Batch  31  loss:  0.002449679421260953
Batch  41  loss:  0.0013799067819491029
Batch  51  loss:  0.0019986953120678663
Batch  61  loss:  0.0026929909363389015
Batch  71  loss:  0.001489790971390903
Batch  81  loss:  0.0019050997216254473
Batch  91  loss:  0.0018895062385126948
Validation on real data: 
LOSS supervised-train 0.0020624513167422265, valid 0.0016219642711803317
EPOCH 38:
Training on simulated data with supervision:
Batch  1  loss:  0.0018460204591974616
Batch  11  loss:  0.0019282868597656488
Batch  21  loss:  0.002672127913683653
Batch  31  loss:  0.0020356408786028624
Batch  41  loss:  0.0017027042340487242
Batch  51  loss:  0.0021139737218618393
Batch  61  loss:  0.0023127037566155195
Batch  71  loss:  0.0020418288186192513
Batch  81  loss:  0.0015449689235538244
Batch  91  loss:  0.0018771644681692123
Validation on real data: 
LOSS supervised-train 0.002097993576899171, valid 0.0019104869570583105
EPOCH 39:
Training on simulated data with supervision:
Batch  1  loss:  0.0020410509314388037
Batch  11  loss:  0.0022759612184017897
Batch  21  loss:  0.0023676613345742226
Batch  31  loss:  0.0019330282229930162
Batch  41  loss:  0.0013107940321788192
Batch  51  loss:  0.0026217654813081026
Batch  61  loss:  0.002059925114735961
Batch  71  loss:  0.0018212752183899283
Batch  81  loss:  0.0015550771495327353
Batch  91  loss:  0.001805996522307396
Validation on real data: 
LOSS supervised-train 0.0020529476064257324, valid 0.0016347026685252786
EPOCH 40:
Training on simulated data with supervision:
Batch  1  loss:  0.0022454934660345316
Batch  11  loss:  0.00255628302693367
Batch  21  loss:  0.0021036164835095406
Batch  31  loss:  0.0019564174581319094
Batch  41  loss:  0.002339469501748681
Batch  51  loss:  0.002232417231425643
Batch  61  loss:  0.002010665601119399
Batch  71  loss:  0.0020888117142021656
Batch  81  loss:  0.0017955421935766935
Batch  91  loss:  0.0022866821382194757
Validation on real data: 
LOSS supervised-train 0.0020358841016422957, valid 0.0015448520425707102
EPOCH 41:
Training on simulated data with supervision:
Batch  1  loss:  0.0018896707333624363
Batch  11  loss:  0.002319631166756153
Batch  21  loss:  0.002631906885653734
Batch  31  loss:  0.0021108994260430336
Batch  41  loss:  0.001550008775666356
Batch  51  loss:  0.0021190836559981108
Batch  61  loss:  0.0019944163504987955
Batch  71  loss:  0.0017089489847421646
Batch  81  loss:  0.0017982207937166095
Batch  91  loss:  0.0014233913971111178
Validation on real data: 
LOSS supervised-train 0.0019440401392057539, valid 0.0012832843931391835
EPOCH 42:
Training on simulated data with supervision:
Batch  1  loss:  0.002108930377289653
Batch  11  loss:  0.0016482017235830426
Batch  21  loss:  0.0024255698081105947
Batch  31  loss:  0.0023702639155089855
Batch  41  loss:  0.00199983362108469
Batch  51  loss:  0.0023091917391866446
Batch  61  loss:  0.0011783092049881816
Batch  71  loss:  0.001887538586743176
Batch  81  loss:  0.0015979281160980463
Batch  91  loss:  0.002352043054997921
Validation on real data: 
LOSS supervised-train 0.002011031846050173, valid 0.002155422233045101
EPOCH 43:
Training on simulated data with supervision:
Batch  1  loss:  0.0019281620625406504
Batch  11  loss:  0.002447360660880804
Batch  21  loss:  0.0019783766474574804
Batch  31  loss:  0.002129896776750684
Batch  41  loss:  0.00203389348462224
Batch  51  loss:  0.002591914962977171
Batch  61  loss:  0.0017850723816081882
Batch  71  loss:  0.0020005886908620596
Batch  81  loss:  0.0017197829438373446
Batch  91  loss:  0.0018246877007186413
Validation on real data: 
LOSS supervised-train 0.001959677222184837, valid 0.001663330476731062
EPOCH 44:
Training on simulated data with supervision:
Batch  1  loss:  0.0022874725982546806
Batch  11  loss:  0.0017365752719342709
Batch  21  loss:  0.0023223229218274355
Batch  31  loss:  0.0022832006216049194
Batch  41  loss:  0.0012848101323470473
Batch  51  loss:  0.002092270180583
Batch  61  loss:  0.0021776161156594753
Batch  71  loss:  0.0018238466000184417
Batch  81  loss:  0.001823220169171691
Batch  91  loss:  0.0019118537893518806
Validation on real data: 
LOSS supervised-train 0.0018928889697417616, valid 0.0014804875245317817
EPOCH 45:
Training on simulated data with supervision:
Batch  1  loss:  0.0023344806395471096
Batch  11  loss:  0.002210495062172413
Batch  21  loss:  0.0030819077510386705
Batch  31  loss:  0.0023191829677671194
Batch  41  loss:  0.001525379135273397
Batch  51  loss:  0.0018386539304628968
Batch  61  loss:  0.0017808278789743781
Batch  71  loss:  0.0016910674748942256
Batch  81  loss:  0.0014835867332294583
Batch  91  loss:  0.001496121403761208
Validation on real data: 
LOSS supervised-train 0.0019311161048244684, valid 0.001549001899547875
EPOCH 46:
Training on simulated data with supervision:
Batch  1  loss:  0.002007941249758005
Batch  11  loss:  0.00179676606785506
Batch  21  loss:  0.002321541542187333
Batch  31  loss:  0.0018920681905001402
Batch  41  loss:  0.0014852297026664019
Batch  51  loss:  0.002201841911301017
Batch  61  loss:  0.0014870549784973264
Batch  71  loss:  0.0014843893004581332
Batch  81  loss:  0.0018116350984200835
Batch  91  loss:  0.002314267447218299
Validation on real data: 
LOSS supervised-train 0.0019007921824231743, valid 0.0015354870120063424
EPOCH 47:
Training on simulated data with supervision:
Batch  1  loss:  0.0017058226512745023
Batch  11  loss:  0.0018828633474186063
Batch  21  loss:  0.002269553253427148
Batch  31  loss:  0.0017435189802199602
Batch  41  loss:  0.0013844072818756104
Batch  51  loss:  0.00251932255923748
Batch  61  loss:  0.0021142626646906137
Batch  71  loss:  0.0016991151496767998
Batch  81  loss:  0.001357939327135682
Batch  91  loss:  0.0013341924641281366
Validation on real data: 
LOSS supervised-train 0.0018479201092850416, valid 0.001734625082463026
EPOCH 48:
Training on simulated data with supervision:
Batch  1  loss:  0.002621773397549987
Batch  11  loss:  0.0017059349920600653
Batch  21  loss:  0.002001796383410692
Batch  31  loss:  0.0018293746979907155
Batch  41  loss:  0.0013684406876564026
Batch  51  loss:  0.0016766285989433527
Batch  61  loss:  0.0021278751082718372
Batch  71  loss:  0.0014806347899138927
Batch  81  loss:  0.0014301410410553217
Batch  91  loss:  0.0015781013062223792
Validation on real data: 
LOSS supervised-train 0.0018451869499403984, valid 0.0016743697924539447
EPOCH 49:
Training on simulated data with supervision:
Batch  1  loss:  0.0023098827805370092
Batch  11  loss:  0.0018645740346983075
Batch  21  loss:  0.0021356302313506603
Batch  31  loss:  0.0017949558096006513
Batch  41  loss:  0.0012655144091695547
Batch  51  loss:  0.0024653151631355286
Batch  61  loss:  0.0016447925008833408
Batch  71  loss:  0.001457007834687829
Batch  81  loss:  0.001599153969436884
Batch  91  loss:  0.0018558435840532184
Validation on real data: 
LOSS supervised-train 0.00188517130096443, valid 0.0014511188492178917
EPOCH 50:
Training on simulated data with supervision:
Batch  1  loss:  0.0019427906954661012
Batch  11  loss:  0.0015182042261585593
Batch  21  loss:  0.002310878597199917
Batch  31  loss:  0.0023689535446465015
Batch  41  loss:  0.0013972771121188998
Batch  51  loss:  0.0034113104920834303
Batch  61  loss:  0.0020981505513191223
Batch  71  loss:  0.001826348016038537
Batch  81  loss:  0.0016026118537411094
Batch  91  loss:  0.0017540669068694115
Validation on real data: 
LOSS supervised-train 0.0018348676071036608, valid 0.001253820606507361
EPOCH 51:
Training on simulated data with supervision:
Batch  1  loss:  0.0019341426668688655
Batch  11  loss:  0.0018057802226394415
Batch  21  loss:  0.0021671722643077374
Batch  31  loss:  0.0019657358061522245
Batch  41  loss:  0.0018038165289908648
Batch  51  loss:  0.0017054954078048468
Batch  61  loss:  0.0017175134271383286
Batch  71  loss:  0.0016051355050876737
Batch  81  loss:  0.0015404135920107365
Batch  91  loss:  0.0014980591367930174
Validation on real data: 
LOSS supervised-train 0.0018076083506457508, valid 0.0018557283328846097
EPOCH 52:
Training on simulated data with supervision:
Batch  1  loss:  0.0016631012549623847
Batch  11  loss:  0.0017116459785029292
Batch  21  loss:  0.0015177461318671703
Batch  31  loss:  0.0017790148267522454
Batch  41  loss:  0.0012722518295049667
Batch  51  loss:  0.002470081439241767
Batch  61  loss:  0.002020392334088683
Batch  71  loss:  0.001738310675136745
Batch  81  loss:  0.0016068812692537904
Batch  91  loss:  0.0017974250949919224
Validation on real data: 
LOSS supervised-train 0.0017946906178258359, valid 0.0017345830565318465
EPOCH 53:
Training on simulated data with supervision:
Batch  1  loss:  0.0015280949883162975
Batch  11  loss:  0.0016775110270828009
Batch  21  loss:  0.0021638369653373957
Batch  31  loss:  0.0019193331245332956
Batch  41  loss:  0.0016867214580997825
Batch  51  loss:  0.0018512686947360635
Batch  61  loss:  0.002401090692728758
Batch  71  loss:  0.0017563909059390426
Batch  81  loss:  0.0014402949018403888
Batch  91  loss:  0.0016062910435721278
Validation on real data: 
LOSS supervised-train 0.001806265995837748, valid 0.0015454989625141025
EPOCH 54:
Training on simulated data with supervision:
Batch  1  loss:  0.0015940599841997027
Batch  11  loss:  0.0015551671385765076
Batch  21  loss:  0.002081065671518445
Batch  31  loss:  0.0024557425640523434
Batch  41  loss:  0.0014930198667570949
Batch  51  loss:  0.0023640126455575228
Batch  61  loss:  0.001959708519279957
Batch  71  loss:  0.0014433020260185003
Batch  81  loss:  0.0014461862156167626
Batch  91  loss:  0.0014525599544867873
Validation on real data: 
LOSS supervised-train 0.0017661839409265666, valid 0.0016769771464169025
EPOCH 55:
Training on simulated data with supervision:
Batch  1  loss:  0.0016516230534762144
Batch  11  loss:  0.0017371901776641607
Batch  21  loss:  0.0021492107771337032
Batch  31  loss:  0.001936220796778798
Batch  41  loss:  0.00172403734177351
Batch  51  loss:  0.002148375380784273
Batch  61  loss:  0.0015667625702917576
Batch  71  loss:  0.0012189679546281695
Batch  81  loss:  0.0014335225569084287
Batch  91  loss:  0.0013006923254579306
Validation on real data: 
LOSS supervised-train 0.0017600936756934971, valid 0.0016332825180143118
EPOCH 56:
Training on simulated data with supervision:
Batch  1  loss:  0.0015501313610002398
Batch  11  loss:  0.001730647636577487
Batch  21  loss:  0.0019045291701331735
Batch  31  loss:  0.0017651267116889358
Batch  41  loss:  0.0011680873576551676
Batch  51  loss:  0.0019902819767594337
Batch  61  loss:  0.0016088613774627447
Batch  71  loss:  0.0016347261844202876
Batch  81  loss:  0.001880658557638526
Batch  91  loss:  0.0014941288391128182
Validation on real data: 
LOSS supervised-train 0.0017187821480911226, valid 0.002629418857395649
EPOCH 57:
Training on simulated data with supervision:
Batch  1  loss:  0.0015963667538017035
Batch  11  loss:  0.0016516461037099361
Batch  21  loss:  0.0023550761397928
Batch  31  loss:  0.001596976537257433
Batch  41  loss:  0.0016273576766252518
Batch  51  loss:  0.002515701809898019
Batch  61  loss:  0.0020076981745660305
Batch  71  loss:  0.0016942655202001333
Batch  81  loss:  0.0017168346093967557
Batch  91  loss:  0.0013867997331544757
Validation on real data: 
LOSS supervised-train 0.0017562557139899583, valid 0.002191567560657859
EPOCH 58:
Training on simulated data with supervision:
Batch  1  loss:  0.00192353839520365
Batch  11  loss:  0.0014140064595267177
Batch  21  loss:  0.0019176304340362549
Batch  31  loss:  0.001827715546824038
Batch  41  loss:  0.0014279508031904697
Batch  51  loss:  0.0015237375628203154
Batch  61  loss:  0.0015826374292373657
Batch  71  loss:  0.001567400642670691
Batch  81  loss:  0.0012797437375411391
Batch  91  loss:  0.001765542896464467
Validation on real data: 
LOSS supervised-train 0.0016505593189503997, valid 0.0014169537462294102
EPOCH 59:
Training on simulated data with supervision:
Batch  1  loss:  0.002129732398316264
Batch  11  loss:  0.0018039101269096136
Batch  21  loss:  0.0025539519265294075
Batch  31  loss:  0.0015719070797786117
Batch  41  loss:  0.0013495146995410323
Batch  51  loss:  0.0016817591385915875
Batch  61  loss:  0.0013567068381235003
Batch  71  loss:  0.0015895902179181576
Batch  81  loss:  0.0011272188276052475
Batch  91  loss:  0.0015527406940236688
Validation on real data: 
LOSS supervised-train 0.0017079026566352695, valid 0.001803061575628817
EPOCH 60:
Training on simulated data with supervision:
Batch  1  loss:  0.001831145491451025
Batch  11  loss:  0.0019720697309821844
Batch  21  loss:  0.0019719242118299007
Batch  31  loss:  0.0020637130364775658
Batch  41  loss:  0.0013940650969743729
Batch  51  loss:  0.002378487028181553
Batch  61  loss:  0.001824400038458407
Batch  71  loss:  0.0017854165052995086
Batch  81  loss:  0.0013640326214954257
Batch  91  loss:  0.0016586494166404009
Validation on real data: 
LOSS supervised-train 0.0017014930513687431, valid 0.002012198558077216
EPOCH 61:
Training on simulated data with supervision:
Batch  1  loss:  0.0017699211603030562
Batch  11  loss:  0.0017008230788633227
Batch  21  loss:  0.0027600291650742292
Batch  31  loss:  0.0021596341393887997
Batch  41  loss:  0.0012444284511730075
Batch  51  loss:  0.0017336129676550627
Batch  61  loss:  0.0021256685722619295
Batch  71  loss:  0.001669077086262405
Batch  81  loss:  0.0011970725608989596
Batch  91  loss:  0.001486077904701233
Validation on real data: 
LOSS supervised-train 0.0017208934255177155, valid 0.001498266588896513
EPOCH 62:
Training on simulated data with supervision:
Batch  1  loss:  0.0016524605453014374
Batch  11  loss:  0.0014377979096025229
Batch  21  loss:  0.0017931137699633837
Batch  31  loss:  0.0019217700464650989
Batch  41  loss:  0.0017438852228224277
Batch  51  loss:  0.001727145747281611
Batch  61  loss:  0.0019890146795660257
Batch  71  loss:  0.0018120190361514688
Batch  81  loss:  0.0015647070249542594
Batch  91  loss:  0.001449835835956037
Validation on real data: 
LOSS supervised-train 0.0017567030992358924, valid 0.001523947692476213
EPOCH 63:
Training on simulated data with supervision:
Batch  1  loss:  0.0018872645450755954
Batch  11  loss:  0.0017228618962690234
Batch  21  loss:  0.0022137057967483997
Batch  31  loss:  0.0018244844395667315
Batch  41  loss:  0.0017771812854334712
Batch  51  loss:  0.0017982623539865017
Batch  61  loss:  0.0019730462227016687
Batch  71  loss:  0.0018851342611014843
Batch  81  loss:  0.001758964965119958
Batch  91  loss:  0.001619434915482998
Validation on real data: 
LOSS supervised-train 0.0016432064515538513, valid 0.0019517537439242005
EPOCH 64:
Training on simulated data with supervision:
Batch  1  loss:  0.0017267637886106968
Batch  11  loss:  0.0014951921766623855
Batch  21  loss:  0.0016269638435915112
Batch  31  loss:  0.0017190932994708419
Batch  41  loss:  0.001486787456087768
Batch  51  loss:  0.0015594407450407743
Batch  61  loss:  0.0012890651123598218
Batch  71  loss:  0.0015733271138742566
Batch  81  loss:  0.001662465394474566
Batch  91  loss:  0.0015367913292720914
Validation on real data: 
LOSS supervised-train 0.0016911091783549637, valid 0.0015397509559988976
EPOCH 65:
Training on simulated data with supervision:
Batch  1  loss:  0.0015300688100978732
Batch  11  loss:  0.0014556142268702388
Batch  21  loss:  0.0013135067420080304
Batch  31  loss:  0.001715211197733879
Batch  41  loss:  0.0013445417862385511
Batch  51  loss:  0.0017749917460605502
Batch  61  loss:  0.0015838625840842724
Batch  71  loss:  0.0016142460517585278
Batch  81  loss:  0.0011237013386562467
Batch  91  loss:  0.0012612679274752736
Validation on real data: 
LOSS supervised-train 0.0016235712647903711, valid 0.002084967913106084
EPOCH 66:
Training on simulated data with supervision:
Batch  1  loss:  0.0017035924829542637
Batch  11  loss:  0.0019097524927929044
Batch  21  loss:  0.0018345775315538049
Batch  31  loss:  0.002082542749121785
Batch  41  loss:  0.0013432609848678112
Batch  51  loss:  0.001953331520780921
Batch  61  loss:  0.001238236902281642
Batch  71  loss:  0.0013230385957285762
Batch  81  loss:  0.0016409665113314986
Batch  91  loss:  0.0012806224403902888
Validation on real data: 
LOSS supervised-train 0.0016759694938082248, valid 0.0017237963620573282
EPOCH 67:
Training on simulated data with supervision:
Batch  1  loss:  0.001471003983169794
Batch  11  loss:  0.0016367104835808277
Batch  21  loss:  0.0018805613508448005
Batch  31  loss:  0.0016671409830451012
Batch  41  loss:  0.0012589613907039165
Batch  51  loss:  0.0020188509952276945
Batch  61  loss:  0.0017360806232318282
Batch  71  loss:  0.001362715964205563
Batch  81  loss:  0.0013873451389372349
Batch  91  loss:  0.0013282005675137043
Validation on real data: 
LOSS supervised-train 0.0016365810844581575, valid 0.0013465426163747907
EPOCH 68:
Training on simulated data with supervision:
Batch  1  loss:  0.001528521766886115
Batch  11  loss:  0.0012974283890798688
Batch  21  loss:  0.0021683312952518463
Batch  31  loss:  0.0019642941188067198
Batch  41  loss:  0.001524970168247819
Batch  51  loss:  0.0024767101276665926
Batch  61  loss:  0.001342577626928687
Batch  71  loss:  0.0012523995246738195
Batch  81  loss:  0.0016189360758289695
Batch  91  loss:  0.001373859471641481
Validation on real data: 
LOSS supervised-train 0.001643745609326288, valid 0.0013926679966971278
EPOCH 69:
Training on simulated data with supervision:
Batch  1  loss:  0.001515932846814394
Batch  11  loss:  0.0019270036136731505
Batch  21  loss:  0.0018789690220728517
Batch  31  loss:  0.0018098761793226004
Batch  41  loss:  0.0012668009148910642
Batch  51  loss:  0.002237880602478981
Batch  61  loss:  0.001328506856225431
Batch  71  loss:  0.0015726409619674087
Batch  81  loss:  0.0011917368974536657
Batch  91  loss:  0.0015679530333727598
Validation on real data: 
LOSS supervised-train 0.0015799249534029513, valid 0.0018307685386389494
EPOCH 70:
Training on simulated data with supervision:
Batch  1  loss:  0.001741805812343955
Batch  11  loss:  0.0017074551433324814
Batch  21  loss:  0.001635458436794579
Batch  31  loss:  0.0018835632363334298
Batch  41  loss:  0.0015213836450129747
Batch  51  loss:  0.0014929105527698994
Batch  61  loss:  0.0015912449453026056
Batch  71  loss:  0.0016416005091741681
Batch  81  loss:  0.001406293478794396
Batch  91  loss:  0.0015067321946844459
Validation on real data: 
LOSS supervised-train 0.0015836059558205307, valid 0.0014654299011453986
EPOCH 71:
Training on simulated data with supervision:
Batch  1  loss:  0.001919963862746954
Batch  11  loss:  0.0015187835087999701
Batch  21  loss:  0.001628572354093194
Batch  31  loss:  0.0019180863164365292
Batch  41  loss:  0.0013058946933597326
Batch  51  loss:  0.0018957003485411406
Batch  61  loss:  0.0017182771116495132
Batch  71  loss:  0.0016121880616992712
Batch  81  loss:  0.0013271085917949677
Batch  91  loss:  0.0012849682243540883
Validation on real data: 
LOSS supervised-train 0.0016297357878647745, valid 0.001298029674217105
EPOCH 72:
Training on simulated data with supervision:
Batch  1  loss:  0.0018037819536402822
Batch  11  loss:  0.0016235533403232694
Batch  21  loss:  0.0014017611974850297
Batch  31  loss:  0.0019758315756917
Batch  41  loss:  0.0013671308988705277
Batch  51  loss:  0.002072446048259735
Batch  61  loss:  0.0015911725349724293
Batch  71  loss:  0.0013030057307332754
Batch  81  loss:  0.0012483189348131418
Batch  91  loss:  0.0014753444120287895
Validation on real data: 
LOSS supervised-train 0.0015527805569581687, valid 0.002005375921726227
EPOCH 73:
Training on simulated data with supervision:
Batch  1  loss:  0.0017858230276033282
Batch  11  loss:  0.0016179411904886365
Batch  21  loss:  0.0018169459654018283
Batch  31  loss:  0.0016096822218969464
Batch  41  loss:  0.0013794430997222662
Batch  51  loss:  0.0017365191597491503
Batch  61  loss:  0.001726619084365666
Batch  71  loss:  0.0014682025648653507
Batch  81  loss:  0.0013057941105216742
Batch  91  loss:  0.0013352340320125222
Validation on real data: 
LOSS supervised-train 0.0015945778123568743, valid 0.0016352515667676926
EPOCH 74:
Training on simulated data with supervision:
Batch  1  loss:  0.0017008435679599643
Batch  11  loss:  0.00152398191858083
Batch  21  loss:  0.001649858895689249
Batch  31  loss:  0.0014917398802936077
Batch  41  loss:  0.00153319351375103
Batch  51  loss:  0.0019094761228188872
Batch  61  loss:  0.0012390033807605505
Batch  71  loss:  0.0014480529353022575
Batch  81  loss:  0.0014631431549787521
Batch  91  loss:  0.0012923522153869271
Validation on real data: 
LOSS supervised-train 0.0016329393116757275, valid 0.0011586361797526479
EPOCH 75:
Training on simulated data with supervision:
Batch  1  loss:  0.0014892449835315347
Batch  11  loss:  0.0015523427864536643
Batch  21  loss:  0.0015285118715837598
Batch  31  loss:  0.001759092789143324
Batch  41  loss:  0.0013969381107017398
Batch  51  loss:  0.0013185580028221011
Batch  61  loss:  0.0014741551131010056
Batch  71  loss:  0.001382462796755135
Batch  81  loss:  0.0011264620115980506
Batch  91  loss:  0.0017115767113864422
Validation on real data: 
LOSS supervised-train 0.0015583833574783056, valid 0.001403521397151053
EPOCH 76:
Training on simulated data with supervision:
Batch  1  loss:  0.0010968426940962672
Batch  11  loss:  0.0013678052928298712
Batch  21  loss:  0.001659984583966434
Batch  31  loss:  0.001269823987968266
Batch  41  loss:  0.001232045004144311
Batch  51  loss:  0.001497166114859283
Batch  61  loss:  0.0014869003789499402
Batch  71  loss:  0.0016000603791326284
Batch  81  loss:  0.001383553841151297
Batch  91  loss:  0.001126311719417572
Validation on real data: 
LOSS supervised-train 0.001504899449646473, valid 0.001987199066206813
EPOCH 77:
Training on simulated data with supervision:
Batch  1  loss:  0.002492265310138464
Batch  11  loss:  0.0018213414587080479
Batch  21  loss:  0.002164514735341072
Batch  31  loss:  0.0016623976407572627
Batch  41  loss:  0.001209396868944168
Batch  51  loss:  0.0018364043207839131
Batch  61  loss:  0.0019852533005177975
Batch  71  loss:  0.0015011531068012118
Batch  81  loss:  0.00170840322971344
Batch  91  loss:  0.0014464131090790033
Validation on real data: 
LOSS supervised-train 0.0016069430252537132, valid 0.0018147098599001765
EPOCH 78:
Training on simulated data with supervision:
Batch  1  loss:  0.0015076559502631426
Batch  11  loss:  0.001103025395423174
Batch  21  loss:  0.0020447669085115194
Batch  31  loss:  0.0018934839172288775
Batch  41  loss:  0.0016327074263244867
Batch  51  loss:  0.0011990910861641169
Batch  61  loss:  0.0015928606735542417
Batch  71  loss:  0.0016216489020735025
Batch  81  loss:  0.001376103376969695
Batch  91  loss:  0.001412605750374496
Validation on real data: 
LOSS supervised-train 0.001547844895394519, valid 0.001486423541791737
EPOCH 79:
Training on simulated data with supervision:
Batch  1  loss:  0.0019530102144926786
Batch  11  loss:  0.0014169950736686587
Batch  21  loss:  0.0016804801998659968
Batch  31  loss:  0.0018992733675986528
Batch  41  loss:  0.0015220663044601679
Batch  51  loss:  0.001349693862721324
Batch  61  loss:  0.0018333072075620294
Batch  71  loss:  0.001157829537987709
Batch  81  loss:  0.001075845560990274
Batch  91  loss:  0.0011072182096540928
Validation on real data: 
LOSS supervised-train 0.0015634961955947801, valid 0.0017731826519593596
EPOCH 80:
Training on simulated data with supervision:
Batch  1  loss:  0.0015847934409976006
Batch  11  loss:  0.001482951920479536
Batch  21  loss:  0.0016498559853062034
Batch  31  loss:  0.0013362681493163109
Batch  41  loss:  0.00155814946629107
Batch  51  loss:  0.0016745813190937042
Batch  61  loss:  0.0015441623982042074
Batch  71  loss:  0.0015246619004756212
Batch  81  loss:  0.0012349731987342238
Batch  91  loss:  0.001266466686502099
Validation on real data: 
LOSS supervised-train 0.0015614112693583594, valid 0.00200590374879539
EPOCH 81:
Training on simulated data with supervision:
Batch  1  loss:  0.001763328560627997
Batch  11  loss:  0.0019721535500139
Batch  21  loss:  0.0017553793732076883
Batch  31  loss:  0.0015342370606958866
Batch  41  loss:  0.0011880729580298066
Batch  51  loss:  0.0013136662309989333
Batch  61  loss:  0.0017046370776370168
Batch  71  loss:  0.0013592928880825639
Batch  81  loss:  0.0013521715300157666
Batch  91  loss:  0.0014364620437845588
Validation on real data: 
LOSS supervised-train 0.0014901725645177066, valid 0.0016646406147629023
EPOCH 82:
Training on simulated data with supervision:
Batch  1  loss:  0.0013430732069537044
Batch  11  loss:  0.001524931169115007
Batch  21  loss:  0.0018399073742330074
Batch  31  loss:  0.0018409168114885688
Batch  41  loss:  0.0009867548942565918
Batch  51  loss:  0.0016857936279848218
Batch  61  loss:  0.0016645934665575624
Batch  71  loss:  0.0016817807918414474
Batch  81  loss:  0.0012319404631853104
Batch  91  loss:  0.001576924230903387
Validation on real data: 
LOSS supervised-train 0.0015967618685681373, valid 0.0016744328895583749
EPOCH 83:
Training on simulated data with supervision:
Batch  1  loss:  0.0013590845046564937
Batch  11  loss:  0.001440290012396872
Batch  21  loss:  0.001890383311547339
Batch  31  loss:  0.0014568695332854986
Batch  41  loss:  0.0013551192823797464
Batch  51  loss:  0.0014682924374938011
Batch  61  loss:  0.0020756965968757868
Batch  71  loss:  0.0015073963440954685
Batch  81  loss:  0.0014658381696790457
Batch  91  loss:  0.0013832167023792863
Validation on real data: 
LOSS supervised-train 0.0016074037394719198, valid 0.0018624990480020642
EPOCH 84:
Training on simulated data with supervision:
Batch  1  loss:  0.0014207735657691956
Batch  11  loss:  0.0013018741738051176
Batch  21  loss:  0.0016023856587707996
Batch  31  loss:  0.001405356335453689
Batch  41  loss:  0.0016438504680991173
Batch  51  loss:  0.0014845015248283744
Batch  61  loss:  0.0014602893497794867
Batch  71  loss:  0.0015236305771395564
Batch  81  loss:  0.0012302098330110312
Batch  91  loss:  0.0011469918536022305
Validation on real data: 
LOSS supervised-train 0.0015471093845553696, valid 0.001766871428117156
EPOCH 85:
Training on simulated data with supervision:
Batch  1  loss:  0.0015263946261256933
Batch  11  loss:  0.0015953376423567533
Batch  21  loss:  0.0015516330022364855
Batch  31  loss:  0.0019342460436746478
Batch  41  loss:  0.0013042837381362915
Batch  51  loss:  0.001648247241973877
Batch  61  loss:  0.001457048929296434
Batch  71  loss:  0.001334310625679791
Batch  81  loss:  0.001331283594481647
Batch  91  loss:  0.0012544202618300915
Validation on real data: 
LOSS supervised-train 0.0014797832252224908, valid 0.0022134219761937857
EPOCH 86:
Training on simulated data with supervision:
Batch  1  loss:  0.0013612877810373902
Batch  11  loss:  0.0018907883204519749
Batch  21  loss:  0.0015457486733794212
Batch  31  loss:  0.0017460361123085022
Batch  41  loss:  0.0013381493045017123
Batch  51  loss:  0.001267236308194697
Batch  61  loss:  0.0014583178563043475
Batch  71  loss:  0.0016071179416030645
Batch  81  loss:  0.0013568641152232885
Batch  91  loss:  0.00119436951354146
Validation on real data: 
LOSS supervised-train 0.0015107450709911064, valid 0.0016115697799250484
EPOCH 87:
Training on simulated data with supervision:
Batch  1  loss:  0.001716388389468193
Batch  11  loss:  0.0013183922274038196
Batch  21  loss:  0.0020330396946519613
Batch  31  loss:  0.001764089334756136
Batch  41  loss:  0.0012993001146242023
Batch  51  loss:  0.0016582076204940677
Batch  61  loss:  0.0012691240990534425
Batch  71  loss:  0.0017025413690134883
Batch  81  loss:  0.0012794517679139972
Batch  91  loss:  0.0013123806565999985
Validation on real data: 
LOSS supervised-train 0.0014902828639606013, valid 0.0017904870910570025
EPOCH 88:
Training on simulated data with supervision:
Batch  1  loss:  0.0017402078956365585
Batch  11  loss:  0.0013676142552867532
Batch  21  loss:  0.0015658753691241145
Batch  31  loss:  0.0010775604750961065
Batch  41  loss:  0.0013543585082516074
Batch  51  loss:  0.0019101869547739625
Batch  61  loss:  0.0015883767046034336
Batch  71  loss:  0.0018827732419595122
Batch  81  loss:  0.0014681105967611074
Batch  91  loss:  0.0014138846891000867
Validation on real data: 
LOSS supervised-train 0.0014987337845377624, valid 0.0017844446701928973
EPOCH 89:
Training on simulated data with supervision:
Batch  1  loss:  0.0014268782688304782
Batch  11  loss:  0.0015344498679041862
Batch  21  loss:  0.0014918431406840682
Batch  31  loss:  0.0013630544999614358
Batch  41  loss:  0.0012309271842241287
Batch  51  loss:  0.001492877141572535
Batch  61  loss:  0.0014988312032073736
Batch  71  loss:  0.0018938323482871056
Batch  81  loss:  0.0013715110253542662
Batch  91  loss:  0.0012815874069929123
Validation on real data: 
LOSS supervised-train 0.001473353371839039, valid 0.002139977877959609
EPOCH 90:
Training on simulated data with supervision:
Batch  1  loss:  0.0017596230609342456
Batch  11  loss:  0.0015135183930397034
Batch  21  loss:  0.00175373419187963
Batch  31  loss:  0.0012085289927199483
Batch  41  loss:  0.0016613908810541034
Batch  51  loss:  0.0016064911615103483
Batch  61  loss:  0.0013244072906672955
Batch  71  loss:  0.0013445388758555055
Batch  81  loss:  0.0017014762852340937
Batch  91  loss:  0.001311127794906497
Validation on real data: 
LOSS supervised-train 0.0014529746363405138, valid 0.0023479152005165815
EPOCH 91:
Training on simulated data with supervision:
Batch  1  loss:  0.0024842347484081984
Batch  11  loss:  0.0010863541392609477
Batch  21  loss:  0.0019313829252496362
Batch  31  loss:  0.0013070633867755532
Batch  41  loss:  0.0016178914811462164
Batch  51  loss:  0.0015604984946548939
Batch  61  loss:  0.0013855700381100178
Batch  71  loss:  0.0012580379843711853
Batch  81  loss:  0.0010601800167933106
Batch  91  loss:  0.0009824606822803617
Validation on real data: 
LOSS supervised-train 0.0015218484174693004, valid 0.00199924036860466
EPOCH 92:
Training on simulated data with supervision:
Batch  1  loss:  0.002175824949517846
Batch  11  loss:  0.00111471489071846
Batch  21  loss:  0.0021237924229353666
Batch  31  loss:  0.0016457875026389956
Batch  41  loss:  0.0009306566207669675
Batch  51  loss:  0.0021768915466964245
Batch  61  loss:  0.0015239641070365906
Batch  71  loss:  0.0014669311931356788
Batch  81  loss:  0.0017586388858035207
Batch  91  loss:  0.0010725436732172966
Validation on real data: 
LOSS supervised-train 0.0015078183088917286, valid 0.0023821478243917227
EPOCH 93:
Training on simulated data with supervision:
Batch  1  loss:  0.0017315391451120377
Batch  11  loss:  0.0013842715416103601
Batch  21  loss:  0.0014425957342609763
Batch  31  loss:  0.0013589365407824516
Batch  41  loss:  0.0014488640008494258
Batch  51  loss:  0.0015880265273153782
Batch  61  loss:  0.0018333507468923926
Batch  71  loss:  0.0010205607395619154
Batch  81  loss:  0.0014039513189345598
Batch  91  loss:  0.0010918885236606002
Validation on real data: 
LOSS supervised-train 0.0014799688395578413, valid 0.0018311039311811328
EPOCH 94:
Training on simulated data with supervision:
Batch  1  loss:  0.0016314717940986156
Batch  11  loss:  0.001332589192315936
Batch  21  loss:  0.0019252526108175516
Batch  31  loss:  0.0012443041196092963
Batch  41  loss:  0.0009339162497781217
Batch  51  loss:  0.0017069857567548752
Batch  61  loss:  0.0015135627472773194
Batch  71  loss:  0.0015839445404708385
Batch  81  loss:  0.0014773171860724688
Batch  91  loss:  0.001048338832333684
Validation on real data: 
LOSS supervised-train 0.001515114817302674, valid 0.0018872508080676198
EPOCH 95:
Training on simulated data with supervision:
Batch  1  loss:  0.0018524887273088098
Batch  11  loss:  0.0013715812237933278
Batch  21  loss:  0.002008994808420539
Batch  31  loss:  0.001812018221244216
Batch  41  loss:  0.001173637225292623
Batch  51  loss:  0.0015356548829004169
Batch  61  loss:  0.0015384118305519223
Batch  71  loss:  0.0016256269300356507
Batch  81  loss:  0.001714338082820177
Batch  91  loss:  0.0011800702195614576
Validation on real data: 
LOSS supervised-train 0.001598031595349312, valid 0.0021475162357091904
EPOCH 96:
Training on simulated data with supervision:
Batch  1  loss:  0.002737359143793583
Batch  11  loss:  0.001577901653945446
Batch  21  loss:  0.0014815604081377387
Batch  31  loss:  0.0011451581958681345
Batch  41  loss:  0.0013634776696562767
Batch  51  loss:  0.0016499420162290335
Batch  61  loss:  0.0013319834833964705
Batch  71  loss:  0.0013865729561075568
Batch  81  loss:  0.002377111930400133
Batch  91  loss:  0.0011526785092428327
Validation on real data: 
LOSS supervised-train 0.0015705829113721848, valid 0.0016418932937085629
EPOCH 97:
Training on simulated data with supervision:
Batch  1  loss:  0.0015947589417919517
Batch  11  loss:  0.001450933050364256
Batch  21  loss:  0.0014692425029352307
Batch  31  loss:  0.0019572365563362837
Batch  41  loss:  0.0011323754442855716
Batch  51  loss:  0.0013818821171298623
Batch  61  loss:  0.001344340736977756
Batch  71  loss:  0.0012409958289936185
Batch  81  loss:  0.0014943964779376984
Batch  91  loss:  0.001425402588211
Validation on real data: 
LOSS supervised-train 0.001483743351418525, valid 0.0012991649564355612
EPOCH 98:
Training on simulated data with supervision:
Batch  1  loss:  0.0016427477821707726
Batch  11  loss:  0.0016129545401781797
Batch  21  loss:  0.0018222014186903834
Batch  31  loss:  0.0014581771101802588
Batch  41  loss:  0.001320005627349019
Batch  51  loss:  0.0021499251015484333
Batch  61  loss:  0.001299035269767046
Batch  71  loss:  0.0011660472955554724
Batch  81  loss:  0.0011270007817074656
Batch  91  loss:  0.0010712053626775742
Validation on real data: 
LOSS supervised-train 0.0015484080789610743, valid 0.0030656245071440935
EPOCH 99:
Training on simulated data with supervision:
Batch  1  loss:  0.002985305618494749
Batch  11  loss:  0.0015919242287054658
Batch  21  loss:  0.0014174820389598608
Batch  31  loss:  0.0014823394594714046
Batch  41  loss:  0.0015434407396242023
Batch  51  loss:  0.0019272493664175272
Batch  61  loss:  0.001418377156369388
Batch  71  loss:  0.001377614215016365
Batch  81  loss:  0.0013236477971076965
Batch  91  loss:  0.0012034645769745111
Validation on real data: 
LOSS supervised-train 0.0015788605966372416, valid 0.0016697796527296305
EPOCH 100:
Training on simulated data with supervision:
Batch  1  loss:  0.0019731270149350166
Batch  11  loss:  0.0013780463486909866
Batch  21  loss:  0.0012589037651196122
Batch  31  loss:  0.0011966538149863482
Batch  41  loss:  0.0012983903288841248
Batch  51  loss:  0.0013898374745622277
Batch  61  loss:  0.001230554305948317
Batch  71  loss:  0.0014869507867842913
Batch  81  loss:  0.0020851825829595327
Batch  91  loss:  0.0013685055309906602
Validation on real data: 
LOSS supervised-train 0.00151232382748276, valid 0.00341875315643847
