Batch  83  loss:  0.032047901302576065
Batch  84  loss:  0.03281514346599579
Batch  85  loss:  0.0363185741007328
Batch  86  loss:  0.0278159286826849
Batch  87  loss:  0.03576438128948212
Batch  88  loss:  0.03847091272473335
Batch  89  loss:  0.03186900541186333
Batch  90  loss:  0.03518686443567276
Batch  91  loss:  0.03247945383191109
Batch  92  loss:  0.03199143707752228
Batch  93  loss:  0.03736407682299614
Batch  94  loss:  0.03281738981604576
Batch  95  loss:  0.030101532116532326
Batch  96  loss:  0.03864803537726402
Batch  97  loss:  0.04116617143154144
Batch  98  loss:  0.03771268576383591
Batch  99  loss:  0.031212342903017998
Batch  100  loss:  0.03600461781024933
Validation: 
LOSS train 0.03880355525761843, val 0.039433691650629044
EPOCH : 5 TIME:  2022-04-18 17:28:47.337523
Training: 
Batch  1  loss:  0.03345321863889694
Batch  2  loss:  0.032983239740133286
Batch  3  loss:  0.03317733108997345
Batch  4  loss:  0.033801693469285965
Batch  5  loss:  0.036187343299388885
Batch  6  loss:  0.035702697932720184
Batch  7  loss:  0.03552393615245819
Batch  8  loss:  0.03848259896039963
Batch  9  loss:  0.03395408019423485
Batch  10  loss:  0.03564653918147087
Batch  11  loss:  0.030246293172240257
Batch  12  loss:  0.03691039979457855
Batch  13  loss:  0.03466765955090523
Batch  14  loss:  0.03247205540537834
Batch  15  loss:  0.03307078406214714
Batch  16  loss:  0.03249339386820793
Batch  17  loss:  0.037226974964141846
Batch  18  loss:  0.032674193382263184
Batch  19  loss:  0.03063131310045719
Batch  20  loss:  0.03252265974879265
Batch  21  loss:  0.033648788928985596
Batch  22  loss:  0.04082702472805977
Batch  23  loss:  0.037834782153367996
Batch  24  loss:  0.026756763458251953
Batch  25  loss:  0.03204384073615074
Batch  26  loss:  0.03897083178162575
Batch  27  loss:  0.0337052121758461
Batch  28  loss:  0.03138667345046997
Batch  29  loss:  0.02715766429901123
Batch  30  loss:  0.03384684398770332
Batch  31  loss:  0.031774383038282394
Batch  32  loss:  0.03553339093923569
Batch  33  loss:  0.03298306092619896
Batch  34  loss:  0.029221266508102417
Batch  35  loss:  0.03173781931400299
Batch  36  loss:  0.03450636938214302
Batch  37  loss:  0.035045888274908066
Batch  38  loss:  0.039773352444171906
Batch  39  loss:  0.032699838280677795
Batch  40  loss:  0.02653169073164463
Batch  41  loss:  0.03266894817352295
Batch  42  loss:  0.03925585746765137
Batch  43  loss:  0.035045698285102844
Batch  44  loss:  0.035932935774326324
Batch  45  loss:  0.03301561623811722
Batch  46  loss:  0.03247426077723503
Batch  47  loss:  0.032024119049310684
Batch  48  loss:  0.032917920500040054
Batch  49  loss:  0.03730909153819084
Batch  50  loss:  0.03127260133624077
Batch  51  loss:  0.03220118582248688
Batch  52  loss:  0.02789011411368847
Batch  53  loss:  0.03147610276937485
Batch  54  loss:  0.03204723447561264
Batch  55  loss:  0.03178216516971588
Batch  56  loss:  0.031772442162036896
Batch  57  loss:  0.03174269571900368
Batch  58  loss:  0.027944335713982582
Batch  59  loss:  0.03146408125758171
Batch  60  loss:  0.03352801129221916
Batch  61  loss:  0.031907401978969574
Batch  62  loss:  0.038527291268110275
Batch  63  loss:  0.028541002422571182
Batch  64  loss:  0.032495684921741486
Batch  65  loss:  0.028325943276286125
Batch  66  loss:  0.035470154136419296
Batch  67  loss:  0.036842480301856995
Batch  68  loss:  0.03405126929283142
Batch  69  loss:  0.03061593510210514
Batch  70  loss:  0.03337889537215233
Batch  71  loss:  0.03058994933962822
Batch  72  loss:  0.027293303981423378
Batch  73  loss:  0.022748388350009918
Batch  74  loss:  0.02821122668683529
Batch  75  loss:  0.03446685150265694
Batch  76  loss:  0.03118451125919819
Batch  77  loss:  0.04449591413140297
Batch  78  loss:  0.025889072567224503
Batch  79  loss:  0.03556307777762413
Batch  80  loss:  0.028922997415065765
Batch  81  loss:  0.030480019748210907
Batch  82  loss:  0.033821213990449905
Batch  83  loss:  0.03629834204912186
Batch  84  loss:  0.039250049740076065
Batch  85  loss:  0.03082404099404812
Batch  86  loss:  0.02605781890451908
Batch  87  loss:  0.037347860634326935
Batch  88  loss:  0.037890397012233734
Batch  89  loss:  0.02914651297032833
Batch  90  loss:  0.03374329209327698
Batch  91  loss:  0.030174845829606056
Batch  92  loss:  0.03143145889043808
Batch  93  loss:  0.03154714033007622
Batch  94  loss:  0.032574668526649475
Batch  95  loss:  0.031017279252409935
Batch  96  loss:  0.03190901502966881
Batch  97  loss:  0.030182313174009323
Batch  98  loss:  0.02514563500881195
Batch  99  loss:  0.023061556741595268
Batch  100  loss:  0.025535015389323235
Validation: 
LOSS train 0.03268539136275649, val 0.030889589339494705
EPOCH : 6 TIME:  2022-04-18 17:47:10.724433
Training: 
Batch  1  loss:  0.03121313452720642
Batch  2  loss:  0.03621973097324371
Batch  3  loss:  0.030135011300444603
Batch  4  loss:  0.02646155096590519
Batch  5  loss:  0.032256122678518295
Batch  6  loss:  0.032591816037893295
Batch  7  loss:  0.03108406998217106
Batch  8  loss:  0.030965860933065414
Batch  9  loss:  0.03262670338153839
Batch  10  loss:  0.033868785947561264
Batch  11  loss:  0.035975828766822815
Batch  12  loss:  0.02681700699031353
Batch  13  loss:  0.0329628586769104
Batch  14  loss:  0.031744420528411865
Batch  15  loss:  0.030349010601639748
Batch  16  loss:  0.02973676100373268
Batch  17  loss:  0.03139001503586769
Batch  18  loss:  0.027734937146306038
Batch  19  loss:  0.030610093846917152
Batch  20  loss:  0.029556816443800926
Batch  21  loss:  0.029698170721530914
Batch  22  loss:  0.02945120260119438
Batch  23  loss:  0.03276114910840988
Batch  24  loss:  0.026279602199792862
Batch  25  loss:  0.03315114229917526
Batch  26  loss:  0.03452898934483528
Batch  27  loss:  0.032575976103544235
Batch  28  loss:  0.034146469086408615
Batch  29  loss:  0.03564434126019478
Batch  30  loss:  0.03436321020126343
Batch  31  loss:  0.0340794213116169
Batch  32  loss:  0.029402198269963264
Batch  33  loss:  0.03257014602422714
Batch  34  loss:  0.0360700748860836
Batch  35  loss:  0.029840761795639992
Batch  36  loss:  0.033162184059619904
Batch  37  loss:  0.0320010744035244
Batch  38  loss:  0.027818169444799423
Batch  39  loss:  0.030230125412344933
Batch  40  loss:  0.029148496687412262
Batch  41  loss:  0.03214925155043602
Batch  42  loss:  0.03195158764719963
Batch  43  loss:  0.02245214395225048
Batch  44  loss:  0.030751248821616173
Batch  45  loss:  0.02731126733124256
Batch  46  loss:  0.030555536970496178
Batch  47  loss:  0.026860341429710388
Batch  48  loss:  0.02713734656572342
Batch  49  loss:  0.02680976316332817
Batch  50  loss:  0.03255543112754822
Batch  51  loss:  0.03109104558825493
Batch  52  loss:  0.03553610295057297
Batch  53  loss:  0.029878968372941017
Batch  54  loss:  0.025788232684135437
Batch  55  loss:  0.02478637918829918
Batch  56  loss:  0.028244368731975555
Batch  57  loss:  0.029689963907003403
Batch  58  loss:  0.02598492056131363
Batch  59  loss:  0.028653280809521675
Batch  60  loss:  0.030795127153396606
Batch  61  loss:  0.031229175627231598
Batch  62  loss:  0.028069550171494484
Batch  63  loss:  0.02871476113796234
Batch  64  loss:  0.027504319325089455
Batch  65  loss:  0.030237838625907898
Batch  66  loss:  0.02452889457345009
Batch  67  loss:  0.027717676013708115
Batch  68  loss:  0.028431391343474388
Batch  69  loss:  0.032690610736608505
Batch  70  loss:  0.029934724792838097
Batch  71  loss:  0.02788698300719261
Batch  72  loss:  0.027549736201763153
Batch  73  loss:  0.027201225981116295
Batch  74  loss:  0.029756439849734306
Batch  75  loss:  0.02678932622075081
Batch  76  loss:  0.02389547973871231
Batch  77  loss:  0.0320153571665287
Batch  78  loss:  0.024760613217949867
Batch  79  loss:  0.03179064020514488
Batch  80  loss:  0.02655017375946045
Batch  81  loss:  0.02164221927523613
Batch  82  loss:  0.03240310773253441
Batch  83  loss:  0.026014532893896103
Batch  84  loss:  0.030167141929268837
Batch  85  loss:  0.03070312924683094
Batch  86  loss:  0.030025260522961617
Batch  87  loss:  0.027264390140771866
Batch  88  loss:  0.0230143740773201
Batch  89  loss:  0.02873499132692814
Batch  90  loss:  0.030226167291402817
Batch  91  loss:  0.020602254197001457
Batch  92  loss:  0.02331089973449707
Batch  93  loss:  0.03052673302590847
Batch  94  loss:  0.024153083562850952
Batch  95  loss:  0.029002999886870384
Batch  96  loss:  0.027250105515122414
Batch  97  loss:  0.028484391048550606
Batch  98  loss:  0.02825109474360943
Batch  99  loss:  0.03028196096420288
Batch  100  loss:  0.028180062770843506
Validation: 
LOSS train 0.029576995670795442, val 0.023569954559206963
EPOCH : 7 TIME:  2022-04-18 18:06:03.194613
Training: 
Batch  1  loss:  0.03313843533396721
Batch  2  loss:  0.02188325673341751
Batch  3  loss:  0.023178009316325188
Batch  4  loss:  0.028892861679196358
Batch  5  loss:  0.02898026444017887
Batch  6  loss:  0.031997449696063995
Batch  7  loss:  0.032041996717453
Batch  8  loss:  0.02931453101336956
Batch  9  loss:  0.02601783722639084
Batch  10  loss:  0.022356539964675903
Batch  11  loss:  0.02840515971183777
Batch  12  loss:  0.030497588217258453
Batch  13  loss:  0.031153639778494835
Batch  14  loss:  0.027746370062232018
Batch  15  loss:  0.025836357846856117
Batch  16  loss:  0.031694196164608
Batch  17  loss:  0.027838101610541344
Batch  18  loss:  0.024750245735049248
Batch  19  loss:  0.025589102879166603
Batch  20  loss:  0.025980304926633835
Batch  21  loss:  0.021475140005350113
Batch  22  loss:  0.025550488382577896
Batch  23  loss:  0.024033181369304657
Batch  24  loss:  0.022050252184271812
Batch  25  loss:  0.026665763929486275
Batch  26  loss:  0.02591683343052864
Batch  27  loss:  0.022388525307178497
Batch  28  loss:  0.02681475132703781
Batch  29  loss:  0.022434718906879425
Batch  30  loss:  0.02969205379486084
Batch  31  loss:  0.02378922887146473
Batch  32  loss:  0.025383083149790764
Batch  33  loss:  0.02948281727731228
Batch  34  loss:  0.030326614156365395
Batch  35  loss:  0.026670236140489578
Batch  36  loss:  0.030261682346463203
Batch  37  loss:  0.028874723240733147
Batch  38  loss:  0.030283961445093155
Batch  39  loss:  0.02806486375629902
Batch  40  loss:  0.02057100273668766
Batch  41  loss:  0.022336320951581
Batch  42  loss:  0.028539156541228294
Batch  43  loss:  0.02329510636627674
Batch  44  loss:  0.026037020608782768
Batch  45  loss:  0.028867026790976524
Batch  46  loss:  0.03375628963112831
Batch  47  loss:  0.022489702329039574
Batch  48  loss:  0.02338624931871891
Batch  49  loss:  0.02628176659345627
Batch  50  loss:  0.024398935958743095
Batch  51  loss:  0.024615230038762093
Batch  52  loss:  0.026327339932322502
Batch  53  loss:  0.027855489403009415
Batch  54  loss:  0.025375599041581154
Batch  55  loss:  0.029773272573947906
Batch  56  loss:  0.03484076261520386
Batch  57  loss:  0.02693457156419754
Batch  58  loss:  0.02324133738875389
Batch  59  loss:  0.023663200438022614
Batch  60  loss:  0.032017242163419724
Batch  61  loss:  0.031666502356529236
Batch  62  loss:  0.032003726810216904
Batch  63  loss:  0.027234695851802826
Batch  64  loss:  0.023227937519550323
Batch  65  loss:  0.0349835529923439
Batch  66  loss:  0.02295222319662571
Batch  67  loss:  0.023716608062386513
Batch  68  loss:  0.030536478385329247
Batch  69  loss:  0.026423973962664604
Batch  70  loss:  0.022738134488463402
Batch  71  loss:  0.02933286689221859
Batch  72  loss:  0.02682766132056713
Batch  73  loss:  0.03193376958370209
Batch  74  loss:  0.030099796131253242
Batch  75  loss:  0.02436145208775997
Batch  76  loss:  0.02509842813014984
Batch  77  loss:  0.030337003991007805
Batch  78  loss:  0.02849223092198372
Batch  79  loss:  0.025773243978619576
Batch  80  loss:  0.027544310316443443
Batch  81  loss:  0.025395726785063744
Batch  82  loss:  0.024540940299630165
Batch  83  loss:  0.018995624035596848
Batch  84  loss:  0.025367507711052895
Batch  85  loss:  0.02739584445953369
Batch  86  loss:  0.023250529542565346
Batch  87  loss:  0.026519902050495148
Batch  88  loss:  0.022898392751812935
Batch  89  loss:  0.028667716309428215
Batch  90  loss:  0.02935917302966118
Batch  91  loss:  0.023486465215682983
Batch  92  loss:  0.02162213809788227
Batch  93  loss:  0.031148632988333702
Batch  94  loss:  0.02777695655822754
Batch  95  loss:  0.02082957699894905
Batch  96  loss:  0.02470558136701584
Batch  97  loss:  0.03193693980574608
Batch  98  loss:  0.029980888590216637
Batch  99  loss:  0.02522837184369564
Batch  100  loss:  0.0265616774559021
Validation: 
LOSS train 0.026830069739371538, val 0.02498895674943924
EPOCH : 8 TIME:  2022-04-18 18:24:44.646399
Training: 
Batch  1  loss:  0.028501225635409355
Batch  2  loss:  0.02586323767900467
Batch  3  loss:  0.02510279044508934
Batch  4  loss:  0.026652995496988297
Batch  5  loss:  0.02315542660653591
Batch  6  loss:  0.024119781330227852
Batch  7  loss:  0.03013855591416359
Batch  8  loss:  0.03229949623346329
Batch  9  loss:  0.02569415047764778
Batch  10  loss:  0.03177884966135025
Batch  11  loss:  0.023555759340524673
Batch  12  loss:  0.02559671364724636
Batch  13  loss:  0.029936980456113815
Batch  14  loss:  0.023383216932415962
Batch  15  loss:  0.02401263266801834
Batch  16  loss:  0.030868839472532272
Batch  17  loss:  0.0222336333245039
Batch  18  loss:  0.02570238523185253
Batch  19  loss:  0.028582384809851646
Batch  20  loss:  0.01989907957613468
Batch  21  loss:  0.022832615301012993
Batch  22  loss:  0.02421467751264572
Batch  23  loss:  0.03353056684136391
Batch  24  loss:  0.028911728411912918
Batch  25  loss:  0.031742945313453674
Batch  26  loss:  0.02630573697388172
Batch  27  loss:  0.02492397651076317
Batch  28  loss:  0.02243736758828163
Batch  29  loss:  0.02458737976849079
Batch  30  loss:  0.024363640695810318
Batch  31  loss:  0.027613922953605652
Batch  32  loss:  0.028391312807798386
Batch  33  loss:  0.031205179169774055
Batch  34  loss:  0.02323024719953537
Batch  35  loss:  0.02487901598215103
Batch  36  loss:  0.026577012613415718
Batch  37  loss:  0.02850692719221115
Batch  38  loss:  0.030780848115682602
Batch  39  loss:  0.02384110912680626
Batch  40  loss:  0.028214458376169205
Batch  41  loss:  0.026462988927960396
Batch  42  loss:  0.02408711425960064
Batch  43  loss:  0.02194957248866558
Batch  44  loss:  0.024340620264410973
Batch  45  loss:  0.026783809065818787
Batch  46  loss:  0.026514379307627678
Batch  47  loss:  0.026819778606295586
Batch  48  loss:  0.02506277896463871
Batch  49  loss:  0.02073155902326107
Batch  50  loss:  0.028407396748661995
Batch  51  loss:  0.027063729241490364
Batch  52  loss:  0.0245998352766037
Batch  53  loss:  0.025063514709472656
Batch  54  loss:  0.02662767842411995
Batch  55  loss:  0.027052173390984535
Batch  56  loss:  0.02153334766626358
Batch  57  loss:  0.028074942529201508
Batch  58  loss:  0.023269252851605415
Batch  59  loss:  0.026314355432987213
Batch  60  loss:  0.028966432437300682
Batch  61  loss:  0.020093083381652832
Batch  62  loss:  0.023900212720036507
Batch  63  loss:  0.020770449191331863
Batch  64  loss:  0.0317654088139534
Batch  65  loss:  0.027496762573719025
Batch  66  loss:  0.021608831360936165
Batch  67  loss:  0.02346775494515896
Batch  68  loss:  0.0207884069532156
Batch  69  loss:  0.02206549048423767
Batch  70  loss:  0.021329840645194054
Batch  71  loss:  0.022336168214678764
Batch  72  loss:  0.020514974370598793
Batch  73  loss:  0.020481031388044357
Batch  74  loss:  0.029246363788843155
Batch  75  loss:  0.022444970905780792
Batch  76  loss:  0.01858079433441162
Batch  77  loss:  0.02565968781709671
Batch  78  loss:  0.024607185274362564
Batch  79  loss:  0.023195261135697365
Batch  80  loss:  0.023427726700901985
Batch  81  loss:  0.027406517416238785
Batch  82  loss:  0.025305800139904022
Batch  83  loss:  0.02954454906284809
Batch  84  loss:  0.021273236721754074
Batch  85  loss:  0.023074233904480934
Batch  86  loss:  0.02052466571331024
Batch  87  loss:  0.02556023746728897
Batch  88  loss:  0.035218868404626846
Batch  89  loss:  0.025483550503849983
Batch  90  loss:  0.02913346141576767
Batch  91  loss:  0.02823384292423725
Batch  92  loss:  0.030436014756560326
Batch  93  loss:  0.02455657348036766
Batch  94  loss:  0.03220623731613159
Batch  95  loss:  0.023500479757785797
Batch  96  loss:  0.026268696412444115
Batch  97  loss:  0.02309289388358593
Batch  98  loss:  0.028020333498716354
Batch  99  loss:  0.02589127980172634
Batch  100  loss:  0.025146257132291794
Validation: 
LOSS train 0.02567552197724581, val 0.027966151013970375
EPOCH : 9 TIME:  2022-04-18 18:43:20.419902
Training: 
Batch  1  loss:  0.029894990846514702
Batch  2  loss:  0.024285752326250076
Batch  3  loss:  0.02662922628223896
Batch  4  loss:  0.02239813841879368
Batch  5  loss:  0.02827407233417034
Batch  6  loss:  0.023412374779582024
Batch  7  loss:  0.027143796905875206
Batch  8  loss:  0.025396347045898438
Batch  9  loss:  0.029386982321739197
Batch  10  loss:  0.02430635131895542
Batch  11  loss:  0.02355792000889778
Batch  12  loss:  0.02856510505080223
Batch  13  loss:  0.026158394291996956
Batch  14  loss:  0.023358603939414024
Batch  15  loss:  0.021918321028351784
Batch  16  loss:  0.022305091843008995
Batch  17  loss:  0.028869442641735077
Batch  18  loss:  0.027119699865579605
Batch  19  loss:  0.02226901426911354
Batch  20  loss:  0.022676758468151093
Batch  21  loss:  0.021677182987332344
Batch  22  loss:  0.02641640044748783
Batch  23  loss:  0.02463902160525322
Batch  24  loss:  0.023301033303141594
Batch  25  loss:  0.020784692838788033
Batch  26  loss:  0.028242139145731926
Batch  27  loss:  0.024408817291259766
Batch  28  loss:  0.027470950037240982
Batch  29  loss:  0.022802310064435005
Batch  30  loss:  0.02951769344508648
Batch  31  loss:  0.0189068466424942
Batch  32  loss:  0.026285596191883087
Batch  33  loss:  0.026013731956481934
Batch  34  loss:  0.026757737621665
Batch  35  loss:  0.02172049693763256
Batch  36  loss:  0.030011335387825966
Batch  37  loss:  0.025260023772716522
Batch  38  loss:  0.020573943853378296
Batch  39  loss:  0.03141562268137932
Batch  40  loss:  0.028875065967440605
Batch  41  loss:  0.03180732950568199
Batch  42  loss:  0.02128339186310768
Batch  43  loss:  0.023373128846287727
Batch  44  loss:  0.02661823481321335
Batch  45  loss:  0.028710197657346725
Batch  46  loss:  0.023911861702799797
Batch  47  loss:  0.025616442784667015
Batch  48  loss:  0.02325179986655712
Batch  49  loss:  0.02229272574186325
Batch  50  loss:  0.029396848753094673
Batch  51  loss:  0.02150074392557144
Batch  52  loss:  0.022231506183743477
Batch  53  loss:  0.025743313133716583
Batch  54  loss:  0.024776514619588852
Batch  55  loss:  0.02416154555976391
Batch  56  loss:  0.02336076647043228
Batch  57  loss:  0.021002642810344696
Batch  58  loss:  0.02371319755911827
Batch  59  loss:  0.026540597900748253
Batch  60  loss:  0.0231314729899168
Batch  61  loss:  0.025927510112524033
Batch  62  loss:  0.022639693692326546
Batch  63  loss:  0.026016853749752045
Batch  64  loss:  0.023984286934137344
Batch  65  loss:  0.02713587135076523
Batch  66  loss:  0.025046486407518387
Batch  67  loss:  0.02577601931989193
Batch  68  loss:  0.031099071726202965
Batch  69  loss:  0.02169499173760414
Batch  70  loss:  0.022330310195684433
Batch  71  loss:  0.019803712144494057
Batch  72  loss:  0.02124098315834999
Batch  73  loss:  0.02372654899954796
Batch  74  loss:  0.01577363722026348
Batch  75  loss:  0.023286888375878334
Batch  76  loss:  0.0200188010931015
Batch  77  loss:  0.02169843204319477
Batch  78  loss:  0.020795654505491257
Batch  79  loss:  0.02968336082994938
Batch  80  loss:  0.023730792105197906
Batch  81  loss:  0.025596417486667633
Batch  82  loss:  0.018412692472338676
Batch  83  loss:  0.02393016219139099
Batch  84  loss:  0.02428366430103779
Batch  85  loss:  0.02328108809888363
Batch  86  loss:  0.02202415093779564
Batch  87  loss:  0.021589726209640503
Batch  88  loss:  0.022271040827035904
Batch  89  loss:  0.02482130564749241
Batch  90  loss:  0.026200881227850914
Batch  91  loss:  0.021472398191690445
Batch  92  loss:  0.028120826929807663
Batch  93  loss:  0.02208891324698925
Batch  94  loss:  0.023073043674230576
Batch  95  loss:  0.024689821526408195
Batch  96  loss:  0.026993416249752045
Batch  97  loss:  0.024010829627513885
Batch  98  loss:  0.02176021970808506
Batch  99  loss:  0.020579606294631958
Batch  100  loss:  0.02140928991138935
Validation: 
LOSS train 0.02443450689315796, val 0.025243239477276802
EPOCH : 10 TIME:  2022-04-18 19:01:38.055807
Training: 
Batch  1  loss:  0.021452976390719414
Batch  2  loss:  0.025297949090600014
Batch  3  loss:  0.031483836472034454
Batch  4  loss:  0.02607276663184166
Batch  5  loss:  0.022390080615878105
Batch  6  loss:  0.02899346500635147
Batch  7  loss:  0.02385532297194004
Batch  8  loss:  0.02033916488289833
Batch  9  loss:  0.020874323323369026
Batch  10  loss:  0.021756120026111603
Batch  11  loss:  0.021736592054367065
Batch  12  loss:  0.02190820872783661
Batch  13  loss:  0.018031299114227295
Batch  14  loss:  0.025868672877550125
Batch  15  loss:  0.022079531103372574
Batch  16  loss:  0.020788783207535744
Batch  17  loss:  0.02034318447113037
Batch  18  loss:  0.019671546295285225
Batch  19  loss:  0.021003035828471184
Batch  20  loss:  0.021526984870433807
Batch  21  loss:  0.024691211059689522
Batch  22  loss:  0.023345770314335823
Batch  23  loss:  0.02002808079123497
Batch  24  loss:  0.016463126987218857
Batch  25  loss:  0.01835111528635025
Batch  26  loss:  0.024054832756519318
Batch  27  loss:  0.020497359335422516
Batch  28  loss:  0.02527180127799511
Batch  29  loss:  0.0246276892721653
Batch  30  loss:  0.021284760907292366
Batch  31  loss:  0.02089570090174675
Batch  32  loss:  0.019749032333493233
Batch  33  loss:  0.02573293074965477
Batch  34  loss:  0.019716903567314148
Batch  35  loss:  0.023131269961595535
Batch  36  loss:  0.020214704796671867
Batch  37  loss:  0.022138813510537148
Batch  38  loss:  0.020759157836437225
Batch  39  loss:  0.0242910236120224
Batch  40  loss:  0.019647542387247086
Batch  41  loss:  0.021306633949279785
Batch  42  loss:  0.027501782402396202
Batch  43  loss:  0.026556624099612236
Batch  44  loss:  0.028405148535966873
Batch  45  loss:  0.022155601531267166
Batch  46  loss:  0.024856260046362877
Batch  47  loss:  0.019559208303689957
Batch  48  loss:  0.02319706417620182
Batch  49  loss:  0.020815689116716385
Batch  50  loss:  0.02033340558409691
Batch  51  loss:  0.020593710243701935
Batch  52  loss:  0.023076632991433144
Batch  53  loss:  0.026763439178466797
Batch  54  loss:  0.02073795720934868
Batch  55  loss:  0.022379612550139427
Batch  56  loss:  0.022955920547246933
Batch  57  loss:  0.019655290991067886
Batch  58  loss:  0.021448306739330292
Batch  59  loss:  0.02420942299067974
Batch  60  loss:  0.017065709456801414
Batch  61  loss:  0.018567033112049103
Batch  62  loss:  0.02210036665201187
Batch  63  loss:  0.024095706641674042
Batch  64  loss:  0.0211946964263916
Batch  65  loss:  0.021358270198106766
Batch  66  loss:  0.02813248336315155
Batch  67  loss:  0.024358132854104042
Batch  68  loss:  0.019293162971735
Batch  69  loss:  0.023133177310228348
Batch  70  loss:  0.026885582134127617
Batch  71  loss:  0.023828404024243355
Batch  72  loss:  0.02137509360909462
Batch  73  loss:  0.02095160447061062
Batch  74  loss:  0.025111280381679535
Batch  75  loss:  0.028105949983000755
Batch  76  loss:  0.017548492178320885
Batch  77  loss:  0.02406366355717182
Batch  78  loss:  0.02174096740782261
Batch  79  loss:  0.02412566915154457
Batch  80  loss:  0.022113706916570663
Batch  81  loss:  0.022900117561221123
Batch  82  loss:  0.021127676591277122
Batch  83  loss:  0.022800710052251816
Batch  84  loss:  0.01911568082869053
Batch  85  loss:  0.021619126200675964
Batch  86  loss:  0.02109917625784874
Batch  87  loss:  0.022463487461209297
Batch  88  loss:  0.02202685736119747
Batch  89  loss:  0.027224160730838776
Batch  90  loss:  0.025453314185142517
Batch  91  loss:  0.022638453170657158
Batch  92  loss:  0.02208000048995018
Batch  93  loss:  0.02332708239555359
Batch  94  loss:  0.028874117881059647
Batch  95  loss:  0.02614905685186386
Batch  96  loss:  0.021612875163555145
Batch  97  loss:  0.023806270211935043
Batch  98  loss:  0.01879120245575905
Batch  99  loss:  0.020348910242319107
Batch  100  loss:  0.025350725278258324
Validation: 
LOSS train 0.022608325369656086, val 0.024131955578923225
EPOCH : 11 TIME:  2022-04-18 19:20:13.749997
Training: 
Batch  1  loss:  0.02246500365436077
Batch  2  loss:  0.025670155882835388
Batch  3  loss:  0.0222220029681921
Batch  4  loss:  0.022088689729571342
Batch  5  loss:  0.02399207465350628
Batch  6  loss:  0.019088033586740494
Batch  7  loss:  0.03143429383635521
Batch  8  loss:  0.022894492372870445
Batch  9  loss:  0.028887923806905746
Batch  10  loss:  0.016208376735448837
Batch  11  loss:  0.024474697187542915
Batch  12  loss:  0.022396793588995934
Batch  13  loss:  0.024599727243185043
Batch  14  loss:  0.02604307420551777
Batch  15  loss:  0.01718883030116558
Batch  16  loss:  0.023203637450933456
Batch  17  loss:  0.020266611129045486
Batch  18  loss:  0.016222301870584488
Batch  19  loss:  0.020158452913165092
Batch  20  loss:  0.020603150129318237
Batch  21  loss:  0.01937517523765564
Batch  22  loss:  0.02435554377734661
Batch  23  loss:  0.02374299056828022
Batch  24  loss:  0.023410934954881668
Batch  25  loss:  0.02509979158639908
Batch  26  loss:  0.02145550772547722
Batch  27  loss:  0.019507164135575294
Batch  28  loss:  0.0226872768253088
Batch  29  loss:  0.030027026310563087
Batch  30  loss:  0.021570537239313126
Batch  31  loss:  0.02008584514260292
Batch  32  loss:  0.025486551225185394
Batch  33  loss:  0.02208324894309044
Batch  34  loss:  0.02510230988264084
Batch  35  loss:  0.02416827715933323
Batch  36  loss:  0.022870609536767006
Batch  37  loss:  0.024031944572925568
Batch  38  loss:  0.021348096430301666
Batch  39  loss:  0.020343158394098282
Batch  40  loss:  0.0190243162214756
Batch  41  loss:  0.022958487272262573
Batch  42  loss:  0.02227100543677807
Batch  43  loss:  0.02143077366054058
Batch  44  loss:  0.025090450420975685
Batch  45  loss:  0.02349839173257351
Batch  46  loss:  0.01813979633152485
Batch  47  loss:  0.02123642899096012
Batch  48  loss:  0.020794590935111046
Batch  49  loss:  0.025964610278606415
Batch  50  loss:  0.016717053949832916
Batch  51  loss:  0.019144877791404724
Batch  52  loss:  0.02208729274570942
Batch  53  loss:  0.023174375295639038
Batch  54  loss:  0.0220680832862854
Batch  55  loss:  0.02120954543352127
Batch  56  loss:  0.024153390899300575
Batch  57  loss:  0.022246703505516052
Batch  58  loss:  0.018147913739085197
Batch  59  loss:  0.02422848716378212
Batch  60  loss:  0.020606981590390205
Batch  61  loss:  0.021295828744769096
Batch  62  loss:  0.02195943333208561
Batch  63  loss:  0.022493509575724602
Batch  64  loss:  0.017538348212838173
Batch  65  loss:  0.01658529043197632
Batch  66  loss:  0.025571279227733612
Batch  67  loss:  0.023894453421235085
Batch  68  loss:  0.021941907703876495
Batch  69  loss:  0.02361283451318741
Batch  70  loss:  0.029764750972390175
Batch  71  loss:  0.022459601983428
Batch  72  loss:  0.022419605404138565
Batch  73  loss:  0.024978168308734894
Batch  74  loss:  0.018519967794418335
Batch  75  loss:  0.021077575162053108
Batch  76  loss:  0.028422296047210693
Batch  77  loss:  0.020884418860077858
Batch  78  loss:  0.024119725450873375
Batch  79  loss:  0.021102432161569595
Batch  80  loss:  0.021516187116503716
Batch  81  loss:  0.02902592346072197
Batch  82  loss:  0.02505548670887947
Batch  83  loss:  0.017817245796322823
Batch  84  loss:  0.016738753765821457
Batch  85  loss:  0.01898319087922573
Batch  86  loss:  0.018887488171458244
Batch  87  loss:  0.016866793856024742
Batch  88  loss:  0.02398112416267395
Batch  89  loss:  0.02100580558180809
Batch  90  loss:  0.01903921365737915
Batch  91  loss:  0.022011855617165565
Batch  92  loss:  0.017419449985027313
Batch  93  loss:  0.020551813766360283
Batch  94  loss:  0.020516347140073776
Batch  95  loss:  0.021482544019818306
Batch  96  loss:  0.01970190554857254
Batch  97  loss:  0.02531120739877224
Batch  98  loss:  0.02053595893085003
Batch  99  loss:  0.026764215901494026
Batch  100  loss:  0.026484224945306778
Validation: 
LOSS train 0.022193920332938433, val 0.021197473630309105
EPOCH : 12 TIME:  2022-04-18 19:38:36.935269
Training: 
Batch  1  loss:  0.02411661110818386
Batch  2  loss:  0.019495487213134766
Batch  3  loss:  0.019230253994464874
Batch  4  loss:  0.018917901441454887
Batch  5  loss:  0.0186848696321249
Batch  6  loss:  0.02011249028146267
Batch  7  loss:  0.021371517330408096
Batch  8  loss:  0.025406228378415108
Batch  9  loss:  0.022002791985869408
Batch  10  loss:  0.026131004095077515
Batch  11  loss:  0.02301315777003765
Batch  12  loss:  0.023885607719421387
Batch  13  loss:  0.020932797342538834
Batch  14  loss:  0.02220037952065468
Batch  15  loss:  0.020134638994932175
Batch  16  loss:  0.019006136804819107
Batch  17  loss:  0.021704716607928276
Batch  18  loss:  0.01924760453402996
Batch  19  loss:  0.0198467168956995
Batch  20  loss:  0.02082550898194313
Batch  21  loss:  0.025103015825152397
Batch  22  loss:  0.028993448242545128
Batch  23  loss:  0.020980650559067726
Batch  24  loss:  0.024391407147049904
Batch  25  loss:  0.027685975655913353
Batch  26  loss:  0.017583956941962242
Batch  27  loss:  0.0261975210160017
Batch  28  loss:  0.024589037522673607
Batch  29  loss:  0.025935478508472443
Batch  30  loss:  0.02646954171359539
Batch  31  loss:  0.0255662202835083
Batch  32  loss:  0.01567455194890499
Batch  33  loss:  0.017848296090960503
Batch  34  loss:  0.024205993860960007
Batch  35  loss:  0.02251610904932022
Batch  36  loss:  0.02236538752913475
Batch  37  loss:  0.018415819853544235
Batch  38  loss:  0.016695747151970863
Batch  39  loss:  0.020716095343232155
Batch  40  loss:  0.019286779686808586
Batch  41  loss:  0.020698182284832
Batch  42  loss:  0.020301666110754013
Batch  43  loss:  0.017004676163196564
Batch  44  loss:  0.026889896020293236
Batch  45  loss:  0.018020087853074074
Batch  46  loss:  0.020065171644091606
Batch  47  loss:  0.019562091678380966
Batch  48  loss:  0.01972094178199768
Batch  49  loss:  0.017530355602502823
Batch  50  loss:  0.021910717710852623
Batch  51  loss:  0.024556180462241173
Batch  52  loss:  0.023915376514196396
Batch  53  loss:  0.020701894536614418
Batch  54  loss:  0.020400524139404297
Batch  55  loss:  0.020437711849808693
Batch  56  loss:  0.020544996485114098
Batch  57  loss:  0.023083968088030815
Batch  58  loss:  0.025789422914385796
Batch  59  loss:  0.02633211575448513
Batch  60  loss:  0.03550431504845619
Batch  61  loss:  0.020786507055163383
Batch  62  loss:  0.018102437257766724
Batch  63  loss:  0.02164543606340885
Batch  64  loss:  0.020343657582998276
Batch  65  loss:  0.026795974001288414
Batch  66  loss:  0.021336505189538002
Batch  67  loss:  0.021783413365483284
Batch  68  loss:  0.021291609853506088
Batch  69  loss:  0.02522837556898594
Batch  70  loss:  0.02083904854953289
Batch  71  loss:  0.017511649057269096
Batch  72  loss:  0.017863711342215538
Batch  73  loss:  0.02086556702852249
Batch  74  loss:  0.018147123977541924
Batch  75  loss:  0.01902284286916256
Batch  76  loss:  0.02381715178489685
Batch  77  loss:  0.023258859291672707
Batch  78  loss:  0.02045976184308529
Batch  79  loss:  0.017383620142936707
Batch  80  loss:  0.019638542085886
Batch  81  loss:  0.019862420856952667
Batch  82  loss:  0.017408816143870354
Batch  83  loss:  0.018998051062226295
Batch  84  loss:  0.019983483478426933
Batch  85  loss:  0.027742858976125717
Batch  86  loss:  0.023640258237719536
Batch  87  loss:  0.02265165001153946
Batch  88  loss:  0.021197080612182617
Batch  89  loss:  0.02032085694372654
Batch  90  loss:  0.01915363036096096
Batch  91  loss:  0.02425355650484562
Batch  92  loss:  0.018113864585757256
Batch  93  loss:  0.023034347221255302
Batch  94  loss:  0.023590436205267906
Batch  95  loss:  0.01793508045375347
Batch  96  loss:  0.02149183303117752
Batch  97  loss:  0.01959165371954441
Batch  98  loss:  0.024673447012901306
Batch  99  loss:  0.021599313244223595
Batch  100  loss:  0.01854868419468403
Validation: 
LOSS train 0.021603708639740943, val 0.024640044197440147
EPOCH : 13 TIME:  2022-04-18 19:57:19.026488
Training: 
Batch  1  loss:  0.018279211595654488
Batch  2  loss:  0.020344192162156105
Batch  3  loss:  0.026154683902859688
Batch  4  loss:  0.024493055418133736
Batch  5  loss:  0.025813616812229156
Batch  6  loss:  0.01710369624197483
Batch  7  loss:  0.0189542043954134
Batch  8  loss:  0.021507205441594124
Batch  9  loss:  0.02238953486084938
Batch  10  loss:  0.019555382430553436
Batch  11  loss:  0.01975473389029503
Batch  12  loss:  0.02458178997039795
Batch  13  loss:  0.020995663478970528
Batch  14  loss:  0.021191800013184547
Batch  15  loss:  0.01786797307431698
Batch  16  loss:  0.018435189500451088
Batch  17  loss:  0.027904165908694267
Batch  18  loss:  0.018527938053011894
Batch  19  loss:  0.016811858862638474
Batch  20  loss:  0.021170716732740402
Batch  21  loss:  0.021138634532690048
Batch  22  loss:  0.022193262353539467
Batch  23  loss:  0.01872009038925171
Batch  24  loss:  0.021039357408881187
Batch  25  loss:  0.016581576317548752
Batch  26  loss:  0.019007425755262375
Batch  27  loss:  0.019911164417862892
Batch  28  loss:  0.019391145557165146
Batch  29  loss:  0.025224868208169937
Batch  30  loss:  0.024509569630026817
Batch  31  loss:  0.019263003021478653
Batch  32  loss:  0.019767800346016884
Batch  33  loss:  0.024365315213799477
Batch  34  loss:  0.02603023312985897
Batch  35  loss:  0.02175607904791832
Batch  36  loss:  0.01773414947092533
Batch  37  loss:  0.018443165346980095
Batch  38  loss:  0.0153104392811656
Batch  39  loss:  0.022135037928819656
Batch  40  loss:  0.016775069758296013
Batch  41  loss:  0.019627684727311134
Batch  42  loss:  0.020344410091638565
Batch  43  loss:  0.019937070086598396
Batch  44  loss:  0.02474638819694519
Batch  45  loss:  0.021091733127832413
Batch  46  loss:  0.02160959504544735
Batch  47  loss:  0.022387905046343803
Batch  48  loss:  0.024110974743962288
Batch  49  loss:  0.021401090547442436
Batch  50  loss:  0.024753183126449585
Batch  51  loss:  0.018298303708434105
Batch  52  loss:  0.018315432593226433
Batch  53  loss:  0.0167958103120327
Batch  54  loss:  0.01977764628827572
Batch  55  loss:  0.017290454357862473
Batch  56  loss:  0.02604171633720398
Batch  57  loss:  0.02228432334959507
Batch  58  loss:  0.017559481784701347
Batch  59  loss:  0.017457356676459312
Batch  60  loss:  0.020248379558324814
Batch  61  loss:  0.020195860415697098
Batch  62  loss:  0.02749369479715824
Batch  63  loss:  0.022449463605880737
Batch  64  loss:  0.02215820737183094
Batch  65  loss:  0.022265799343585968
Batch  66  loss:  0.018551761284470558
Batch  67  loss:  0.02085859887301922
Batch  68  loss:  0.020029382780194283
Batch  69  loss:  0.022775407880544662
Batch  70  loss:  0.01673559658229351
Batch  71  loss:  0.021741393953561783
Batch  72  loss:  0.024130916222929955
Batch  73  loss:  0.021475400775671005
Batch  74  loss:  0.02234811708331108
Batch  75  loss:  0.020428121089935303
Batch  76  loss:  0.021646341308951378
Batch  77  loss:  0.023204559460282326
Batch  78  loss:  0.021470369771122932
Batch  79  loss:  0.01941123977303505
Batch  80  loss:  0.01931421458721161
Batch  81  loss:  0.02020668424665928
Batch  82  loss:  0.018917862325906754
Batch  83  loss:  0.01799846440553665
Batch  84  loss:  0.01882735639810562
Batch  85  loss:  0.02499627321958542
Batch  86  loss:  0.02300983853638172
Batch  87  loss:  0.026259969919919968
Batch  88  loss:  0.01728622429072857
Batch  89  loss:  0.021198149770498276
Batch  90  loss:  0.016011809930205345
Batch  91  loss:  0.017609667032957077
Batch  92  loss:  0.016564078629016876
Batch  93  loss:  0.020306037738919258
Batch  94  loss:  0.024604663252830505
Batch  95  loss:  0.025517143309116364
Batch  96  loss:  0.01761396788060665
Batch  97  loss:  0.025400599464774132
Batch  98  loss:  0.025243252515792847
Batch  99  loss:  0.017546214163303375
Batch  100  loss:  0.017661061137914658
Validation: 
LOSS train 0.02086676704697311, val 0.018032187595963478
EPOCH : 14 TIME:  2022-04-18 20:16:18.277556
Training: 
Batch  1  loss:  0.022761685773730278
Batch  2  loss:  0.0228741355240345
Batch  3  loss:  0.01797102950513363
Batch  4  loss:  0.01744958758354187
Batch  5  loss:  0.021225091069936752
Batch  6  loss:  0.016712132841348648
Batch  7  loss:  0.02888982743024826
Batch  8  loss:  0.015579945407807827
Batch  9  loss:  0.021077558398246765
Batch  10  loss:  0.019619014114141464
Batch  11  loss:  0.024025950580835342
Batch  12  loss:  0.021707862615585327
Batch  13  loss:  0.018942803144454956
Batch  14  loss:  0.02037588134407997
Batch  15  loss:  0.020325183868408203
Batch  16  loss:  0.01969088427722454
Batch  17  loss:  0.02252313308417797
Batch  18  loss:  0.025240514427423477
Batch  19  loss:  0.01602497138082981
Batch  20  loss:  0.017405055463314056
Batch  21  loss:  0.019852744415402412
Batch  22  loss:  0.022371210157871246
Batch  23  loss:  0.021443314850330353
Batch  24  loss:  0.016554441303014755
Batch  25  loss:  0.0223004762083292
Batch  26  loss:  0.020593460649251938
Batch  27  loss:  0.021358881145715714
Batch  28  loss:  0.019060935825109482
Batch  29  loss:  0.02451736479997635
Batch  30  loss:  0.019118499010801315
Batch  31  loss:  0.01857832260429859
Batch  32  loss:  0.02433440089225769
Batch  33  loss:  0.022446274757385254
Batch  34  loss:  0.021580100059509277
Batch  35  loss:  0.017517942935228348
Batch  36  loss:  0.017900343984365463
Batch  37  loss:  0.022412141785025597
Batch  38  loss:  0.019585195928812027
Batch  39  loss:  0.015622696839272976
Batch  40  loss:  0.023513730615377426
Batch  41  loss:  0.016468869522213936
Batch  42  loss:  0.016473757103085518
Batch  43  loss:  0.021068047732114792
Batch  44  loss:  0.019655002281069756
Batch  45  loss:  0.01929493062198162
Batch  46  loss:  0.017417756840586662
Batch  47  loss:  0.02164054661989212
Batch  48  loss:  0.023554500192403793
Batch  49  loss:  0.01903284154832363
Batch  50  loss:  0.01827794499695301
Batch  51  loss:  0.01903674192726612
Batch  52  loss:  0.018106568604707718
Batch  53  loss:  0.017241913825273514
Batch  54  loss:  0.01876370795071125
Batch  55  loss:  0.02133079059422016
Batch  56  loss:  0.01994158700108528
Batch  57  loss:  0.015494748018682003
Batch  58  loss:  0.017641453072428703
Batch  59  loss:  0.02078941836953163
Batch  60  loss:  0.01512192003428936
Batch  61  loss:  0.019915733486413956
Batch  62  loss:  0.01996375061571598
Batch  63  loss:  0.019798826426267624
Batch  64  loss:  0.01749102957546711
Batch  65  loss:  0.016507701948285103
Batch  66  loss:  0.019403552636504173
Batch  67  loss:  0.01811162382364273
Batch  68  loss:  0.02120864763855934
Batch  69  loss:  0.01547201257199049
Batch  70  loss:  0.01821795664727688
Batch  71  loss:  0.02034398540854454
Batch  72  loss:  0.019525999203324318
Batch  73  loss:  0.016866741701960564
Batch  74  loss:  0.02410675399005413
Batch  75  loss:  0.01819191873073578
Batch  76  loss:  0.016516663134098053
Batch  77  loss:  0.017663225531578064
Batch  78  loss:  0.02258675917983055
Batch  79  loss:  0.023308048024773598
Batch  80  loss:  0.0253961943089962
Batch  81  loss:  0.020777570083737373
Batch  82  loss:  0.019679052755236626
Batch  83  loss:  0.02059517428278923
Batch  84  loss:  0.02403748407959938
Batch  85  loss:  0.023492777720093727
Batch  86  loss:  0.016907190904021263
Batch  87  loss:  0.01624431274831295
Batch  88  loss:  0.018471648916602135
Batch  89  loss:  0.021045757457613945
Batch  90  loss:  0.015758849680423737
Batch  91  loss:  0.017846377566456795
Batch  92  loss:  0.021889500319957733
Batch  93  loss:  0.017793050035834312
Batch  94  loss:  0.015206805430352688
Batch  95  loss:  0.01801697164773941
Batch  96  loss:  0.02102803811430931
Batch  97  loss:  0.021392114460468292
Batch  98  loss:  0.015564022585749626
Batch  99  loss:  0.026022212579846382
Batch  100  loss:  0.016698163002729416
Validation: 
LOSS train 0.01976501972414553, val 0.019244248047471046
EPOCH : 15 TIME:  2022-04-18 20:34:51.674946
Training: 
Batch  1  loss:  0.020413154736161232
Batch  2  loss:  0.019972113892436028
Batch  3  loss:  0.016386553645133972
Batch  4  loss:  0.02125724032521248
Batch  5  loss:  0.024350013583898544
Batch  6  loss:  0.01812843419611454
Batch  7  loss:  0.01420580130070448
Batch  8  loss:  0.024514731019735336
Batch  9  loss:  0.018206341192126274
Batch  10  loss:  0.021351119503378868
Batch  11  loss:  0.0175697710365057
Batch  12  loss:  0.020490895956754684
Batch  13  loss:  0.02177976630628109
Batch  14  loss:  0.019746635109186172
Batch  15  loss:  0.022733887657523155
Batch  16  loss:  0.014134975150227547
Batch  17  loss:  0.02146105468273163
Batch  18  loss:  0.017349064350128174
Batch  19  loss:  0.01486621331423521
Batch  20  loss:  0.022908253595232964
Batch  21  loss:  0.01583634875714779
Batch  22  loss:  0.0180430319160223
Batch  23  loss:  0.02387266978621483
Batch  24  loss:  0.01797582022845745
Batch  25  loss:  0.021723493933677673
Batch  26  loss:  0.017013363540172577
Batch  27  loss:  0.01759508065879345
Batch  28  loss:  0.02222851850092411
Batch  29  loss:  0.020454686135053635
Batch  30  loss:  0.01666860468685627
Batch  31  loss:  0.021497663110494614
Batch  32  loss:  0.019688095897436142
Batch  33  loss:  0.020080476999282837
Batch  34  loss:  0.021427448838949203
Batch  35  loss:  0.02624030038714409
Batch  36  loss:  0.02614402212202549
Batch  37  loss:  0.018278388306498528
Batch  38  loss:  0.023588256910443306
Batch  39  loss:  0.01823626458644867
Batch  40  loss:  0.019787706434726715
Batch  41  loss:  0.017633942887187004
Batch  42  loss:  0.019281042739748955
Batch  43  loss:  0.015965111553668976
Batch  44  loss:  0.02439851313829422
Batch  45  loss:  0.019468169659376144
Batch  46  loss:  0.020657896995544434
Batch  47  loss:  0.016460280865430832
Batch  48  loss:  0.017218520864844322
Batch  49  loss:  0.02069671079516411
Batch  50  loss:  0.017855782061815262
Batch  51  loss:  0.019047226756811142
Batch  52  loss:  0.02192322164773941
Batch  53  loss:  0.017078518867492676
Batch  54  loss:  0.016630899161100388
Batch  55  loss:  0.020392220467329025
Batch  56  loss:  0.01691708341240883
Batch  57  loss:  0.019178451970219612
Batch  58  loss:  0.019855529069900513
Batch  59  loss:  0.019616585224866867
Batch  60  loss:  0.017748933285474777
Batch  61  loss:  0.017394738271832466
Batch  62  loss:  0.02003651112318039
Batch  63  loss:  0.025712374597787857
Batch  64  loss:  0.01761072315275669
Batch  65  loss:  0.01992405764758587
Batch  66  loss:  0.018997501581907272
Batch  67  loss:  0.020433053374290466
Batch  68  loss:  0.019236532971262932
Batch  69  loss:  0.018025202676653862
Batch  70  loss:  0.019157137721776962
Batch  71  loss:  0.0155723188072443
Batch  72  loss:  0.02035842090845108
Batch  73  loss:  0.01699507050216198
Batch  74  loss:  0.021735891699790955
Batch  75  loss:  0.02386430650949478
Batch  76  loss:  0.018639840185642242
Batch  77  loss:  0.020840592682361603
Batch  78  loss:  0.017161091789603233
Batch  79  loss:  0.019208047538995743
Batch  80  loss:  0.02405116893351078
Batch  81  loss:  0.018788862973451614
Batch  82  loss:  0.016200613230466843
Batch  83  loss:  0.018182341009378433
Batch  84  loss:  0.025911852717399597
Batch  85  loss:  0.021426459774374962
Batch  86  loss:  0.019944949075579643
Batch  87  loss:  0.019339514896273613
Batch  88  loss:  0.02430046536028385
Batch  89  loss:  0.018778180703520775
Batch  90  loss:  0.0189241673797369
Batch  91  loss:  0.01771337166428566
Batch  92  loss:  0.01989179477095604
Batch  93  loss:  0.016687704250216484
Batch  94  loss:  0.0192046407610178
Batch  95  loss:  0.0182086993008852
Batch  96  loss:  0.020395034924149513
Batch  97  loss:  0.016687095165252686
Batch  98  loss:  0.016644390299916267
Batch  99  loss:  0.025566061958670616
Batch  100  loss:  0.021395370364189148
Validation: 
LOSS train 0.019653770569711924, val 0.016955813392996788
EPOCH : 16 TIME:  2022-04-18 20:53:09.066644
Training: 
Batch  1  loss:  0.02240137755870819
Batch  2  loss:  0.018538368865847588
Batch  3  loss:  0.016214773058891296
Batch  4  loss:  0.018933231011033058
Batch  5  loss:  0.01754807122051716
Batch  6  loss:  0.020136110484600067
Batch  7  loss:  0.018117187544703484
Batch  8  loss:  0.01966807246208191
Batch  9  loss:  0.01827862486243248
Batch  10  loss:  0.019890196621418
Batch  11  loss:  0.012734286487102509
Batch  12  loss:  0.02169153466820717
Batch  13  loss:  0.017865054309368134
Batch  14  loss:  0.01593433879315853
Batch  15  loss:  0.018421124666929245
Batch  16  loss:  0.016860416159033775
Batch  17  loss:  0.022665049880743027
Batch  18  loss:  0.020457744598388672
Batch  19  loss:  0.020203381776809692
Batch  20  loss:  0.01906607113778591
Batch  21  loss:  0.01785328797996044
Batch  22  loss:  0.017933545634150505
Batch  23  loss:  0.01514747366309166
Batch  24  loss:  0.015535295009613037
Batch  25  loss:  0.017704525962471962
Batch  26  loss:  0.015207714401185513
Batch  27  loss:  0.01771172322332859
Batch  28  loss:  0.016667990013957024
Batch  29  loss:  0.020662112161517143
Batch  30  loss:  0.019937880337238312
Batch  31  loss:  0.016669059172272682
Batch  32  loss:  0.02111363783478737
Batch  33  loss:  0.017619293183088303
Batch  34  loss:  0.021889830008149147
Batch  35  loss:  0.018941087648272514
Batch  36  loss:  0.01926354318857193
Batch  37  loss:  0.023097028955817223
Batch  38  loss:  0.0169651061296463
Batch  39  loss:  0.01822930946946144
Batch  40  loss:  0.02161564864218235
Batch  41  loss:  0.020665287971496582
Batch  42  loss:  0.01684025302529335
Batch  43  loss:  0.021558303385972977
Batch  44  loss:  0.01578749157488346
Batch  45  loss:  0.020520001649856567
Batch  46  loss:  0.022149380296468735
Batch  47  loss:  0.017590126022696495
Batch  48  loss:  0.016908016055822372
Batch  49  loss:  0.021257251501083374
Batch  50  loss:  0.020914562046527863
Batch  51  loss:  0.01750604435801506
Batch  52  loss:  0.020876692607998848
Batch  53  loss:  0.017661452293395996
Batch  54  loss:  0.020710773766040802
Batch  55  loss:  0.017221026122570038
Batch  56  loss:  0.018708229064941406
Batch  57  loss:  0.015679098665714264
Batch  58  loss:  0.01661371998488903
Batch  59  loss:  0.020245136693120003
Batch  60  loss:  0.02004152536392212
Batch  61  loss:  0.019790535792708397
Batch  62  loss:  0.015593287535011768
Batch  63  loss:  0.020008202642202377
Batch  64  loss:  0.016728488728404045
Batch  65  loss:  0.014681946486234665
Batch  66  loss:  0.016481416299939156
Batch  67  loss:  0.016731474548578262
Batch  68  loss:  0.01925741508603096
Batch  69  loss:  0.013471859507262707
Batch  70  loss:  0.017533594742417336
Batch  71  loss:  0.022211888805031776
Batch  72  loss:  0.01548612117767334
Batch  73  loss:  0.014751538634300232
Batch  74  loss:  0.022798355668783188
Batch  75  loss:  0.02160864695906639
Batch  76  loss:  0.017583634704351425
Batch  77  loss:  0.01883539743721485
Batch  78  loss:  0.019159037619829178
Batch  79  loss:  0.017147712409496307
Batch  80  loss:  0.016910064965486526
Batch  81  loss:  0.01613732986152172
Batch  82  loss:  0.014777669683098793
Batch  83  loss:  0.016831886023283005
Batch  84  loss:  0.023357335478067398
Batch  85  loss:  0.022494085133075714
Batch  86  loss:  0.01846359856426716
Batch  87  loss:  0.01891152746975422
Batch  88  loss:  0.013501739129424095
Batch  89  loss:  0.01827692613005638
Batch  90  loss:  0.02119913510978222
Batch  91  loss:  0.016524236649274826
Batch  92  loss:  0.0182271096855402
Batch  93  loss:  0.019880665466189384
Batch  94  loss:  0.018419992178678513
Batch  95  loss:  0.01599571853876114
Batch  96  loss:  0.020056607201695442
Batch  97  loss:  0.02084921859204769
Batch  98  loss:  0.018259281292557716
Batch  99  loss:  0.014805173501372337
Batch  100  loss:  0.019186044111847878
Validation: 
LOSS train 0.018477703807875515, val 0.01590070128440857
EPOCH : 17 TIME:  2022-04-18 21:11:23.744139
Training: 
Batch  1  loss:  0.019615985453128815
Batch  2  loss:  0.02283482998609543
Batch  3  loss:  0.021291283890604973
Batch  4  loss:  0.018248556181788445
Batch  5  loss:  0.014147762209177017
Batch  6  loss:  0.01576833613216877
Batch  7  loss:  0.019470039755105972
Batch  8  loss:  0.020139142870903015
Batch  9  loss:  0.015566463582217693
Batch  10  loss:  0.01923115737736225
Batch  11  loss:  0.019704563543200493
Batch  12  loss:  0.02231677435338497
Batch  13  loss:  0.0126012759283185
Batch  14  loss:  0.017711743712425232
Batch  15  loss:  0.01549482997506857
Batch  16  loss:  0.01837799698114395
Batch  17  loss:  0.022415781393647194
Batch  18  loss:  0.019175546243786812
Batch  19  loss:  0.020350929349660873
Batch  20  loss:  0.01727931760251522
Batch  21  loss:  0.020281843841075897
Batch  22  loss:  0.01584009826183319
Batch  23  loss:  0.018298819661140442
Batch  24  loss:  0.014793972484767437
Batch  25  loss:  0.01804380863904953
Batch  26  loss:  0.01482060644775629
Batch  27  loss:  0.0182325541973114
Batch  28  loss:  0.02685859613120556
Batch  29  loss:  0.020908359438180923
Batch  30  loss:  0.015864796936511993
Batch  31  loss:  0.014096145518124104
Batch  32  loss:  0.016655726358294487
Batch  33  loss:  0.01950935646891594
Batch  34  loss:  0.024883979931473732
Batch  35  loss:  0.017349520698189735
Batch  36  loss:  0.019145561382174492
Batch  37  loss:  0.02108045294880867
Batch  38  loss:  0.01871892809867859
Batch  39  loss:  0.014473413117229939
Batch  40  loss:  0.012788529507815838
Batch  41  loss:  0.018175175413489342
Batch  42  loss:  0.017787015065550804
Batch  43  loss:  0.01728232391178608
Batch  44  loss:  0.020240308716893196
Batch  45  loss:  0.019170910120010376
Batch  46  loss:  0.01647263765335083
Batch  47  loss:  0.01495410967618227
Batch  48  loss:  0.019238106906414032
Batch  49  loss:  0.01796715334057808
Batch  50  loss:  0.01626911200582981
Batch  51  loss:  0.019314484670758247
Batch  52  loss:  0.0214865542948246
Batch  53  loss:  0.018334202468395233
Batch  54  loss:  0.018275244161486626
Batch  55  loss:  0.02118007466197014
Batch  56  loss:  0.019912011921405792
Batch  57  loss:  0.01685629040002823
Batch  58  loss:  0.016789553686976433
Batch  59  loss:  0.014044965617358685
Batch  60  loss:  0.016679784283041954
Batch  61  loss:  0.017832690849900246
Batch  62  loss:  0.020452337339520454
Batch  63  loss:  0.017660047858953476
Batch  64  loss:  0.018236007541418076
Batch  65  loss:  0.018984846770763397
Batch  66  loss:  0.017373450100421906
Batch  67  loss:  0.01501366589218378
Batch  68  loss:  0.018951131030917168
Batch  69  loss:  0.017027070745825768
Batch  70  loss:  0.017349962145090103
Batch  71  loss:  0.01633337140083313
Batch  72  loss:  0.017321761697530746
Batch  73  loss:  0.021405959501862526
Batch  74  loss:  0.01753980852663517
Batch  75  loss:  0.02142341062426567
Batch  76  loss:  0.01747955195605755
Batch  77  loss:  0.017595581710338593
Batch  78  loss:  0.01561078242957592
Batch  79  loss:  0.018344683572649956
Batch  80  loss:  0.01566789485514164
Batch  81  loss:  0.01771325059235096
Batch  82  loss:  0.019565243273973465
Batch  83  loss:  0.02034003660082817
Batch  84  loss:  0.018824579194188118
Batch  85  loss:  0.01675218529999256
Batch  86  loss:  0.019706733524799347
Batch  87  loss:  0.0171869695186615
Batch  88  loss:  0.01569300703704357
Batch  89  loss:  0.024387216195464134
Batch  90  loss:  0.01310273539274931
Batch  91  loss:  0.017323290929198265
Batch  92  loss:  0.017295269295573235
Batch  93  loss:  0.017184589058160782
Batch  94  loss:  0.016486134380102158
Batch  95  loss:  0.017518844455480576
Batch  96  loss:  0.017285533249378204
Batch  97  loss:  0.019343895837664604
Batch  98  loss:  0.019498653709888458
Batch  99  loss:  0.02020612731575966
Batch  100  loss:  0.02083737403154373
Validation: 
LOSS train 0.018186730910092593, val 0.014703829772770405
EPOCH : 18 TIME:  2022-04-18 21:29:36.404623
Training: 
Batch  1  loss:  0.0169989001005888
Batch  2  loss:  0.013672654516994953
Batch  3  loss:  0.015923263505101204
Batch  4  loss:  0.019304092973470688
Batch  5  loss:  0.01982615888118744
Batch  6  loss:  0.01565905101597309
Batch  7  loss:  0.016140857711434364
Batch  8  loss:  0.016754906624555588
Batch  9  loss:  0.02024054527282715
Batch  10  loss:  0.01712866686284542
Batch  11  loss:  0.021865829825401306
Batch  12  loss:  0.016613805666565895
Batch  13  loss:  0.01789787784218788
Batch  14  loss:  0.022287534549832344
Batch  15  loss:  0.016171613708138466
Batch  16  loss:  0.020071934908628464
Batch  17  loss:  0.01769237406551838
Batch  18  loss:  0.015608997084200382
Batch  19  loss:  0.016915371641516685
Batch  20  loss:  0.015766380354762077
Batch  21  loss:  0.01667085662484169
Batch  22  loss:  0.017565017566084862
Batch  23  loss:  0.01634318009018898
Batch  24  loss:  0.013486411422491074
Batch  25  loss:  0.019525248557329178
Batch  26  loss:  0.02227935567498207
Batch  27  loss:  0.01716768369078636
Batch  28  loss:  0.018910624086856842
Batch  29  loss:  0.021828198805451393
Batch  30  loss:  0.01946965418756008
Batch  31  loss:  0.01983685977756977
Batch  32  loss:  0.013924152590334415
Batch  33  loss:  0.02534409426152706
Batch  34  loss:  0.017319614067673683
Batch  35  loss:  0.01575303263962269
Batch  36  loss:  0.018335144966840744
Batch  37  loss:  0.022267945110797882
Batch  38  loss:  0.01956002786755562
Batch  39  loss:  0.014420623891055584
Batch  40  loss:  0.01853303425014019
Batch  41  loss:  0.016985945403575897
Batch  42  loss:  0.017734723165631294
Batch  43  loss:  0.015340682119131088
Batch  44  loss:  0.023808645084500313
Batch  45  loss:  0.02000688575208187
Batch  46  loss:  0.01900196447968483
Batch  47  loss:  0.01548193022608757
Batch  48  loss:  0.015081471763551235
Batch  49  loss:  0.020134391263127327
Batch  50  loss:  0.015314140357077122
Batch  51  loss:  0.017589222639799118
Batch  52  loss:  0.01765107363462448
Batch  53  loss:  0.017851674929261208
Batch  54  loss:  0.018352089449763298
Batch  55  loss:  0.018744729459285736
Batch  56  loss:  0.013011513277888298
Batch  57  loss:  0.016861604526638985
Batch  58  loss:  0.01702006720006466
Batch  59  loss:  0.01871240697801113
Batch  60  loss:  0.022284507751464844
Batch  61  loss:  0.018551256507635117
Batch  62  loss:  0.021671772003173828
Batch  63  loss:  0.015489587560296059
Batch  64  loss:  0.019116438925266266
Batch  65  loss:  0.016846925020217896
Batch  66  loss:  0.01811041496694088
Batch  67  loss:  0.015459730289876461
Batch  68  loss:  0.017514163628220558
Batch  69  loss:  0.016757823526859283
Batch  70  loss:  0.021661294624209404
Batch  71  loss:  0.012782251462340355
Batch  72  loss:  0.019699107855558395
Batch  73  loss:  0.016832703724503517
Batch  74  loss:  0.018148357048630714
Batch  75  loss:  0.01690622977912426
Batch  76  loss:  0.022114301100373268
Batch  77  loss:  0.017623035237193108
Batch  78  loss:  0.014237086288630962
Batch  79  loss:  0.019714850932359695
Batch  80  loss:  0.01872183382511139
Batch  81  loss:  0.01810445636510849
Batch  82  loss:  0.017264485359191895
Batch  83  loss:  0.018613111227750778
Batch  84  loss:  0.019549455493688583
Batch  85  loss:  0.016830088570713997
Batch  86  loss:  0.014312120154500008
Batch  87  loss:  0.019582288339734077
Batch  88  loss:  0.01713855192065239
Batch  89  loss:  0.016849739477038383
Batch  90  loss:  0.021652504801750183
Batch  91  loss:  0.027079500257968903
Batch  92  loss:  0.01390124298632145
Batch  93  loss:  0.022029642015695572
Batch  94  loss:  0.017471706494688988
Batch  95  loss:  0.015495974570512772
Batch  96  loss:  0.019568420946598053
Batch  97  loss:  0.022304875776171684
Batch  98  loss:  0.01784623973071575
Batch  99  loss:  0.015242054127156734
Batch  100  loss:  0.016120318323373795
Validation: 
LOSS train 0.01802967187948525, val 0.020024260506033897
EPOCH : 19 TIME:  2022-04-18 21:48:22.958957
Training: 
Batch  1  loss:  0.020192528143525124
Batch  2  loss:  0.01763361692428589
Batch  3  loss:  0.018626470118761063
Batch  4  loss:  0.013806006871163845
Batch  5  loss:  0.021482529118657112
Batch  6  loss:  0.020120399072766304
Batch  7  loss:  0.019907498732209206
Batch  8  loss:  0.017377672716975212
Batch  9  loss:  0.020034510642290115
Batch  10  loss:  0.016769926995038986
Batch  11  loss:  0.018403630703687668
Batch  12  loss:  0.016346899792551994
Batch  13  loss:  0.017189061269164085
Batch  14  loss:  0.01873035915195942
Batch  15  loss:  0.018726184964179993
Batch  16  loss:  0.01682041399180889
Batch  17  loss:  0.01465645432472229
Batch  18  loss:  0.013779770582914352
Batch  19  loss:  0.019704576581716537
Batch  20  loss:  0.02022555284202099
Batch  21  loss:  0.019861659035086632
Batch  22  loss:  0.023644160479307175
Batch  23  loss:  0.01686117984354496
Batch  24  loss:  0.0135350888594985
Batch  25  loss:  0.02148578129708767
Batch  26  loss:  0.016701802611351013
Batch  27  loss:  0.015542919747531414
Batch  28  loss:  0.014237903989851475
Batch  29  loss:  0.01810302771627903
Batch  30  loss:  0.015010030008852482
Batch  31  loss:  0.015045495703816414
Batch  32  loss:  0.016916388645768166
Batch  33  loss:  0.014177593402564526
Batch  34  loss:  0.02066200040280819
Batch  35  loss:  0.015321733430027962
Batch  36  loss:  0.015259944833815098
Batch  37  loss:  0.01599639654159546
Batch  38  loss:  0.02131868340075016
Batch  39  loss:  0.019956126809120178
Batch  40  loss:  0.013303634710609913
Batch  41  loss:  0.014804058708250523
Batch  42  loss:  0.02356978878378868
Batch  43  loss:  0.01550117414444685
Batch  44  loss:  0.015547188930213451
Batch  45  loss:  0.021504396572709084
Batch  46  loss:  0.01638510636985302
Batch  47  loss:  0.020539183169603348
Batch  48  loss:  0.02177748642861843
Batch  49  loss:  0.02217528596520424
Batch  50  loss:  0.01779348962008953
Batch  51  loss:  0.01762242801487446
Batch  52  loss:  0.014971759170293808
Batch  53  loss:  0.019029507413506508
Batch  54  loss:  0.01901453174650669
Batch  55  loss:  0.01675979606807232
Batch  56  loss:  0.016793834045529366
Batch  57  loss:  0.01528033334761858
Batch  58  loss:  0.01583368331193924
Batch  59  loss:  0.01745990477502346
Batch  60  loss:  0.016234898939728737
Batch  61  loss:  0.014795529656112194
Batch  62  loss:  0.018400464206933975
Batch  63  loss:  0.018981706351041794
Batch  64  loss:  0.016965089365839958
Batch  65  loss:  0.018181949853897095
Batch  66  loss:  0.017824295908212662
Batch  67  loss:  0.02046998403966427
Batch  68  loss:  0.014252987690269947
Batch  69  loss:  0.016545932739973068
Batch  70  loss:  0.013732144609093666
Batch  71  loss:  0.014763440936803818
Batch  72  loss:  0.018423711881041527
Batch  73  loss:  0.01684720255434513
Batch  74  loss:  0.018134402111172676
Batch  75  loss:  0.016043543815612793
Batch  76  loss:  0.017929021269083023
Batch  77  loss:  0.017217839136719704
Batch  78  loss:  0.013795113191008568
Batch  79  loss:  0.015567855909466743
Batch  80  loss:  0.0177922286093235
Batch  81  loss:  0.016130484640598297
Batch  82  loss:  0.01696467585861683
Batch  83  loss:  0.01923510804772377
Batch  84  loss:  0.02004699595272541
Batch  85  loss:  0.01548624038696289
Batch  86  loss:  0.01923709362745285
Batch  87  loss:  0.017504187300801277
Batch  88  loss:  0.018982702866196632
Batch  89  loss:  0.020239075645804405
Batch  90  loss:  0.020638959482312202
Batch  91  loss:  0.017432577908039093
Batch  92  loss:  0.019012482836842537
Batch  93  loss:  0.018611837178468704
Batch  94  loss:  0.01789863221347332
Batch  95  loss:  0.015631744638085365
Batch  96  loss:  0.018052462488412857
Batch  97  loss:  0.01847521774470806
Batch  98  loss:  0.014475652948021889
Batch  99  loss:  0.02453741431236267
Batch  100  loss:  0.02128322795033455
Validation: 
LOSS train 0.017686146683990954, val 0.01910446770489216
EPOCH : 20 TIME:  2022-04-18 22:06:49.804135
Training: 
Batch  1  loss:  0.019272634759545326
Batch  2  loss:  0.013066829182207584
Batch  3  loss:  0.023657679557800293
Batch  4  loss:  0.02088739350438118
Batch  5  loss:  0.01687641628086567
Batch  6  loss:  0.015233457088470459
Batch  7  loss:  0.018309209495782852
Batch  8  loss:  0.024415137246251106
Batch  9  loss:  0.014139094389975071
Batch  10  loss:  0.01217365451157093
Batch  11  loss:  0.01880609057843685
Batch  12  loss:  0.01551508903503418
Batch  13  loss:  0.024005208164453506
Batch  14  loss:  0.01471087895333767
Batch  15  loss:  0.01303992047905922
Batch  16  loss:  0.014414845034480095
Batch  17  loss:  0.015231091529130936
Batch  18  loss:  0.01651000790297985
Batch  19  loss:  0.014180796220898628
Batch  20  loss:  0.017823614180088043
Batch  21  loss:  0.01959739811718464
Batch  22  loss:  0.010378694161772728
Batch  23  loss:  0.01216112356632948
Batch  24  loss:  0.020375141873955727
Batch  25  loss:  0.017638660967350006
Batch  26  loss:  0.014279193244874477
Batch  27  loss:  0.018921198323369026
Batch  28  loss:  0.02014109119772911
Batch  29  loss:  0.016451120376586914
Batch  30  loss:  0.014316546730697155
Batch  31  loss:  0.01779923588037491
Batch  32  loss:  0.01712673343718052
Batch  33  loss:  0.015230359509587288
Batch  34  loss:  0.013668167404830456
Batch  35  loss:  0.016335083171725273
Batch  36  loss:  0.016336584463715553
Batch  37  loss:  0.02076541818678379
Batch  38  loss:  0.018487824127078056
Batch  39  loss:  0.01775169186294079
Batch  40  loss:  0.02014613151550293
Batch  41  loss:  0.014791140332818031
Batch  42  loss:  0.014491376467049122
Batch  43  loss:  0.019484201446175575
Batch  44  loss:  0.01654590666294098
Batch  45  loss:  0.01699288934469223
Batch  46  loss:  0.014218859374523163
Batch  47  loss:  0.016801727935671806
Batch  48  loss:  0.016450855880975723
Batch  49  loss:  0.016799313947558403
Batch  50  loss:  0.012634305283427238
Batch  51  loss:  0.015111912041902542
Batch  52  loss:  0.014120389707386494
Batch  53  loss:  0.017318885773420334
Batch  54  loss:  0.01353953592479229
Batch  55  loss:  0.013254323042929173
Batch  56  loss:  0.016637904569506645
Batch  57  loss:  0.013911464251577854
Batch  58  loss:  0.01650356315076351
Batch  59  loss:  0.012458360753953457
Batch  60  loss:  0.014588677324354649
Batch  61  loss:  0.018923861905932426
Batch  62  loss:  0.017036644741892815
Batch  63  loss:  0.016900120303034782
Batch  64  loss:  0.020464420318603516
Batch  65  loss:  0.018011119216680527
Batch  66  loss:  0.018040427938103676
Batch  67  loss:  0.02231169119477272
Batch  68  loss:  0.01712171547114849
Batch  69  loss:  0.0199577696621418
Batch  70  loss:  0.023708632215857506
Batch  71  loss:  0.014806106686592102
Batch  72  loss:  0.021351870149374008
Batch  73  loss:  0.018722686916589737
Batch  74  loss:  0.018461814150214195
Batch  75  loss:  0.01861562579870224
Batch  76  loss:  0.018815239891409874
Batch  77  loss:  0.015716921538114548
Batch  78  loss:  0.01727977767586708
Batch  79  loss:  0.016666637733578682
Batch  80  loss:  0.017774252220988274
Batch  81  loss:  0.013244982808828354
Batch  82  loss:  0.017020732164382935
Batch  83  loss:  0.017704958096146584
Batch  84  loss:  0.02064463123679161
Batch  85  loss:  0.01707378774881363
Batch  86  loss:  0.017146633937954903
Batch  87  loss:  0.017173007130622864
Batch  88  loss:  0.018349431455135345
Batch  89  loss:  0.019087307155132294
Batch  90  loss:  0.017972666770219803
Batch  91  loss:  0.016219578683376312
Batch  92  loss:  0.02031693235039711
Batch  93  loss:  0.015727510675787926
Batch  94  loss:  0.013912150636315346
Batch  95  loss:  0.014070749282836914
Batch  96  loss:  0.015278263948857784
Batch  97  loss:  0.021221647039055824
Batch  98  loss:  0.01835736073553562
Batch  99  loss:  0.017223581671714783
Batch  100  loss:  0.01548804622143507
Validation: 
LOSS train 0.017007533349096774, val 0.014329208992421627
Mon 18 Apr 2022 10:25:09 PM EDT
GUITAR:: POINT TRANSFORMER
INFO - 2022-04-18 22:25:11,700 - utils - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO - 2022-04-18 22:25:11,700 - utils - NumExpr defaulting to 8 threads.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  guitar
--------------------
Running self_supervised_training:  2022-04-18 22:25:11.847855
--------------------
device is  cuda
--------------------
Number of trainable parameters:  893083
EPOCH : 1 TIME:  2022-04-18 22:25:13.626048
Training: 
Batch  1  loss:  0.358206182718277
Batch  2  loss:  0.16137194633483887
Batch  3  loss:  0.23788097500801086
Batch  4  loss:  0.30152082443237305
Batch  5  loss:  0.1632734090089798
Batch  6  loss:  0.10531356930732727
Batch  7  loss:  0.14801451563835144
Batch  8  loss:  0.1702893078327179
Batch  9  loss:  0.13398997485637665
Batch  10  loss:  0.0903419479727745
Batch  11  loss:  0.06880345940589905
Batch  12  loss:  0.07010568678379059
Batch  13  loss:  0.07841560989618301
Batch  14  loss:  0.08067728579044342
Batch  15  loss:  0.07876873761415482
Batch  16  loss:  0.0748462826013565
Batch  17  loss:  0.06254446506500244
Batch  18  loss:  0.050282638520002365
Batch  19  loss:  0.04227606579661369
Batch  20  loss:  0.04249928891658783
Batch  21  loss:  0.038192786276340485
Batch  22  loss:  0.03912530839443207
Batch  23  loss:  0.041214894503355026
Batch  24  loss:  0.03721385821700096
Batch  25  loss:  0.037554845213890076
Batch  26  loss:  0.036260392516851425
Batch  27  loss:  0.03897187113761902
Batch  28  loss:  0.03820332512259483
Batch  29  loss:  0.03331558033823967
Batch  30  loss:  0.03200477734208107
Batch  31  loss:  0.030565781518816948
Batch  32  loss:  0.029220227152109146
Batch  33  loss:  0.028367532417178154
Batch  34  loss:  0.027300966903567314
Batch  35  loss:  0.027926728129386902
Batch  36  loss:  0.0256641935557127
Batch  37  loss:  0.023790039122104645
Batch  38  loss:  0.025864027440547943
Batch  39  loss:  0.02483096532523632
Batch  40  loss:  0.025075718760490417
Batch  41  loss:  0.025682661682367325
Batch  42  loss:  0.026616109535098076
Batch  43  loss:  0.02563525177538395
Batch  44  loss:  0.025790639221668243
Batch  45  loss:  0.026306597515940666
Batch  46  loss:  0.023677434772253036
Batch  47  loss:  0.022820333018898964
Batch  48  loss:  0.021382946521043777
Batch  49  loss:  0.023014230653643608
Batch  50  loss:  0.023485111072659492
Batch  51  loss:  0.023629188537597656
Batch  52  loss:  0.023853320628404617
Batch  53  loss:  0.025822089985013008
Batch  54  loss:  0.02083340659737587
Batch  55  loss:  0.021839972585439682
Batch  56  loss:  0.023042256012558937
Batch  57  loss:  0.02178574912250042
Batch  58  loss:  0.0179375521838665
Batch  59  loss:  0.02240290865302086
Batch  60  loss:  0.02090965211391449
Batch  61  loss:  0.02222503162920475
Batch  62  loss:  0.021437082439661026
Batch  63  loss:  0.02078581228852272
Batch  64  loss:  0.02000466361641884
Batch  65  loss:  0.019076986238360405
Batch  66  loss:  0.019460143521428108
Batch  67  loss:  0.01816260814666748
Batch  68  loss:  0.020161692053079605
Batch  69  loss:  0.017940951511263847
Batch  70  loss:  0.01904831826686859
Batch  71  loss:  0.01767694763839245
Batch  72  loss:  0.018649758771061897
Batch  73  loss:  0.018306974321603775
Batch  74  loss:  0.017554691061377525
Batch  75  loss:  0.019161639735102654
Batch  76  loss:  0.018173620104789734
Batch  77  loss:  0.017231348901987076
Batch  78  loss:  0.020226703956723213
Batch  79  loss:  0.01779111661016941
Batch  80  loss:  0.01714910753071308
Batch  81  loss:  0.01711162179708481
Batch  82  loss:  0.0166507288813591
Batch  83  loss:  0.01710049994289875
Batch  84  loss:  0.01810312271118164
Batch  85  loss:  0.018024077638983727
Batch  86  loss:  0.01864038221538067
Batch  87  loss:  0.01654021255671978
Batch  88  loss:  0.016868850216269493
Batch  89  loss:  0.018309928476810455
Batch  90  loss:  0.01653820089995861
Batch  91  loss:  0.01761138252913952
Batch  92  loss:  0.01761075295507908
Batch  93  loss:  0.01727287657558918
Batch  94  loss:  0.01578430086374283
Batch  95  loss:  0.016216637566685677
Batch  96  loss:  0.016232265159487724
Batch  97  loss:  0.016420679166913033
Batch  98  loss:  0.015443812124431133
Batch  99  loss:  0.0167232695966959
Batch  100  loss:  0.016809506341814995
Validation: 
LOSS train 0.043467917116358876, val 0.015949783846735954
EPOCH : 2 TIME:  2022-04-18 22:38:20.409146
Training: 
Batch  1  loss:  0.016700992360711098
Batch  2  loss:  0.017459481954574585
Batch  3  loss:  0.0156827662140131
Batch  4  loss:  0.017247596755623817
Batch  5  loss:  0.015723899006843567
Batch  6  loss:  0.015897396951913834
Batch  7  loss:  0.016741635277867317
Batch  8  loss:  0.015275015495717525
Batch  9  loss:  0.016392549499869347
Batch  10  loss:  0.01756344921886921
Batch  11  loss:  0.016271209344267845
Batch  12  loss:  0.015927493572235107
Batch  13  loss:  0.015329158864915371
Batch  14  loss:  0.014471962116658688
Batch  15  loss:  0.015635346993803978
Batch  16  loss:  0.01623501628637314
Batch  17  loss:  0.015055805444717407
Batch  18  loss:  0.016443710774183273
Batch  19  loss:  0.015270342119038105
Batch  20  loss:  0.016865674406290054
Batch  21  loss:  0.015977568924427032
Batch  22  loss:  0.015539082698523998
Batch  23  loss:  0.015675581991672516
Batch  24  loss:  0.015561103820800781
Batch  25  loss:  0.015859780833125114
Batch  26  loss:  0.01501496136188507
Batch  27  loss:  0.016171475872397423
Batch  28  loss:  0.015149950981140137
Batch  29  loss:  0.015650564804673195
Batch  30  loss:  0.017133085057139397
Batch  31  loss:  0.017562435939908028
Batch  32  loss:  0.014162372797727585
Batch  33  loss:  0.016167068853974342
Batch  34  loss:  0.015089718624949455
Batch  35  loss:  0.014852928929030895
Batch  36  loss:  0.015483547933399677
Batch  37  loss:  0.014840909279882908
Batch  38  loss:  0.014342458918690681
Batch  39  loss:  0.017306175082921982
Batch  40  loss:  0.014450756832957268
Batch  41  loss:  0.014799841679632664
Batch  42  loss:  0.017120175063610077
Batch  43  loss:  0.014161752536892891
Batch  44  loss:  0.014395869337022305
Batch  45  loss:  0.015084711834788322
Batch  46  loss:  0.014085594564676285
Batch  47  loss:  0.014845266006886959
Batch  48  loss:  0.015635428950190544
Batch  49  loss:  0.017628774046897888
Batch  50  loss:  0.014469611458480358
Batch  51  loss:  0.017063306644558907
Batch  52  loss:  0.015798572450876236
Batch  53  loss:  0.013807876035571098
Batch  54  loss:  0.015365168452262878
Batch  55  loss:  0.014689858071506023
Batch  56  loss:  0.014743315987288952
Batch  57  loss:  0.014691482298076153
Batch  58  loss:  0.015381119213998318
Batch  59  loss:  0.015835341066122055
Batch  60  loss:  0.015909399837255478
Batch  61  loss:  0.014478063210844994
Batch  62  loss:  0.015154712833464146
Batch  63  loss:  0.014094247482717037
Batch  64  loss:  0.015177757479250431
Batch  65  loss:  0.014728279784321785
Batch  66  loss:  0.01634431816637516
Batch  67  loss:  0.01523061003535986
Batch  68  loss:  0.016896121203899384
Batch  69  loss:  0.014055024832487106
Batch  70  loss:  0.014399112202227116
Batch  71  loss:  0.015094312839210033
Batch  72  loss:  0.014907599426805973
Batch  73  loss:  0.015056478790938854
Batch  74  loss:  0.015036992728710175
Batch  75  loss:  0.014305021613836288
Batch  76  loss:  0.01535852998495102
Batch  77  loss:  0.014884025789797306
Batch  78  loss:  0.014447147957980633
Batch  79  loss:  0.014893674291670322
Batch  80  loss:  0.014972950331866741
Batch  81  loss:  0.0152907008305192
Batch  82  loss:  0.014659908600151539
Batch  83  loss:  0.014896886423230171
Batch  84  loss:  0.014274241402745247
Batch  85  loss:  0.01470821350812912
Batch  86  loss:  0.015822527930140495
Batch  87  loss:  0.01482473686337471
Batch  88  loss:  0.01609150692820549
Batch  89  loss:  0.013476062566041946
Batch  90  loss:  0.01358045544475317
Batch  91  loss:  0.013844418339431286
Batch  92  loss:  0.014596568420529366
Batch  93  loss:  0.014736703597009182
Batch  94  loss:  0.014515357092022896
Batch  95  loss:  0.014768932946026325
Batch  96  loss:  0.015029669739305973
Batch  97  loss:  0.014683534391224384
Batch  98  loss:  0.014330222271382809
Batch  99  loss:  0.01380747277289629
Batch  100  loss:  0.015081812627613544
Validation: 
LOSS train 0.015322294114157557, val 0.013893592171370983
EPOCH : 3 TIME:  2022-04-18 22:51:27.002569
Training: 
Batch  1  loss:  0.013342717662453651
Batch  2  loss:  0.014349684119224548
Batch  3  loss:  0.01403073500841856
Batch  4  loss:  0.015536668710410595
Batch  5  loss:  0.014473862014710903
Batch  6  loss:  0.015584922395646572
Batch  7  loss:  0.013911091722548008
Batch  8  loss:  0.014873161911964417
Batch  9  loss:  0.014318433590233326
Batch  10  loss:  0.013925368897616863
Batch  11  loss:  0.014874884858727455
Batch  12  loss:  0.015322712250053883
Batch  13  loss:  0.014335881918668747
Batch  14  loss:  0.01394370011985302
Batch  15  loss:  0.014874003827571869
Batch  16  loss:  0.01406470499932766
Batch  17  loss:  0.01446186751127243
Batch  18  loss:  0.014663934707641602
Batch  19  loss:  0.014528317376971245
Batch  20  loss:  0.01368029322475195
Batch  21  loss:  0.0136244036257267
Batch  22  loss:  0.013630388304591179
Batch  23  loss:  0.014017168432474136
Batch  24  loss:  0.013850770890712738
Batch  25  loss:  0.015191729180514812
Batch  26  loss:  0.014546186663210392
Batch  27  loss:  0.013875403441488743
Batch  28  loss:  0.014036688953638077
Batch  29  loss:  0.014782792888581753
Batch  30  loss:  0.013434778898954391
Batch  31  loss:  0.014565484598279
Batch  32  loss:  0.01453450322151184
Batch  33  loss:  0.01400868222117424
Batch  34  loss:  0.01404347363859415
Batch  35  loss:  0.014178161509335041
Batch  36  loss:  0.013873149640858173
Batch  37  loss:  0.01416698656976223
Batch  38  loss:  0.013615641742944717
Batch  39  loss:  0.013479967601597309
Batch  40  loss:  0.014262796379625797
Batch  41  loss:  0.014306792058050632
Batch  42  loss:  0.013256332837045193
Batch  43  loss:  0.01406469102948904
Batch  44  loss:  0.012928475625813007
Batch  45  loss:  0.015132956206798553
Batch  46  loss:  0.014491581358015537
Batch  47  loss:  0.014734634198248386
Batch  48  loss:  0.01501216646283865
Batch  49  loss:  0.013169221580028534
Batch  50  loss:  0.014489135704934597
Batch  51  loss:  0.014012185856699944
Batch  52  loss:  0.013629531487822533
Batch  53  loss:  0.013739998452365398
Batch  54  loss:  0.013716163113713264
Batch  55  loss:  0.013612782582640648
Batch  56  loss:  0.012979516759514809
Batch  57  loss:  0.013897474855184555
Batch  58  loss:  0.014497163705527782
Batch  59  loss:  0.013092605397105217
Batch  60  loss:  0.013192599639296532
Batch  61  loss:  0.012923046946525574
Batch  62  loss:  0.013268062844872475
Batch  63  loss:  0.014023913070559502
Batch  64  loss:  0.013331837020814419
Batch  65  loss:  0.012964713387191296
Batch  66  loss:  0.01497461274266243
Batch  67  loss:  0.014761295169591904
Batch  68  loss:  0.01313330139964819
Batch  69  loss:  0.014240963384509087
Batch  70  loss:  0.013297740370035172
Batch  71  loss:  0.013757060281932354
Batch  72  loss:  0.013756911270320415
Batch  73  loss:  0.012941400520503521
Batch  74  loss:  0.013653521426022053
Batch  75  loss:  0.01299502607434988
Batch  76  loss:  0.014074807055294514
Batch  77  loss:  0.013231609016656876
Batch  78  loss:  0.01383224967867136
Batch  79  loss:  0.01328688208013773
Batch  80  loss:  0.013266329653561115
Batch  81  loss:  0.014237336814403534
Batch  82  loss:  0.013675146736204624
Batch  83  loss:  0.013761291280388832
Batch  84  loss:  0.014583239331841469
Batch  85  loss:  0.014064631424844265
Batch  86  loss:  0.014206442050635815
Batch  87  loss:  0.014042936265468597
Batch  88  loss:  0.013818751089274883
Batch  89  loss:  0.013602650724351406
Batch  90  loss:  0.01372319646179676
Batch  91  loss:  0.013856327161192894
Batch  92  loss:  0.013322150334715843
Batch  93  loss:  0.013752982020378113
Batch  94  loss:  0.013642572797834873
Batch  95  loss:  0.01278194971382618
Batch  96  loss:  0.014341861009597778
Batch  97  loss:  0.013138179667294025
Batch  98  loss:  0.013543380424380302
Batch  99  loss:  0.013481438159942627
Batch  100  loss:  0.013486726209521294
Validation: 
LOSS train 0.013955505872145295, val 0.01375400647521019
EPOCH : 4 TIME:  2022-04-18 23:04:33.669110
Training: 
Batch  1  loss:  0.014127071015536785
Batch  2  loss:  0.014044864103198051
Batch  3  loss:  0.012897840701043606
Batch  4  loss:  0.013439030386507511
Batch  5  loss:  0.013425939716398716
Batch  6  loss:  0.013860332779586315
Batch  7  loss:  0.013961730524897575
Batch  8  loss:  0.012847058475017548
Batch  9  loss:  0.013995861634612083
Batch  10  loss:  0.013494725339114666
Batch  11  loss:  0.014065075665712357
Batch  12  loss:  0.013516273349523544
Batch  13  loss:  0.01411542110145092
Batch  14  loss:  0.01385616883635521
Batch  15  loss:  0.014452934265136719
Batch  16  loss:  0.013407140970230103
Batch  17  loss:  0.012351573444902897
Batch  18  loss:  0.012992160394787788
Batch  19  loss:  0.012674819678068161
Batch  20  loss:  0.013277024030685425
Batch  21  loss:  0.01324119046330452
Batch  22  loss:  0.013160987757146358
Batch  23  loss:  0.014315907843410969
Batch  24  loss:  0.013327940367162228
Batch  25  loss:  0.014163295738399029
Batch  26  loss:  0.013944678008556366
Batch  27  loss:  0.013901302590966225
Batch  28  loss:  0.012228226289153099
Batch  29  loss:  0.01488703303039074
Batch  30  loss:  0.013675272464752197
Batch  31  loss:  0.01294131763279438
Batch  32  loss:  0.012528153136372566
Batch  33  loss:  0.013558482751250267
Batch  34  loss:  0.012567086145281792
Batch  35  loss:  0.013370056636631489
Batch  36  loss:  0.012987052090466022
Batch  37  loss:  0.013327227905392647
Batch  38  loss:  0.014172856695950031
Batch  39  loss:  0.01384759321808815
Batch  40  loss:  0.014169848524034023
Batch  41  loss:  0.012615518644452095
Batch  42  loss:  0.0162871852517128
Batch  43  loss:  0.014121142216026783
Batch  44  loss:  0.013072090223431587
Batch  45  loss:  0.012851547449827194
Batch  46  loss:  0.01427444163709879
Batch  47  loss:  0.013509493321180344
Batch  48  loss:  0.01343551091849804
Batch  49  loss:  0.01330655999481678
Batch  50  loss:  0.014012537896633148
Batch  51  loss:  0.013178667984902859
Batch  52  loss:  0.013069086708128452
Batch  53  loss:  0.014203318394720554
Batch  54  loss:  0.013286436907947063
Batch  55  loss:  0.014302522875368595
Batch  56  loss:  0.01365022361278534
Batch  57  loss:  0.013097398914396763
Batch  58  loss:  0.013144130818545818
Batch  59  loss:  0.013006565161049366
Batch  60  loss:  0.013322864659130573
Batch  61  loss:  0.012996342964470387
Batch  62  loss:  0.013619191013276577
Batch  63  loss:  0.01253069844096899
Batch  64  loss:  0.01322189997881651
Batch  65  loss:  0.012998034246265888
Batch  66  loss:  0.0136990612372756
Batch  67  loss:  0.01408821064978838
Batch  68  loss:  0.013609268702566624
Batch  69  loss:  0.01254489365965128
Batch  70  loss:  0.013507872819900513
Batch  71  loss:  0.013679093681275845
Batch  72  loss:  0.01267663761973381
Batch  73  loss:  0.013471905142068863
Batch  74  loss:  0.013209625147283077
Batch  75  loss:  0.012874062173068523
Batch  76  loss:  0.014697245322167873
Batch  77  loss:  0.01235211081802845
Batch  78  loss:  0.013685247860848904
Batch  79  loss:  0.01394953578710556
Batch  80  loss:  0.013091989792883396
Batch  81  loss:  0.013045532628893852
Batch  82  loss:  0.013122563250362873
Batch  83  loss:  0.012811951339244843
Batch  84  loss:  0.012637610547244549
Batch  85  loss:  0.013348715379834175
Batch  86  loss:  0.013623212464153767
Batch  87  loss:  0.012832528911530972
Batch  88  loss:  0.012389895506203175
Batch  89  loss:  0.013055066578090191
Batch  90  loss:  0.012117616832256317
Batch  91  loss:  0.013210327364504337
Batch  92  loss:  0.014803112484514713
Batch  93  loss:  0.013287320733070374
Batch  94  loss:  0.013398881070315838
Batch  95  loss:  0.013161699287593365
Batch  96  loss:  0.013232138007879257
Batch  97  loss:  0.012927870266139507
Batch  98  loss:  0.01298160757869482
Batch  99  loss:  0.014496284537017345
Batch  100  loss:  0.013447501696646214
Validation: 
LOSS train 0.013433001628145575, val 0.012587539851665497
EPOCH : 5 TIME:  2022-04-18 23:17:37.597012
Training: 
Batch  1  loss:  0.013301915489137173
Batch  2  loss:  0.01315896026790142
Batch  3  loss:  0.01306325662881136
Batch  4  loss:  0.013393229804933071
Batch  5  loss:  0.012865298427641392
Batch  6  loss:  0.01319773867726326
Batch  7  loss:  0.013931257650256157
Batch  8  loss:  0.012906194664537907
Batch  9  loss:  0.013026830740272999
Batch  10  loss:  0.012836810201406479
Batch  11  loss:  0.012936882674694061
Batch  12  loss:  0.013233110308647156
Batch  13  loss:  0.012783972546458244
Batch  14  loss:  0.013444744981825352
Batch  15  loss:  0.013321630656719208
Batch  16  loss:  0.012869049794971943
Batch  17  loss:  0.012984784319996834
Batch  18  loss:  0.012603654526174068
Batch  19  loss:  0.0124615877866745
Batch  20  loss:  0.013023961335420609
Batch  21  loss:  0.013286275789141655
Batch  22  loss:  0.012057186104357243
Batch  23  loss:  0.012641338631510735
Batch  24  loss:  0.012822476215660572
Batch  25  loss:  0.013491776771843433
Batch  26  loss:  0.012984659522771835
Batch  27  loss:  0.013003828004002571
Batch  28  loss:  0.013349947519600391
Batch  29  loss:  0.012734446674585342
Batch  30  loss:  0.01304030604660511
Batch  31  loss:  0.01256221067160368
Batch  32  loss:  0.014473306946456432
Batch  33  loss:  0.013607600703835487
Batch  34  loss:  0.0131662143394351
Batch  35  loss:  0.012614412233233452
Batch  36  loss:  0.012561481446027756
Batch  37  loss:  0.013327577151358128
Batch  38  loss:  0.013683618046343327
Batch  39  loss:  0.012246093712747097
Batch  40  loss:  0.012782573699951172
Batch  41  loss:  0.014159581623971462
Batch  42  loss:  0.012714773416519165
Batch  43  loss:  0.013093127869069576
Batch  44  loss:  0.013899898156523705
Batch  45  loss:  0.013181273825466633
Batch  46  loss:  0.01266720425337553
Batch  47  loss:  0.012222801335155964
Batch  48  loss:  0.012924443930387497
Batch  49  loss:  0.013596657663583755
Batch  50  loss:  0.012478425167500973
Batch  51  loss:  0.012428794987499714
Batch  52  loss:  0.012597962282598019
Batch  53  loss:  0.013632725924253464
Batch  54  loss:  0.012798953801393509
Batch  55  loss:  0.012409517541527748
Batch  56  loss:  0.013220857828855515
Batch  57  loss:  0.013257665559649467
Batch  58  loss:  0.012257195077836514
Batch  59  loss:  0.012763283215463161
Batch  60  loss:  0.012776082381606102
Batch  61  loss:  0.01345538068562746
Batch  62  loss:  0.012477868236601353
Batch  63  loss:  0.012301991693675518
Batch  64  loss:  0.01354221161454916
Batch  65  loss:  0.012660305015742779
Batch  66  loss:  0.013214337639510632
Batch  67  loss:  0.013462013565003872
Batch  68  loss:  0.013174393214285374
Batch  69  loss:  0.013275082223117352
Batch  70  loss:  0.01313029695302248
Batch  71  loss:  0.013124514371156693
Batch  72  loss:  0.01342754065990448
Batch  73  loss:  0.013534597121179104
Batch  74  loss:  0.012769833207130432
Batch  75  loss:  0.013266840018332005
Batch  76  loss:  0.013383228331804276
Batch  77  loss:  0.014068040996789932
Batch  78  loss:  0.013969849795103073
Batch  79  loss:  0.013459967449307442
Batch  80  loss:  0.01310547161847353
Batch  81  loss:  0.013575677759945393
Batch  82  loss:  0.012318977154791355
Batch  83  loss:  0.012598657049238682
Batch  84  loss:  0.013089971616864204
Batch  85  loss:  0.012574117630720139
Batch  86  loss:  0.012691800482571125
Batch  87  loss:  0.013472666032612324
Batch  88  loss:  0.01307378988713026
Batch  89  loss:  0.013241142965853214
Batch  90  loss:  0.012867911718785763
Batch  91  loss:  0.013210637494921684
Batch  92  loss:  0.01364760473370552
Batch  93  loss:  0.01285702083259821
Batch  94  loss:  0.01308451872318983
Batch  95  loss:  0.012574334628880024
Batch  96  loss:  0.013148448430001736
Batch  97  loss:  0.012413720600306988
Batch  98  loss:  0.013707373291254044
Batch  99  loss:  0.013446640223264694
Batch  100  loss:  0.012793329544365406
Validation: 
LOSS train 0.013060555327683687, val 0.012689204886555672
EPOCH : 6 TIME:  2022-04-18 23:30:35.987248
Training: 
Batch  1  loss:  0.013403204269707203
Batch  2  loss:  0.012894594110548496
Batch  3  loss:  0.01253393106162548
Batch  4  loss:  0.011835858225822449
Batch  5  loss:  0.012444600462913513
Batch  6  loss:  0.012632585130631924
Batch  7  loss:  0.012474562972784042
Batch  8  loss:  0.012846133671700954
Batch  9  loss:  0.012837845832109451
Batch  10  loss:  0.013244003988802433
Batch  11  loss:  0.01279121357947588
Batch  12  loss:  0.012895474210381508
Batch  13  loss:  0.012723390012979507
Batch  14  loss:  0.01377194095402956
Batch  15  loss:  0.012211945839226246
Batch  16  loss:  0.012765084393322468
Batch  17  loss:  0.013208986259996891
Batch  18  loss:  0.012813684530556202
Batch  19  loss:  0.012796970084309578
Batch  20  loss:  0.012615928426384926
Batch  21  loss:  0.013082343153655529
Batch  22  loss:  0.012961606495082378
Batch  23  loss:  0.012563387863337994
Batch  24  loss:  0.013458635658025742
Batch  25  loss:  0.013465199619531631
Batch  26  loss:  0.013020913116633892
Batch  27  loss:  0.014437849633395672
Batch  28  loss:  0.012525956146419048
Batch  29  loss:  0.012467066757380962
Batch  30  loss:  0.012642327696084976
Batch  31  loss:  0.01292926725000143
Batch  32  loss:  0.012895862571895123
Batch  33  loss:  0.013052632100880146
Batch  34  loss:  0.012491388246417046
Batch  35  loss:  0.012443608604371548
Batch  36  loss:  0.012765852734446526
Batch  37  loss:  0.013085479848086834
Batch  38  loss:  0.012720718048512936
Batch  39  loss:  0.012884709984064102
Batch  40  loss:  0.013343948870897293
Batch  41  loss:  0.012159034609794617
Batch  42  loss:  0.013147899881005287
Batch  43  loss:  0.013018820434808731
Batch  44  loss:  0.012949981726706028
Batch  45  loss:  0.012988206930458546
Batch  46  loss:  0.012743880040943623
Batch  47  loss:  0.012921176850795746
Batch  48  loss:  0.012419340200722218
Batch  49  loss:  0.012071113102138042
Batch  50  loss:  0.01349139679223299
Batch  51  loss:  0.013147896155714989
Batch  52  loss:  0.014052136801183224
Batch  53  loss:  0.012760350480675697
Batch  54  loss:  0.012599525041878223
Batch  55  loss:  0.013073001988232136
Batch  56  loss:  0.013495917432010174
Batch  57  loss:  0.012649718672037125
Batch  58  loss:  0.012696526944637299
Batch  59  loss:  0.013075539842247963
Batch  60  loss:  0.01271725818514824
Batch  61  loss:  0.013371937908232212
Batch  62  loss:  0.012517101131379604
Batch  63  loss:  0.012767016887664795
Batch  64  loss:  0.013014552183449268
Batch  65  loss:  0.013349690474569798
Batch  66  loss:  0.013066518120467663
Batch  67  loss:  0.013361348770558834
Batch  68  loss:  0.013565546832978725
Batch  69  loss:  0.011932105757296085
Batch  70  loss:  0.012255022302269936
Batch  71  loss:  0.012156777083873749
Batch  72  loss:  0.012507149949669838
Batch  73  loss:  0.01254463754594326
Batch  74  loss:  0.012985595501959324
Batch  75  loss:  0.013825183734297752
Batch  76  loss:  0.01213631872087717
Batch  77  loss:  0.01300402358174324
Batch  78  loss:  0.01416147593408823
Batch  79  loss:  0.012811536900699139
Batch  80  loss:  0.012843751348555088
Batch  81  loss:  0.012661287561058998
Batch  82  loss:  0.012593128718435764
Batch  83  loss:  0.012669172137975693
Batch  84  loss:  0.012857218272984028
Batch  85  loss:  0.01370381098240614
Batch  86  loss:  0.013020907528698444
Batch  87  loss:  0.012528620660305023
Batch  88  loss:  0.012836731038987637
Batch  89  loss:  0.012533635832369328
Batch  90  loss:  0.012943090870976448
Batch  91  loss:  0.013395387679338455
Batch  92  loss:  0.012601498514413834
Batch  93  loss:  0.013873616233468056
Batch  94  loss:  0.011959442868828773
Batch  95  loss:  0.012326167896389961
Batch  96  loss:  0.012914101593196392
Batch  97  loss:  0.01200038380920887
Batch  98  loss:  0.011995409615337849
Batch  99  loss:  0.012401235289871693
Batch  100  loss:  0.012532687745988369
Validation: 
LOSS train 0.012856842400506139, val 0.012630953453481197
EPOCH : 7 TIME:  2022-04-18 23:43:35.549978
Training: 
Batch  1  loss:  0.012648506090044975
Batch  2  loss:  0.012358791194856167
Batch  3  loss:  0.012717096135020256
Batch  4  loss:  0.011930475011467934
Batch  5  loss:  0.011963386088609695
Batch  6  loss:  0.01291649043560028
Batch  7  loss:  0.01313981506973505
Batch  8  loss:  0.012123637832701206
Batch  9  loss:  0.012124614790081978
Batch  10  loss:  0.012090028263628483
Batch  11  loss:  0.012825787998735905
Batch  12  loss:  0.012044191360473633
Batch  13  loss:  0.013664966449141502
Batch  14  loss:  0.01268677692860365
Batch  15  loss:  0.012249801307916641
Batch  16  loss:  0.012692316435277462
Batch  17  loss:  0.012210705317556858
Batch  18  loss:  0.012125940062105656
Batch  19  loss:  0.012558544985949993
Batch  20  loss:  0.012695550918579102
Batch  21  loss:  0.01231477502733469
Batch  22  loss:  0.011996964924037457
Batch  23  loss:  0.012225700542330742
Batch  24  loss:  0.012202332727611065
Batch  25  loss:  0.012010655365884304
Batch  26  loss:  0.012709390372037888
Batch  27  loss:  0.01289447396993637
Batch  28  loss:  0.011790511198341846
Batch  29  loss:  0.01305814553052187
Batch  30  loss:  0.011463510803878307
Batch  31  loss:  0.012076154351234436
Batch  32  loss:  0.012634174898266792
Batch  33  loss:  0.012663591653108597
Batch  34  loss:  0.012725288979709148
Batch  35  loss:  0.012661914341151714
Batch  36  loss:  0.01188705861568451
Batch  37  loss:  0.012097273021936417
Batch  38  loss:  0.012145747430622578
Batch  39  loss:  0.012169656343758106
Batch  40  loss:  0.012055299244821072
Batch  41  loss:  0.012308461591601372
Batch  42  loss:  0.012427379377186298
Batch  43  loss:  0.013055231422185898
Batch  44  loss:  0.012412182986736298
Batch  45  loss:  0.012127308174967766
Batch  46  loss:  0.012591294944286346
Batch  47  loss:  0.012169137597084045
Batch  48  loss:  0.01253192126750946
Batch  49  loss:  0.012292184866964817
Batch  50  loss:  0.012298624031245708
Batch  51  loss:  0.013230767101049423
Batch  52  loss:  0.012228299863636494
Batch  53  loss:  0.012973389588296413
Batch  54  loss:  0.012294800952076912
Batch  55  loss:  0.012649936601519585
Batch  56  loss:  0.01270696334540844
Batch  57  loss:  0.012695041485130787
Batch  58  loss:  0.012287525460124016
Batch  59  loss:  0.012744703330099583
Batch  60  loss:  0.013371684588491917
Batch  61  loss:  0.012975121848285198
Batch  62  loss:  0.011549405753612518
Batch  63  loss:  0.012801354750990868
Batch  64  loss:  0.01215060893446207
Batch  65  loss:  0.012389915063977242
Batch  66  loss:  0.012901697307825089
Batch  67  loss:  0.01226880308240652
Batch  68  loss:  0.011605066247284412
Batch  69  loss:  0.012556757777929306
Batch  70  loss:  0.01304575428366661
Batch  71  loss:  0.012361008673906326
Batch  72  loss:  0.013090265914797783
Batch  73  loss:  0.0123002203181386
Batch  74  loss:  0.013302267529070377
Batch  75  loss:  0.012989790178835392
Batch  76  loss:  0.012601684778928757
Batch  77  loss:  0.012417244724929333
Batch  78  loss:  0.012068532407283783
Batch  79  loss:  0.011962068267166615
Batch  80  loss:  0.012856721878051758
Batch  81  loss:  0.012635980732738972
Batch  82  loss:  0.01248259749263525
Batch  83  loss:  0.011715460568666458
Batch  84  loss:  0.012394542805850506
Batch  85  loss:  0.013148968107998371
Batch  86  loss:  0.013465180061757565
Batch  87  loss:  0.012289984151721
Batch  88  loss:  0.012851868756115437
Batch  89  loss:  0.011645861901342869
Batch  90  loss:  0.012450789101421833
Batch  91  loss:  0.011905389837920666
Batch  92  loss:  0.012499775737524033
Batch  93  loss:  0.011845750734210014
Batch  94  loss:  0.012694812379777431
Batch  95  loss:  0.012413935735821724
Batch  96  loss:  0.013355331495404243
Batch  97  loss:  0.012430244125425816
Batch  98  loss:  0.013335459865629673
Batch  99  loss:  0.011918196454644203
Batch  100  loss:  0.012095863930881023
Validation: 
LOSS train 0.012467171642929315, val 0.011969291605055332
EPOCH : 8 TIME:  2022-04-18 23:56:33.138924
Training: 
Batch  1  loss:  0.012570466846227646
Batch  2  loss:  0.013253607787191868
Batch  3  loss:  0.012710556387901306
Batch  4  loss:  0.011784023605287075
Batch  5  loss:  0.013046960346400738
Batch  6  loss:  0.012286805547773838
Batch  7  loss:  0.01146437507122755
Batch  8  loss:  0.01287789549678564
Batch  9  loss:  0.01258582342416048
Batch  10  loss:  0.011664561927318573
Batch  11  loss:  0.011904665268957615
Batch  12  loss:  0.012789740227162838
Batch  13  loss:  0.01268844772130251
Batch  14  loss:  0.012739155441522598
Batch  15  loss:  0.013214478269219398
Batch  16  loss:  0.01243537850677967
Batch  17  loss:  0.012311124242842197
Batch  18  loss:  0.013487953692674637
Batch  19  loss:  0.012201456353068352
Batch  20  loss:  0.012368257157504559
Batch  21  loss:  0.012674550525844097
Batch  22  loss:  0.01239391416311264
Batch  23  loss:  0.012279250659048557
Batch  24  loss:  0.012997986748814583
Batch  25  loss:  0.01252054050564766
Batch  26  loss:  0.01307156402617693
Batch  27  loss:  0.012457286939024925
Batch  28  loss:  0.013404271565377712
Batch  29  loss:  0.012790919281542301
Batch  30  loss:  0.012450947426259518
Batch  31  loss:  0.012352881021797657
Batch  32  loss:  0.011975965462625027
Batch  33  loss:  0.01219483558088541
Batch  34  loss:  0.012089226394891739
Batch  35  loss:  0.012053178623318672
Batch  36  loss:  0.011819002218544483
Batch  37  loss:  0.012816056609153748
Batch  38  loss:  0.01149833295494318
Batch  39  loss:  0.012236029841005802
Batch  40  loss:  0.013043281622231007
Batch  41  loss:  0.012285620905458927
Batch  42  loss:  0.012849630787968636
Batch  43  loss:  0.011851766146719456
Batch  44  loss:  0.012309698387980461
Batch  45  loss:  0.012816745787858963
Batch  46  loss:  0.012651938945055008
Batch  47  loss:  0.012111324816942215
Batch  48  loss:  0.012876695021986961
Batch  49  loss:  0.013291175477206707
Batch  50  loss:  0.012836002744734287
Batch  51  loss:  0.012542175129055977
Batch  52  loss:  0.012489239685237408
Batch  53  loss:  0.012628932483494282
Batch  54  loss:  0.011693364940583706
Batch  55  loss:  0.011889755725860596
Batch  56  loss:  0.01270995382219553
Batch  57  loss:  0.01248118095099926
Batch  58  loss:  0.011759983375668526
Batch  59  loss:  0.012529699131846428
Batch  60  loss:  0.01189990621060133
Batch  61  loss:  0.012563933618366718
Batch  62  loss:  0.012412230484187603
Batch  63  loss:  0.01170083973556757
Batch  64  loss:  0.01222269982099533
Batch  65  loss:  0.012155143544077873
Batch  66  loss:  0.012695025652647018
Batch  67  loss:  0.012221402488648891
Batch  68  loss:  0.012519430369138718
Batch  69  loss:  0.01240849494934082
Batch  70  loss:  0.012270462699234486
Batch  71  loss:  0.012177660129964352
Batch  72  loss:  0.012135706841945648
Batch  73  loss:  0.012141640298068523
Batch  74  loss:  0.01260856818407774
Batch  75  loss:  0.012452901341021061
Batch  76  loss:  0.011660343036055565
Batch  77  loss:  0.012528636492788792
Batch  78  loss:  0.011852261610329151
Batch  79  loss:  0.012010149657726288
Batch  80  loss:  0.012336608953773975
Batch  81  loss:  0.011376305483281612
Batch  82  loss:  0.012237029150128365
Batch  83  loss:  0.012342951260507107
Batch  84  loss:  0.012337871827185154
Batch  85  loss:  0.012486092746257782
Batch  86  loss:  0.012340034358203411
Batch  87  loss:  0.012218818068504333
Batch  88  loss:  0.011945921927690506
Batch  89  loss:  0.013039249926805496
Batch  90  loss:  0.013050957582890987
Batch  91  loss:  0.012163457460701466
Batch  92  loss:  0.012257716618478298
Batch  93  loss:  0.012574448250234127
Batch  94  loss:  0.013172175735235214
Batch  95  loss:  0.011748207733035088
Batch  96  loss:  0.012713441625237465
Batch  97  loss:  0.011951417662203312
Batch  98  loss:  0.01188258733600378
Batch  99  loss:  0.012331136502325535
Batch  100  loss:  0.012891685590147972
Validation: 
LOSS train 0.012401461927220225, val 0.012108473107218742
EPOCH : 9 TIME:  2022-04-19 00:09:30.421440
Training: 
Batch  1  loss:  0.012554413639008999
Batch  2  loss:  0.012581201270222664
Batch  3  loss:  0.012069440446794033
Batch  4  loss:  0.01247779093682766
Batch  5  loss:  0.011731144972145557
Batch  6  loss:  0.012224859558045864
Batch  7  loss:  0.01217315811663866
Batch  8  loss:  0.01255969237536192
Batch  9  loss:  0.013131431303918362
Batch  10  loss:  0.013094017282128334
Batch  11  loss:  0.013794338330626488
Batch  12  loss:  0.013097045011818409
Batch  13  loss:  0.01140627358108759
Batch  14  loss:  0.012126816436648369
Batch  15  loss:  0.012359344400465488
Batch  16  loss:  0.011898672208189964
Batch  17  loss:  0.011874156072735786
Batch  18  loss:  0.011886305175721645
Batch  19  loss:  0.011911463923752308
Batch  20  loss:  0.012046515941619873
Batch  21  loss:  0.011977158486843109
Batch  22  loss:  0.01210425142198801
Batch  23  loss:  0.012216432020068169
Batch  24  loss:  0.012405669316649437
Batch  25  loss:  0.01233257632702589
Batch  26  loss:  0.012517521157860756
Batch  27  loss:  0.013023295439779758
Batch  28  loss:  0.01265817228704691
Batch  29  loss:  0.012678084895014763
Batch  30  loss:  0.012184004299342632
Batch  31  loss:  0.012304608710110188
Batch  32  loss:  0.012114154174923897
Batch  33  loss:  0.011659747920930386
Batch  34  loss:  0.011636280454695225
Batch  35  loss:  0.012679843232035637
Batch  36  loss:  0.011362049728631973
Batch  37  loss:  0.012905322946608067
Batch  38  loss:  0.012478737160563469
Batch  39  loss:  0.012100930325686932
Batch  40  loss:  0.012074732221662998
Batch  41  loss:  0.011731256730854511
Batch  42  loss:  0.01266526710242033
Batch  43  loss:  0.011910834349691868
Batch  44  loss:  0.011773045174777508
Batch  45  loss:  0.012257200665771961
Batch  46  loss:  0.012329591438174248
Batch  47  loss:  0.012065455317497253
Batch  48  loss:  0.012505855411291122
Batch  49  loss:  0.011846039444208145
Batch  50  loss:  0.01251872070133686
Batch  51  loss:  0.012156185694038868
Batch  52  loss:  0.011981477029621601
Batch  53  loss:  0.011212927289307117
Batch  54  loss:  0.011864905245602131
Batch  55  loss:  0.011423779651522636
Batch  56  loss:  0.012273942120373249
Batch  57  loss:  0.0119839608669281
Batch  58  loss:  0.011821437627077103
Batch  59  loss:  0.01181123312562704
Batch  60  loss:  0.013164084404706955
Batch  61  loss:  0.011982588097453117
Batch  62  loss:  0.012175577692687511
Batch  63  loss:  0.012328777462244034
Batch  64  loss:  0.01198031660169363
Batch  65  loss:  0.011979673057794571
Batch  66  loss:  0.01196668203920126
Batch  67  loss:  0.0123963113874197
Batch  68  loss:  0.012168941088020802
Batch  69  loss:  0.011347915977239609
Batch  70  loss:  0.01202046126127243
Batch  71  loss:  0.011527867056429386
Batch  72  loss:  0.013212490826845169
Batch  73  loss:  0.012529815547168255
Batch  74  loss:  0.012046434916555882
Batch  75  loss:  0.011838757432997227
Batch  76  loss:  0.011878099292516708
Batch  77  loss:  0.011723064817488194
Batch  78  loss:  0.011683383025228977
Batch  79  loss:  0.011909826658666134
Batch  80  loss:  0.012220914475619793
Batch  81  loss:  0.011806914582848549
Batch  82  loss:  0.012685224413871765
Batch  83  loss:  0.012053144164383411
Batch  84  loss:  0.012369041331112385
Batch  85  loss:  0.011693057604134083
Batch  86  loss:  0.012156237848103046
Batch  87  loss:  0.012281411327421665
Batch  88  loss:  0.011931977234780788
Batch  89  loss:  0.011888504028320312
Batch  90  loss:  0.011860650964081287
Batch  91  loss:  0.012085629627108574
Batch  92  loss:  0.012383769266307354
Batch  93  loss:  0.01217908039689064
Batch  94  loss:  0.011705450713634491
Batch  95  loss:  0.012370062060654163
Batch  96  loss:  0.012844209559261799
Batch  97  loss:  0.01185512077063322
Batch  98  loss:  0.011508963070809841
Batch  99  loss:  0.01266944408416748
Batch  100  loss:  0.012396630831062794
Validation: 
LOSS train 0.012173512754961847, val 0.011620272882282734
EPOCH : 10 TIME:  2022-04-19 00:22:33.492358
Training: 
Batch  1  loss:  0.012053634971380234
Batch  2  loss:  0.012218494899570942
Batch  3  loss:  0.012689986266195774
Batch  4  loss:  0.012429463677108288
Batch  5  loss:  0.01173087116330862
Batch  6  loss:  0.012029177509248257
Batch  7  loss:  0.012873581610620022
Batch  8  loss:  0.012370524927973747
Batch  9  loss:  0.01168516743928194
Batch  10  loss:  0.012945289723575115
Batch  11  loss:  0.01212643925100565
Batch  12  loss:  0.012319792993366718
Batch  13  loss:  0.012240555137395859
Batch  14  loss:  0.012315649539232254
Batch  15  loss:  0.012123031541705132
Batch  16  loss:  0.011923973448574543
Batch  17  loss:  0.0119736073538661
Batch  18  loss:  0.013309638947248459
Batch  19  loss:  0.01250066515058279
Batch  20  loss:  0.012169859372079372
Batch  21  loss:  0.01225657295435667
Batch  22  loss:  0.012360002845525742
Batch  23  loss:  0.0114476652815938
Batch  24  loss:  0.012618926353752613
Batch  25  loss:  0.012922718189656734
Batch  26  loss:  0.011708387173712254
Batch  27  loss:  0.012425852008163929
Batch  28  loss:  0.01169066596776247
Batch  29  loss:  0.011984742246568203
Batch  30  loss:  0.012161352671682835
Batch  31  loss:  0.011749128811061382
Batch  32  loss:  0.012006605975329876
Batch  33  loss:  0.012389037758111954
Batch  34  loss:  0.011469872668385506
Batch  35  loss:  0.012056594714522362
Batch  36  loss:  0.012229730375111103
Batch  37  loss:  0.012269639410078526
Batch  38  loss:  0.011946222744882107
Batch  39  loss:  0.011910490691661835
Batch  40  loss:  0.011880268342792988
Batch  41  loss:  0.012033338658511639
Batch  42  loss:  0.011729298159480095
Batch  43  loss:  0.011631122790277004
Batch  44  loss:  0.011408375576138496
Batch  45  loss:  0.011248703114688396
Batch  46  loss:  0.011445999145507812
Batch  47  loss:  0.011613656766712666
Batch  48  loss:  0.01199587807059288
Batch  49  loss:  0.011998748406767845
Batch  50  loss:  0.012162144295871258
Batch  51  loss:  0.011572452262043953
Batch  52  loss:  0.01155082881450653
Batch  53  loss:  0.011318156495690346
Batch  54  loss:  0.01287970319390297
Batch  55  loss:  0.011813445948064327
Batch  56  loss:  0.012149369344115257
Batch  57  loss:  0.01211437676101923
Batch  58  loss:  0.012096087448298931
Batch  59  loss:  0.011915910057723522
Batch  60  loss:  0.012382504530251026
Batch  61  loss:  0.012078036554157734
Batch  62  loss:  0.011857044883072376
Batch  63  loss:  0.012229187414050102
Batch  64  loss:  0.011715871281921864
Batch  65  loss:  0.012537588365375996
Batch  66  loss:  0.012166542932391167
Batch  67  loss:  0.011653956025838852
Batch  68  loss:  0.01165112666785717
Batch  69  loss:  0.011827794834971428
Batch  70  loss:  0.012154895812273026
Batch  71  loss:  0.011899171397089958
Batch  72  loss:  0.012186097912490368
Batch  73  loss:  0.011550716124475002
Batch  74  loss:  0.012043430469930172
Batch  75  loss:  0.012747688218951225
Batch  76  loss:  0.0118789691478014
Batch  77  loss:  0.011980867013335228
Batch  78  loss:  0.011594557203352451
Batch  79  loss:  0.013051245361566544
Batch  80  loss:  0.012636161409318447
Batch  81  loss:  0.01255073957145214
Batch  82  loss:  0.011073433794081211
Batch  83  loss:  0.01198930200189352
Batch  84  loss:  0.01166237611323595
Batch  85  loss:  0.011908728629350662
Batch  86  loss:  0.012174747884273529
Batch  87  loss:  0.012231993488967419
Batch  88  loss:  0.011881818063557148
Batch  89  loss:  0.011962084099650383
Batch  90  loss:  0.011946595273911953
Batch  91  loss:  0.011807892471551895
Batch  92  loss:  0.011065897531807423
Batch  93  loss:  0.011623391881585121
Batch  94  loss:  0.012314721010625362
Batch  95  loss:  0.012205636128783226
Batch  96  loss:  0.01228696946054697
Batch  97  loss:  0.011476995423436165
Batch  98  loss:  0.012049002572894096
Batch  99  loss:  0.012631048448383808
Batch  100  loss:  0.012095307931303978
Validation: 
LOSS train 0.012049536127597094, val 0.011805557645857334
EPOCH : 11 TIME:  2022-04-19 00:35:33.415313
Training: 
Batch  1  loss:  0.01117321103811264
Batch  2  loss:  0.011477670632302761
Batch  3  loss:  0.011767811141908169
Batch  4  loss:  0.012131202034652233
Batch  5  loss:  0.011688987724483013
Batch  6  loss:  0.011568603105843067
Batch  7  loss:  0.012076892890036106
Batch  8  loss:  0.012470094487071037
Batch  9  loss:  0.011446146294474602
Batch  10  loss:  0.011807574890553951
Batch  11  loss:  0.012103435583412647
Batch  12  loss:  0.012075972743332386
Batch  13  loss:  0.011800953187048435
Batch  14  loss:  0.011976780369877815
Batch  15  loss:  0.01182679831981659
Batch  16  loss:  0.012073449790477753
Batch  17  loss:  0.012155034579336643
Batch  18  loss:  0.011802001856267452
Batch  19  loss:  0.01185564510524273
Batch  20  loss:  0.011668117716908455
Batch  21  loss:  0.012331969104707241
Batch  22  loss:  0.012102263048291206
Batch  23  loss:  0.011964973993599415
Batch  24  loss:  0.012050013057887554
Batch  25  loss:  0.011625601910054684
Batch  26  loss:  0.011663693934679031
Batch  27  loss:  0.01208812091499567
Batch  28  loss:  0.01118922233581543
Batch  29  loss:  0.012412702664732933
Batch  30  loss:  0.011720850132405758
Batch  31  loss:  0.011564311571419239
Batch  32  loss:  0.011679207906126976
Batch  33  loss:  0.011190530844032764
Batch  34  loss:  0.011849583126604557
Batch  35  loss:  0.01215913612395525
Batch  36  loss:  0.012158198282122612
Batch  37  loss:  0.01133856363594532
Batch  38  loss:  0.011730232276022434
Batch  39  loss:  0.012463907711207867
Batch  40  loss:  0.012591881677508354
Batch  41  loss:  0.01332036778330803
Batch  42  loss:  0.011396575719118118
Batch  43  loss:  0.01189111452549696
Batch  44  loss:  0.012213065288960934
Batch  45  loss:  0.012009461410343647
Batch  46  loss:  0.01233088318258524
Batch  47  loss:  0.011432004161179066
Batch  48  loss:  0.012589991092681885
Batch  49  loss:  0.011873790994286537
Batch  50  loss:  0.011613595299422741
Batch  51  loss:  0.011976552195847034
Batch  52  loss:  0.011850557290017605
Batch  53  loss:  0.012047138065099716
Batch  54  loss:  0.013261843472719193
Batch  55  loss:  0.012006365694105625
Batch  56  loss:  0.011891177855432034
Batch  57  loss:  0.012215728871524334
Batch  58  loss:  0.011421377770602703
Batch  59  loss:  0.011205105111002922
Batch  60  loss:  0.011548745445907116
Batch  61  loss:  0.012366849929094315
Batch  62  loss:  0.012035056948661804
Batch  63  loss:  0.011478237807750702
Batch  64  loss:  0.01151274610310793
Batch  65  loss:  0.01181530486792326
Batch  66  loss:  0.01264906581491232
Batch  67  loss:  0.011665812693536282
Batch  68  loss:  0.012079247273504734
Batch  69  loss:  0.012037325650453568
Batch  70  loss:  0.012183272279798985
Batch  71  loss:  0.01161342952400446
Batch  72  loss:  0.01119387336075306
Batch  73  loss:  0.011698746122419834
Batch  74  loss:  0.012845028191804886
Batch  75  loss:  0.01229219138622284
Batch  76  loss:  0.011804475449025631
Batch  77  loss:  0.012803692370653152
Batch  78  loss:  0.011540617793798447
Batch  79  loss:  0.011938899755477905
Batch  80  loss:  0.011489336378872395
Batch  81  loss:  0.011725762858986855
Batch  82  loss:  0.012559698894619942
Batch  83  loss:  0.01230548694729805
Batch  84  loss:  0.01171712577342987
Batch  85  loss:  0.012054011225700378
Batch  86  loss:  0.012162480503320694
Batch  87  loss:  0.01159833837300539
Batch  88  loss:  0.011718976311385632
Batch  89  loss:  0.011340845376253128
Batch  90  loss:  0.01195502933114767
Batch  91  loss:  0.01212152186781168
Batch  92  loss:  0.0120081203058362
Batch  93  loss:  0.011880990117788315
Batch  94  loss:  0.011916153132915497
Batch  95  loss:  0.011969957500696182
Batch  96  loss:  0.011991217732429504
Batch  97  loss:  0.012614408507943153
Batch  98  loss:  0.011908404529094696
Batch  99  loss:  0.012020496651530266
Batch  100  loss:  0.011601072736084461
Validation: 
LOSS train 0.011931300973519682, val 0.011246123351156712
EPOCH : 12 TIME:  2022-04-19 00:48:26.568530
Training: 
Batch  1  loss:  0.011961283162236214
Batch  2  loss:  0.011541797779500484
Batch  3  loss:  0.011478165164589882
Batch  4  loss:  0.011163932271301746
Batch  5  loss:  0.012055343948304653
Batch  6  loss:  0.011047025211155415
Batch  7  loss:  0.01233218889683485
Batch  8  loss:  0.01126918662339449
Batch  9  loss:  0.012003891170024872
Batch  10  loss:  0.011934597045183182
Batch  11  loss:  0.010877235792577267
Batch  12  loss:  0.011531001888215542
Batch  13  loss:  0.012074565514922142
Batch  14  loss:  0.011546315625309944
Batch  15  loss:  0.01181428600102663
Batch  16  loss:  0.011522375978529453
Batch  17  loss:  0.011384389363229275
Batch  18  loss:  0.01168149709701538
Batch  19  loss:  0.011501831002533436
Batch  20  loss:  0.011746794916689396
Batch  21  loss:  0.012172725051641464
Batch  22  loss:  0.011652748100459576
Batch  23  loss:  0.011837325058877468
Batch  24  loss:  0.011500142514705658
Batch  25  loss:  0.011248781345784664
Batch  26  loss:  0.010949524119496346
Batch  27  loss:  0.011813608929514885
Batch  28  loss:  0.011437110602855682
Batch  29  loss:  0.01159745641052723
Batch  30  loss:  0.01120320986956358
Batch  31  loss:  0.012536553665995598
Batch  32  loss:  0.011467120610177517
Batch  33  loss:  0.011012294329702854
Batch  34  loss:  0.010696964338421822
Batch  35  loss:  0.011675585992634296
Batch  36  loss:  0.011619232594966888
Batch  37  loss:  0.011306000873446465
Batch  38  loss:  0.01178218238055706
Batch  39  loss:  0.011210978962481022
Batch  40  loss:  0.011847476474940777
Batch  41  loss:  0.01150560937821865
Batch  42  loss:  0.011968596838414669
Batch  43  loss:  0.012191032990813255
Batch  44  loss:  0.01148623414337635
Batch  45  loss:  0.010770753026008606
Batch  46  loss:  0.011301080696284771
Batch  47  loss:  0.01166445016860962
Batch  48  loss:  0.012123697437345982
Batch  49  loss:  0.011915159411728382
Batch  50  loss:  0.011881078593432903
Batch  51  loss:  0.011338477022945881
Batch  52  loss:  0.011202382855117321
Batch  53  loss:  0.011875046417117119
Batch  54  loss:  0.011960172094404697
Batch  55  loss:  0.011244835332036018
Batch  56  loss:  0.01156679354608059
Batch  57  loss:  0.011794738471508026
Batch  58  loss:  0.011911089532077312
Batch  59  loss:  0.011493242345750332
Batch  60  loss:  0.01187131181359291
Batch  61  loss:  0.011476910673081875
Batch  62  loss:  0.011574860662221909
Batch  63  loss:  0.011836717836558819
Batch  64  loss:  0.011684589087963104
Batch  65  loss:  0.01230316050350666
Batch  66  loss:  0.012306668795645237
Batch  67  loss:  0.011997719295322895
Batch  68  loss:  0.011600317433476448
Batch  69  loss:  0.011819903738796711
Batch  70  loss:  0.011744069866836071
Batch  71  loss:  0.011428074911236763
Batch  72  loss:  0.011374284513294697
Batch  73  loss:  0.011397953145205975
Batch  74  loss:  0.011610953137278557
Batch  75  loss:  0.0128905288875103
Batch  76  loss:  0.011766109615564346
Batch  77  loss:  0.011559726670384407
Batch  78  loss:  0.011738138273358345
Batch  79  loss:  0.011468918062746525
Batch  80  loss:  0.012689655646681786
Batch  81  loss:  0.011699370108544827
Batch  82  loss:  0.01179418433457613
Batch  83  loss:  0.01175207830965519
Batch  84  loss:  0.011677596718072891
Batch  85  loss:  0.011841882951557636
Batch  86  loss:  0.011745765805244446
Batch  87  loss:  0.011904705315828323
Batch  88  loss:  0.01099679246544838
Batch  89  loss:  0.011684679426252842
Batch  90  loss:  0.012289249338209629
Batch  91  loss:  0.011803588829934597
Batch  92  loss:  0.012098865583539009
Batch  93  loss:  0.012028743512928486
Batch  94  loss:  0.011346421204507351
Batch  95  loss:  0.012368500232696533
Batch  96  loss:  0.011785430833697319
Batch  97  loss:  0.011432275176048279
Batch  98  loss:  0.011680187657475471
Batch  99  loss:  0.011352007277309895
Batch  100  loss:  0.011080717667937279
Validation: 
LOSS train 0.011667328123003244, val 0.011567180044949055
EPOCH : 13 TIME:  2022-04-19 01:01:18.918261
Training: 
Batch  1  loss:  0.011570450849831104
Batch  2  loss:  0.011731247417628765
Batch  3  loss:  0.0123567134141922
Batch  4  loss:  0.011637702584266663
Batch  5  loss:  0.011657106690108776
Batch  6  loss:  0.011357887648046017
Batch  7  loss:  0.011786023154854774
Batch  8  loss:  0.011860904283821583
Batch  9  loss:  0.011671783402562141
Batch  10  loss:  0.011073879897594452
Batch  11  loss:  0.011433407664299011
Batch  12  loss:  0.0120963454246521
Batch  13  loss:  0.011284157633781433
Batch  14  loss:  0.011830391362309456
Batch  15  loss:  0.012543492950499058
Batch  16  loss:  0.011326084844768047
Batch  17  loss:  0.011306888423860073
Batch  18  loss:  0.011976306326687336
Batch  19  loss:  0.011704878881573677
Batch  20  loss:  0.01140345074236393
Batch  21  loss:  0.012030488811433315
Batch  22  loss:  0.011029662564396858
Batch  23  loss:  0.011775043793022633
Batch  24  loss:  0.012180796824395657
Batch  25  loss:  0.012157156132161617
Batch  26  loss:  0.01104078721255064
Batch  27  loss:  0.011448702774941921
Batch  28  loss:  0.011804926209151745
Batch  29  loss:  0.012570707127451897
Batch  30  loss:  0.012580947950482368
Batch  31  loss:  0.011887283064424992
Batch  32  loss:  0.011811641976237297
Batch  33  loss:  0.012072231620550156
Batch  34  loss:  0.01248203869909048
Batch  35  loss:  0.01215854287147522
Batch  36  loss:  0.011103328317403793
Batch  37  loss:  0.01168061327189207
Batch  38  loss:  0.010763544589281082
Batch  39  loss:  0.010976538062095642
Batch  40  loss:  0.011059426702558994
Batch  41  loss:  0.011736061424016953
Batch  42  loss:  0.011814659461379051
Batch  43  loss:  0.011123738251626492
Batch  44  loss:  0.01207214966416359
Batch  45  loss:  0.011563081294298172
Batch  46  loss:  0.012036919593811035
Batch  47  loss:  0.01170671172440052
Batch  48  loss:  0.012423139996826649
Batch  49  loss:  0.012101403437554836
Batch  50  loss:  0.011880774050951004
Batch  51  loss:  0.011872346512973309
Batch  52  loss:  0.012081164866685867
Batch  53  loss:  0.011793994344770908
Batch  54  loss:  0.011676046997308731
Batch  55  loss:  0.011304504238069057
Batch  56  loss:  0.011631791479885578
Batch  57  loss:  0.011718313209712505
Batch  58  loss:  0.011386333033442497
Batch  59  loss:  0.011714710853993893
Batch  60  loss:  0.011963716708123684
Batch  61  loss:  0.011319160461425781
Batch  62  loss:  0.01188682485371828
Batch  63  loss:  0.011326820589601994
Batch  64  loss:  0.011851091869175434
Batch  65  loss:  0.012012806721031666
Batch  66  loss:  0.011099133640527725
Batch  67  loss:  0.011498866602778435
Batch  68  loss:  0.011353373527526855
Batch  69  loss:  0.011674998328089714
Batch  70  loss:  0.011633000336587429
Batch  71  loss:  0.011806952767074108
Batch  72  loss:  0.01115820836275816
Batch  73  loss:  0.011600534431636333
Batch  74  loss:  0.011681582778692245
Batch  75  loss:  0.011564881540834904
Batch  76  loss:  0.011578739620745182
Batch  77  loss:  0.011445273645222187
Batch  78  loss:  0.011338694952428341
Batch  79  loss:  0.01158152800053358
Batch  80  loss:  0.011936780996620655
Batch  81  loss:  0.01107937190681696
Batch  82  loss:  0.011953095905482769
Batch  83  loss:  0.011719558387994766
Batch  84  loss:  0.011623358353972435
Batch  85  loss:  0.011488804593682289
Batch  86  loss:  0.012202762998640537
Batch  87  loss:  0.01170477457344532
Batch  88  loss:  0.011707932688295841
Batch  89  loss:  0.01139423344284296
Batch  90  loss:  0.01165094505995512
Batch  91  loss:  0.011719423346221447
Batch  92  loss:  0.012089427560567856
Batch  93  loss:  0.011304833926260471
Batch  94  loss:  0.011650958098471165
Batch  95  loss:  0.011166946962475777
Batch  96  loss:  0.011753988452255726
Batch  97  loss:  0.011720521375536919
Batch  98  loss:  0.012340137735009193
Batch  99  loss:  0.01138430554419756
Batch  100  loss:  0.01146351546049118
Validation: 
LOSS train 0.011682932237163187, val 0.01145952194929123
EPOCH : 14 TIME:  2022-04-19 01:14:15.257129
Training: 
Batch  1  loss:  0.011998765170574188
Batch  2  loss:  0.011406135745346546
Batch  3  loss:  0.011354696936905384
Batch  4  loss:  0.011725175194442272
Batch  5  loss:  0.01141485758125782
Batch  6  loss:  0.011138592846691608
Batch  7  loss:  0.011992826126515865
Batch  8  loss:  0.012141742743551731
Batch  9  loss:  0.011506471782922745
Batch  10  loss:  0.01187820453196764
Batch  11  loss:  0.011731884442269802
Batch  12  loss:  0.011141854338347912
Batch  13  loss:  0.010934657417237759
Batch  14  loss:  0.01231510378420353
Batch  15  loss:  0.011370353400707245
Batch  16  loss:  0.012233534827828407
Batch  17  loss:  0.01202126033604145
Batch  18  loss:  0.011090016923844814
Batch  19  loss:  0.01193443313241005
Batch  20  loss:  0.01202660333365202
Batch  21  loss:  0.01229159813374281
Batch  22  loss:  0.010874943807721138
Batch  23  loss:  0.010948927141726017
Batch  24  loss:  0.01191783882677555
Batch  25  loss:  0.012755479663610458
Batch  26  loss:  0.011607448570430279
Batch  27  loss:  0.012034732848405838
Batch  28  loss:  0.01113939844071865
Batch  29  loss:  0.011941959150135517
Batch  30  loss:  0.012234191410243511
Batch  31  loss:  0.012107612565159798
Batch  32  loss:  0.012082588858902454
Batch  33  loss:  0.011404220946133137
Batch  34  loss:  0.01163807138800621
Batch  35  loss:  0.011794227175414562
Batch  36  loss:  0.012715235352516174
Batch  37  loss:  0.010625455528497696
Batch  38  loss:  0.011659270152449608
Batch  39  loss:  0.011705461889505386
Batch  40  loss:  0.01146042812615633
Batch  41  loss:  0.011104249395430088
Batch  42  loss:  0.01225353591144085
Batch  43  loss:  0.011405648663640022
Batch  44  loss:  0.010960433632135391
Batch  45  loss:  0.011654956266283989
Batch  46  loss:  0.011842984706163406
Batch  47  loss:  0.011434467509388924
Batch  48  loss:  0.011260705068707466
Batch  49  loss:  0.011709707789123058
Batch  50  loss:  0.011810168623924255
Batch  51  loss:  0.01139567606151104
Batch  52  loss:  0.012494878843426704
Batch  53  loss:  0.011417895555496216
Batch  54  loss:  0.011612236499786377
Batch  55  loss:  0.011369160376489162
Batch  56  loss:  0.011784051544964314
Batch  57  loss:  0.011299378238618374
Batch  58  loss:  0.011330392211675644
Batch  59  loss:  0.011478517204523087
Batch  60  loss:  0.01249179057776928
Batch  61  loss:  0.012352135963737965
Batch  62  loss:  0.01155098620802164
Batch  63  loss:  0.01222254242748022
Batch  64  loss:  0.011530833318829536
Batch  65  loss:  0.012009069323539734
Batch  66  loss:  0.011468589305877686
Batch  67  loss:  0.011223177425563335
Batch  68  loss:  0.012051732279360294
Batch  69  loss:  0.012120206840336323
Batch  70  loss:  0.011999933049082756
Batch  71  loss:  0.010904654860496521
Batch  72  loss:  0.012573749758303165
Batch  73  loss:  0.011557000689208508
Batch  74  loss:  0.011199499480426311
Batch  75  loss:  0.011066999286413193
Batch  76  loss:  0.011768913827836514
Batch  77  loss:  0.012091532349586487
Batch  78  loss:  0.012074948288500309
Batch  79  loss:  0.011720487847924232
Batch  80  loss:  0.011238690465688705
Batch  81  loss:  0.011614826507866383
Batch  82  loss:  0.011511163786053658
Batch  83  loss:  0.011226368136703968
Batch  84  loss:  0.011644139885902405
Batch  85  loss:  0.011737743392586708
Batch  86  loss:  0.011793981306254864
Batch  87  loss:  0.011720751412212849
Batch  88  loss:  0.011094734072685242
Batch  89  loss:  0.011129103600978851
Batch  90  loss:  0.0116525087505579
Batch  91  loss:  0.01155033428221941
Batch  92  loss:  0.01225406862795353
Batch  93  loss:  0.01186798233538866
Batch  94  loss:  0.01151964906603098
Batch  95  loss:  0.011367635801434517
Batch  96  loss:  0.012169099412858486
Batch  97  loss:  0.011635925620794296
Batch  98  loss:  0.011452487669885159
Batch  99  loss:  0.011470741592347622
Batch  100  loss:  0.011038471944630146
Validation: 
LOSS train 0.011665844954550267, val 0.011035972274839878
EPOCH : 15 TIME:  2022-04-19 01:27:10.023393
Training: 
Batch  1  loss:  0.011396653018891811
Batch  2  loss:  0.01128824707120657
Batch  3  loss:  0.010989049449563026
Batch  4  loss:  0.01178074348717928
Batch  5  loss:  0.011193945072591305
Batch  6  loss:  0.011879410594701767
Batch  7  loss:  0.010938504710793495
Batch  8  loss:  0.011411217972636223
Batch  9  loss:  0.011913781054317951
Batch  10  loss:  0.011669148690998554
Batch  11  loss:  0.011876587755978107
Batch  12  loss:  0.010939695872366428
Batch  13  loss:  0.011834557168185711
Batch  14  loss:  0.01120696496218443
Batch  15  loss:  0.011777445673942566
Batch  16  loss:  0.011922593228518963
Batch  17  loss:  0.011714844033122063
Batch  18  loss:  0.012360991910099983
Batch  19  loss:  0.01140710711479187
Batch  20  loss:  0.01131707988679409
Batch  21  loss:  0.011978213675320148
Batch  22  loss:  0.011482576839625835
Batch  23  loss:  0.011537322774529457
Batch  24  loss:  0.011664292775094509
Batch  25  loss:  0.011782596819102764
Batch  26  loss:  0.011816788464784622
Batch  27  loss:  0.011349951848387718
Batch  28  loss:  0.011411680839955807
Batch  29  loss:  0.010713450610637665
Batch  30  loss:  0.01204046793282032
Batch  31  loss:  0.011651759967207909
Batch  32  loss:  0.011517243459820747
Batch  33  loss:  0.011535710655152798
Batch  34  loss:  0.011600283905863762
Batch  35  loss:  0.011316882446408272
Batch  36  loss:  0.0115458769723773
Batch  37  loss:  0.011653721332550049
Batch  38  loss:  0.011736765503883362
Batch  39  loss:  0.011598186567425728
Batch  40  loss:  0.01093695405870676
Batch  41  loss:  0.011570828035473824
Batch  42  loss:  0.01151589211076498
Batch  43  loss:  0.011503135785460472
Batch  44  loss:  0.011769418604671955
Batch  45  loss:  0.012697916477918625
Batch  46  loss:  0.011651620268821716
Batch  47  loss:  0.012562284246087074
Batch  48  loss:  0.011654075235128403
Batch  49  loss:  0.011906731873750687
Batch  50  loss:  0.012706541456282139
Batch  51  loss:  0.011661040596663952
Batch  52  loss:  0.01160105038434267
Batch  53  loss:  0.011702427640557289
Batch  54  loss:  0.011885583400726318
Batch  55  loss:  0.011466210708022118
Batch  56  loss:  0.011049151420593262
Batch  57  loss:  0.010634001344442368
Batch  58  loss:  0.012595687992870808
Batch  59  loss:  0.01220106240361929
Batch  60  loss:  0.011139403097331524
Batch  61  loss:  0.011350130662322044
Batch  62  loss:  0.01324119046330452
Batch  63  loss:  0.011630040593445301
Batch  64  loss:  0.011149904690682888
Batch  65  loss:  0.012105841189622879
Batch  66  loss:  0.012389921583235264
Batch  67  loss:  0.011615011841058731
Batch  68  loss:  0.011207489296793938
Batch  69  loss:  0.011965346522629261
Batch  70  loss:  0.011259078979492188
Batch  71  loss:  0.011843714863061905
Batch  72  loss:  0.011936559341847897
Batch  73  loss:  0.011596517637372017
Batch  74  loss:  0.01186134573072195
Batch  75  loss:  0.011238844133913517
Batch  76  loss:  0.011546873487532139
Batch  77  loss:  0.011647449806332588
Batch  78  loss:  0.012058857828378677
Batch  79  loss:  0.011508533731102943
Batch  80  loss:  0.011334633454680443
Batch  81  loss:  0.012296230532228947
Batch  82  loss:  0.011409733444452286
Batch  83  loss:  0.011073964647948742
Batch  84  loss:  0.012076337821781635
Batch  85  loss:  0.011178540997207165
Batch  86  loss:  0.011369659565389156
Batch  87  loss:  0.011718385852873325
Batch  88  loss:  0.011393223889172077
Batch  89  loss:  0.01158976648002863
Batch  90  loss:  0.011617803014814854
Batch  91  loss:  0.010987626388669014
Batch  92  loss:  0.011355482041835785
Batch  93  loss:  0.011757342144846916
Batch  94  loss:  0.010431377217173576
Batch  95  loss:  0.011359112337231636
Batch  96  loss:  0.011595658957958221
Batch  97  loss:  0.011884357780218124
Batch  98  loss:  0.012160727754235268
Batch  99  loss:  0.011399973183870316
Batch  100  loss:  0.011826494708657265
Validation: 
LOSS train 0.011621324438601732, val 0.011058064177632332
EPOCH : 16 TIME:  2022-04-19 01:40:07.881335
Training: 
Batch  1  loss:  0.011489169672131538
Batch  2  loss:  0.01173470914363861
Batch  3  loss:  0.01158265769481659
Batch  4  loss:  0.011581439524888992
Batch  5  loss:  0.011499323882162571
Batch  6  loss:  0.011421880684792995
Batch  7  loss:  0.01207564678043127
Batch  8  loss:  0.01170473825186491
Batch  9  loss:  0.011185003444552422
Batch  10  loss:  0.010961107909679413
Batch  11  loss:  0.011949990876019001
Batch  12  loss:  0.011842814274132252
Batch  13  loss:  0.011305815540254116
Batch  14  loss:  0.011174527928233147
Batch  15  loss:  0.011606552638113499
Batch  16  loss:  0.011781302280724049
Batch  17  loss:  0.01194782741367817
Batch  18  loss:  0.011195877566933632
Batch  19  loss:  0.011393556371331215
Batch  20  loss:  0.011392918415367603
Batch  21  loss:  0.011259939521551132
Batch  22  loss:  0.011465728282928467
Batch  23  loss:  0.01173805445432663
Batch  24  loss:  0.011063464917242527
Batch  25  loss:  0.011274607852101326
Batch  26  loss:  0.010819589719176292
Batch  27  loss:  0.011104552075266838
Batch  28  loss:  0.011325187049806118
Batch  29  loss:  0.011670541949570179
Batch  30  loss:  0.011541825719177723
Batch  31  loss:  0.011268188245594501
Batch  32  loss:  0.010742860846221447
Batch  33  loss:  0.011473959311842918
Batch  34  loss:  0.01118987426161766
Batch  35  loss:  0.011865527369081974
Batch  36  loss:  0.011737806722521782
Batch  37  loss:  0.011112162843346596
Batch  38  loss:  0.011705947108566761
Batch  39  loss:  0.011712530627846718
Batch  40  loss:  0.011676878668367863
Batch  41  loss:  0.011123964563012123
Batch  42  loss:  0.011504428461194038
Batch  43  loss:  0.011161701753735542
Batch  44  loss:  0.011450906284153461
Batch  45  loss:  0.011613977141678333
Batch  46  loss:  0.012437667697668076
Batch  47  loss:  0.010872691869735718
Batch  48  loss:  0.011998825706541538
Batch  49  loss:  0.011353690177202225
Batch  50  loss:  0.011604971252381802
Batch  51  loss:  0.011974654160439968
Batch  52  loss:  0.011351549066603184
Batch  53  loss:  0.011189959943294525
Batch  54  loss:  0.011260868050158024
Batch  55  loss:  0.011198947206139565
Batch  56  loss:  0.011347164399921894
Batch  57  loss:  0.01153556164354086
Batch  58  loss:  0.012136239558458328
Batch  59  loss:  0.01142306812107563
Batch  60  loss:  0.011237461119890213
Batch  61  loss:  0.011479652486741543
Batch  62  loss:  0.01143612153828144
Batch  63  loss:  0.011769764125347137
Batch  64  loss:  0.011185918003320694
Batch  65  loss:  0.011529465205967426
Batch  66  loss:  0.01169593632221222
Batch  67  loss:  0.01153707504272461
Batch  68  loss:  0.011929853819310665
Batch  69  loss:  0.01106242649257183
Batch  70  loss:  0.011642198078334332
Batch  71  loss:  0.011535129509866238
Batch  72  loss:  0.011950809508562088
Batch  73  loss:  0.011397108435630798
Batch  74  loss:  0.01096715684980154
Batch  75  loss:  0.011202411726117134
Batch  76  loss:  0.01090044155716896
Batch  77  loss:  0.011429516598582268
Batch  78  loss:  0.011009881272912025
Batch  79  loss:  0.011347434483468533
Batch  80  loss:  0.011575202457606792
Batch  81  loss:  0.011495458893477917
Batch  82  loss:  0.011865868233144283
Batch  83  loss:  0.011044921353459358
Batch  84  loss:  0.012403824366629124
Batch  85  loss:  0.011739049106836319
Batch  86  loss:  0.012142159976065159
Batch  87  loss:  0.011021091602742672
Batch  88  loss:  0.012557920068502426
Batch  89  loss:  0.011518997140228748
Batch  90  loss:  0.012689764611423016
Batch  91  loss:  0.011635666713118553
Batch  92  loss:  0.011147571727633476
Batch  93  loss:  0.011553325690329075
Batch  94  loss:  0.011708860285580158
Batch  95  loss:  0.011793424375355244
Batch  96  loss:  0.012344212271273136
Batch  97  loss:  0.011312279850244522
Batch  98  loss:  0.011318191885948181
Batch  99  loss:  0.01138509251177311
Batch  100  loss:  0.010814416222274303
Validation: 
LOSS train 0.011504299864172935, val 0.011103350669145584
EPOCH : 17 TIME:  2022-04-19 01:53:02.553537
Training: 
Batch  1  loss:  0.011317203752696514
Batch  2  loss:  0.0105657409876585
Batch  3  loss:  0.01157170720398426
Batch  4  loss:  0.011570942588150501
Batch  5  loss:  0.01111568696796894
Batch  6  loss:  0.011752385646104813
Batch  7  loss:  0.011147555895149708
Batch  8  loss:  0.011369647458195686
Batch  9  loss:  0.011776968836784363
Batch  10  loss:  0.012201220728456974
Batch  11  loss:  0.011627160012722015
Batch  12  loss:  0.011603537946939468
Batch  13  loss:  0.011888121254742146
Batch  14  loss:  0.011231236159801483
Batch  15  loss:  0.01116739958524704
Batch  16  loss:  0.011052880436182022
Batch  17  loss:  0.011643697507679462
Batch  18  loss:  0.011391044594347477
Batch  19  loss:  0.011937262490391731
Batch  20  loss:  0.011432950384914875
Batch  21  loss:  0.012124919332563877
Batch  22  loss:  0.010962462984025478
Batch  23  loss:  0.011255414225161076
Batch  24  loss:  0.011885828338563442
Batch  25  loss:  0.011398086324334145
Batch  26  loss:  0.012321810238063335
Batch  27  loss:  0.011105809360742569
Batch  28  loss:  0.011076549999415874
Batch  29  loss:  0.011748228222131729
Batch  30  loss:  0.011593575589358807
Batch  31  loss:  0.011455116793513298
Batch  32  loss:  0.010769584216177464
Batch  33  loss:  0.011947446502745152
Batch  34  loss:  0.011309734545648098
Batch  35  loss:  0.011762147769331932
Batch  36  loss:  0.011003696359694004
Batch  37  loss:  0.011132780462503433
Batch  38  loss:  0.011177167296409607
Batch  39  loss:  0.011438288725912571
Batch  40  loss:  0.011912995018064976
Batch  41  loss:  0.011382881551980972
Batch  42  loss:  0.011659604497253895
Batch  43  loss:  0.011887624859809875
Batch  44  loss:  0.010920471511781216
Batch  45  loss:  0.010950613766908646
Batch  46  loss:  0.012399828992784023
Batch  47  loss:  0.01086367852985859
Batch  48  loss:  0.011473897844552994
Batch  49  loss:  0.011231043376028538
Batch  50  loss:  0.01089434139430523
Batch  51  loss:  0.01115866843611002
Batch  52  loss:  0.011327109299600124
Batch  53  loss:  0.011599941179156303
Batch  54  loss:  0.011420232243835926
Batch  55  loss:  0.011369896121323109
Batch  56  loss:  0.01101926900446415
Batch  57  loss:  0.010992512106895447
Batch  58  loss:  0.011360966600477695
Batch  59  loss:  0.011968123726546764
Batch  60  loss:  0.011789033189415932
Batch  61  loss:  0.011009735986590385
Batch  62  loss:  0.011273061856627464
Batch  63  loss:  0.01191867794841528
Batch  64  loss:  0.010786160826683044
Batch  65  loss:  0.011536756530404091
Batch  66  loss:  0.011164486408233643
Batch  67  loss:  0.010586374439299107
Batch  68  loss:  0.01160961203277111
Batch  69  loss:  0.011725995689630508
Batch  70  loss:  0.011569282971322536
Batch  71  loss:  0.011466262862086296
Batch  72  loss:  0.011408893391489983
Batch  73  loss:  0.011219201609492302
Batch  74  loss:  0.011073334142565727
Batch  75  loss:  0.01148460153490305
Batch  76  loss:  0.011599449440836906
Batch  77  loss:  0.011141679249703884
Batch  78  loss:  0.010725321248173714
Batch  79  loss:  0.012041617184877396
Batch  80  loss:  0.010841302573680878
Batch  81  loss:  0.010549766942858696
Batch  82  loss:  0.012061383575201035
Batch  83  loss:  0.012180437333881855
Batch  84  loss:  0.011654183268547058
Batch  85  loss:  0.011341537348926067
Batch  86  loss:  0.01141718402504921
Batch  87  loss:  0.010933651588857174
Batch  88  loss:  0.01146745029836893
Batch  89  loss:  0.010982902720570564
Batch  90  loss:  0.011139108799397945
Batch  91  loss:  0.011405047960579395
Batch  92  loss:  0.01090607512742281
Batch  93  loss:  0.011314962059259415
Batch  94  loss:  0.01157381385564804
Batch  95  loss:  0.011023752391338348
Batch  96  loss:  0.011083689518272877
Batch  97  loss:  0.011891822330653667
Batch  98  loss:  0.0114792725071311
Batch  99  loss:  0.011737342923879623
Batch  100  loss:  0.011656645685434341
Validation: 
LOSS train 0.011403975691646338, val 0.010787919163703918
EPOCH : 18 TIME:  2022-04-19 02:05:55.843764
Training: 
Batch  1  loss:  0.011464125476777554
Batch  2  loss:  0.011530284769833088
Batch  3  loss:  0.011938614770770073
Batch  4  loss:  0.01148033607751131
Batch  5  loss:  0.01147651206701994
Batch  6  loss:  0.012091168202459812
Batch  7  loss:  0.011258391663432121
Batch  8  loss:  0.011970109306275845
Batch  9  loss:  0.012231594882905483
Batch  10  loss:  0.010998756624758244
Batch  11  loss:  0.011543304659426212
Batch  12  loss:  0.011184101924300194
Batch  13  loss:  0.011925246566534042
Batch  14  loss:  0.0109421880915761
Batch  15  loss:  0.010839219205081463
Batch  16  loss:  0.011400730349123478
Batch  17  loss:  0.010833164677023888
Batch  18  loss:  0.012031404301524162
Batch  19  loss:  0.010927279479801655
Batch  20  loss:  0.010859707370400429
Batch  21  loss:  0.011419149115681648
Batch  22  loss:  0.011059117503464222
Batch  23  loss:  0.011185945942997932
Batch  24  loss:  0.0108947129920125
Batch  25  loss:  0.01113860122859478
Batch  26  loss:  0.01138897892087698
Batch  27  loss:  0.011442570015788078
Batch  28  loss:  0.011390002444386482
Batch  29  loss:  0.011436240747570992
Batch  30  loss:  0.010760154575109482
Batch  31  loss:  0.012112405151128769
Batch  32  loss:  0.010703308507800102
Batch  33  loss:  0.010255289264023304
Batch  34  loss:  0.011605639010667801
Batch  35  loss:  0.01182012539356947
Batch  36  loss:  0.011083080433309078
Batch  37  loss:  0.011523148976266384
Batch  38  loss:  0.011763419955968857
Batch  39  loss:  0.011234241537749767
Batch  40  loss:  0.011124134063720703
Batch  41  loss:  0.011338612996041775
Batch  42  loss:  0.011645701713860035
Batch  43  loss:  0.011458281427621841
Batch  44  loss:  0.010391980409622192
Batch  45  loss:  0.011502978391945362
Batch  46  loss:  0.01112430915236473
Batch  47  loss:  0.011210719123482704
Batch  48  loss:  0.011362030170857906
Batch  49  loss:  0.010776432231068611
Batch  50  loss:  0.010974514298141003
Batch  51  loss:  0.011128592304885387
Batch  52  loss:  0.011101038195192814
Batch  53  loss:  0.01181483268737793
Batch  54  loss:  0.011303141713142395
Batch  55  loss:  0.011526393704116344
Batch  56  loss:  0.01129855215549469
Batch  57  loss:  0.011412899009883404
Batch  58  loss:  0.010694031603634357
Batch  59  loss:  0.011050743982195854
Batch  60  loss:  0.01153494417667389
Batch  61  loss:  0.010807936079800129
Batch  62  loss:  0.011950616724789143
Batch  63  loss:  0.012019271962344646
Batch  64  loss:  0.011303114704787731
Batch  65  loss:  0.011353173293173313
Batch  66  loss:  0.011617218144237995
Batch  67  loss:  0.011029331013560295
Batch  68  loss:  0.011186578311026096
Batch  69  loss:  0.011787940748035908
Batch  70  loss:  0.010959498584270477
Batch  71  loss:  0.011158180423080921
Batch  72  loss:  0.011592065915465355
Batch  73  loss:  0.010659303516149521
Batch  74  loss:  0.011537251062691212
Batch  75  loss:  0.01206289418041706
Batch  76  loss:  0.011269382201135159
Batch  77  loss:  0.011254815384745598
Batch  78  loss:  0.010865101590752602
Batch  79  loss:  0.011482694186270237
Batch  80  loss:  0.011049927212297916
Batch  81  loss:  0.01141453254967928
Batch  82  loss:  0.011079750955104828
Batch  83  loss:  0.011470858938992023
Batch  84  loss:  0.01113887783139944
Batch  85  loss:  0.011231758631765842
Batch  86  loss:  0.011730498634278774
Batch  87  loss:  0.011526934802532196
Batch  88  loss:  0.011118602938950062
Batch  89  loss:  0.011423048563301563
Batch  90  loss:  0.011219694279134274
Batch  91  loss:  0.0107258390635252
Batch  92  loss:  0.01090629119426012
Batch  93  loss:  0.011901823803782463
Batch  94  loss:  0.011103206314146519
Batch  95  loss:  0.011453617364168167
Batch  96  loss:  0.01111560221761465
Batch  97  loss:  0.011421945877373219
Batch  98  loss:  0.011596870608627796
Batch  99  loss:  0.011855757795274258
Batch  100  loss:  0.0114503288641572
Validation: 
LOSS train 0.011327493721619248, val 0.011001761071383953
EPOCH : 19 TIME:  2022-04-19 02:18:53.598239
Training: 
Batch  1  loss:  0.011258332058787346
Batch  2  loss:  0.010998339392244816
Batch  3  loss:  0.011310843750834465
Batch  4  loss:  0.011040305718779564
Batch  5  loss:  0.01107847224920988
Batch  6  loss:  0.011280357837677002
Batch  7  loss:  0.011761371977627277
Batch  8  loss:  0.011371388100087643
Batch  9  loss:  0.011369971558451653
Batch  10  loss:  0.011228445917367935
Batch  11  loss:  0.01062885019928217
Batch  12  loss:  0.01114293746650219
Batch  13  loss:  0.011318609118461609
Batch  14  loss:  0.011247383430600166
Batch  15  loss:  0.011936883442103863
Batch  16  loss:  0.01193224173039198
Batch  17  loss:  0.010441472753882408
Batch  18  loss:  0.011904619634151459
Batch  19  loss:  0.011405476368963718
Batch  20  loss:  0.01140779908746481
Batch  21  loss:  0.011563539505004883
Batch  22  loss:  0.010935180820524693
Batch  23  loss:  0.011054559610784054
Batch  24  loss:  0.011182963848114014
Batch  25  loss:  0.011124083772301674
Batch  26  loss:  0.01061331294476986
Batch  27  loss:  0.011402451433241367
Batch  28  loss:  0.011042837053537369
Batch  29  loss:  0.012414494529366493
Batch  30  loss:  0.011564912274479866
Batch  31  loss:  0.01120662223547697
Batch  32  loss:  0.011309679597616196
Batch  33  loss:  0.011010903865098953
Batch  34  loss:  0.011902260594069958
Batch  35  loss:  0.011050446890294552
Batch  36  loss:  0.011895228177309036
Batch  37  loss:  0.010965798050165176
Batch  38  loss:  0.010727064684033394
Batch  39  loss:  0.01120045967400074
Batch  40  loss:  0.01077660545706749
Batch  41  loss:  0.011246087029576302
Batch  42  loss:  0.011444849893450737
Batch  43  loss:  0.011293304152786732
Batch  44  loss:  0.010986490175127983
Batch  45  loss:  0.011420111171901226
Batch  46  loss:  0.012455370277166367
Batch  47  loss:  0.011466248892247677
Batch  48  loss:  0.010904812254011631
Batch  49  loss:  0.010761954821646214
Batch  50  loss:  0.011439982801675797
Batch  51  loss:  0.011098533868789673
Batch  52  loss:  0.01113303191959858
Batch  53  loss:  0.011145529337227345
Batch  54  loss:  0.01187623105943203
Batch  55  loss:  0.010902199894189835
Batch  56  loss:  0.011506406590342522
Batch  57  loss:  0.012462484650313854
Batch  58  loss:  0.010579454712569714
Batch  59  loss:  0.011603469960391521
Batch  60  loss:  0.010548810474574566
Batch  61  loss:  0.011139240115880966
Batch  62  loss:  0.011135616339743137
Batch  63  loss:  0.010827960446476936
Batch  64  loss:  0.011485404334962368
Batch  65  loss:  0.011262716725468636
Batch  66  loss:  0.01167500950396061
Batch  67  loss:  0.011040742509067059
Batch  68  loss:  0.011987103149294853
Batch  69  loss:  0.011281227692961693
Batch  70  loss:  0.011320923455059528
Batch  71  loss:  0.01157083734869957
Batch  72  loss:  0.011158114299178123
Batch  73  loss:  0.011536112986505032
Batch  74  loss:  0.011250539682805538
Batch  75  loss:  0.01125514879822731
Batch  76  loss:  0.010717512108385563
Batch  77  loss:  0.011070077307522297
Batch  78  loss:  0.010829061269760132
Batch  79  loss:  0.011856879107654095
Batch  80  loss:  0.01168875303119421
Batch  81  loss:  0.011330511420965195
Batch  82  loss:  0.011165985837578773
Batch  83  loss:  0.010792046785354614
Batch  84  loss:  0.010999386198818684
Batch  85  loss:  0.010780546814203262
Batch  86  loss:  0.011235053651034832
Batch  87  loss:  0.01068150345236063
Batch  88  loss:  0.012007152661681175
Batch  89  loss:  0.011880121193826199
Batch  90  loss:  0.01166286040097475
Batch  91  loss:  0.010961579158902168
Batch  92  loss:  0.011574606411159039
Batch  93  loss:  0.010767240077257156
Batch  94  loss:  0.0114178117364645
Batch  95  loss:  0.010888054035604
Batch  96  loss:  0.011695913970470428
Batch  97  loss:  0.01184975728392601
Batch  98  loss:  0.011852019466459751
Batch  99  loss:  0.011054609902203083
Batch  100  loss:  0.011678888462483883
Validation: 
LOSS train 0.011296474998816848, val 0.010923625901341438
EPOCH : 20 TIME:  2022-04-19 02:31:52.613803
Training: 
Batch  1  loss:  0.010774650610983372
Batch  2  loss:  0.011060993187129498
Batch  3  loss:  0.011344718746840954
Batch  4  loss:  0.010931872762739658
Batch  5  loss:  0.011194473132491112
Batch  6  loss:  0.011282648891210556
Batch  7  loss:  0.011679910123348236
Batch  8  loss:  0.010644483380019665
Batch  9  loss:  0.011795599944889545
Batch  10  loss:  0.011526820249855518
Batch  11  loss:  0.011453866958618164
Batch  12  loss:  0.011575168929994106
Batch  13  loss:  0.011177020147442818
Batch  14  loss:  0.011257912963628769
Batch  15  loss:  0.011442088522017002
Batch  16  loss:  0.01149873435497284
Batch  17  loss:  0.011226126924157143
Batch  18  loss:  0.011473073624074459
Batch  19  loss:  0.01188846118748188
Batch  20  loss:  0.011619926430284977
Batch  21  loss:  0.011012223549187183
Batch  22  loss:  0.011573945172131062
Batch  23  loss:  0.010906166397035122
Batch  24  loss:  0.011281496845185757
Batch  25  loss:  0.010940209031105042
Batch  26  loss:  0.011494374834001064
Batch  27  loss:  0.011542179621756077
Batch  28  loss:  0.01160610094666481
Batch  29  loss:  0.010937930084764957
Batch  30  loss:  0.01162503007799387
Batch  31  loss:  0.011531932279467583
Batch  32  loss:  0.011074459180235863
Batch  33  loss:  0.011943357065320015
Batch  34  loss:  0.011804508976638317
Batch  35  loss:  0.011581639759242535
Batch  36  loss:  0.011440393514931202
Batch  37  loss:  0.01109930407255888
Batch  38  loss:  0.011090293526649475
Batch  39  loss:  0.011153212748467922
Batch  40  loss:  0.01177449245005846
Batch  41  loss:  0.011462834663689137
Batch  42  loss:  0.011295272037386894
Batch  43  loss:  0.010526100173592567
Batch  44  loss:  0.011376702226698399
Batch  45  loss:  0.010347043164074421
Batch  46  loss:  0.01172757986932993
Batch  47  loss:  0.011056337505578995
Batch  48  loss:  0.01090770959854126
Batch  49  loss:  0.01077840756624937
Batch  50  loss:  0.011061502620577812
Batch  51  loss:  0.010813160799443722
Batch  52  loss:  0.01107033807784319
Batch  53  loss:  0.011516728438436985
Batch  54  loss:  0.011621213518083096
Batch  55  loss:  0.011326305568218231
Batch  56  loss:  0.010813860222697258
Batch  57  loss:  0.011020970530807972
Batch  58  loss:  0.010600287467241287
Batch  59  loss:  0.011491074226796627
Batch  60  loss:  0.010783770121634007
Batch  61  loss:  0.011221778579056263
Batch  62  loss:  0.01107327826321125
Batch  63  loss:  0.010947967879474163
Batch  64  loss:  0.01123103965073824
Batch  65  loss:  0.011016149073839188
Batch  66  loss:  0.011434648185968399
Batch  67  loss:  0.01064024306833744
Batch  68  loss:  0.010550482198596
Batch  69  loss:  0.011315254494547844
Batch  70  loss:  0.011758527718484402
Batch  71  loss:  0.010677985846996307
Batch  72  loss:  0.011196515522897243
Batch  73  loss:  0.010758837684988976
Batch  74  loss:  0.011684521101415157
Batch  75  loss:  0.011191096156835556
Batch  76  loss:  0.011329826898872852
Batch  77  loss:  0.01102629117667675
Batch  78  loss:  0.011189035139977932
Batch  79  loss:  0.011094118468463421
Batch  80  loss:  0.011436251923441887
Batch  81  loss:  0.010685049928724766
Batch  82  loss:  0.011073140427470207
Batch  83  loss:  0.010898623615503311
Batch  84  loss:  0.011188661679625511
Batch  85  loss:  0.011321621015667915
Batch  86  loss:  0.011352228932082653
Batch  87  loss:  0.011888275854289532
Batch  88  loss:  0.01066148653626442
Batch  89  loss:  0.010891914367675781
Batch  90  loss:  0.010944673791527748
Batch  91  loss:  0.011826020665466785
Batch  92  loss:  0.011117801070213318
Batch  93  loss:  0.010754618793725967
Batch  94  loss:  0.011017090640962124
Batch  95  loss:  0.011275365948677063
Batch  96  loss:  0.010719630867242813
Batch  97  loss:  0.011173997074365616
Batch  98  loss:  0.011084861122071743
Batch  99  loss:  0.01089910976588726
Batch  100  loss:  0.01087813451886177
Validation: 
LOSS train 0.011202851552516222, val 0.010686895810067654
Tue 19 Apr 2022 02:44:49 AM EDT
LAPTOP:: POINT TRANSFORMER
INFO - 2022-04-19 02:44:51,399 - utils - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO - 2022-04-19 02:44:51,399 - utils - NumExpr defaulting to 8 threads.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  laptop
--------------------
Running self_supervised_training:  2022-04-19 02:44:51.541477
--------------------
device is  cuda
--------------------
Number of trainable parameters:  888466
EPOCH : 1 TIME:  2022-04-19 02:44:53.263095
Training: 
Batch  1  loss:  0.6162847280502319
Batch  2  loss:  0.42027735710144043
Batch  3  loss:  0.6366214156150818
Batch  4  loss:  0.2918061316013336
Batch  5  loss:  0.3904836177825928
Batch  6  loss:  0.41069862246513367
Batch  7  loss:  0.31275370717048645
Batch  8  loss:  0.2398817390203476
Batch  9  loss:  0.2344459444284439
Batch  10  loss:  0.23240020871162415
Batch  11  loss:  0.24126537144184113
Batch  12  loss:  0.23489566147327423
Batch  13  loss:  0.2215842455625534
Batch  14  loss:  0.22303132712841034
Batch  15  loss:  0.2138194590806961
Batch  16  loss:  0.21055208146572113
Batch  17  loss:  0.210881769657135
Batch  18  loss:  0.2026873379945755
Batch  19  loss:  0.20598775148391724
Batch  20  loss:  0.2095261961221695
Batch  21  loss:  0.1992260366678238
Batch  22  loss:  0.20262861251831055
Batch  23  loss:  0.20596474409103394
Batch  24  loss:  0.19559766352176666
Batch  25  loss:  0.21108245849609375
Batch  26  loss:  0.20512837171554565
Batch  27  loss:  0.2124633938074112
Batch  28  loss:  0.21312877535820007
Batch  29  loss:  0.20441505312919617
Batch  30  loss:  0.20444706082344055
Batch  31  loss:  0.20240506529808044
Batch  32  loss:  0.20029674470424652
Batch  33  loss:  0.20679371058940887
Batch  34  loss:  0.20067405700683594
Batch  35  loss:  0.20444633066654205
Batch  36  loss:  0.19808387756347656
Batch  37  loss:  0.20326972007751465
Batch  38  loss:  0.19906704127788544
Batch  39  loss:  0.19762976467609406
Batch  40  loss:  0.20371970534324646
Batch  41  loss:  0.19886115193367004
Batch  42  loss:  0.19904865324497223
Batch  43  loss:  0.20034730434417725
Batch  44  loss:  0.19194087386131287
Batch  45  loss:  0.20078414678573608
Batch  46  loss:  0.1945035457611084
Batch  47  loss:  0.19972872734069824
Batch  48  loss:  0.19969569146633148
Batch  49  loss:  0.19598117470741272
Batch  50  loss:  0.2005038559436798
Batch  51  loss:  0.1952611356973648
Batch  52  loss:  0.19520503282546997
Batch  53  loss:  0.19572369754314423
Batch  54  loss:  0.19726060330867767
Batch  55  loss:  0.1956394463777542
Batch  56  loss:  0.197159081697464
Batch  57  loss:  0.1959582269191742
Batch  58  loss:  0.19561095535755157
Batch  59  loss:  0.1952895075082779
Batch  60  loss:  0.19484230875968933
Batch  61  loss:  0.19422049820423126
Batch  62  loss:  0.19474636018276215
Batch  63  loss:  0.19345638155937195
Batch  64  loss:  0.19484683871269226
Batch  65  loss:  0.19425025582313538
Batch  66  loss:  0.18732598423957825
Batch  67  loss:  0.19485709071159363
Batch  68  loss:  0.19358676671981812
Batch  69  loss:  0.1937706023454666
Batch  70  loss:  0.1903713494539261
Batch  71  loss:  0.19119781255722046
Batch  72  loss:  0.1942768096923828
Batch  73  loss:  0.19434897601604462
Batch  74  loss:  0.19219085574150085
Batch  75  loss:  0.19070659577846527
Batch  76  loss:  0.19666624069213867
Batch  77  loss:  0.1926879584789276
Batch  78  loss:  0.19054585695266724
Batch  79  loss:  0.18973617255687714
Batch  80  loss:  0.19225504994392395
Batch  81  loss:  0.19203972816467285
Batch  82  loss:  0.19040866196155548
Batch  83  loss:  0.1849742978811264
Batch  84  loss:  0.1964927464723587
Batch  85  loss:  0.19127030670642853
Batch  86  loss:  0.1902647316455841
Batch  87  loss:  0.18858139216899872
Batch  88  loss:  0.18892362713813782
Batch  89  loss:  0.19545739889144897
Batch  90  loss:  0.1882697194814682
Batch  91  loss:  0.19245107471942902
Batch  92  loss:  0.18995675444602966
Batch  93  loss:  0.1893354058265686
Batch  94  loss:  0.1896522492170334
Batch  95  loss:  0.19044849276542664
Batch  96  loss:  0.18795879185199738
Batch  97  loss:  0.19279983639717102
Batch  98  loss:  0.18987753987312317
Batch  99  loss:  0.18819354474544525
Batch  100  loss:  0.1897517591714859
Validation: 
LOSS train 0.21632850497961045, val 0.19124118983745575
EPOCH : 2 TIME:  2022-04-19 02:50:31.482266
Training: 
Batch  1  loss:  0.1874919831752777
Batch  2  loss:  0.19151519238948822
Batch  3  loss:  0.18801119923591614
Batch  4  loss:  0.18964584171772003
Batch  5  loss:  0.18547813594341278
Batch  6  loss:  0.18570823967456818
Batch  7  loss:  0.19175662100315094
Batch  8  loss:  0.18566304445266724
Batch  9  loss:  0.19104279577732086
Batch  10  loss:  0.19083556532859802
Batch  11  loss:  0.19150996208190918
Batch  12  loss:  0.18501152098178864
Batch  13  loss:  0.18846724927425385
Batch  14  loss:  0.19122302532196045
Batch  15  loss:  0.18992747366428375
Batch  16  loss:  0.1920928955078125
Batch  17  loss:  0.18762972950935364
Batch  18  loss:  0.18518050014972687
Batch  19  loss:  0.19024497270584106
Batch  20  loss:  0.18425855040550232
Batch  21  loss:  0.19004233181476593
Batch  22  loss:  0.18897171318531036
Batch  23  loss:  0.18509554862976074
Batch  24  loss:  0.18939892947673798
Batch  25  loss:  0.18849655985832214
Batch  26  loss:  0.1877918243408203
Batch  27  loss:  0.18939590454101562
Batch  28  loss:  0.1882648766040802
Batch  29  loss:  0.1877526044845581
Batch  30  loss:  0.19087764620780945
Batch  31  loss:  0.18398602306842804
Batch  32  loss:  0.18627387285232544
Batch  33  loss:  0.19058887660503387
Batch  34  loss:  0.1878303438425064
Batch  35  loss:  0.18713665008544922
Batch  36  loss:  0.18977920711040497
Batch  37  loss:  0.1858341246843338
Batch  38  loss:  0.18793898820877075
Batch  39  loss:  0.1828782558441162
Batch  40  loss:  0.18707264959812164
Batch  41  loss:  0.1865544468164444
Batch  42  loss:  0.18645323812961578
Batch  43  loss:  0.1873985230922699
Batch  44  loss:  0.18817248940467834
Batch  45  loss:  0.1918502002954483
Batch  46  loss:  0.18509936332702637
Batch  47  loss:  0.1886972039937973
Batch  48  loss:  0.18631397187709808
Batch  49  loss:  0.18749448657035828
Batch  50  loss:  0.18730714917182922
Batch  51  loss:  0.18824508786201477
Batch  52  loss:  0.18662725389003754
Batch  53  loss:  0.1883753091096878
Batch  54  loss:  0.18171817064285278
Batch  55  loss:  0.18353229761123657
Batch  56  loss:  0.18575221300125122
Batch  57  loss:  0.19014625251293182
Batch  58  loss:  0.1838645190000534
Batch  59  loss:  0.18759796023368835
Batch  60  loss:  0.1890788972377777
Batch  61  loss:  0.18909260630607605
Batch  62  loss:  0.18313366174697876
Batch  63  loss:  0.18412473797798157
Batch  64  loss:  0.18599723279476166
Batch  65  loss:  0.18659506738185883
Batch  66  loss:  0.1874188929796219
Batch  67  loss:  0.187754824757576
Batch  68  loss:  0.1887044459581375
Batch  69  loss:  0.18375003337860107
Batch  70  loss:  0.18807542324066162
Batch  71  loss:  0.1920645833015442
Batch  72  loss:  0.1882203072309494
Batch  73  loss:  0.1892804652452469
Batch  74  loss:  0.1889459639787674
Batch  75  loss:  0.18701858818531036
Batch  76  loss:  0.1880979835987091
Batch  77  loss:  0.18733060359954834
Batch  78  loss:  0.1800142377614975
Batch  79  loss:  0.1879604607820511
Batch  80  loss:  0.18366806209087372
Batch  81  loss:  0.17904703319072723
Batch  82  loss:  0.18628565967082977
Batch  83  loss:  0.18807455897331238
Batch  84  loss:  0.1851338893175125
Batch  85  loss:  0.17790912091732025
Batch  86  loss:  0.18684141337871552
Batch  87  loss:  0.1892697662115097
Batch  88  loss:  0.1833595633506775
Batch  89  loss:  0.18422973155975342
Batch  90  loss:  0.18174989521503448
Batch  91  loss:  0.18560847640037537
Batch  92  loss:  0.19087731838226318
Batch  93  loss:  0.1830221712589264
Batch  94  loss:  0.18675722181797028
Batch  95  loss:  0.18166422843933105
Batch  96  loss:  0.18399055302143097
Batch  97  loss:  0.18422822654247284
Batch  98  loss:  0.1863461136817932
Batch  99  loss:  0.18535423278808594
Batch  100  loss:  0.18688106536865234
Validation: 
LOSS train 0.1870225888490677, val 0.18642105162143707
EPOCH : 3 TIME:  2022-04-19 02:56:10.450446
Training: 
Batch  1  loss:  0.18442684412002563
Batch  2  loss:  0.1804845631122589
Batch  3  loss:  0.1851608008146286
Batch  4  loss:  0.18314129114151
Batch  5  loss:  0.18458658456802368
Batch  6  loss:  0.190108522772789
Batch  7  loss:  0.18501651287078857
Batch  8  loss:  0.1882665753364563
Batch  9  loss:  0.18371263146400452
Batch  10  loss:  0.18308860063552856
Batch  11  loss:  0.1841498166322708
Batch  12  loss:  0.1822395622730255
Batch  13  loss:  0.1855064332485199
Batch  14  loss:  0.18635842204093933
Batch  15  loss:  0.18845266103744507
Batch  16  loss:  0.18128179013729095
Batch  17  loss:  0.19053031504154205
Batch  18  loss:  0.18794755637645721
Batch  19  loss:  0.18273493647575378
Batch  20  loss:  0.1841956228017807
Batch  21  loss:  0.18666861951351166
Batch  22  loss:  0.18602736294269562
Batch  23  loss:  0.1795959174633026
Batch  24  loss:  0.1874484419822693
Batch  25  loss:  0.18409757316112518
Batch  26  loss:  0.1861438751220703
Batch  27  loss:  0.18584518134593964
Batch  28  loss:  0.1816009134054184
Batch  29  loss:  0.18776099383831024
Batch  30  loss:  0.1888035386800766
Batch  31  loss:  0.18035966157913208
Batch  32  loss:  0.18471580743789673
Batch  33  loss:  0.18134799599647522
Batch  34  loss:  0.1833348572254181
Batch  35  loss:  0.18860803544521332
Batch  36  loss:  0.18745718896389008
Batch  37  loss:  0.18472816050052643
Batch  38  loss:  0.17982620000839233
Batch  39  loss:  0.18462736904621124
Batch  40  loss:  0.19118519127368927
Batch  41  loss:  0.18751151859760284
Batch  42  loss:  0.18371979892253876
Batch  43  loss:  0.18291957676410675
Batch  44  loss:  0.18239793181419373
Batch  45  loss:  0.18801181018352509
Batch  46  loss:  0.18623635172843933
Batch  47  loss:  0.1791127324104309
Batch  48  loss:  0.18568004667758942
Batch  49  loss:  0.1832875907421112
Batch  50  loss:  0.18157300353050232
Batch  51  loss:  0.18145863711833954
Batch  52  loss:  0.18586650490760803
Batch  53  loss:  0.18427760899066925
Batch  54  loss:  0.18926791846752167
Batch  55  loss:  0.18135888874530792
Batch  56  loss:  0.18869715929031372
Batch  57  loss:  0.17974506318569183
Batch  58  loss:  0.18593746423721313
Batch  59  loss:  0.18391506373882294
Batch  60  loss:  0.18722353875637054
Batch  61  loss:  0.18144404888153076
Batch  62  loss:  0.1807677000761032
Batch  63  loss:  0.1806221306324005
Batch  64  loss:  0.1882389783859253
Batch  65  loss:  0.1853133589029312
Batch  66  loss:  0.186258926987648
Batch  67  loss:  0.18607768416404724
Batch  68  loss:  0.1819709688425064
Batch  69  loss:  0.1894717514514923
Batch  70  loss:  0.18308621644973755
Batch  71  loss:  0.18812690675258636
Batch  72  loss:  0.18557769060134888
Batch  73  loss:  0.18530398607254028
Batch  74  loss:  0.18308383226394653
Batch  75  loss:  0.18248435854911804
Batch  76  loss:  0.18060679733753204
Batch  77  loss:  0.18222381174564362
Batch  78  loss:  0.18712139129638672
Batch  79  loss:  0.18392620980739594
Batch  80  loss:  0.18463987112045288
Batch  81  loss:  0.17986972630023956
Batch  82  loss:  0.18451575934886932
Batch  83  loss:  0.17824849486351013
Batch  84  loss:  0.1822836697101593
Batch  85  loss:  0.18320445716381073
Batch  86  loss:  0.175467848777771
Batch  87  loss:  0.17895522713661194
Batch  88  loss:  0.18890269100666046
Batch  89  loss:  0.184866264462471
Batch  90  loss:  0.1819174885749817
Batch  91  loss:  0.1869683563709259
Batch  92  loss:  0.18578559160232544
Batch  93  loss:  0.18143165111541748
Batch  94  loss:  0.1800537407398224
Batch  95  loss:  0.18086831271648407
Batch  96  loss:  0.18466441333293915
Batch  97  loss:  0.1775265783071518
Batch  98  loss:  0.18186518549919128
Batch  99  loss:  0.18149623274803162
Batch  100  loss:  0.18097907304763794
Validation: 
LOSS train 0.18415988519787788, val 0.1882036179304123
EPOCH : 4 TIME:  2022-04-19 03:01:47.828911
Training: 
Batch  1  loss:  0.17896048724651337
Batch  2  loss:  0.18004894256591797
Batch  3  loss:  0.18263189494609833
Batch  4  loss:  0.1775721311569214
Batch  5  loss:  0.17822596430778503
Batch  6  loss:  0.18310828506946564
Batch  7  loss:  0.18350248038768768
Batch  8  loss:  0.18766936659812927
Batch  9  loss:  0.17844261229038239
Batch  10  loss:  0.18420810997486115
Batch  11  loss:  0.17703916132450104
Batch  12  loss:  0.18072573840618134
Batch  13  loss:  0.1810390055179596
Batch  14  loss:  0.1822945773601532
Batch  15  loss:  0.18707869946956635
Batch  16  loss:  0.18393611907958984
Batch  17  loss:  0.1854989230632782
Batch  18  loss:  0.18438570201396942
Batch  19  loss:  0.17708836495876312
Batch  20  loss:  0.17938220500946045
Batch  21  loss:  0.18268969655036926
Batch  22  loss:  0.17484161257743835
Batch  23  loss:  0.18678706884384155
Batch  24  loss:  0.17641960084438324
Batch  25  loss:  0.18277838826179504
Batch  26  loss:  0.17935922741889954
Batch  27  loss:  0.17403443157672882
Batch  28  loss:  0.18557998538017273
Batch  29  loss:  0.1746932566165924
Batch  30  loss:  0.1795775592327118
Batch  31  loss:  0.18430937826633453
Batch  32  loss:  0.18822863698005676
Batch  33  loss:  0.18165789544582367
Batch  34  loss:  0.17834264039993286
Batch  35  loss:  0.17695243656635284
Batch  36  loss:  0.17917022109031677
Batch  37  loss:  0.17989438772201538
Batch  38  loss:  0.18061886727809906
Batch  39  loss:  0.18643249571323395
Batch  40  loss:  0.1788874715566635
Batch  41  loss:  0.17550571262836456
Batch  42  loss:  0.1787300705909729
Batch  43  loss:  0.18439777195453644
Batch  44  loss:  0.17940694093704224
Batch  45  loss:  0.17972654104232788
Batch  46  loss:  0.1789020448923111
Batch  47  loss:  0.17331072688102722
Batch  48  loss:  0.18925315141677856
Batch  49  loss:  0.18169204890727997
Batch  50  loss:  0.17959453165531158
Batch  51  loss:  0.1807248592376709
Batch  52  loss:  0.18175315856933594
Batch  53  loss:  0.17608827352523804
Batch  54  loss:  0.1827768236398697
Batch  55  loss:  0.18477767705917358
Batch  56  loss:  0.18172746896743774
Batch  57  loss:  0.18305282294750214
Batch  58  loss:  0.17479856312274933
Batch  59  loss:  0.17595885694026947
Batch  60  loss:  0.1853393167257309
Batch  61  loss:  0.18415813148021698
Batch  62  loss:  0.18835479021072388
Batch  63  loss:  0.1813473254442215
Batch  64  loss:  0.18708541989326477
Batch  65  loss:  0.17693789303302765
Batch  66  loss:  0.17971889674663544
Batch  67  loss:  0.17792260646820068
Batch  68  loss:  0.17831794917583466
Batch  69  loss:  0.1852380484342575
Batch  70  loss:  0.18040461838245392
Batch  71  loss:  0.18351279199123383
Batch  72  loss:  0.18088698387145996
Batch  73  loss:  0.17981894314289093
Batch  74  loss:  0.17750467360019684
Batch  75  loss:  0.18320117890834808
Batch  76  loss:  0.17846204340457916
Batch  77  loss:  0.18240033090114594
Batch  78  loss:  0.17948397994041443
Batch  79  loss:  0.17941729724407196
Batch  80  loss:  0.17345499992370605
Batch  81  loss:  0.1784912794828415
Batch  82  loss:  0.18010245263576508
Batch  83  loss:  0.1828220635652542
Batch  84  loss:  0.18603278696537018
Batch  85  loss:  0.17846842110157013
Batch  86  loss:  0.17369495332241058
Batch  87  loss:  0.17352744936943054
Batch  88  loss:  0.1845017969608307
Batch  89  loss:  0.18439200520515442
Batch  90  loss:  0.17596435546875
Batch  91  loss:  0.18207988142967224
Batch  92  loss:  0.17622508108615875
Batch  93  loss:  0.1765865683555603
Batch  94  loss:  0.17948070168495178
Batch  95  loss:  0.17943118512630463
Batch  96  loss:  0.1739504188299179
Batch  97  loss:  0.180253803730011
Batch  98  loss:  0.17204859852790833
Batch  99  loss:  0.17485865950584412
Batch  100  loss:  0.18604080379009247
Validation: 
LOSS train 0.1804819355905056, val 0.17389549314975739
EPOCH : 5 TIME:  2022-04-19 03:07:26.609136
Training: 
Batch  1  loss:  0.1802050918340683
Batch  2  loss:  0.1633116900920868
Batch  3  loss:  0.17459623515605927
Batch  4  loss:  0.1712232232093811
Batch  5  loss:  0.17644964158535004
Batch  6  loss:  0.17128130793571472
Batch  7  loss:  0.17409372329711914
Batch  8  loss:  0.17878314852714539
Batch  9  loss:  0.1738700419664383
Batch  10  loss:  0.17506176233291626
Batch  11  loss:  0.17534804344177246
Batch  12  loss:  0.1689894050359726
Batch  13  loss:  0.1701715588569641
Batch  14  loss:  0.17505790293216705
Batch  15  loss:  0.18240530788898468
Batch  16  loss:  0.1702502816915512
Batch  17  loss:  0.16989459097385406
Batch  18  loss:  0.16845600306987762
Batch  19  loss:  0.1746462732553482
Batch  20  loss:  0.17343340814113617
Batch  21  loss:  0.17037749290466309
Batch  22  loss:  0.17178967595100403
Batch  23  loss:  0.16032256186008453
Batch  24  loss:  0.17675931751728058
Batch  25  loss:  0.17462550103664398
Batch  26  loss:  0.17335979640483856
Batch  27  loss:  0.16909503936767578
Batch  28  loss:  0.18069113790988922
Batch  29  loss:  0.1733880639076233
Batch  30  loss:  0.17494215071201324
Batch  31  loss:  0.1721394658088684
Batch  32  loss:  0.17903758585453033
Batch  33  loss:  0.17930202186107635
Batch  34  loss:  0.1649416834115982
Batch  35  loss:  0.17404985427856445
Batch  36  loss:  0.16552649438381195
Batch  37  loss:  0.17722289264202118
Batch  38  loss:  0.17065605521202087
Batch  39  loss:  0.16878722608089447
Batch  40  loss:  0.17540760338306427
Batch  41  loss:  0.16309188306331635
Batch  42  loss:  0.17685039341449738
Batch  43  loss:  0.17504997551441193
Batch  44  loss:  0.1708078384399414
Batch  45  loss:  0.1796945333480835
Batch  46  loss:  0.16960474848747253
Batch  47  loss:  0.16798558831214905
Batch  48  loss:  0.17259955406188965
Batch  49  loss:  0.17257505655288696
Batch  50  loss:  0.15900620818138123
Batch  51  loss:  0.16481974720954895
Batch  52  loss:  0.16335545480251312
Batch  53  loss:  0.1630636304616928
Batch  54  loss:  0.17527733743190765
Batch  55  loss:  0.1674243062734604
Batch  56  loss:  0.1669095903635025
Batch  57  loss:  0.1587003767490387
Batch  58  loss:  0.15746597945690155
Batch  59  loss:  0.16350838541984558
Batch  60  loss:  0.16372893750667572
Batch  61  loss:  0.1612396538257599
Batch  62  loss:  0.17068415880203247
Batch  63  loss:  0.15068241953849792
Batch  64  loss:  0.1567801982164383
Batch  65  loss:  0.16705703735351562
Batch  66  loss:  0.1595923900604248
Batch  67  loss:  0.1672707349061966
Batch  68  loss:  0.17145130038261414
Batch  69  loss:  0.15896418690681458
Batch  70  loss:  0.1564086377620697
Batch  71  loss:  0.14478683471679688
Batch  72  loss:  0.15990383923053741
Batch  73  loss:  0.1633969396352768
Batch  74  loss:  0.15544739365577698
Batch  75  loss:  0.15702630579471588
Batch  76  loss:  0.16558809578418732
Batch  77  loss:  0.16166792809963226
Batch  78  loss:  0.17821849882602692
Batch  79  loss:  0.1574980467557907
Batch  80  loss:  0.1558619737625122
Batch  81  loss:  0.15838541090488434
Batch  82  loss:  0.15634240210056305
Batch  83  loss:  0.16784286499023438
Batch  84  loss:  0.1651533991098404
Batch  85  loss:  0.14799268543720245
Batch  86  loss:  0.1636638343334198
Batch  87  loss:  0.15623164176940918
Batch  88  loss:  0.1594248265028
Batch  89  loss:  0.16313156485557556
Batch  90  loss:  0.15440043807029724
Batch  91  loss:  0.18256333470344543
Batch  92  loss:  0.15395793318748474
Batch  93  loss:  0.1470959484577179
Batch  94  loss:  0.1693454384803772
Batch  95  loss:  0.1616481989622116
Batch  96  loss:  0.15651845932006836
Batch  97  loss:  0.15120810270309448
Batch  98  loss:  0.15673626959323883
Batch  99  loss:  0.16459938883781433
Batch  100  loss:  0.1510370522737503
Validation: 
LOSS train 0.16678275555372238, val 0.18344001471996307
EPOCH : 6 TIME:  2022-04-19 03:13:06.040960
Training: 
Batch  1  loss:  0.16730166971683502
Batch  2  loss:  0.16626650094985962
Batch  3  loss:  0.15097802877426147
Batch  4  loss:  0.14435003697872162
Batch  5  loss:  0.15079697966575623
Batch  6  loss:  0.16154098510742188
Batch  7  loss:  0.16966910660266876
Batch  8  loss:  0.1561470329761505
Batch  9  loss:  0.16038784384727478
Batch  10  loss:  0.15890976786613464
Batch  11  loss:  0.15249750018119812
Batch  12  loss:  0.15975238382816315
Batch  13  loss:  0.15303008258342743
Batch  14  loss:  0.1496119201183319
Batch  15  loss:  0.14685945212841034
Batch  16  loss:  0.14994218945503235
Batch  17  loss:  0.16381095349788666
Batch  18  loss:  0.15895669162273407
Batch  19  loss:  0.17023257911205292
Batch  20  loss:  0.15796630084514618
Batch  21  loss:  0.1474982351064682
Batch  22  loss:  0.1428699642419815
Batch  23  loss:  0.1406039595603943
Batch  24  loss:  0.17090041935443878
Batch  25  loss:  0.14701269567012787
Batch  26  loss:  0.15326400101184845
Batch  27  loss:  0.14961378276348114
Batch  28  loss:  0.16932889819145203
Batch  29  loss:  0.15812718868255615
Batch  30  loss:  0.1592475175857544
Batch  31  loss:  0.1526777744293213
Batch  32  loss:  0.16097919642925262
Batch  33  loss:  0.14942073822021484
Batch  34  loss:  0.16726256906986237
Batch  35  loss:  0.15678441524505615
Batch  36  loss:  0.15311452746391296
Batch  37  loss:  0.16252823173999786
Batch  38  loss:  0.14719970524311066
Batch  39  loss:  0.14731940627098083
Batch  40  loss:  0.13458679616451263
Batch  41  loss:  0.1474580317735672
Batch  42  loss:  0.14728595316410065
Batch  43  loss:  0.16576392948627472
Batch  44  loss:  0.14541305601596832
Batch  45  loss:  0.14788022637367249
Batch  46  loss:  0.1672099232673645
Batch  47  loss:  0.14383414387702942
Batch  48  loss:  0.15638889372348785
Batch  49  loss:  0.13682374358177185
Batch  50  loss:  0.1445239931344986
Batch  51  loss:  0.14838992059230804
Batch  52  loss:  0.14562545716762543
Batch  53  loss:  0.14444106817245483
Batch  54  loss:  0.13386331498622894
Batch  55  loss:  0.14792491495609283
Batch  56  loss:  0.1536654680967331
Batch  57  loss:  0.1557469516992569
Batch  58  loss:  0.145505890250206
Batch  59  loss:  0.16651767492294312
Batch  60  loss:  0.13440357148647308
Batch  61  loss:  0.14341585338115692
Batch  62  loss:  0.1545601785182953
Batch  63  loss:  0.13295431435108185
Batch  64  loss:  0.15476104617118835
Batch  65  loss:  0.14152877032756805
Batch  66  loss:  0.14779427647590637
Batch  67  loss:  0.15534885227680206
Batch  68  loss:  0.13852211833000183
Batch  69  loss:  0.13631387054920197
Batch  70  loss:  0.14857645332813263
Batch  71  loss:  0.14593257009983063
Batch  72  loss:  0.14445529878139496
Batch  73  loss:  0.15039119124412537
Batch  74  loss:  0.12798890471458435
Batch  75  loss:  0.14550891518592834
Batch  76  loss:  0.13921895623207092
Batch  77  loss:  0.167103573679924
Batch  78  loss:  0.16300290822982788
Batch  79  loss:  0.13816890120506287
Batch  80  loss:  0.1606060415506363
Batch  81  loss:  0.14668947458267212
Batch  82  loss:  0.1350404918193817
Batch  83  loss:  0.1426444947719574
Batch  84  loss:  0.13530738651752472
Batch  85  loss:  0.1386898010969162
Batch  86  loss:  0.15565261244773865
Batch  87  loss:  0.14476382732391357
Batch  88  loss:  0.13107427954673767
Batch  89  loss:  0.15517650544643402
Batch  90  loss:  0.13994275033473969
Batch  91  loss:  0.14448362588882446
Batch  92  loss:  0.13266398012638092
Batch  93  loss:  0.1424206644296646
Batch  94  loss:  0.15698157250881195
Batch  95  loss:  0.15387797355651855
Batch  96  loss:  0.13749083876609802
Batch  97  loss:  0.1524823009967804
Batch  98  loss:  0.14955270290374756
Batch  99  loss:  0.1434480845928192
Batch  100  loss:  0.13821539282798767
Validation: 
LOSS train 0.1501076591014862, val 0.17991691827774048
EPOCH : 7 TIME:  2022-04-19 03:18:42.336084
Training: 
Batch  1  loss:  0.140476793050766
Batch  2  loss:  0.13427089154720306
Batch  3  loss:  0.13807475566864014
Batch  4  loss:  0.14252911508083344
Batch  5  loss:  0.139367938041687
Batch  6  loss:  0.13756445050239563
Batch  7  loss:  0.12608438730239868
Batch  8  loss:  0.1289466768503189
Batch  9  loss:  0.12435808032751083
Batch  10  loss:  0.12354058772325516
Batch  11  loss:  0.12512528896331787
Batch  12  loss:  0.1428811401128769
Batch  13  loss:  0.1302795708179474
Batch  14  loss:  0.12051144987344742
Batch  15  loss:  0.12469586730003357
Batch  16  loss:  0.1497652530670166
Batch  17  loss:  0.1598571389913559
Batch  18  loss:  0.11744674295186996
Batch  19  loss:  0.11411330848932266
Batch  20  loss:  0.12588629126548767
Batch  21  loss:  0.14257512986660004
Batch  22  loss:  0.12057237327098846
Batch  23  loss:  0.12117183208465576
Batch  24  loss:  0.12682433426380157
Batch  25  loss:  0.12274821847677231
Batch  26  loss:  0.1298014372587204
Batch  27  loss:  0.1424105018377304
Batch  28  loss:  0.13127905130386353
Batch  29  loss:  0.1402910351753235
Batch  30  loss:  0.12019048631191254
Batch  31  loss:  0.1207948848605156
Batch  32  loss:  0.1263219118118286
Batch  33  loss:  0.1330709457397461
Batch  34  loss:  0.13554704189300537
Batch  35  loss:  0.13738752901554108
Batch  36  loss:  0.13922128081321716
Batch  37  loss:  0.1319136619567871
Batch  38  loss:  0.11394283920526505
Batch  39  loss:  0.10973206162452698
Batch  40  loss:  0.11681530624628067
Batch  41  loss:  0.12406118214130402
Batch  42  loss:  0.10957738757133484
Batch  43  loss:  0.1325862556695938
Batch  44  loss:  0.1302052140235901
Batch  45  loss:  0.12400705367326736
Batch  46  loss:  0.13113054633140564
Batch  47  loss:  0.1309301108121872
Batch  48  loss:  0.10436557233333588
Batch  49  loss:  0.11282407492399216
Batch  50  loss:  0.12286008894443512
Batch  51  loss:  0.13508076965808868
Batch  52  loss:  0.13973671197891235
Batch  53  loss:  0.12986236810684204
Batch  54  loss:  0.1410038024187088
Batch  55  loss:  0.12745989859104156
Batch  56  loss:  0.14744211733341217
Batch  57  loss:  0.1470489203929901
Batch  58  loss:  0.15160633623600006
Batch  59  loss:  0.115696482360363
Batch  60  loss:  0.13908323645591736
Batch  61  loss:  0.1447433978319168
Batch  62  loss:  0.1003909632563591
Batch  63  loss:  0.12148170173168182
Batch  64  loss:  0.11252149194478989
Batch  65  loss:  0.13000395894050598
Batch  66  loss:  0.11767236143350601
Batch  67  loss:  0.12452329695224762
Batch  68  loss:  0.10039075464010239
Batch  69  loss:  0.12453500181436539
Batch  70  loss:  0.12129205465316772
Batch  71  loss:  0.142134889960289
Batch  72  loss:  0.1321023851633072
Batch  73  loss:  0.13644370436668396
Batch  74  loss:  0.12292402237653732
Batch  75  loss:  0.12340331822633743
Batch  76  loss:  0.13036572933197021
Batch  77  loss:  0.12271565198898315
Batch  78  loss:  0.12200377881526947
Batch  79  loss:  0.12000234425067902
Batch  80  loss:  0.1331615447998047
Batch  81  loss:  0.10634778439998627
Batch  82  loss:  0.12986178696155548
Batch  83  loss:  0.11685958504676819
Batch  84  loss:  0.12567685544490814
Batch  85  loss:  0.13000079989433289
Batch  86  loss:  0.11628169566392899
Batch  87  loss:  0.13051806390285492
Batch  88  loss:  0.1100180596113205
Batch  89  loss:  0.13776320219039917
Batch  90  loss:  0.1388411968946457
Batch  91  loss:  0.11597834527492523
Batch  92  loss:  0.1263163685798645
Batch  93  loss:  0.11392185091972351
Batch  94  loss:  0.11045228689908981
Batch  95  loss:  0.12412329763174057
Batch  96  loss:  0.1346924751996994
Batch  97  loss:  0.10768435895442963
Batch  98  loss:  0.1041167676448822
Batch  99  loss:  0.10068372637033463
Batch  100  loss:  0.11152532696723938
Validation: 
LOSS train 0.12679431907832622, val 0.17151786386966705
EPOCH : 8 TIME:  2022-04-19 03:24:20.457321
Training: 
Batch  1  loss:  0.11938430368900299
Batch  2  loss:  0.12676285207271576
Batch  3  loss:  0.1378965973854065
Batch  4  loss:  0.1161675825715065
Batch  5  loss:  0.11950644850730896
Batch  6  loss:  0.1303190141916275
Batch  7  loss:  0.1056249812245369
Batch  8  loss:  0.10062160342931747
Batch  9  loss:  0.13876305520534515
Batch  10  loss:  0.13723154366016388
Batch  11  loss:  0.10326230525970459
Batch  12  loss:  0.11370933055877686
Batch  13  loss:  0.12805786728858948
Batch  14  loss:  0.13297249376773834
Batch  15  loss:  0.09797094762325287
Batch  16  loss:  0.11506997048854828
Batch  17  loss:  0.13599541783332825
Batch  18  loss:  0.11727763712406158
Batch  19  loss:  0.12505817413330078
Batch  20  loss:  0.10236985236406326
Batch  21  loss:  0.1399657130241394
Batch  22  loss:  0.11371831595897675
Batch  23  loss:  0.13485950231552124
Batch  24  loss:  0.11342744529247284
Batch  25  loss:  0.13680487871170044
Batch  26  loss:  0.125298410654068
Batch  27  loss:  0.14128826558589935
Batch  28  loss:  0.12683524191379547
Batch  29  loss:  0.1291332244873047
Batch  30  loss:  0.11973905563354492
Batch  31  loss:  0.12212162464857101
Batch  32  loss:  0.12263204157352448
Batch  33  loss:  0.11717681586742401
Batch  34  loss:  0.13068048655986786
Batch  35  loss:  0.12394417822360992
Batch  36  loss:  0.11046019941568375
Batch  37  loss:  0.11374368518590927
Batch  38  loss:  0.12677836418151855
Batch  39  loss:  0.11450894176959991
Batch  40  loss:  0.09920907765626907
Batch  41  loss:  0.11404529213905334
Batch  42  loss:  0.10368148237466812
Batch  43  loss:  0.10732900351285934
Batch  44  loss:  0.14215292036533356
Batch  45  loss:  0.11305353790521622
Batch  46  loss:  0.10437826067209244
Batch  47  loss:  0.10938528925180435
Batch  48  loss:  0.11379680037498474
Batch  49  loss:  0.11655055731534958
Batch  50  loss:  0.09994290769100189
Batch  51  loss:  0.12206289172172546
Batch  52  loss:  0.12180774658918381
Batch  53  loss:  0.11162175238132477
Batch  54  loss:  0.1255343109369278
Batch  55  loss:  0.11467402428388596
Batch  56  loss:  0.10237029939889908
Batch  57  loss:  0.12189998477697372
Batch  58  loss:  0.11616640537977219
Batch  59  loss:  0.10780958086252213
Batch  60  loss:  0.14433173835277557
Batch  61  loss:  0.1165214329957962
Batch  62  loss:  0.10702557116746902
Batch  63  loss:  0.09520743787288666
Batch  64  loss:  0.10497491806745529
Batch  65  loss:  0.13597439229488373
Batch  66  loss:  0.12082774937152863
Batch  67  loss:  0.09917747229337692
Batch  68  loss:  0.13460591435432434
Batch  69  loss:  0.09522779285907745
Batch  70  loss:  0.12140469998121262
Batch  71  loss:  0.1046667993068695
Batch  72  loss:  0.11422275006771088
Batch  73  loss:  0.09921140223741531
Batch  74  loss:  0.10300841927528381
Batch  75  loss:  0.10977809876203537
Batch  76  loss:  0.10368531942367554
Batch  77  loss:  0.09872659295797348
Batch  78  loss:  0.12816891074180603
Batch  79  loss:  0.136479452252388
Batch  80  loss:  0.12944179773330688
Batch  81  loss:  0.11354073137044907
Batch  82  loss:  0.11763408035039902
Batch  83  loss:  0.11408322304487228
Batch  84  loss:  0.12193558365106583
Batch  85  loss:  0.1340380609035492
Batch  86  loss:  0.11124828457832336
Batch  87  loss:  0.11100181192159653
Batch  88  loss:  0.12522941827774048
Batch  89  loss:  0.10585241764783859
Batch  90  loss:  0.10263127833604813
Batch  91  loss:  0.12189514935016632
Batch  92  loss:  0.1145099401473999
Batch  93  loss:  0.0907011553645134
Batch  94  loss:  0.10034547746181488
Batch  95  loss:  0.1177973523736
Batch  96  loss:  0.12452677637338638
Batch  97  loss:  0.11975694447755814
Batch  98  loss:  0.10084685683250427
Batch  99  loss:  0.11782718449831009
Batch  100  loss:  0.09795575588941574
Validation: 
LOSS train 0.11700636640191078, val 0.15486103296279907
EPOCH : 9 TIME:  2022-04-19 03:29:57.739727
Training: 
Batch  1  loss:  0.11764710396528244
Batch  2  loss:  0.1155417412519455
Batch  3  loss:  0.10076753050088882
Batch  4  loss:  0.11794984340667725
Batch  5  loss:  0.100140281021595
Batch  6  loss:  0.11689452826976776
Batch  7  loss:  0.11903849244117737
Batch  8  loss:  0.13312353193759918
Batch  9  loss:  0.12145248055458069
Batch  10  loss:  0.1115349754691124
Batch  11  loss:  0.13347581028938293
Batch  12  loss:  0.11158335208892822
Batch  13  loss:  0.11482785642147064
Batch  14  loss:  0.11751192063093185
Batch  15  loss:  0.1031765341758728
Batch  16  loss:  0.09738138318061829
Batch  17  loss:  0.08014213293790817
Batch  18  loss:  0.13041645288467407
Batch  19  loss:  0.11323600262403488
Batch  20  loss:  0.11159811913967133
Batch  21  loss:  0.11549703031778336
Batch  22  loss:  0.12870174646377563
Batch  23  loss:  0.08323224633932114
Batch  24  loss:  0.12442981451749802
Batch  25  loss:  0.10733577609062195
Batch  26  loss:  0.09588614106178284
Batch  27  loss:  0.1187833696603775
Batch  28  loss:  0.0999017134308815
Batch  29  loss:  0.10290855169296265
Batch  30  loss:  0.09267331659793854
Batch  31  loss:  0.11525344848632812
Batch  32  loss:  0.09159599989652634
Batch  33  loss:  0.12023826688528061
Batch  34  loss:  0.10267719626426697
Batch  35  loss:  0.12044833600521088
Batch  36  loss:  0.10441190749406815
Batch  37  loss:  0.12263721227645874
Batch  38  loss:  0.10089853405952454
Batch  39  loss:  0.12914244830608368
Batch  40  loss:  0.09930697083473206
Batch  41  loss:  0.09828098863363266
Batch  42  loss:  0.10428129136562347
Batch  43  loss:  0.11649356782436371
Batch  44  loss:  0.11495687067508698
Batch  45  loss:  0.09679511934518814
Batch  46  loss:  0.11025580763816833
Batch  47  loss:  0.12127910554409027
Batch  48  loss:  0.10408569127321243
Batch  49  loss:  0.10076778382062912
Batch  50  loss:  0.10526446998119354
Batch  51  loss:  0.11258403211832047
Batch  52  loss:  0.12163674831390381
Batch  53  loss:  0.12127311527729034
Batch  54  loss:  0.13107489049434662
Batch  55  loss:  0.11796694248914719
Batch  56  loss:  0.11247776448726654
Batch  57  loss:  0.10589536279439926
Batch  58  loss:  0.08675192296504974
Batch  59  loss:  0.12459474056959152
Batch  60  loss:  0.11016158759593964
Batch  61  loss:  0.10799594968557358
Batch  62  loss:  0.13222263753414154
Batch  63  loss:  0.1136912927031517
Batch  64  loss:  0.09853502362966537
Batch  65  loss:  0.1009724959731102
Batch  66  loss:  0.11428187787532806
Batch  67  loss:  0.11713697016239166
Batch  68  loss:  0.10710305720567703
Batch  69  loss:  0.1221657544374466
Batch  70  loss:  0.10667246580123901
Batch  71  loss:  0.12052533775568008
Batch  72  loss:  0.1187639981508255
Batch  73  loss:  0.10398664325475693
Batch  74  loss:  0.0930667296051979
Batch  75  loss:  0.11263429373502731
Batch  76  loss:  0.11128293722867966
Batch  77  loss:  0.1429516226053238
Batch  78  loss:  0.1177663803100586
Batch  79  loss:  0.11888551712036133
Batch  80  loss:  0.09406398236751556
Batch  81  loss:  0.12603066861629486
Batch  82  loss:  0.14140470325946808
Batch  83  loss:  0.11182737350463867
Batch  84  loss:  0.12489400804042816
Batch  85  loss:  0.11021770536899567
Batch  86  loss:  0.1233801618218422
Batch  87  loss:  0.11390985548496246
Batch  88  loss:  0.09888719022274017
Batch  89  loss:  0.10718146711587906
Batch  90  loss:  0.1234319657087326
Batch  91  loss:  0.11079388111829758
Batch  92  loss:  0.10827680677175522
Batch  93  loss:  0.12547831237316132
Batch  94  loss:  0.12399670481681824
Batch  95  loss:  0.1166534274816513
Batch  96  loss:  0.1142265647649765
Batch  97  loss:  0.12500227987766266
Batch  98  loss:  0.10759082436561584
Batch  99  loss:  0.11308548599481583
Batch  100  loss:  0.09784457832574844
Validation: 
LOSS train 0.11239096835255623, val 0.11016090214252472
EPOCH : 10 TIME:  2022-04-19 03:35:34.291122
Training: 
Batch  1  loss:  0.1108383759856224
Batch  2  loss:  0.11178641021251678
Batch  3  loss:  0.12692323327064514
Batch  4  loss:  0.1105160340666771
Batch  5  loss:  0.10551885515451431
Batch  6  loss:  0.13902044296264648
Batch  7  loss:  0.11263231188058853
Batch  8  loss:  0.10257165879011154
Batch  9  loss:  0.10612346231937408
Batch  10  loss:  0.11862631887197495
Batch  11  loss:  0.10069690644741058
Batch  12  loss:  0.1067858561873436
Batch  13  loss:  0.12300866097211838
Batch  14  loss:  0.09914321452379227
Batch  15  loss:  0.1158510074019432
Batch  16  loss:  0.09968391060829163
Batch  17  loss:  0.10751727968454361
Batch  18  loss:  0.09934590011835098
Batch  19  loss:  0.10721009224653244
Batch  20  loss:  0.11764243990182877
Batch  21  loss:  0.10812045633792877
Batch  22  loss:  0.0980018600821495
Batch  23  loss:  0.09656575322151184
Batch  24  loss:  0.1225239634513855
Batch  25  loss:  0.1007598489522934
Batch  26  loss:  0.12938423454761505
Batch  27  loss:  0.09876666963100433
Batch  28  loss:  0.1196846291422844
Batch  29  loss:  0.1084103211760521
Batch  30  loss:  0.095934197306633
Batch  31  loss:  0.11677005141973495
Batch  32  loss:  0.103613942861557
Batch  33  loss:  0.13595446944236755
Batch  34  loss:  0.11137929558753967
Batch  35  loss:  0.11562078446149826
Batch  36  loss:  0.10751667618751526
Batch  37  loss:  0.09394200891256332
Batch  38  loss:  0.09418167173862457
Batch  39  loss:  0.09738189727067947
Batch  40  loss:  0.11460159718990326
Batch  41  loss:  0.10518644750118256
Batch  42  loss:  0.119178406894207
Batch  43  loss:  0.09802842885255814
Batch  44  loss:  0.11407514661550522
Batch  45  loss:  0.09175445139408112
Batch  46  loss:  0.12091413140296936
Batch  47  loss:  0.11232858151197433
Batch  48  loss:  0.1408044546842575
Batch  49  loss:  0.12768715620040894
Batch  50  loss:  0.1047104224562645
Batch  51  loss:  0.14464615285396576
Batch  52  loss:  0.1301705539226532
Batch  53  loss:  0.09173741191625595
Batch  54  loss:  0.12514230608940125
Batch  55  loss:  0.1172284185886383
Batch  56  loss:  0.09900137782096863
Batch  57  loss:  0.10576637834310532
Batch  58  loss:  0.09991203993558884
Batch  59  loss:  0.1196705773472786
Batch  60  loss:  0.12460116297006607
Batch  61  loss:  0.12875719368457794
Batch  62  loss:  0.12486207485198975
Batch  63  loss:  0.10241691023111343
Batch  64  loss:  0.12228129804134369
Batch  65  loss:  0.11732568591833115
Batch  66  loss:  0.13891498744487762
Batch  67  loss:  0.11346998065710068
Batch  68  loss:  0.12180566787719727
Batch  69  loss:  0.13640350103378296
Batch  70  loss:  0.1103062704205513
Batch  71  loss:  0.1081940233707428
Batch  72  loss:  0.10363103449344635
Batch  73  loss:  0.1282769739627838
Batch  74  loss:  0.14293242990970612
Batch  75  loss:  0.09714947640895844
Batch  76  loss:  0.10548583418130875
Batch  77  loss:  0.09968990087509155
Batch  78  loss:  0.11499350517988205
Batch  79  loss:  0.12259324640035629
Batch  80  loss:  0.11052975058555603
Batch  81  loss:  0.12682385742664337
Batch  82  loss:  0.1008666604757309
Batch  83  loss:  0.103133924305439
Batch  84  loss:  0.11995675414800644
Batch  85  loss:  0.10789307951927185
Batch  86  loss:  0.11114005744457245
Batch  87  loss:  0.10702863335609436
Batch  88  loss:  0.09889377653598785
Batch  89  loss:  0.10287901759147644
Batch  90  loss:  0.10943349450826645
Batch  91  loss:  0.1156708225607872
Batch  92  loss:  0.11191005259752274
Batch  93  loss:  0.1218346357345581
Batch  94  loss:  0.1014409065246582
Batch  95  loss:  0.11409308761358261
Batch  96  loss:  0.08609145879745483
Batch  97  loss:  0.09740819036960602
Batch  98  loss:  0.11131192743778229
Batch  99  loss:  0.09750258922576904
Batch  100  loss:  0.09750917553901672
Validation: 
LOSS train 0.11175942555069923, val 0.10674058645963669
EPOCH : 11 TIME:  2022-04-19 03:41:09.122970
Training: 
Batch  1  loss:  0.09280675649642944
Batch  2  loss:  0.10031557828187943
Batch  3  loss:  0.11564609408378601
Batch  4  loss:  0.09669842571020126
Batch  5  loss:  0.08989951014518738
Batch  6  loss:  0.11229208111763
Batch  7  loss:  0.0911596491932869
Batch  8  loss:  0.08907408267259598
Batch  9  loss:  0.1048252061009407
Batch  10  loss:  0.1084095761179924
Batch  11  loss:  0.09501966089010239
Batch  12  loss:  0.08342189341783524
Batch  13  loss:  0.11064361035823822
Batch  14  loss:  0.1259084790945053
Batch  15  loss:  0.1261705905199051
Batch  16  loss:  0.1188746839761734
Batch  17  loss:  0.11632943898439407
Batch  18  loss:  0.1115049347281456
Batch  19  loss:  0.10089118778705597
Batch  20  loss:  0.1057119369506836
Batch  21  loss:  0.0932123139500618
Batch  22  loss:  0.10800584405660629
Batch  23  loss:  0.09997814148664474
Batch  24  loss:  0.1140274778008461
Batch  25  loss:  0.10668853670358658
Batch  26  loss:  0.09176352620124817
Batch  27  loss:  0.10192147642374039
Batch  28  loss:  0.12765349447727203
Batch  29  loss:  0.10681463032960892
Batch  30  loss:  0.10694299638271332
Batch  31  loss:  0.10838976502418518
Batch  32  loss:  0.11377926915884018
Batch  33  loss:  0.10076514631509781
Batch  34  loss:  0.12339683622121811
Batch  35  loss:  0.12488475441932678
Batch  36  loss:  0.09531137347221375
Batch  37  loss:  0.11038433760404587
Batch  38  loss:  0.08889778703451157
Batch  39  loss:  0.128668874502182
Batch  40  loss:  0.09707789123058319
Batch  41  loss:  0.10497333109378815
Batch  42  loss:  0.10368475317955017
Batch  43  loss:  0.09092708677053452
Batch  44  loss:  0.11454030126333237
Batch  45  loss:  0.10012955218553543
Batch  46  loss:  0.08561024814844131
Batch  47  loss:  0.09394405782222748
Batch  48  loss:  0.0950932651758194
Batch  49  loss:  0.11454910784959793
Batch  50  loss:  0.11076556891202927
Batch  51  loss:  0.08991868793964386
Batch  52  loss:  0.11860883235931396
Batch  53  loss:  0.11967615783214569
Batch  54  loss:  0.09760219603776932
Batch  55  loss:  0.0958276316523552
Batch  56  loss:  0.11169492453336716
Batch  57  loss:  0.12654000520706177
Batch  58  loss:  0.0988064557313919
Batch  59  loss:  0.11772732436656952
Batch  60  loss:  0.11389848589897156
Batch  61  loss:  0.09136451780796051
Batch  62  loss:  0.11225921660661697
Batch  63  loss:  0.09067338705062866
Batch  64  loss:  0.13074827194213867
Batch  65  loss:  0.10648315399885178
Batch  66  loss:  0.1055348590016365
Batch  67  loss:  0.10242610424757004
Batch  68  loss:  0.10851244628429413
Batch  69  loss:  0.137522891163826
Batch  70  loss:  0.106760174036026
Batch  71  loss:  0.10363263636827469
Batch  72  loss:  0.0934414267539978
Batch  73  loss:  0.09944478422403336
Batch  74  loss:  0.09284932911396027
Batch  75  loss:  0.10576347261667252
Batch  76  loss:  0.08867321163415909
Batch  77  loss:  0.10970707982778549
Batch  78  loss:  0.11756753921508789
Batch  79  loss:  0.09842188656330109
Batch  80  loss:  0.08628035336732864
Batch  81  loss:  0.11420376598834991
Batch  82  loss:  0.11861635744571686
Batch  83  loss:  0.0988171398639679
Batch  84  loss:  0.12580756843090057
Batch  85  loss:  0.09751956909894943
Batch  86  loss:  0.10896655917167664
Batch  87  loss:  0.1189500093460083
Batch  88  loss:  0.08805687725543976
Batch  89  loss:  0.07803146541118622
Batch  90  loss:  0.09649818390607834
Batch  91  loss:  0.09599101543426514
Batch  92  loss:  0.101847343146801
Batch  93  loss:  0.09162957966327667
Batch  94  loss:  0.10946670174598694
Batch  95  loss:  0.13352283835411072
Batch  96  loss:  0.12146159261465073
Batch  97  loss:  0.08719448000192642
Batch  98  loss:  0.09167209267616272
Batch  99  loss:  0.10828959941864014
Batch  100  loss:  0.12339399009943008
Validation: 
LOSS train 0.10546689294278622, val 0.10267098248004913
EPOCH : 12 TIME:  2022-04-19 03:46:44.818080
Training: 
Batch  1  loss:  0.10117223858833313
Batch  2  loss:  0.1040930449962616
Batch  3  loss:  0.0984933078289032
Batch  4  loss:  0.0995553582906723
Batch  5  loss:  0.10587753355503082
Batch  6  loss:  0.09560643881559372
Batch  7  loss:  0.11613770574331284
Batch  8  loss:  0.11926738172769547
Batch  9  loss:  0.10041987150907516
Batch  10  loss:  0.10524957627058029
Batch  11  loss:  0.0814557820558548
Batch  12  loss:  0.10905216634273529
Batch  13  loss:  0.09729032218456268
Batch  14  loss:  0.10509824752807617
Batch  15  loss:  0.11370397359132767
Batch  16  loss:  0.1247587651014328
Batch  17  loss:  0.12170212715864182
Batch  18  loss:  0.10296891629695892
Batch  19  loss:  0.09995092451572418
Batch  20  loss:  0.09352708607912064
Batch  21  loss:  0.10249103605747223
Batch  22  loss:  0.1125626340508461
Batch  23  loss:  0.13184860348701477
Batch  24  loss:  0.09717349708080292
Batch  25  loss:  0.09773324429988861
Batch  26  loss:  0.08318231254816055
Batch  27  loss:  0.10226041823625565
Batch  28  loss:  0.11458564549684525
Batch  29  loss:  0.09578517824411392
Batch  30  loss:  0.12439613044261932
Batch  31  loss:  0.09347999095916748
Batch  32  loss:  0.0868925079703331
Batch  33  loss:  0.08642680943012238
Batch  34  loss:  0.08439258486032486
Batch  35  loss:  0.13363753259181976
Batch  36  loss:  0.09108342975378036
Batch  37  loss:  0.10007491707801819
Batch  38  loss:  0.11518837511539459
Batch  39  loss:  0.10958191007375717
Batch  40  loss:  0.10941869765520096
Batch  41  loss:  0.07703147083520889
Batch  42  loss:  0.11312805861234665
Batch  43  loss:  0.09646619856357574
Batch  44  loss:  0.0985642671585083
Batch  45  loss:  0.09216134995222092
Batch  46  loss:  0.13168364763259888
Batch  47  loss:  0.11048494279384613
Batch  48  loss:  0.1119375005364418
Batch  49  loss:  0.084830641746521
Batch  50  loss:  0.10577117651700974
Batch  51  loss:  0.09151319414377213
Batch  52  loss:  0.09200471639633179
Batch  53  loss:  0.10220751166343689
Batch  54  loss:  0.10270334035158157
Batch  55  loss:  0.1181529238820076
Batch  56  loss:  0.11086513102054596
Batch  57  loss:  0.09184396266937256
Batch  58  loss:  0.0998273566365242
Batch  59  loss:  0.117782361805439
Batch  60  loss:  0.10201042145490646
Batch  61  loss:  0.10734597593545914
Batch  62  loss:  0.1362442523241043
Batch  63  loss:  0.12147471308708191
Batch  64  loss:  0.12963663041591644
Batch  65  loss:  0.08887162804603577
Batch  66  loss:  0.0892835408449173
Batch  67  loss:  0.11371976882219315
Batch  68  loss:  0.09273898601531982
Batch  69  loss:  0.1346752941608429
Batch  70  loss:  0.1196095272898674
Batch  71  loss:  0.10780569165945053
Batch  72  loss:  0.11181492358446121
Batch  73  loss:  0.10528893023729324
Batch  74  loss:  0.12227437645196915
Batch  75  loss:  0.12265903502702713
Batch  76  loss:  0.13554322719573975
Batch  77  loss:  0.10088605433702469
Batch  78  loss:  0.14013130962848663
Batch  79  loss:  0.12238139659166336
Batch  80  loss:  0.10714838653802872
Batch  81  loss:  0.10019031167030334
Batch  82  loss:  0.08735181391239166
Batch  83  loss:  0.11528243869543076
Batch  84  loss:  0.10974880307912827
Batch  85  loss:  0.1227906197309494
Batch  86  loss:  0.11887340247631073
Batch  87  loss:  0.10540826618671417
Batch  88  loss:  0.11216318607330322
Batch  89  loss:  0.1158609539270401
Batch  90  loss:  0.09956275671720505
Batch  91  loss:  0.10518281906843185
Batch  92  loss:  0.09476352483034134
Batch  93  loss:  0.11399761587381363
Batch  94  loss:  0.11031202971935272
Batch  95  loss:  0.09906738996505737
Batch  96  loss:  0.11791785806417465
Batch  97  loss:  0.12290972471237183
Batch  98  loss:  0.12684392929077148
Batch  99  loss:  0.10556980222463608
Batch  100  loss:  0.11411425471305847
Validation: 
LOSS train 0.10725991547107697, val 0.10318950563669205
EPOCH : 13 TIME:  2022-04-19 03:52:17.845598
Training: 
Batch  1  loss:  0.11132308840751648
Batch  2  loss:  0.10255498439073563
Batch  3  loss:  0.1006375253200531
Batch  4  loss:  0.09156600385904312
Batch  5  loss:  0.09881050139665604
Batch  6  loss:  0.10232444852590561
Batch  7  loss:  0.10839448869228363
Batch  8  loss:  0.09911035001277924
Batch  9  loss:  0.10155582427978516
Batch  10  loss:  0.11309026181697845
Batch  11  loss:  0.11621097475290298
Batch  12  loss:  0.08845652639865875
Batch  13  loss:  0.10827615857124329
Batch  14  loss:  0.11361794173717499
Batch  15  loss:  0.09349954128265381
Batch  16  loss:  0.0849524661898613
Batch  17  loss:  0.1034717857837677
Batch  18  loss:  0.11615042388439178
Batch  19  loss:  0.10554986447095871
Batch  20  loss:  0.10850164294242859
Batch  21  loss:  0.11121036112308502
Batch  22  loss:  0.13489733636379242
Batch  23  loss:  0.09267222881317139
Batch  24  loss:  0.09765376895666122
Batch  25  loss:  0.08272489160299301
Batch  26  loss:  0.09949136525392532
Batch  27  loss:  0.10831083357334137
Batch  28  loss:  0.09159429371356964
Batch  29  loss:  0.09367639571428299
Batch  30  loss:  0.1284325271844864
Batch  31  loss:  0.10544031858444214
Batch  32  loss:  0.09511951357126236
Batch  33  loss:  0.10769263654947281
Batch  34  loss:  0.11114976555109024
Batch  35  loss:  0.11454492062330246
Batch  36  loss:  0.10791811347007751
Batch  37  loss:  0.09590987861156464
Batch  38  loss:  0.10844415426254272
Batch  39  loss:  0.10408631712198257
Batch  40  loss:  0.10908716171979904
Batch  41  loss:  0.08715896308422089
Batch  42  loss:  0.08438174426555634
Batch  43  loss:  0.08325664699077606
Batch  44  loss:  0.0922883003950119
Batch  45  loss:  0.09494005888700485
Batch  46  loss:  0.08590167760848999
Batch  47  loss:  0.09737091511487961
Batch  48  loss:  0.09667723625898361
Batch  49  loss:  0.10223061591386795
Batch  50  loss:  0.10987987369298935
Batch  51  loss:  0.08997386693954468
Batch  52  loss:  0.1049104630947113
Batch  53  loss:  0.10073438286781311
Batch  54  loss:  0.09197037667036057
Batch  55  loss:  0.118290014564991
Batch  56  loss:  0.10945937782526016
Batch  57  loss:  0.10474303364753723
Batch  58  loss:  0.11712967604398727
Batch  59  loss:  0.09692951291799545
Batch  60  loss:  0.09027563035488129
Batch  61  loss:  0.09260304272174835
Batch  62  loss:  0.0957271158695221
Batch  63  loss:  0.10341985523700714
Batch  64  loss:  0.10214747488498688
Batch  65  loss:  0.09190473705530167
Batch  66  loss:  0.11336440593004227
Batch  67  loss:  0.10300824791193008
Batch  68  loss:  0.10687440633773804
Batch  69  loss:  0.08425770699977875
Batch  70  loss:  0.11444484442472458
Batch  71  loss:  0.09912963956594467
Batch  72  loss:  0.10389348864555359
Batch  73  loss:  0.09867130219936371
Batch  74  loss:  0.08519050478935242
Batch  75  loss:  0.125648632645607
Batch  76  loss:  0.11180970072746277
Batch  77  loss:  0.11230827122926712
Batch  78  loss:  0.12218128889799118
Batch  79  loss:  0.11362943053245544
Batch  80  loss:  0.11692769825458527
Batch  81  loss:  0.10387144982814789
Batch  82  loss:  0.11563775688409805
Batch  83  loss:  0.10079095512628555
Batch  84  loss:  0.10628625750541687
Batch  85  loss:  0.11139485985040665
Batch  86  loss:  0.09810613095760345
Batch  87  loss:  0.10418263077735901
Batch  88  loss:  0.1141887754201889
Batch  89  loss:  0.11119458079338074
Batch  90  loss:  0.10742620378732681
Batch  91  loss:  0.09675589948892593
Batch  92  loss:  0.08524410426616669
Batch  93  loss:  0.11197233200073242
Batch  94  loss:  0.11011097580194473
Batch  95  loss:  0.09252750128507614
Batch  96  loss:  0.08662041276693344
Batch  97  loss:  0.08919048309326172
Batch  98  loss:  0.09720220416784286
Batch  99  loss:  0.10168296843767166
Batch  100  loss:  0.11509464681148529
Validation: 
LOSS train 0.10283236876130104, val 0.14523376524448395
EPOCH : 14 TIME:  2022-04-19 03:57:55.200561
Training: 
Batch  1  loss:  0.10400933772325516
Batch  2  loss:  0.0842064842581749
Batch  3  loss:  0.09786363691091537
Batch  4  loss:  0.09682688862085342
Batch  5  loss:  0.11120668798685074
Batch  6  loss:  0.09754161536693573
Batch  7  loss:  0.09178639948368073
Batch  8  loss:  0.08710336685180664
Batch  9  loss:  0.10351177304983139
Batch  10  loss:  0.11358513683080673
Batch  11  loss:  0.13591013848781586
Batch  12  loss:  0.08787993341684341
Batch  13  loss:  0.09988737106323242
Batch  14  loss:  0.102755106985569
Batch  15  loss:  0.10465680062770844
Batch  16  loss:  0.10215930640697479
Batch  17  loss:  0.11070580035448074
Batch  18  loss:  0.12713541090488434
Batch  19  loss:  0.11037952452898026
Batch  20  loss:  0.11497233808040619
Batch  21  loss:  0.11460103839635849
Batch  22  loss:  0.10638318955898285
Batch  23  loss:  0.10886187106370926
Batch  24  loss:  0.0892045870423317
Batch  25  loss:  0.0981210395693779
Batch  26  loss:  0.11647868901491165
Batch  27  loss:  0.10285048931837082
Batch  28  loss:  0.10292436182498932
Batch  29  loss:  0.11703866720199585
Batch  30  loss:  0.10986334830522537
Batch  31  loss:  0.11010522395372391
Batch  32  loss:  0.11046254634857178
Batch  33  loss:  0.09380601346492767
Batch  34  loss:  0.09784238785505295
Batch  35  loss:  0.10028059035539627
Batch  36  loss:  0.11164016276597977
Batch  37  loss:  0.08677642792463303
Batch  38  loss:  0.11538168787956238
Batch  39  loss:  0.0925937071442604
Batch  40  loss:  0.08570079505443573
Batch  41  loss:  0.12832953035831451
Batch  42  loss:  0.10288606584072113
Batch  43  loss:  0.10798690468072891
Batch  44  loss:  0.09905271977186203
Batch  45  loss:  0.1053089126944542
Batch  46  loss:  0.10620012134313583
Batch  47  loss:  0.08673039078712463
Batch  48  loss:  0.10003186762332916
Batch  49  loss:  0.12297307699918747
Batch  50  loss:  0.12961341440677643
Batch  51  loss:  0.12476342916488647
Batch  52  loss:  0.13028858602046967
Batch  53  loss:  0.12667076289653778
Batch  54  loss:  0.11588962376117706
Batch  55  loss:  0.10566850751638412
Batch  56  loss:  0.09716957807540894
Batch  57  loss:  0.09666524827480316
Batch  58  loss:  0.11885112524032593
Batch  59  loss:  0.12211965024471283
Batch  60  loss:  0.0914367288351059
Batch  61  loss:  0.09801950305700302
Batch  62  loss:  0.0964866429567337
Batch  63  loss:  0.10541864484548569
Batch  64  loss:  0.09112363308668137
Batch  65  loss:  0.09507813304662704
Batch  66  loss:  0.10622413456439972
Batch  67  loss:  0.08059249818325043
Batch  68  loss:  0.11257676780223846
Batch  69  loss:  0.11032912880182266
Batch  70  loss:  0.07815388590097427
Batch  71  loss:  0.12840230762958527
Batch  72  loss:  0.1066548153758049
Batch  73  loss:  0.11271529644727707
Batch  74  loss:  0.12993258237838745
Batch  75  loss:  0.09412231296300888
Batch  76  loss:  0.10215461254119873
Batch  77  loss:  0.09443366527557373
Batch  78  loss:  0.10513138771057129
Batch  79  loss:  0.09796889871358871
Batch  80  loss:  0.11476428806781769
Batch  81  loss:  0.08343927562236786
Batch  82  loss:  0.09001108258962631
Batch  83  loss:  0.10408250242471695
Batch  84  loss:  0.1015322282910347
Batch  85  loss:  0.09891783446073532
Batch  86  loss:  0.10703595727682114
Batch  87  loss:  0.10821524262428284
Batch  88  loss:  0.10232020914554596
Batch  89  loss:  0.09454359114170074
Batch  90  loss:  0.10834182798862457
Batch  91  loss:  0.08790330588817596
Batch  92  loss:  0.08813834190368652
Batch  93  loss:  0.0951063334941864
Batch  94  loss:  0.10268938541412354
Batch  95  loss:  0.10884462296962738
Batch  96  loss:  0.08853549510240555
Batch  97  loss:  0.08959346264600754
Batch  98  loss:  0.10427895933389664
Batch  99  loss:  0.09143487364053726
Batch  100  loss:  0.09601611644029617
Validation: 
LOSS train 0.1038489991426468, val 0.19490034878253937
EPOCH : 15 TIME:  2022-04-19 04:03:30.114825
Training: 
Batch  1  loss:  0.10712634772062302
Batch  2  loss:  0.10265886038541794
Batch  3  loss:  0.08270853012800217
Batch  4  loss:  0.09186654537916183
Batch  5  loss:  0.13568736612796783
Batch  6  loss:  0.09609837085008621
Batch  7  loss:  0.1053462028503418
Batch  8  loss:  0.09476493299007416
Batch  9  loss:  0.10118251293897629
Batch  10  loss:  0.12236373871564865
Batch  11  loss:  0.10642468929290771
Batch  12  loss:  0.10704242438077927
Batch  13  loss:  0.1278664469718933
Batch  14  loss:  0.12433724850416183
Batch  15  loss:  0.12239454686641693
Batch  16  loss:  0.12103267759084702
Batch  17  loss:  0.09717229008674622
Batch  18  loss:  0.10082156956195831
Batch  19  loss:  0.10050210356712341
Batch  20  loss:  0.09697475284337997
Batch  21  loss:  0.0775965079665184
Batch  22  loss:  0.08327674865722656
Batch  23  loss:  0.11108338832855225
Batch  24  loss:  0.09960965812206268
Batch  25  loss:  0.11166056245565414
Batch  26  loss:  0.08702843636274338
Batch  27  loss:  0.09143128246068954
Batch  28  loss:  0.10889308899641037
Batch  29  loss:  0.11928027868270874
Batch  30  loss:  0.10359267145395279
Batch  31  loss:  0.1257612556219101
Batch  32  loss:  0.09528633207082748
Batch  33  loss:  0.10195218026638031
Batch  34  loss:  0.10776158422231674
Batch  35  loss:  0.07293214648962021
Batch  36  loss:  0.10094519704580307
Batch  37  loss:  0.10285135358572006
Batch  38  loss:  0.11100669950246811
Batch  39  loss:  0.10974302142858505
Batch  40  loss:  0.09775839745998383
Batch  41  loss:  0.0990498885512352
Batch  42  loss:  0.09477546811103821
Batch  43  loss:  0.12001634389162064
Batch  44  loss:  0.09643054753541946
Batch  45  loss:  0.10447967052459717
Batch  46  loss:  0.10303612798452377
Batch  47  loss:  0.09103404730558395
Batch  48  loss:  0.10392579436302185
Batch  49  loss:  0.09830787777900696
Batch  50  loss:  0.10665587335824966
Batch  51  loss:  0.08442957699298859
Batch  52  loss:  0.09788797795772552
Batch  53  loss:  0.11381693929433823
Batch  54  loss:  0.07192674279212952
Batch  55  loss:  0.10531428456306458
Batch  56  loss:  0.12172175198793411
Batch  57  loss:  0.1036057099699974
Batch  58  loss:  0.11349645256996155
Batch  59  loss:  0.10063377022743225
Batch  60  loss:  0.10278484225273132
Batch  61  loss:  0.11527951806783676
Batch  62  loss:  0.08713556081056595
Batch  63  loss:  0.10484971851110458
Batch  64  loss:  0.10550875216722488
Batch  65  loss:  0.0960688367486
Batch  66  loss:  0.09724090993404388
Batch  67  loss:  0.08817391842603683
Batch  68  loss:  0.07443571090698242
Batch  69  loss:  0.08366061002016068
Batch  70  loss:  0.0976160317659378
Batch  71  loss:  0.09667526185512543
Batch  72  loss:  0.09860853850841522
Batch  73  loss:  0.12184172123670578
Batch  74  loss:  0.09958138316869736
Batch  75  loss:  0.08279485255479813
Batch  76  loss:  0.07607023417949677
Batch  77  loss:  0.06980154663324356
Batch  78  loss:  0.11973085254430771
Batch  79  loss:  0.10786508023738861
Batch  80  loss:  0.09722958505153656
Batch  81  loss:  0.10940317064523697
Batch  82  loss:  0.08479166775941849
Batch  83  loss:  0.07762686908245087
Batch  84  loss:  0.08879178762435913
Batch  85  loss:  0.08836841583251953
Batch  86  loss:  0.08371399343013763
Batch  87  loss:  0.08594188839197159
Batch  88  loss:  0.09279561787843704
Batch  89  loss:  0.09437249600887299
Batch  90  loss:  0.07739400863647461
Batch  91  loss:  0.09095104783773422
Batch  92  loss:  0.08822177350521088
Batch  93  loss:  0.10090822726488113
Batch  94  loss:  0.08132635802030563
Batch  95  loss:  0.09035737812519073
Batch  96  loss:  0.09945441782474518
Batch  97  loss:  0.10488356649875641
Batch  98  loss:  0.08829966187477112
Batch  99  loss:  0.10982130467891693
Batch  100  loss:  0.09620409458875656
Validation: 
LOSS train 0.09950949005782604, val 0.12232477962970734
EPOCH : 16 TIME:  2022-04-19 04:09:08.070272
Training: 
Batch  1  loss:  0.10434100776910782
Batch  2  loss:  0.10965751111507416
Batch  3  loss:  0.10648781806230545
Batch  4  loss:  0.10529091954231262
Batch  5  loss:  0.09171247482299805
Batch  6  loss:  0.0848843902349472
Batch  7  loss:  0.10616687685251236
Batch  8  loss:  0.13675537705421448
Batch  9  loss:  0.12970121204853058
Batch  10  loss:  0.07608869671821594
Batch  11  loss:  0.09639430791139603
Batch  12  loss:  0.08981332927942276
Batch  13  loss:  0.1030576080083847
Batch  14  loss:  0.09902560710906982
Batch  15  loss:  0.10742807388305664
Batch  16  loss:  0.10734464228153229
Batch  17  loss:  0.10655256360769272
Batch  18  loss:  0.09542769193649292
Batch  19  loss:  0.11311239004135132
Batch  20  loss:  0.09421457350254059
Batch  21  loss:  0.11229705810546875
Batch  22  loss:  0.11647454649209976
Batch  23  loss:  0.09235212951898575
Batch  24  loss:  0.07803699374198914
Batch  25  loss:  0.08903472870588303
Batch  26  loss:  0.09625531733036041
Batch  27  loss:  0.10173233598470688
Batch  28  loss:  0.09405435621738434
Batch  29  loss:  0.11086515337228775
Batch  30  loss:  0.08301230520009995
Batch  31  loss:  0.09116517752408981
Batch  32  loss:  0.08358810096979141
Batch  33  loss:  0.07416896522045135
Batch  34  loss:  0.08164283633232117
Batch  35  loss:  0.09284118562936783
Batch  36  loss:  0.11630385369062424
Batch  37  loss:  0.1101173385977745
Batch  38  loss:  0.07228808850049973
Batch  39  loss:  0.12400419265031815
Batch  40  loss:  0.10486126691102982
Batch  41  loss:  0.07522139698266983
Batch  42  loss:  0.08375291526317596
Batch  43  loss:  0.0977369174361229
Batch  44  loss:  0.09328588098287582
Batch  45  loss:  0.07453592866659164
Batch  46  loss:  0.08094702661037445
Batch  47  loss:  0.08301807940006256
Batch  48  loss:  0.0779995322227478
Batch  49  loss:  0.09516189992427826
Batch  50  loss:  0.11677892506122589
Batch  51  loss:  0.10718945413827896
Batch  52  loss:  0.10517095029354095
Batch  53  loss:  0.11209867149591446
Batch  54  loss:  0.08839228749275208
Batch  55  loss:  0.0986800268292427
Batch  56  loss:  0.10076921433210373
Batch  57  loss:  0.10806843638420105
Batch  58  loss:  0.08922390639781952
Batch  59  loss:  0.08778945356607437
Batch  60  loss:  0.10133986920118332
Batch  61  loss:  0.10088907182216644
Batch  62  loss:  0.11698702722787857
Batch  63  loss:  0.11313466727733612
Batch  64  loss:  0.1068630963563919
Batch  65  loss:  0.11283853650093079
Batch  66  loss:  0.08782140165567398
Batch  67  loss:  0.07500679045915604
Batch  68  loss:  0.10434538871049881
Batch  69  loss:  0.10966915637254715
Batch  70  loss:  0.07709264755249023
Batch  71  loss:  0.11300439387559891
Batch  72  loss:  0.09435918927192688
Batch  73  loss:  0.10572263598442078
Batch  74  loss:  0.09922730922698975
Batch  75  loss:  0.09768053889274597
Batch  76  loss:  0.08613307029008865
Batch  77  loss:  0.1086452305316925
Batch  78  loss:  0.09744895994663239
Batch  79  loss:  0.106142558157444
Batch  80  loss:  0.09985531866550446
Batch  81  loss:  0.08362609893083572
Batch  82  loss:  0.10313016921281815
Batch  83  loss:  0.07479358464479446
Batch  84  loss:  0.08761151880025864
Batch  85  loss:  0.11678752303123474
Batch  86  loss:  0.09772990643978119
Batch  87  loss:  0.09346939623355865
Batch  88  loss:  0.10434386879205704
Batch  89  loss:  0.09979619830846786
Batch  90  loss:  0.10765311121940613
Batch  91  loss:  0.09815745055675507
Batch  92  loss:  0.095591239631176
Batch  93  loss:  0.1109348013997078
Batch  94  loss:  0.087837815284729
Batch  95  loss:  0.0995519831776619
Batch  96  loss:  0.108332060277462
Batch  97  loss:  0.0857725515961647
Batch  98  loss:  0.09598501771688461
Batch  99  loss:  0.12138629704713821
Batch  100  loss:  0.08250554651021957
Validation: 
LOSS train 0.09833576902747154, val 0.10533319413661957
EPOCH : 17 TIME:  2022-04-19 04:14:45.424165
Training: 
Batch  1  loss:  0.11746377497911453
Batch  2  loss:  0.11050523817539215
Batch  3  loss:  0.08680745959281921
Batch  4  loss:  0.10385698080062866
Batch  5  loss:  0.09238883852958679
Batch  6  loss:  0.07598379999399185
Batch  7  loss:  0.09474758058786392
Batch  8  loss:  0.09647035598754883
Batch  9  loss:  0.10997079312801361
Batch  10  loss:  0.07356369495391846
Batch  11  loss:  0.10735692083835602
Batch  12  loss:  0.09453757107257843
Batch  13  loss:  0.12568442523479462
Batch  14  loss:  0.11481821537017822
Batch  15  loss:  0.0889933779835701
Batch  16  loss:  0.08665475994348526
Batch  17  loss:  0.1111983209848404
Batch  18  loss:  0.10101676732301712
Batch  19  loss:  0.10652744770050049
Batch  20  loss:  0.11017150431871414
Batch  21  loss:  0.08888889104127884
Batch  22  loss:  0.07256828993558884
Batch  23  loss:  0.09960591048002243
Batch  24  loss:  0.0836934745311737
Batch  25  loss:  0.10748086869716644
Batch  26  loss:  0.1025429293513298
Batch  27  loss:  0.09452691674232483
Batch  28  loss:  0.07562632113695145
Batch  29  loss:  0.10312972962856293
Batch  30  loss:  0.07755828648805618
Batch  31  loss:  0.09005922079086304
Batch  32  loss:  0.1248508021235466
Batch  33  loss:  0.1184549331665039
Batch  34  loss:  0.1261274367570877
Batch  35  loss:  0.0865170955657959
Batch  36  loss:  0.11558826267719269
Batch  37  loss:  0.09835898131132126
Batch  38  loss:  0.07738170772790909
Batch  39  loss:  0.09635578095912933
Batch  40  loss:  0.11038067936897278
Batch  41  loss:  0.14129072427749634
Batch  42  loss:  0.09683558344841003
Batch  43  loss:  0.11593736708164215
Batch  44  loss:  0.0859011858701706
Batch  45  loss:  0.09091980755329132
Batch  46  loss:  0.1053108349442482
Batch  47  loss:  0.0837477296590805
Batch  48  loss:  0.09515334665775299
Batch  49  loss:  0.10643144696950912
Batch  50  loss:  0.11651033908128738
Batch  51  loss:  0.11109430342912674
Batch  52  loss:  0.106501005589962
Batch  53  loss:  0.08458030223846436
Batch  54  loss:  0.10662692040205002
Batch  55  loss:  0.13452385365962982
Batch  56  loss:  0.12586908042430878
Batch  57  loss:  0.09595800191164017
Batch  58  loss:  0.1019572839140892
Batch  59  loss:  0.10024501383304596
Batch  60  loss:  0.1098082885146141
Batch  61  loss:  0.082949198782444
Batch  62  loss:  0.09288104623556137
Batch  63  loss:  0.08593069016933441
Batch  64  loss:  0.09642324596643448
Batch  65  loss:  0.11011264473199844
Batch  66  loss:  0.11553595215082169
Batch  67  loss:  0.12486496567726135
Batch  68  loss:  0.10888639092445374
Batch  69  loss:  0.09320325404405594
Batch  70  loss:  0.08726195991039276
Batch  71  loss:  0.11426535993814468
Batch  72  loss:  0.08838439732789993
Batch  73  loss:  0.08579608052968979
Batch  74  loss:  0.09419166296720505
Batch  75  loss:  0.0807051807641983
Batch  76  loss:  0.08446124196052551
Batch  77  loss:  0.0960114449262619
Batch  78  loss:  0.09549043327569962
Batch  79  loss:  0.08716971427202225
Batch  80  loss:  0.09403716772794724
Batch  81  loss:  0.09349630028009415
Batch  82  loss:  0.07654343545436859
Batch  83  loss:  0.09741144627332687
Batch  84  loss:  0.10846180468797684
Batch  85  loss:  0.09335589408874512
Batch  86  loss:  0.09386414289474487
Batch  87  loss:  0.0866910070180893
Batch  88  loss:  0.09484115242958069
Batch  89  loss:  0.11342229694128036
Batch  90  loss:  0.09335941821336746
Batch  91  loss:  0.10271226614713669
Batch  92  loss:  0.09327927976846695
Batch  93  loss:  0.08531393110752106
Batch  94  loss:  0.09290968626737595
Batch  95  loss:  0.11314723640680313
Batch  96  loss:  0.08596785366535187
Batch  97  loss:  0.11619888246059418
Batch  98  loss:  0.09161306917667389
Batch  99  loss:  0.10586123168468475
Batch  100  loss:  0.09205751121044159
Validation: 
LOSS train 0.09922686643898487, val 0.09028217941522598
EPOCH : 18 TIME:  2022-04-19 04:20:22.111721
Training: 
Batch  1  loss:  0.07916806638240814
Batch  2  loss:  0.08699645102024078
Batch  3  loss:  0.07640307396650314
Batch  4  loss:  0.0736384466290474
Batch  5  loss:  0.07996245473623276
Batch  6  loss:  0.08839419484138489
Batch  7  loss:  0.10686036199331284
Batch  8  loss:  0.10474729537963867
Batch  9  loss:  0.08425453305244446
Batch  10  loss:  0.13694444298744202
Batch  11  loss:  0.07734978199005127
Batch  12  loss:  0.10380681604146957
Batch  13  loss:  0.09332726895809174
Batch  14  loss:  0.09161370992660522
Batch  15  loss:  0.10704541951417923
Batch  16  loss:  0.11161649227142334
Batch  17  loss:  0.10459037870168686
Batch  18  loss:  0.10791152715682983
Batch  19  loss:  0.09316893666982651
Batch  20  loss:  0.09799374639987946
Batch  21  loss:  0.0867941826581955
Batch  22  loss:  0.09161416441202164
Batch  23  loss:  0.0958312377333641
Batch  24  loss:  0.10977598279714584
Batch  25  loss:  0.09296519309282303
Batch  26  loss:  0.0701836496591568
Batch  27  loss:  0.097287118434906
Batch  28  loss:  0.11687488108873367
Batch  29  loss:  0.11083453893661499
Batch  30  loss:  0.11078620702028275
Batch  31  loss:  0.09422875940799713
Batch  32  loss:  0.09524508565664291
Batch  33  loss:  0.08931253850460052
Batch  34  loss:  0.09852111339569092
Batch  35  loss:  0.07663516700267792
Batch  36  loss:  0.11625495553016663
Batch  37  loss:  0.08968295156955719
Batch  38  loss:  0.09343507885932922
Batch  39  loss:  0.0989709421992302
Batch  40  loss:  0.10183174908161163
Batch  41  loss:  0.09178127348423004
Batch  42  loss:  0.10130822658538818
Batch  43  loss:  0.09675148129463196
Batch  44  loss:  0.08743119984865189
Batch  45  loss:  0.09271049499511719
Batch  46  loss:  0.10533967614173889
Batch  47  loss:  0.10895242542028427
Batch  48  loss:  0.10346657782793045
Batch  49  loss:  0.0904058888554573
Batch  50  loss:  0.09824246913194656
Batch  51  loss:  0.106344074010849
Batch  52  loss:  0.09765790402889252
Batch  53  loss:  0.08903517574071884
Batch  54  loss:  0.11737960577011108
Batch  55  loss:  0.08668790757656097
Batch  56  loss:  0.1083628311753273
Batch  57  loss:  0.09439671039581299
Batch  58  loss:  0.1002805083990097
Batch  59  loss:  0.08291475474834442
Batch  60  loss:  0.09346810728311539
Batch  61  loss:  0.07644934207201004
Batch  62  loss:  0.08960554003715515
Batch  63  loss:  0.07289155572652817
Batch  64  loss:  0.08751299977302551
Batch  65  loss:  0.09904168546199799
Batch  66  loss:  0.10505727678537369
Batch  67  loss:  0.0819532573223114
Batch  68  loss:  0.07451888173818588
Batch  69  loss:  0.08720283210277557
Batch  70  loss:  0.09816411137580872
Batch  71  loss:  0.09899404644966125
Batch  72  loss:  0.09080738574266434
Batch  73  loss:  0.09159551560878754
Batch  74  loss:  0.09319831430912018
Batch  75  loss:  0.09479415416717529
Batch  76  loss:  0.1196196973323822
Batch  77  loss:  0.10154207050800323
Batch  78  loss:  0.1047871857881546
Batch  79  loss:  0.10255282372236252
Batch  80  loss:  0.09067153930664062
Batch  81  loss:  0.08870036154985428
Batch  82  loss:  0.11063317954540253
Batch  83  loss:  0.10765895247459412
Batch  84  loss:  0.09507497400045395
Batch  85  loss:  0.09573442488908768
Batch  86  loss:  0.08656524121761322
Batch  87  loss:  0.08768977969884872
Batch  88  loss:  0.10822371393442154
Batch  89  loss:  0.11302818357944489
Batch  90  loss:  0.09502746164798737
Batch  91  loss:  0.10078838467597961
Batch  92  loss:  0.1108943447470665
Batch  93  loss:  0.09182965010404587
Batch  94  loss:  0.07270237058401108
Batch  95  loss:  0.0915227085351944
Batch  96  loss:  0.08075232058763504
Batch  97  loss:  0.08389659970998764
Batch  98  loss:  0.10668991506099701
Batch  99  loss:  0.08783414959907532
Batch  100  loss:  0.07400839030742645
Validation: 
LOSS train 0.09545991510152817, val 0.1280025690793991
EPOCH : 19 TIME:  2022-04-19 04:26:00.903630
Training: 
Batch  1  loss:  0.1161772683262825
Batch  2  loss:  0.07524765282869339
Batch  3  loss:  0.10857119411230087
Batch  4  loss:  0.0896478146314621
Batch  5  loss:  0.10863655060529709
Batch  6  loss:  0.0833326056599617
Batch  7  loss:  0.09003637731075287
Batch  8  loss:  0.08388523012399673
Batch  9  loss:  0.08998492360115051
Batch  10  loss:  0.10549966245889664
Batch  11  loss:  0.10525122284889221
Batch  12  loss:  0.12274239957332611
Batch  13  loss:  0.10000433772802353
Batch  14  loss:  0.07811908423900604
Batch  15  loss:  0.10593677312135696
Batch  16  loss:  0.10024905949831009
Batch  17  loss:  0.1261882483959198
Batch  18  loss:  0.12051771581172943
Batch  19  loss:  0.08260901272296906
Batch  20  loss:  0.11972735822200775
Batch  21  loss:  0.0928540974855423
Batch  22  loss:  0.08535163849592209
Batch  23  loss:  0.0977213904261589
Batch  24  loss:  0.10034748166799545
Batch  25  loss:  0.10609236359596252
Batch  26  loss:  0.09329701215028763
Batch  27  loss:  0.09958451241254807
Batch  28  loss:  0.09785180538892746
Batch  29  loss:  0.10466291010379791
Batch  30  loss:  0.08293050527572632
Batch  31  loss:  0.09826391190290451
Batch  32  loss:  0.09956365078687668
Batch  33  loss:  0.07334106415510178
Batch  34  loss:  0.09564930945634842
Batch  35  loss:  0.0915030911564827
Batch  36  loss:  0.11254838854074478
Batch  37  loss:  0.06751132011413574
Batch  38  loss:  0.10366245359182358
Batch  39  loss:  0.09287340193986893
Batch  40  loss:  0.10047133266925812
Batch  41  loss:  0.07210955768823624
Batch  42  loss:  0.07620911300182343
Batch  43  loss:  0.09957320988178253
Batch  44  loss:  0.10637352615594864
Batch  45  loss:  0.090733103454113
Batch  46  loss:  0.06722275912761688
Batch  47  loss:  0.08189211785793304
Batch  48  loss:  0.09015987068414688
Batch  49  loss:  0.10033605247735977
Batch  50  loss:  0.09651484340429306
Batch  51  loss:  0.07777880877256393
Batch  52  loss:  0.10137256979942322
Batch  53  loss:  0.11443254351615906
Batch  54  loss:  0.10264882445335388
Batch  55  loss:  0.07873411476612091
Batch  56  loss:  0.09059341251850128
Batch  57  loss:  0.09994146972894669
Batch  58  loss:  0.07931409031152725
Batch  59  loss:  0.08823348581790924
Batch  60  loss:  0.0724862590432167
Batch  61  loss:  0.0921894833445549
Batch  62  loss:  0.07627902925014496
Batch  63  loss:  0.09003644436597824
Batch  64  loss:  0.0832798182964325
Batch  65  loss:  0.05907038226723671
Batch  66  loss:  0.07272209972143173
Batch  67  loss:  0.07657754421234131
Batch  68  loss:  0.0945783331990242
Batch  69  loss:  0.08187692612409592
Batch  70  loss:  0.08723077923059464
Batch  71  loss:  0.0950644388794899
Batch  72  loss:  0.08682715892791748
Batch  73  loss:  0.10150550305843353
Batch  74  loss:  0.10670372098684311
Batch  75  loss:  0.09973279386758804
Batch  76  loss:  0.09833735972642899
Batch  77  loss:  0.09684471040964127
Batch  78  loss:  0.1051950454711914
Batch  79  loss:  0.11244986951351166
Batch  80  loss:  0.08785777539014816
Batch  81  loss:  0.08454705774784088
Batch  82  loss:  0.12975123524665833
Batch  83  loss:  0.09497752040624619
Batch  84  loss:  0.11942639201879501
Batch  85  loss:  0.1060517281293869
Batch  86  loss:  0.07351358234882355
Batch  87  loss:  0.09848905354738235
Batch  88  loss:  0.09092695266008377
Batch  89  loss:  0.09834232181310654
Batch  90  loss:  0.10646714270114899
Batch  91  loss:  0.08408330380916595
Batch  92  loss:  0.08956800401210785
Batch  93  loss:  0.1094140037894249
Batch  94  loss:  0.096999391913414
Batch  95  loss:  0.10333079844713211
Batch  96  loss:  0.09394504129886627
Batch  97  loss:  0.0865311473608017
Batch  98  loss:  0.09168197214603424
Batch  99  loss:  0.11723193526268005
Batch  100  loss:  0.08923565596342087
Validation: 
LOSS train 0.09463979322463274, val 0.12703584134578705
EPOCH : 20 TIME:  2022-04-19 04:31:38.547059
Training: 
Batch  1  loss:  0.08702543377876282
Batch  2  loss:  0.08346295356750488
Batch  3  loss:  0.09426932781934738
Batch  4  loss:  0.06104442477226257
Batch  5  loss:  0.10169336944818497
Batch  6  loss:  0.07373826205730438
Batch  7  loss:  0.11403016746044159
Batch  8  loss:  0.08971082419157028
Batch  9  loss:  0.09633181244134903
Batch  10  loss:  0.08919829875230789
Batch  11  loss:  0.12006820738315582
Batch  12  loss:  0.09731092303991318
Batch  13  loss:  0.11744816601276398
Batch  14  loss:  0.10409463942050934
Batch  15  loss:  0.08920085430145264
Batch  16  loss:  0.09649903327226639
Batch  17  loss:  0.09413497895002365
Batch  18  loss:  0.09768562018871307
Batch  19  loss:  0.102300725877285
Batch  20  loss:  0.1009104922413826
Batch  21  loss:  0.0808752030134201
Batch  22  loss:  0.08109957724809647
Batch  23  loss:  0.07814822345972061
Batch  24  loss:  0.1087975949048996
Batch  25  loss:  0.0872342437505722
Batch  26  loss:  0.09022479504346848
Batch  27  loss:  0.0932602733373642
Batch  28  loss:  0.11161236464977264
Batch  29  loss:  0.0918809175491333
Batch  30  loss:  0.0872722715139389
Batch  31  loss:  0.09424722194671631
Batch  32  loss:  0.10116006433963776
Batch  33  loss:  0.08079095929861069
Batch  34  loss:  0.10266661643981934
Batch  35  loss:  0.07802793383598328
Batch  36  loss:  0.08613229542970657
Batch  37  loss:  0.07827600836753845
Batch  38  loss:  0.08112087845802307
Batch  39  loss:  0.10683172196149826
Batch  40  loss:  0.08394449949264526
Batch  41  loss:  0.0947757214307785
Batch  42  loss:  0.08459337800741196
Batch  43  loss:  0.10604112595319748
Batch  44  loss:  0.0917058140039444
Batch  45  loss:  0.0819934606552124
Batch  46  loss:  0.09692743420600891
Batch  47  loss:  0.07734953612089157
Batch  48  loss:  0.08405594527721405
Batch  49  loss:  0.08672582358121872
Batch  50  loss:  0.1179313063621521
Batch  51  loss:  0.09406107664108276
Batch  52  loss:  0.08587520569562912
Batch  53  loss:  0.0918014794588089
Batch  54  loss:  0.10386324673891068
Batch  55  loss:  0.0961081013083458
Batch  56  loss:  0.08784713596105576
Batch  57  loss:  0.08795848488807678
Batch  58  loss:  0.11808690428733826
Batch  59  loss:  0.09983751177787781
Batch  60  loss:  0.10699837654829025
Batch  61  loss:  0.06945972144603729
Batch  62  loss:  0.09029626846313477
Batch  63  loss:  0.1035095602273941
Batch  64  loss:  0.08894767612218857
Batch  65  loss:  0.07934647798538208
Batch  66  loss:  0.10343014448881149
Batch  67  loss:  0.105504110455513
Batch  68  loss:  0.087946318089962
Batch  69  loss:  0.07831568270921707
Batch  70  loss:  0.07851997017860413
Batch  71  loss:  0.08522024750709534
Batch  72  loss:  0.06535209715366364
Batch  73  loss:  0.08631091564893723
Batch  74  loss:  0.09568651765584946
Batch  75  loss:  0.07203847914934158
Batch  76  loss:  0.09065812826156616
Batch  77  loss:  0.07443004101514816
Batch  78  loss:  0.10395614057779312
Batch  79  loss:  0.11598032712936401
Batch  80  loss:  0.09544026851654053
Batch  81  loss:  0.08188428729772568
Batch  82  loss:  0.08557750284671783
Batch  83  loss:  0.09193799644708633
Batch  84  loss:  0.07466333359479904
Batch  85  loss:  0.07671178877353668
Batch  86  loss:  0.09125865995883942
Batch  87  loss:  0.09160890430212021
Batch  88  loss:  0.10220671445131302
Batch  89  loss:  0.07297264039516449
Batch  90  loss:  0.0752888023853302
Batch  91  loss:  0.10125188529491425
Batch  92  loss:  0.08977404981851578
Batch  93  loss:  0.07897499948740005
Batch  94  loss:  0.0939122810959816
Batch  95  loss:  0.08064097911119461
Batch  96  loss:  0.07795338332653046
Batch  97  loss:  0.10429900139570236
Batch  98  loss:  0.08373191952705383
Batch  99  loss:  0.102401502430439
Batch  100  loss:  0.08795841038227081
Validation: 
LOSS train 0.09123657383024693, val 0.13811206817626953
Tue 19 Apr 2022 04:37:16 AM EDT
SKATEBOARD:: POINT TRANSFORMER
INFO - 2022-04-19 04:37:18,485 - utils - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO - 2022-04-19 04:37:18,485 - utils - NumExpr defaulting to 8 threads.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  skateboard
--------------------
Running self_supervised_training:  2022-04-19 04:37:18.629039
--------------------
device is  cuda
--------------------
Number of trainable parameters:  894622
EPOCH : 1 TIME:  2022-04-19 04:37:20.309446
Training: 
Batch  1  loss:  0.655309796333313
Batch  2  loss:  0.22357064485549927
Batch  3  loss:  0.3700953722000122
Batch  4  loss:  0.5327147841453552
Batch  5  loss:  0.3308809995651245
Batch  6  loss:  0.19023723900318146
Batch  7  loss:  0.19296503067016602
Batch  8  loss:  0.23082396388053894
Batch  9  loss:  0.2475648671388626
Batch  10  loss:  0.2327343374490738
Batch  11  loss:  0.21179541945457458
Batch  12  loss:  0.18199974298477173
Batch  13  loss:  0.1753670871257782
Batch  14  loss:  0.16253861784934998
Batch  15  loss:  0.16613197326660156
Batch  16  loss:  0.16071143746376038
Batch  17  loss:  0.1593819409608841
Batch  18  loss:  0.15916872024536133
Batch  19  loss:  0.1601637303829193
Batch  20  loss:  0.16003839671611786
Batch  21  loss:  0.15669777989387512
Batch  22  loss:  0.15621592104434967
Batch  23  loss:  0.15385204553604126
Batch  24  loss:  0.1543334573507309
Batch  25  loss:  0.15262912213802338
Batch  26  loss:  0.15054678916931152
Batch  27  loss:  0.15020418167114258
Batch  28  loss:  0.14919954538345337
Batch  29  loss:  0.15237440168857574
Batch  30  loss:  0.14852114021778107
Batch  31  loss:  0.15033061802387238
Batch  32  loss:  0.1478411853313446
Batch  33  loss:  0.1497528851032257
Batch  34  loss:  0.15127944946289062
Batch  35  loss:  0.14781454205513
Batch  36  loss:  0.14832429587841034
Batch  37  loss:  0.14821214973926544
Batch  38  loss:  0.14990635216236115
Batch  39  loss:  0.14862236380577087
Batch  40  loss:  0.1469477415084839
Batch  41  loss:  0.14636875689029694
Batch  42  loss:  0.14627261459827423
Batch  43  loss:  0.14684727787971497
Batch  44  loss:  0.1460576057434082
Batch  45  loss:  0.14614474773406982
Batch  46  loss:  0.14393387734889984
Batch  47  loss:  0.14377643167972565
Batch  48  loss:  0.14626829326152802
Batch  49  loss:  0.14886638522148132
Batch  50  loss:  0.14487838745117188
Batch  51  loss:  0.14404474198818207
Batch  52  loss:  0.14533236622810364
Batch  53  loss:  0.14425301551818848
Batch  54  loss:  0.14701291918754578
Batch  55  loss:  0.1452789306640625
Batch  56  loss:  0.1447671800851822
Batch  57  loss:  0.14484067261219025
Batch  58  loss:  0.1455872654914856
Batch  59  loss:  0.14296603202819824
Batch  60  loss:  0.1419338434934616
Batch  61  loss:  0.14318618178367615
Batch  62  loss:  0.14325088262557983
Batch  63  loss:  0.14476941525936127
Batch  64  loss:  0.14439326524734497
Batch  65  loss:  0.14369094371795654
Batch  66  loss:  0.142710879445076
Batch  67  loss:  0.14237989485263824
Batch  68  loss:  0.14448785781860352
Batch  69  loss:  0.14282754063606262
Batch  70  loss:  0.1456133872270584
Batch  71  loss:  0.1441868394613266
Batch  72  loss:  0.14379967749118805
Batch  73  loss:  0.14381101727485657
Batch  74  loss:  0.14343956112861633
Batch  75  loss:  0.1443151831626892
Batch  76  loss:  0.14284858107566833
Batch  77  loss:  0.14113913476467133
Batch  78  loss:  0.14364735782146454
Batch  79  loss:  0.145062655210495
Batch  80  loss:  0.142782062292099
Batch  81  loss:  0.14149503409862518
Batch  82  loss:  0.1447531133890152
Batch  83  loss:  0.14495256543159485
Batch  84  loss:  0.144294872879982
Batch  85  loss:  0.14183884859085083
Batch  86  loss:  0.14440999925136566
Batch  87  loss:  0.14483803510665894
Batch  88  loss:  0.14575831592082977
Batch  89  loss:  0.1439816802740097
Batch  90  loss:  0.1402430236339569
Batch  91  loss:  0.14595133066177368
Batch  92  loss:  0.14423325657844543
Batch  93  loss:  0.14451660215854645
Batch  94  loss:  0.1409555822610855
Batch  95  loss:  0.14272619783878326
Batch  96  loss:  0.14473596215248108
Batch  97  loss:  0.14484943449497223
Batch  98  loss:  0.1452513039112091
Batch  99  loss:  0.1444786936044693
Batch  100  loss:  0.14125430583953857
Validation: 
LOSS train 0.16571091890335082, val 0.14328216016292572
EPOCH : 2 TIME:  2022-04-19 04:44:19.535047
Training: 
Batch  1  loss:  0.14536435902118683
Batch  2  loss:  0.14471536874771118
Batch  3  loss:  0.14223496615886688
Batch  4  loss:  0.14490906894207
Batch  5  loss:  0.14530320465564728
Batch  6  loss:  0.14442138373851776
Batch  7  loss:  0.14413104951381683
Batch  8  loss:  0.14571842551231384
Batch  9  loss:  0.14492328464984894
Batch  10  loss:  0.1438400000333786
Batch  11  loss:  0.14398932456970215
Batch  12  loss:  0.14121918380260468
Batch  13  loss:  0.14481262862682343
Batch  14  loss:  0.14131303131580353
Batch  15  loss:  0.14342239499092102
Batch  16  loss:  0.14359092712402344
Batch  17  loss:  0.14459234476089478
Batch  18  loss:  0.1423192322254181
Batch  19  loss:  0.1434607058763504
Batch  20  loss:  0.1411803960800171
Batch  21  loss:  0.1424354463815689
Batch  22  loss:  0.14587078988552094
Batch  23  loss:  0.1448090523481369
Batch  24  loss:  0.14267665147781372
Batch  25  loss:  0.14428706467151642
Batch  26  loss:  0.14344695210456848
Batch  27  loss:  0.14246122539043427
Batch  28  loss:  0.14197252690792084
Batch  29  loss:  0.1444515734910965
Batch  30  loss:  0.14536084234714508
Batch  31  loss:  0.14533083140850067
Batch  32  loss:  0.14476357400417328
Batch  33  loss:  0.14468513429164886
Batch  34  loss:  0.1406693160533905
Batch  35  loss:  0.14369940757751465
Batch  36  loss:  0.14403964579105377
Batch  37  loss:  0.14249320328235626
Batch  38  loss:  0.14536866545677185
Batch  39  loss:  0.14094063639640808
Batch  40  loss:  0.14278416335582733
Batch  41  loss:  0.14550039172172546
Batch  42  loss:  0.14315477013587952
Batch  43  loss:  0.14412842690944672
Batch  44  loss:  0.1449654996395111
Batch  45  loss:  0.14388670027256012
Batch  46  loss:  0.14428745210170746
Batch  47  loss:  0.14218854904174805
Batch  48  loss:  0.14542923867702484
Batch  49  loss:  0.14309214055538177
Batch  50  loss:  0.14465482532978058
Batch  51  loss:  0.14234115183353424
Batch  52  loss:  0.1451064795255661
Batch  53  loss:  0.1463824212551117
Batch  54  loss:  0.14447981119155884
Batch  55  loss:  0.14449623227119446
Batch  56  loss:  0.14477817714214325
Batch  57  loss:  0.14405035972595215
Batch  58  loss:  0.14315053820610046
Batch  59  loss:  0.14346268773078918
Batch  60  loss:  0.14322268962860107
Batch  61  loss:  0.1420045793056488
Batch  62  loss:  0.14609165489673615
Batch  63  loss:  0.14541593194007874
Batch  64  loss:  0.14248526096343994
Batch  65  loss:  0.1436167061328888
Batch  66  loss:  0.1423725038766861
Batch  67  loss:  0.14436163008213043
Batch  68  loss:  0.1431894600391388
Batch  69  loss:  0.14575572311878204
Batch  70  loss:  0.14337998628616333
Batch  71  loss:  0.14354020357131958
Batch  72  loss:  0.14357051253318787
Batch  73  loss:  0.14524468779563904
Batch  74  loss:  0.14318513870239258
Batch  75  loss:  0.14305409789085388
Batch  76  loss:  0.1448138803243637
Batch  77  loss:  0.14441625773906708
Batch  78  loss:  0.14192399382591248
Batch  79  loss:  0.14361873269081116
Batch  80  loss:  0.14474545419216156
Batch  81  loss:  0.14635330438613892
Batch  82  loss:  0.14332443475723267
Batch  83  loss:  0.14219637215137482
Batch  84  loss:  0.14595605432987213
Batch  85  loss:  0.14576135575771332
Batch  86  loss:  0.14277945458889008
Batch  87  loss:  0.14300169050693512
Batch  88  loss:  0.1477297693490982
Batch  89  loss:  0.14450940489768982
Batch  90  loss:  0.14363636076450348
Batch  91  loss:  0.1458502858877182
Batch  92  loss:  0.14333561062812805
Batch  93  loss:  0.14411082863807678
Batch  94  loss:  0.1451382040977478
Batch  95  loss:  0.14459781348705292
Batch  96  loss:  0.14119532704353333
Batch  97  loss:  0.14498910307884216
Batch  98  loss:  0.14344871044158936
Batch  99  loss:  0.14432670176029205
Batch  100  loss:  0.1444752812385559
Validation: 
LOSS train 0.1439426898956299, val 0.14586955308914185
EPOCH : 3 TIME:  2022-04-19 04:51:11.590626
Training: 
Batch  1  loss:  0.14388629794120789
Batch  2  loss:  0.14381112158298492
Batch  3  loss:  0.1454562395811081
Batch  4  loss:  0.14613358676433563
Batch  5  loss:  0.14363931119441986
Batch  6  loss:  0.1430327147245407
Batch  7  loss:  0.1437569558620453
Batch  8  loss:  0.14469534158706665
Batch  9  loss:  0.1455855369567871
Batch  10  loss:  0.14379821717739105
Batch  11  loss:  0.14397281408309937
Batch  12  loss:  0.14533880352973938
Batch  13  loss:  0.14223472774028778
Batch  14  loss:  0.14590497314929962
Batch  15  loss:  0.14456667006015778
Batch  16  loss:  0.14423969388008118
Batch  17  loss:  0.14403600990772247
Batch  18  loss:  0.1442565619945526
Batch  19  loss:  0.1436741203069687
Batch  20  loss:  0.14008556306362152
Batch  21  loss:  0.14494958519935608
Batch  22  loss:  0.14513351023197174
Batch  23  loss:  0.1452052742242813
Batch  24  loss:  0.1431904286146164
Batch  25  loss:  0.1421433985233307
Batch  26  loss:  0.14410372078418732
Batch  27  loss:  0.1433877944946289
Batch  28  loss:  0.14366844296455383
Batch  29  loss:  0.14718782901763916
Batch  30  loss:  0.14221611618995667
Batch  31  loss:  0.14344607293605804
Batch  32  loss:  0.14301545917987823
Batch  33  loss:  0.14553634822368622
Batch  34  loss:  0.1436515897512436
Batch  35  loss:  0.14512912929058075
Batch  36  loss:  0.14733272790908813
Batch  37  loss:  0.1445418745279312
Batch  38  loss:  0.14439477026462555
Batch  39  loss:  0.1442805677652359
Batch  40  loss:  0.14499545097351074
Batch  41  loss:  0.14385244250297546
Batch  42  loss:  0.14311790466308594
Batch  43  loss:  0.14356350898742676
Batch  44  loss:  0.14365726709365845
Batch  45  loss:  0.14418095350265503
Batch  46  loss:  0.1435239017009735
Batch  47  loss:  0.14300476014614105
Batch  48  loss:  0.14348703622817993
Batch  49  loss:  0.14264194667339325
Batch  50  loss:  0.1459847390651703
Batch  51  loss:  0.1450645476579666
Batch  52  loss:  0.14574240148067474
Batch  53  loss:  0.14408378303050995
Batch  54  loss:  0.1431688666343689
Batch  55  loss:  0.14186102151870728
Batch  56  loss:  0.14579859375953674
Batch  57  loss:  0.14370928704738617
Batch  58  loss:  0.14423133432865143
Batch  59  loss:  0.14413921535015106
Batch  60  loss:  0.14229610562324524
Batch  61  loss:  0.14558109641075134
Batch  62  loss:  0.14291395246982574
Batch  63  loss:  0.14699725806713104
Batch  64  loss:  0.14406071603298187
Batch  65  loss:  0.1433420032262802
Batch  66  loss:  0.14609646797180176
Batch  67  loss:  0.14412595331668854
Batch  68  loss:  0.1423283964395523
Batch  69  loss:  0.14219224452972412
Batch  70  loss:  0.14121171832084656
Batch  71  loss:  0.14323101937770844
Batch  72  loss:  0.1417005956172943
Batch  73  loss:  0.14511266350746155
Batch  74  loss:  0.14351069927215576
Batch  75  loss:  0.14354431629180908
Batch  76  loss:  0.14396443963050842
Batch  77  loss:  0.14686883985996246
Batch  78  loss:  0.14264273643493652
Batch  79  loss:  0.14328405261039734
Batch  80  loss:  0.1432313472032547
Batch  81  loss:  0.14390972256660461
Batch  82  loss:  0.14460045099258423
Batch  83  loss:  0.14360816776752472
Batch  84  loss:  0.14447283744812012
Batch  85  loss:  0.1445247083902359
Batch  86  loss:  0.14311020076274872
Batch  87  loss:  0.14220404624938965
Batch  88  loss:  0.14287345111370087
Batch  89  loss:  0.14258058369159698
Batch  90  loss:  0.1430041640996933
Batch  91  loss:  0.14323292672634125
Batch  92  loss:  0.14374661445617676
Batch  93  loss:  0.1427643746137619
Batch  94  loss:  0.1439822018146515
Batch  95  loss:  0.1434926986694336
Batch  96  loss:  0.14227399230003357
Batch  97  loss:  0.143253356218338
Batch  98  loss:  0.14480911195278168
Batch  99  loss:  0.1427561342716217
Batch  100  loss:  0.14636778831481934
Validation: 
LOSS train 0.14394233018159866, val 0.14497943222522736
EPOCH : 4 TIME:  2022-04-19 04:58:05.545476
Training: 
Batch  1  loss:  0.14413350820541382
Batch  2  loss:  0.14382579922676086
Batch  3  loss:  0.14290805160999298
Batch  4  loss:  0.14442650973796844
Batch  5  loss:  0.14410124719142914
Batch  6  loss:  0.14163334667682648
Batch  7  loss:  0.14341124892234802
Batch  8  loss:  0.1432333141565323
Batch  9  loss:  0.14205090701580048
Batch  10  loss:  0.14399509131908417
Batch  11  loss:  0.1457442194223404
Batch  12  loss:  0.14452190697193146
Batch  13  loss:  0.14642678201198578
Batch  14  loss:  0.14226551353931427
Batch  15  loss:  0.14598160982131958
Batch  16  loss:  0.14233584702014923
Batch  17  loss:  0.14562276005744934
Batch  18  loss:  0.1445067971944809
Batch  19  loss:  0.14157438278198242
Batch  20  loss:  0.14294476807117462
Batch  21  loss:  0.1419999599456787
Batch  22  loss:  0.14374028146266937
Batch  23  loss:  0.14174747467041016
Batch  24  loss:  0.14380817115306854
Batch  25  loss:  0.14440608024597168
Batch  26  loss:  0.14175720512866974
Batch  27  loss:  0.14479531347751617
Batch  28  loss:  0.14549627900123596
Batch  29  loss:  0.14374560117721558
Batch  30  loss:  0.14094151556491852
Batch  31  loss:  0.1422489434480667
Batch  32  loss:  0.1444658786058426
Batch  33  loss:  0.14504985511302948
Batch  34  loss:  0.1439124196767807
Batch  35  loss:  0.143596351146698
Batch  36  loss:  0.14330419898033142
Batch  37  loss:  0.14441625773906708
Batch  38  loss:  0.1434784084558487
Batch  39  loss:  0.14347286522388458
Batch  40  loss:  0.1463281661272049
Batch  41  loss:  0.14199644327163696
Batch  42  loss:  0.14279137551784515
Batch  43  loss:  0.14391684532165527
Batch  44  loss:  0.14420723915100098
Batch  45  loss:  0.1431857794523239
Batch  46  loss:  0.1442483365535736
Batch  47  loss:  0.1428125500679016
Batch  48  loss:  0.14183977246284485
Batch  49  loss:  0.14330562949180603
Batch  50  loss:  0.1441783607006073
Batch  51  loss:  0.14321649074554443
Batch  52  loss:  0.14350906014442444
Batch  53  loss:  0.14465805888175964
Batch  54  loss:  0.14537197351455688
Batch  55  loss:  0.1458980292081833
Batch  56  loss:  0.1436792016029358
Batch  57  loss:  0.14249520003795624
Batch  58  loss:  0.1446591168642044
Batch  59  loss:  0.14235013723373413
Batch  60  loss:  0.14395049214363098
Batch  61  loss:  0.14405912160873413
Batch  62  loss:  0.14467871189117432
Batch  63  loss:  0.1453770250082016
Batch  64  loss:  0.14255116879940033
Batch  65  loss:  0.14656226336956024
Batch  66  loss:  0.14342889189720154
Batch  67  loss:  0.1451115608215332
Batch  68  loss:  0.14365822076797485
Batch  69  loss:  0.14308324456214905
Batch  70  loss:  0.1439237892627716
Batch  71  loss:  0.14290539920330048
Batch  72  loss:  0.14453035593032837
Batch  73  loss:  0.14470888674259186
Batch  74  loss:  0.14241482317447662
Batch  75  loss:  0.1442563384771347
Batch  76  loss:  0.1437099277973175
Batch  77  loss:  0.14152485132217407
Batch  78  loss:  0.14577718079090118
Batch  79  loss:  0.14663481712341309
Batch  80  loss:  0.14664769172668457
Batch  81  loss:  0.14572006464004517
Batch  82  loss:  0.14465422928333282
Batch  83  loss:  0.14412875473499298
Batch  84  loss:  0.14247579872608185
Batch  85  loss:  0.14464154839515686
Batch  86  loss:  0.1421358585357666
Batch  87  loss:  0.14456139504909515
Batch  88  loss:  0.14346852898597717
Batch  89  loss:  0.14371995627880096
Batch  90  loss:  0.14357724785804749
Batch  91  loss:  0.14057202637195587
Batch  92  loss:  0.14294219017028809
Batch  93  loss:  0.14283864200115204
Batch  94  loss:  0.1442498117685318
Batch  95  loss:  0.14536546170711517
Batch  96  loss:  0.14551621675491333
Batch  97  loss:  0.1450667381286621
Batch  98  loss:  0.14215658605098724
Batch  99  loss:  0.14445285499095917
Batch  100  loss:  0.14295563101768494
Validation: 
LOSS train 0.14381368741393089, val 0.1459355354309082
EPOCH : 5 TIME:  2022-04-19 05:04:57.146326
Training: 
Batch  1  loss:  0.1425456553697586
Batch  2  loss:  0.1434500813484192
Batch  3  loss:  0.1449519693851471
Batch  4  loss:  0.14319942891597748
Batch  5  loss:  0.14435657858848572
Batch  6  loss:  0.14473102986812592
Batch  7  loss:  0.14309892058372498
Batch  8  loss:  0.1450655460357666
Batch  9  loss:  0.14321260154247284
Batch  10  loss:  0.14411285519599915
Batch  11  loss:  0.14245295524597168
Batch  12  loss:  0.14489316940307617
Batch  13  loss:  0.14392006397247314
Batch  14  loss:  0.14396941661834717
Batch  15  loss:  0.14096294343471527
Batch  16  loss:  0.14239893853664398
Batch  17  loss:  0.14417043328285217
Batch  18  loss:  0.14440375566482544
Batch  19  loss:  0.14417555928230286
Batch  20  loss:  0.14341029524803162
Batch  21  loss:  0.14470389485359192
Batch  22  loss:  0.14316396415233612
Batch  23  loss:  0.1426471471786499
Batch  24  loss:  0.1436469852924347
Batch  25  loss:  0.14429150521755219
Batch  26  loss:  0.14341162145137787
Batch  27  loss:  0.14690732955932617
Batch  28  loss:  0.14351524412631989
Batch  29  loss:  0.14372991025447845
Batch  30  loss:  0.14346489310264587
Batch  31  loss:  0.14395484328269958
Batch  32  loss:  0.14462606608867645
Batch  33  loss:  0.1429184079170227
Batch  34  loss:  0.14257267117500305
Batch  35  loss:  0.14387443661689758
Batch  36  loss:  0.14481686055660248
Batch  37  loss:  0.14341135323047638
Batch  38  loss:  0.14432334899902344
Batch  39  loss:  0.14436770975589752
Batch  40  loss:  0.14391353726387024
Batch  41  loss:  0.14231230318546295
Batch  42  loss:  0.14462660253047943
Batch  43  loss:  0.14192591607570648
Batch  44  loss:  0.1434004157781601
Batch  45  loss:  0.14520764350891113
Batch  46  loss:  0.14169210195541382
Batch  47  loss:  0.1426422894001007
Batch  48  loss:  0.1438661366701126
Batch  49  loss:  0.14538417756557465
Batch  50  loss:  0.14459742605686188
Batch  51  loss:  0.14312906563282013
Batch  52  loss:  0.14433564245700836
Batch  53  loss:  0.1463317722082138
Batch  54  loss:  0.1416168510913849
Batch  55  loss:  0.14389775693416595
Batch  56  loss:  0.1453794687986374
Batch  57  loss:  0.14318813383579254
Batch  58  loss:  0.1452418565750122
Batch  59  loss:  0.144646555185318
Batch  60  loss:  0.14527055621147156
Batch  61  loss:  0.14301060140132904
Batch  62  loss:  0.14242318272590637
Batch  63  loss:  0.14503397047519684
Batch  64  loss:  0.1440439522266388
Batch  65  loss:  0.14282195270061493
Batch  66  loss:  0.14472393691539764
Batch  67  loss:  0.14550185203552246
Batch  68  loss:  0.14496666193008423
Batch  69  loss:  0.14474116265773773
Batch  70  loss:  0.1449519395828247
Batch  71  loss:  0.14331769943237305
Batch  72  loss:  0.14446668326854706
Batch  73  loss:  0.14389710128307343
Batch  74  loss:  0.14363692700862885
Batch  75  loss:  0.14500339329242706
Batch  76  loss:  0.14388294517993927
Batch  77  loss:  0.14427873492240906
Batch  78  loss:  0.14362750947475433
Batch  79  loss:  0.14237026870250702
Batch  80  loss:  0.14048852026462555
Batch  81  loss:  0.14342084527015686
Batch  82  loss:  0.14328846335411072
Batch  83  loss:  0.14464811980724335
Batch  84  loss:  0.14245359599590302
Batch  85  loss:  0.14479666948318481
Batch  86  loss:  0.14474858343601227
Batch  87  loss:  0.14545266330242157
Batch  88  loss:  0.14616823196411133
Batch  89  loss:  0.14398367702960968
Batch  90  loss:  0.14276014268398285
Batch  91  loss:  0.14402471482753754
Batch  92  loss:  0.14528240263462067
Batch  93  loss:  0.1449381411075592
Batch  94  loss:  0.14559097588062286
Batch  95  loss:  0.14394725859165192
Batch  96  loss:  0.14371265470981598
Batch  97  loss:  0.14430740475654602
Batch  98  loss:  0.14465048909187317
Batch  99  loss:  0.1424759477376938
Batch  100  loss:  0.1446746289730072
Validation: 
LOSS train 0.14392951205372811, val 0.14388397336006165
EPOCH : 6 TIME:  2022-04-19 05:11:51.646141
Training: 
Batch  1  loss:  0.14425402879714966
Batch  2  loss:  0.14422065019607544
Batch  3  loss:  0.14374159276485443
Batch  4  loss:  0.14301341772079468
Batch  5  loss:  0.14347338676452637
Batch  6  loss:  0.1440589874982834
Batch  7  loss:  0.14405161142349243
Batch  8  loss:  0.1439635455608368
Batch  9  loss:  0.14474810659885406
Batch  10  loss:  0.14504200220108032
Batch  11  loss:  0.14231276512145996
Batch  12  loss:  0.1437833309173584
Batch  13  loss:  0.14510460197925568
Batch  14  loss:  0.1436571627855301
Batch  15  loss:  0.1415283977985382
Batch  16  loss:  0.14265280961990356
Batch  17  loss:  0.1443815380334854
Batch  18  loss:  0.14383630454540253
Batch  19  loss:  0.1447211354970932
Batch  20  loss:  0.1434880495071411
Batch  21  loss:  0.14136920869350433
Batch  22  loss:  0.14240875840187073
Batch  23  loss:  0.14514194428920746
Batch  24  loss:  0.1442447006702423
Batch  25  loss:  0.14567407965660095
Batch  26  loss:  0.14336013793945312
Batch  27  loss:  0.1456931084394455
Batch  28  loss:  0.14427942037582397
Batch  29  loss:  0.14521054923534393
Batch  30  loss:  0.14232924580574036
Batch  31  loss:  0.14482749998569489
Batch  32  loss:  0.14349998533725739
Batch  33  loss:  0.14188706874847412
Batch  34  loss:  0.14287196099758148
Batch  35  loss:  0.14246830344200134
Batch  36  loss:  0.1415073126554489
Batch  37  loss:  0.1427595168352127
Batch  38  loss:  0.14504751563072205
Batch  39  loss:  0.14382857084274292
Batch  40  loss:  0.142327219247818
Batch  41  loss:  0.14539800584316254
Batch  42  loss:  0.13909660279750824
Batch  43  loss:  0.141781285405159
Batch  44  loss:  0.14551611244678497
Batch  45  loss:  0.14368289709091187
Batch  46  loss:  0.14306384325027466
Batch  47  loss:  0.1410413533449173
Batch  48  loss:  0.14325609803199768
Batch  49  loss:  0.14714407920837402
Batch  50  loss:  0.14521533250808716
Batch  51  loss:  0.14573641121387482
Batch  52  loss:  0.14406239986419678
Batch  53  loss:  0.14489786326885223
Batch  54  loss:  0.14625078439712524
Batch  55  loss:  0.14379552006721497
Batch  56  loss:  0.1449558436870575
Batch  57  loss:  0.14432458579540253
Batch  58  loss:  0.142841637134552
Batch  59  loss:  0.14347249269485474
Batch  60  loss:  0.14305876195430756
Batch  61  loss:  0.14553329348564148
Batch  62  loss:  0.1441849172115326
Batch  63  loss:  0.14410339295864105
Batch  64  loss:  0.14457067847251892
Batch  65  loss:  0.14350628852844238
Batch  66  loss:  0.14267362654209137
Batch  67  loss:  0.1435319185256958
Batch  68  loss:  0.1441449522972107
Batch  69  loss:  0.14325164258480072
Batch  70  loss:  0.1435130089521408
Batch  71  loss:  0.14682622253894806
Batch  72  loss:  0.14197896420955658
Batch  73  loss:  0.14420795440673828
Batch  74  loss:  0.14407718181610107
Batch  75  loss:  0.14263497292995453
Batch  76  loss:  0.144436314702034
Batch  77  loss:  0.14374572038650513
Batch  78  loss:  0.14455126225948334
Batch  79  loss:  0.1430647224187851
Batch  80  loss:  0.14632514119148254
Batch  81  loss:  0.14364981651306152
Batch  82  loss:  0.1443846970796585
Batch  83  loss:  0.1433786302804947
Batch  84  loss:  0.14240708947181702
Batch  85  loss:  0.1447034478187561
Batch  86  loss:  0.14606569707393646
Batch  87  loss:  0.1441204994916916
Batch  88  loss:  0.14104606211185455
Batch  89  loss:  0.1446835845708847
Batch  90  loss:  0.1440589874982834
Batch  91  loss:  0.14363732933998108
Batch  92  loss:  0.14344541728496552
Batch  93  loss:  0.14107750356197357
Batch  94  loss:  0.14321266114711761
Batch  95  loss:  0.14368301630020142
Batch  96  loss:  0.1448272466659546
Batch  97  loss:  0.14551308751106262
Batch  98  loss:  0.1434653103351593
Batch  99  loss:  0.14318343997001648
Batch  100  loss:  0.14411501586437225
Validation: 
LOSS train 0.14380882158875466, val 0.14225631952285767
EPOCH : 7 TIME:  2022-04-19 05:18:47.131417
Training: 
Batch  1  loss:  0.14448007941246033
Batch  2  loss:  0.14571836590766907
Batch  3  loss:  0.14484666287899017
Batch  4  loss:  0.1435413509607315
Batch  5  loss:  0.14154238998889923
Batch  6  loss:  0.14472301304340363
Batch  7  loss:  0.14368414878845215
Batch  8  loss:  0.14174729585647583
Batch  9  loss:  0.14409862458705902
Batch  10  loss:  0.1463022530078888
Batch  11  loss:  0.14375953376293182
Batch  12  loss:  0.1445602923631668
Batch  13  loss:  0.14548027515411377
Batch  14  loss:  0.14114287495613098
Batch  15  loss:  0.14333221316337585
Batch  16  loss:  0.14482071995735168
Batch  17  loss:  0.14400598406791687
Batch  18  loss:  0.14345738291740417
Batch  19  loss:  0.14189770817756653
Batch  20  loss:  0.14166073501110077
Batch  21  loss:  0.14516960084438324
Batch  22  loss:  0.14320026338100433
Batch  23  loss:  0.1427612155675888
Batch  24  loss:  0.14432285726070404
Batch  25  loss:  0.14657863974571228
Batch  26  loss:  0.14483018219470978
Batch  27  loss:  0.14349564909934998
Batch  28  loss:  0.1442161202430725
Batch  29  loss:  0.14483481645584106
Batch  30  loss:  0.14281541109085083
Batch  31  loss:  0.1470872312784195
Batch  32  loss:  0.14512035250663757
Batch  33  loss:  0.14334504306316376
Batch  34  loss:  0.14215920865535736
Batch  35  loss:  0.14619103074073792
Batch  36  loss:  0.1439104527235031
Batch  37  loss:  0.14303605258464813
Batch  38  loss:  0.14534561336040497
Batch  39  loss:  0.14236460626125336
Batch  40  loss:  0.14427991211414337
Batch  41  loss:  0.14147496223449707
Batch  42  loss:  0.14812718331813812
Batch  43  loss:  0.14411866664886475
Batch  44  loss:  0.14412051439285278
Batch  45  loss:  0.14210695028305054
Batch  46  loss:  0.14592093229293823
Batch  47  loss:  0.1439390778541565
Batch  48  loss:  0.14286549389362335
Batch  49  loss:  0.14391382038593292
Batch  50  loss:  0.14408111572265625
Batch  51  loss:  0.14359372854232788
Batch  52  loss:  0.1436700075864792
Batch  53  loss:  0.14418520033359528
Batch  54  loss:  0.14523755013942719
Batch  55  loss:  0.14572447538375854
Batch  56  loss:  0.14527449011802673
Batch  57  loss:  0.1434510052204132
Batch  58  loss:  0.14272759854793549
Batch  59  loss:  0.14379605650901794
Batch  60  loss:  0.14279337227344513
Batch  61  loss:  0.142120361328125
Batch  62  loss:  0.1419709026813507
Batch  63  loss:  0.14603881537914276
Batch  64  loss:  0.14440175890922546
Batch  65  loss:  0.1420872062444687
Batch  66  loss:  0.14618264138698578
Batch  67  loss:  0.14437070488929749
Batch  68  loss:  0.14393411576747894
Batch  69  loss:  0.1433555632829666
Batch  70  loss:  0.1450882852077484
Batch  71  loss:  0.1430601328611374
Batch  72  loss:  0.14474093914031982
Batch  73  loss:  0.1432831883430481
Batch  74  loss:  0.14531023800373077
Batch  75  loss:  0.14386825263500214
Batch  76  loss:  0.14283373951911926
Batch  77  loss:  0.14246302843093872
Batch  78  loss:  0.14358820021152496
Batch  79  loss:  0.14352145791053772
Batch  80  loss:  0.1446867436170578
Batch  81  loss:  0.14410905539989471
Batch  82  loss:  0.14375945925712585
Batch  83  loss:  0.14363138377666473
Batch  84  loss:  0.14304517209529877
Batch  85  loss:  0.1441679149866104
Batch  86  loss:  0.14587731659412384
Batch  87  loss:  0.14340147376060486
Batch  88  loss:  0.14669619500637054
Batch  89  loss:  0.14253687858581543
Batch  90  loss:  0.14335522055625916
Batch  91  loss:  0.1437673270702362
Batch  92  loss:  0.14612606167793274
Batch  93  loss:  0.1427212655544281
Batch  94  loss:  0.14398299157619476
Batch  95  loss:  0.14402176439762115
Batch  96  loss:  0.1436014175415039
Batch  97  loss:  0.14380958676338196
Batch  98  loss:  0.14727681875228882
Batch  99  loss:  0.14403985440731049
Batch  100  loss:  0.14304985105991364
Validation: 
LOSS train 0.14400871649384497, val 0.14341311156749725
EPOCH : 8 TIME:  2022-04-19 05:25:35.473034
Training: 
Batch  1  loss:  0.14486704766750336
Batch  2  loss:  0.14479006826877594
Batch  3  loss:  0.1451244354248047
Batch  4  loss:  0.14560531079769135
Batch  5  loss:  0.141383096575737
Batch  6  loss:  0.14534805715084076
Batch  7  loss:  0.14445272088050842
Batch  8  loss:  0.14543354511260986
Batch  9  loss:  0.1435321569442749
Batch  10  loss:  0.14688460528850555
Batch  11  loss:  0.14487914741039276
Batch  12  loss:  0.1446448117494583
Batch  13  loss:  0.14503969252109528
Batch  14  loss:  0.14464278519153595
Batch  15  loss:  0.1436242312192917
Batch  16  loss:  0.1402381807565689
Batch  17  loss:  0.1429688185453415
Batch  18  loss:  0.14398226141929626
Batch  19  loss:  0.1457342654466629
Batch  20  loss:  0.14447279274463654
Batch  21  loss:  0.14266961812973022
Batch  22  loss:  0.14307352900505066
Batch  23  loss:  0.1446758210659027
Batch  24  loss:  0.14443188905715942
Batch  25  loss:  0.14235767722129822
Batch  26  loss:  0.1450653076171875
Batch  27  loss:  0.14420805871486664
Batch  28  loss:  0.1440129578113556
Batch  29  loss:  0.1433929055929184
Batch  30  loss:  0.1433040350675583
Batch  31  loss:  0.14395968616008759
Batch  32  loss:  0.14389891922473907
Batch  33  loss:  0.1447610855102539
Batch  34  loss:  0.14432944357395172
Batch  35  loss:  0.14424856007099152
Batch  36  loss:  0.1424422413110733
Batch  37  loss:  0.1431957483291626
Batch  38  loss:  0.1421753615140915
Batch  39  loss:  0.1459493488073349
Batch  40  loss:  0.14359422028064728
Batch  41  loss:  0.14366182684898376
Batch  42  loss:  0.1465296894311905
Batch  43  loss:  0.14510855078697205
Batch  44  loss:  0.14386023581027985
Batch  45  loss:  0.14435437321662903
Batch  46  loss:  0.14474686980247498
Batch  47  loss:  0.1402978003025055
Batch  48  loss:  0.14358647167682648
Batch  49  loss:  0.14393718540668488
Batch  50  loss:  0.1454320251941681
Batch  51  loss:  0.14129938185214996
Batch  52  loss:  0.14305676519870758
Batch  53  loss:  0.14332835376262665
Batch  54  loss:  0.14163772761821747
Batch  55  loss:  0.14266876876354218
Batch  56  loss:  0.1447013020515442
Batch  57  loss:  0.14334703981876373
Batch  58  loss:  0.14416877925395966
Batch  59  loss:  0.142893448472023
Batch  60  loss:  0.14423400163650513
Batch  61  loss:  0.1428605616092682
Batch  62  loss:  0.14425382018089294
Batch  63  loss:  0.14680172502994537
Batch  64  loss:  0.14456355571746826
Batch  65  loss:  0.14618800580501556
Batch  66  loss:  0.14182202517986298
Batch  67  loss:  0.1452113538980484
Batch  68  loss:  0.14404863119125366
Batch  69  loss:  0.14271330833435059
Batch  70  loss:  0.14659476280212402
Batch  71  loss:  0.14385464787483215
Batch  72  loss:  0.14189419150352478
Batch  73  loss:  0.14559924602508545
Batch  74  loss:  0.1447969377040863
Batch  75  loss:  0.14312440156936646
Batch  76  loss:  0.1456863135099411
Batch  77  loss:  0.14418810606002808
Batch  78  loss:  0.14189539849758148
Batch  79  loss:  0.143313929438591
Batch  80  loss:  0.14230258762836456
Batch  81  loss:  0.1446073055267334
Batch  82  loss:  0.14258113503456116
Batch  83  loss:  0.14099906384944916
Batch  84  loss:  0.14459919929504395
Batch  85  loss:  0.1441642791032791
Batch  86  loss:  0.14588172733783722
Batch  87  loss:  0.14356616139411926
Batch  88  loss:  0.1431145817041397
Batch  89  loss:  0.14394602179527283
Batch  90  loss:  0.14585532248020172
Batch  91  loss:  0.14236994087696075
Batch  92  loss:  0.14287596940994263
Batch  93  loss:  0.14712253212928772
Batch  94  loss:  0.14492250978946686
Batch  95  loss:  0.14585211873054504
Batch  96  loss:  0.1457383781671524
Batch  97  loss:  0.1422535628080368
Batch  98  loss:  0.14387592673301697
Batch  99  loss:  0.14382882416248322
Batch  100  loss:  0.14534834027290344
Validation: 
LOSS train 0.14401461452245712, val 0.1420968770980835
EPOCH : 9 TIME:  2022-04-19 05:32:24.726644
Training: 
Batch  1  loss:  0.14157237112522125
Batch  2  loss:  0.14524929225444794
Batch  3  loss:  0.1434171199798584
Batch  4  loss:  0.1451217085123062
Batch  5  loss:  0.14570827782154083
Batch  6  loss:  0.1424175351858139
Batch  7  loss:  0.14400212466716766
Batch  8  loss:  0.1432895064353943
Batch  9  loss:  0.1444862335920334
Batch  10  loss:  0.14355939626693726
Batch  11  loss:  0.1389210820198059
Batch  12  loss:  0.1458224207162857
Batch  13  loss:  0.14147599041461945
Batch  14  loss:  0.14320331811904907
Batch  15  loss:  0.14227332174777985
Batch  16  loss:  0.14440694451332092
Batch  17  loss:  0.14642833173274994
Batch  18  loss:  0.14417684078216553
Batch  19  loss:  0.14492526650428772
Batch  20  loss:  0.14295083284378052
Batch  21  loss:  0.14328688383102417
Batch  22  loss:  0.1432265192270279
Batch  23  loss:  0.14342184364795685
Batch  24  loss:  0.14349588751792908
Batch  25  loss:  0.1452992558479309
Batch  26  loss:  0.14637480676174164
Batch  27  loss:  0.1457347273826599
Batch  28  loss:  0.1456400603055954
Batch  29  loss:  0.14186082780361176
Batch  30  loss:  0.14408980309963226
Batch  31  loss:  0.14601755142211914
Batch  32  loss:  0.14289848506450653
Batch  33  loss:  0.143752783536911
Batch  34  loss:  0.1457250714302063
Batch  35  loss:  0.14321574568748474
Batch  36  loss:  0.14501439034938812
Batch  37  loss:  0.14788442850112915
Batch  38  loss:  0.14537806808948517
Batch  39  loss:  0.14461641013622284
Batch  40  loss:  0.14322778582572937
Batch  41  loss:  0.1460915207862854
Batch  42  loss:  0.14380629360675812
Batch  43  loss:  0.14387567341327667
Batch  44  loss:  0.14340591430664062
Batch  45  loss:  0.14366348087787628
Batch  46  loss:  0.14240103960037231
Batch  47  loss:  0.14347980916500092
Batch  48  loss:  0.14478617906570435
Batch  49  loss:  0.14370006322860718
Batch  50  loss:  0.14347417652606964
Batch  51  loss:  0.14323550462722778
Batch  52  loss:  0.14234155416488647
Batch  53  loss:  0.1455450803041458
Batch  54  loss:  0.14251866936683655
Batch  55  loss:  0.14279018342494965
Batch  56  loss:  0.14312948286533356
Batch  57  loss:  0.1441401094198227
Batch  58  loss:  0.1436532586812973
Batch  59  loss:  0.1446751207113266
Batch  60  loss:  0.14444120228290558
Batch  61  loss:  0.1456647515296936
Batch  62  loss:  0.14203615486621857
Batch  63  loss:  0.14484699070453644
Batch  64  loss:  0.14204654097557068
Batch  65  loss:  0.1419815868139267
Batch  66  loss:  0.14374375343322754
Batch  67  loss:  0.14387477934360504
Batch  68  loss:  0.14363433420658112
Batch  69  loss:  0.14299647510051727
Batch  70  loss:  0.14429129660129547
Batch  71  loss:  0.14452628791332245
Batch  72  loss:  0.1437976062297821
Batch  73  loss:  0.1429145634174347
Batch  74  loss:  0.1467624306678772
Batch  75  loss:  0.14306892454624176
Batch  76  loss:  0.14459505677223206
Batch  77  loss:  0.14401182532310486
Batch  78  loss:  0.14252886176109314
Batch  79  loss:  0.14500579237937927
Batch  80  loss:  0.14336936175823212
Batch  81  loss:  0.14351864159107208
Batch  82  loss:  0.14149856567382812
Batch  83  loss:  0.14484882354736328
Batch  84  loss:  0.14178727567195892
Batch  85  loss:  0.14355914294719696
Batch  86  loss:  0.14459142088890076
Batch  87  loss:  0.1442318856716156
Batch  88  loss:  0.14426584541797638
Batch  89  loss:  0.14506717026233673
Batch  90  loss:  0.14612877368927002
Batch  91  loss:  0.1441335827112198
Batch  92  loss:  0.14656424522399902
Batch  93  loss:  0.14390721917152405
Batch  94  loss:  0.14553725719451904
Batch  95  loss:  0.1455138772726059
Batch  96  loss:  0.14238986372947693
Batch  97  loss:  0.14326350390911102
Batch  98  loss:  0.1437206268310547
Batch  99  loss:  0.14321748912334442
Batch  100  loss:  0.14312998950481415
Validation: 
LOSS train 0.14395296141505243, val 0.1450650691986084
EPOCH : 10 TIME:  2022-04-19 05:39:13.792492
Training: 
Batch  1  loss:  0.14480827748775482
Batch  2  loss:  0.14481057226657867
Batch  3  loss:  0.14300866425037384
Batch  4  loss:  0.14183247089385986
Batch  5  loss:  0.14367082715034485
Batch  6  loss:  0.14479051530361176
Batch  7  loss:  0.14382889866828918
Batch  8  loss:  0.14629140496253967
Batch  9  loss:  0.1438893973827362
Batch  10  loss:  0.1431388407945633
Batch  11  loss:  0.14143452048301697
Batch  12  loss:  0.1451084166765213
Batch  13  loss:  0.14226169884204865
Batch  14  loss:  0.14212638139724731
Batch  15  loss:  0.1447249799966812
Batch  16  loss:  0.14308330416679382
Batch  17  loss:  0.1451595276594162
Batch  18  loss:  0.14446786046028137
Batch  19  loss:  0.14130999147891998
Batch  20  loss:  0.14311166107654572
Batch  21  loss:  0.14334073662757874
Batch  22  loss:  0.1437448263168335
Batch  23  loss:  0.14559994637966156
Batch  24  loss:  0.14213398098945618
Batch  25  loss:  0.14374643564224243
Batch  26  loss:  0.1453922986984253
Batch  27  loss:  0.1414903849363327
Batch  28  loss:  0.145211860537529
Batch  29  loss:  0.1433040350675583
Batch  30  loss:  0.14478683471679688
Batch  31  loss:  0.14266765117645264
Batch  32  loss:  0.14573374390602112
Batch  33  loss:  0.14667999744415283
Batch  34  loss:  0.14569191634655
Batch  35  loss:  0.14323997497558594
Batch  36  loss:  0.1466348022222519
Batch  37  loss:  0.14262951910495758
Batch  38  loss:  0.14552272856235504
Batch  39  loss:  0.1450202465057373
Batch  40  loss:  0.1483955979347229
Batch  41  loss:  0.14220671355724335
Batch  42  loss:  0.14253336191177368
Batch  43  loss:  0.14403921365737915
Batch  44  loss:  0.14242751896381378
Batch  45  loss:  0.14515787363052368
Batch  46  loss:  0.14431796967983246
Batch  47  loss:  0.14368836581707
Batch  48  loss:  0.1447174996137619
Batch  49  loss:  0.14578813314437866
Batch  50  loss:  0.14180679619312286
Batch  51  loss:  0.14202937483787537
Batch  52  loss:  0.14654278755187988
Batch  53  loss:  0.14133936166763306
Batch  54  loss:  0.14494161307811737
Batch  55  loss:  0.1449500471353531
Batch  56  loss:  0.14313311874866486
Batch  57  loss:  0.14438237249851227
Batch  58  loss:  0.14457489550113678
Batch  59  loss:  0.14410486817359924
Batch  60  loss:  0.14652666449546814
Batch  61  loss:  0.14452789723873138
Batch  62  loss:  0.1456434726715088
Batch  63  loss:  0.1429370641708374
Batch  64  loss:  0.14161811769008636
Batch  65  loss:  0.14345918595790863
Batch  66  loss:  0.1428031474351883
Batch  67  loss:  0.1461578905582428
Batch  68  loss:  0.14266343414783478
Batch  69  loss:  0.14586177468299866
Batch  70  loss:  0.14187560975551605
Batch  71  loss:  0.14480243623256683
Batch  72  loss:  0.1432061791419983
Batch  73  loss:  0.1446932703256607
Batch  74  loss:  0.14448201656341553
Batch  75  loss:  0.1423875242471695
Batch  76  loss:  0.14223569631576538
Batch  77  loss:  0.14351655542850494
Batch  78  loss:  0.14536328613758087
Batch  79  loss:  0.14406557381153107
Batch  80  loss:  0.14181694388389587
Batch  81  loss:  0.14287281036376953
Batch  82  loss:  0.1427312195301056
Batch  83  loss:  0.14242403209209442
Batch  84  loss:  0.14129769802093506
Batch  85  loss:  0.14468400180339813
Batch  86  loss:  0.14394497871398926
Batch  87  loss:  0.14415179193019867
Batch  88  loss:  0.14286445081233978
Batch  89  loss:  0.144718199968338
Batch  90  loss:  0.14484979212284088
Batch  91  loss:  0.14195479452610016
Batch  92  loss:  0.14574144780635834
Batch  93  loss:  0.14300847053527832
Batch  94  loss:  0.14358468353748322
Batch  95  loss:  0.14275327324867249
Batch  96  loss:  0.14117781817913055
Batch  97  loss:  0.14303924143314362
Batch  98  loss:  0.14494024217128754
Batch  99  loss:  0.14475707709789276
Batch  100  loss:  0.14460358023643494
Validation: 
LOSS train 0.143872509598732, val 0.14623986184597015
EPOCH : 11 TIME:  2022-04-19 05:45:55.977482
Training: 
Batch  1  loss:  0.14241020381450653
Batch  2  loss:  0.14332737028598785
Batch  3  loss:  0.1460331678390503
Batch  4  loss:  0.14556272327899933
Batch  5  loss:  0.14589475095272064
Batch  6  loss:  0.14517274498939514
Batch  7  loss:  0.14313001930713654
Batch  8  loss:  0.14466239511966705
Batch  9  loss:  0.14168128371238708
Batch  10  loss:  0.14275558292865753
Batch  11  loss:  0.14190281927585602
Batch  12  loss:  0.14214493334293365
Batch  13  loss:  0.14329160749912262
Batch  14  loss:  0.14416709542274475
Batch  15  loss:  0.1449570506811142
Batch  16  loss:  0.14660942554473877
Batch  17  loss:  0.1428169310092926
Batch  18  loss:  0.14309711754322052
Batch  19  loss:  0.1437988579273224
Batch  20  loss:  0.14337706565856934
Batch  21  loss:  0.1461832970380783
Batch  22  loss:  0.14385734498500824
Batch  23  loss:  0.14826078712940216
Batch  24  loss:  0.14529894292354584
Batch  25  loss:  0.14500083029270172
Batch  26  loss:  0.14396429061889648
Batch  27  loss:  0.14434923231601715
Batch  28  loss:  0.14507488906383514
Batch  29  loss:  0.1437843143939972
Batch  30  loss:  0.14539329707622528
Batch  31  loss:  0.14448444545269012
Batch  32  loss:  0.14345033466815948
Batch  33  loss:  0.14268113672733307
Batch  34  loss:  0.14474749565124512
Batch  35  loss:  0.1437874436378479
Batch  36  loss:  0.14326757192611694
Batch  37  loss:  0.1446753591299057
Batch  38  loss:  0.14075584709644318
Batch  39  loss:  0.1433863490819931
Batch  40  loss:  0.14352653920650482
Batch  41  loss:  0.1441708654165268
Batch  42  loss:  0.14443574845790863
Batch  43  loss:  0.14257502555847168
Batch  44  loss:  0.1457410305738449
Batch  45  loss:  0.1446002572774887
Batch  46  loss:  0.1457696557044983
Batch  47  loss:  0.14451448619365692
Batch  48  loss:  0.14420416951179504
Batch  49  loss:  0.14565713703632355
Batch  50  loss:  0.14614294469356537
Batch  51  loss:  0.14468544721603394
Batch  52  loss:  0.1434396207332611
Batch  53  loss:  0.1455216407775879
Batch  54  loss:  0.14476530253887177
Batch  55  loss:  0.14377078413963318
Batch  56  loss:  0.14443883299827576
Batch  57  loss:  0.1435329020023346
Batch  58  loss:  0.14404797554016113
Batch  59  loss:  0.14657296240329742
Batch  60  loss:  0.1471022367477417
Batch  61  loss:  0.14438369870185852
Batch  62  loss:  0.1417963206768036
Batch  63  loss:  0.14397898316383362
Batch  64  loss:  0.142501100897789
Batch  65  loss:  0.1419074833393097
Batch  66  loss:  0.14272110164165497
Batch  67  loss:  0.14530536532402039
Batch  68  loss:  0.14276540279388428
Batch  69  loss:  0.14294062554836273
Batch  70  loss:  0.14240509271621704
Batch  71  loss:  0.1422290951013565
Batch  72  loss:  0.14180199801921844
Batch  73  loss:  0.1409347504377365
Batch  74  loss:  0.14443491399288177
Batch  75  loss:  0.14716275036334991
Batch  76  loss:  0.14476697146892548
Batch  77  loss:  0.1424146592617035
Batch  78  loss:  0.14500631392002106
Batch  79  loss:  0.14258138835430145
Batch  80  loss:  0.142489492893219
Batch  81  loss:  0.14527131617069244
Batch  82  loss:  0.1445399820804596
Batch  83  loss:  0.1428709775209427
Batch  84  loss:  0.14541125297546387
Batch  85  loss:  0.14394895732402802
Batch  86  loss:  0.14400020241737366
Batch  87  loss:  0.1394333690404892
Batch  88  loss:  0.1446642428636551
Batch  89  loss:  0.14376871287822723
Batch  90  loss:  0.14399254322052002
Batch  91  loss:  0.14366306364536285
Batch  92  loss:  0.1422877460718155
Batch  93  loss:  0.14316187798976898
Batch  94  loss:  0.14453434944152832
Batch  95  loss:  0.14475266635417938
Batch  96  loss:  0.14288780093193054
Batch  97  loss:  0.1445879489183426
Batch  98  loss:  0.14531227946281433
Batch  99  loss:  0.1435515433549881
Batch  100  loss:  0.14284288883209229
Validation: 
LOSS train 0.14398423150181772, val 0.14393965899944305
EPOCH : 12 TIME:  2022-04-19 05:52:45.572051
Training: 
Batch  1  loss:  0.1438525915145874
Batch  2  loss:  0.14482776820659637
Batch  3  loss:  0.14451757073402405
Batch  4  loss:  0.14247407019138336
Batch  5  loss:  0.1427178680896759
Batch  6  loss:  0.14365872740745544
Batch  7  loss:  0.14332853257656097
Batch  8  loss:  0.14399667084217072
Batch  9  loss:  0.1443512886762619
Batch  10  loss:  0.14568212628364563
Batch  11  loss:  0.1434563398361206
Batch  12  loss:  0.1436343640089035
Batch  13  loss:  0.1413622945547104
Batch  14  loss:  0.14344391226768494
Batch  15  loss:  0.14585131406784058
Batch  16  loss:  0.1441958248615265
Batch  17  loss:  0.14391295611858368
Batch  18  loss:  0.14334844052791595
Batch  19  loss:  0.14326098561286926
Batch  20  loss:  0.1434154212474823
Batch  21  loss:  0.14708206057548523
Batch  22  loss:  0.14240318536758423
Batch  23  loss:  0.14501728117465973
Batch  24  loss:  0.1418718844652176
Batch  25  loss:  0.1429377943277359
Batch  26  loss:  0.14392845332622528
Batch  27  loss:  0.14052210748195648
Batch  28  loss:  0.14337287843227386
Batch  29  loss:  0.14537562429904938
Batch  30  loss:  0.14398126304149628
Batch  31  loss:  0.1443088948726654
Batch  32  loss:  0.14514774084091187
Batch  33  loss:  0.1460132896900177
Batch  34  loss:  0.14271754026412964
Batch  35  loss:  0.1436622142791748
Batch  36  loss:  0.14359699189662933
Batch  37  loss:  0.14155016839504242
Batch  38  loss:  0.1428358405828476
Batch  39  loss:  0.1447383016347885
Batch  40  loss:  0.14501357078552246
Batch  41  loss:  0.1437935084104538
Batch  42  loss:  0.14661206305027008
Batch  43  loss:  0.14342361688613892
Batch  44  loss:  0.1432904154062271
Batch  45  loss:  0.14409731328487396
Batch  46  loss:  0.14543652534484863
Batch  47  loss:  0.1416378617286682
Batch  48  loss:  0.14404116570949554
Batch  49  loss:  0.14571836590766907
Batch  50  loss:  0.1419239044189453
Batch  51  loss:  0.14615237712860107
Batch  52  loss:  0.14489047229290009
Batch  53  loss:  0.14349792897701263
Batch  54  loss:  0.14655549824237823
Batch  55  loss:  0.1419728398323059
Batch  56  loss:  0.14167341589927673
Batch  57  loss:  0.14179940521717072
Batch  58  loss:  0.14136217534542084
Batch  59  loss:  0.14458809792995453
Batch  60  loss:  0.14293533563613892
Batch  61  loss:  0.14369864761829376
Batch  62  loss:  0.14003285765647888
Batch  63  loss:  0.14256015419960022
Batch  64  loss:  0.14297817647457123
Batch  65  loss:  0.1431649923324585
Batch  66  loss:  0.14238415658473969
Batch  67  loss:  0.1440892368555069
Batch  68  loss:  0.14056766033172607
Batch  69  loss:  0.1433132290840149
Batch  70  loss:  0.14403828978538513
Batch  71  loss:  0.14199014008045197
Batch  72  loss:  0.14356650412082672
Batch  73  loss:  0.14286215603351593
Batch  74  loss:  0.1435721218585968
Batch  75  loss:  0.14176052808761597
Batch  76  loss:  0.14423707127571106
Batch  77  loss:  0.14368782937526703
Batch  78  loss:  0.14314323663711548
Batch  79  loss:  0.14277352392673492
Batch  80  loss:  0.14548125863075256
Batch  81  loss:  0.14295218884944916
Batch  82  loss:  0.14359930157661438
Batch  83  loss:  0.1454799473285675
Batch  84  loss:  0.14375247061252594
Batch  85  loss:  0.14214439690113068
Batch  86  loss:  0.14300990104675293
Batch  87  loss:  0.14380000531673431
Batch  88  loss:  0.14643384516239166
Batch  89  loss:  0.1443863958120346
Batch  90  loss:  0.1452072262763977
Batch  91  loss:  0.1438862681388855
Batch  92  loss:  0.14456205070018768
Batch  93  loss:  0.14504142105579376
Batch  94  loss:  0.14200088381767273
Batch  95  loss:  0.1442645937204361
Batch  96  loss:  0.14505191147327423
Batch  97  loss:  0.1431608945131302
Batch  98  loss:  0.1420290321111679
Batch  99  loss:  0.1434577852487564
Batch  100  loss:  0.1427835375070572
Validation: 
LOSS train 0.1436367426812649, val 0.141365185379982
EPOCH : 13 TIME:  2022-04-19 05:59:29.350236
Training: 
Batch  1  loss:  0.1447719782590866
Batch  2  loss:  0.14378628134727478
Batch  3  loss:  0.14377281069755554
Batch  4  loss:  0.1431342214345932
Batch  5  loss:  0.14380113780498505
Batch  6  loss:  0.14449167251586914
Batch  7  loss:  0.14118129014968872
Batch  8  loss:  0.14330171048641205
Batch  9  loss:  0.14456306397914886
Batch  10  loss:  0.14202775061130524
Batch  11  loss:  0.1436057686805725
Batch  12  loss:  0.14566710591316223
Batch  13  loss:  0.1446567326784134
Batch  14  loss:  0.14572669565677643
Batch  15  loss:  0.143400177359581
Batch  16  loss:  0.1427839696407318
Batch  17  loss:  0.14674364030361176
Batch  18  loss:  0.14377865195274353
Batch  19  loss:  0.14284813404083252
Batch  20  loss:  0.14222452044487
Batch  21  loss:  0.14458894729614258
Batch  22  loss:  0.14517882466316223
Batch  23  loss:  0.1425301730632782
Batch  24  loss:  0.14388512074947357
Batch  25  loss:  0.14254716038703918
Batch  26  loss:  0.14384829998016357
Batch  27  loss:  0.13990633189678192
Batch  28  loss:  0.1437613070011139
Batch  29  loss:  0.14502084255218506
Batch  30  loss:  0.143233522772789
Batch  31  loss:  0.144910991191864
Batch  32  loss:  0.14633549749851227
Batch  33  loss:  0.14429223537445068
Batch  34  loss:  0.1449599266052246
Batch  35  loss:  0.14552880823612213
Batch  36  loss:  0.14115679264068604
Batch  37  loss:  0.1450304239988327
Batch  38  loss:  0.1415289044380188
Batch  39  loss:  0.14737959206104279
Batch  40  loss:  0.1433754712343216
Batch  41  loss:  0.14390413463115692
Batch  42  loss:  0.14519143104553223
Batch  43  loss:  0.14303696155548096
Batch  44  loss:  0.14262589812278748
Batch  45  loss:  0.14425522089004517
Batch  46  loss:  0.14420661330223083
Batch  47  loss:  0.14467860758304596
Batch  48  loss:  0.14355610311031342
Batch  49  loss:  0.14424915611743927
Batch  50  loss:  0.14418144524097443
Batch  51  loss:  0.1446170061826706
Batch  52  loss:  0.1429726779460907
Batch  53  loss:  0.143998920917511
Batch  54  loss:  0.1456144154071808
Batch  55  loss:  0.1451614499092102
Batch  56  loss:  0.14454923570156097
Batch  57  loss:  0.1447746753692627
Batch  58  loss:  0.14425721764564514
Batch  59  loss:  0.14279067516326904
Batch  60  loss:  0.14415988326072693
Batch  61  loss:  0.14464476704597473
Batch  62  loss:  0.14098869264125824
Batch  63  loss:  0.14264589548110962
Batch  64  loss:  0.14191484451293945
Batch  65  loss:  0.14327050745487213
Batch  66  loss:  0.14371204376220703
Batch  67  loss:  0.1449882984161377
Batch  68  loss:  0.14088477194309235
Batch  69  loss:  0.14450761675834656
Batch  70  loss:  0.14412303268909454
Batch  71  loss:  0.1433211863040924
Batch  72  loss:  0.14554311335086823
Batch  73  loss:  0.14414049685001373
Batch  74  loss:  0.1444740891456604
Batch  75  loss:  0.143271341919899
Batch  76  loss:  0.14703044295310974
Batch  77  loss:  0.14252914488315582
Batch  78  loss:  0.14411690831184387
Batch  79  loss:  0.1428137570619583
Batch  80  loss:  0.14606505632400513
Batch  81  loss:  0.1428605616092682
Batch  82  loss:  0.1425158679485321
Batch  83  loss:  0.14310497045516968
Batch  84  loss:  0.14385809004306793
Batch  85  loss:  0.14332315325737
Batch  86  loss:  0.14253269135951996
Batch  87  loss:  0.14484013617038727
Batch  88  loss:  0.14330363273620605
Batch  89  loss:  0.14351819455623627
Batch  90  loss:  0.1432117223739624
Batch  91  loss:  0.142792209982872
Batch  92  loss:  0.14318637549877167
Batch  93  loss:  0.14355647563934326
Batch  94  loss:  0.14376884698867798
Batch  95  loss:  0.14264264702796936
Batch  96  loss:  0.14567147195339203
Batch  97  loss:  0.1443013995885849
Batch  98  loss:  0.14458613097667694
Batch  99  loss:  0.14234809577465057
Batch  100  loss:  0.14357000589370728
Validation: 
LOSS train 0.14382526904344559, val 0.14329387247562408
EPOCH : 14 TIME:  2022-04-19 06:06:11.786160
Training: 
Batch  1  loss:  0.14287668466567993
Batch  2  loss:  0.1420256644487381
Batch  3  loss:  0.1459604650735855
Batch  4  loss:  0.14523504674434662
Batch  5  loss:  0.14303070306777954
Batch  6  loss:  0.14232730865478516
Batch  7  loss:  0.14166614413261414
Batch  8  loss:  0.14666754007339478
Batch  9  loss:  0.1436937153339386
Batch  10  loss:  0.14534258842468262
Batch  11  loss:  0.14324723184108734
Batch  12  loss:  0.1424373984336853
Batch  13  loss:  0.14270150661468506
Batch  14  loss:  0.14432033896446228
Batch  15  loss:  0.1436958611011505
Batch  16  loss:  0.14418332278728485
Batch  17  loss:  0.1452249139547348
Batch  18  loss:  0.1441267877817154
Batch  19  loss:  0.14414627850055695
Batch  20  loss:  0.1434331238269806
Batch  21  loss:  0.14545923471450806
Batch  22  loss:  0.14471372961997986
Batch  23  loss:  0.14207294583320618
Batch  24  loss:  0.1453607678413391
Batch  25  loss:  0.1444340944290161
Batch  26  loss:  0.14012165367603302
Batch  27  loss:  0.1448562741279602
Batch  28  loss:  0.1430787295103073
Batch  29  loss:  0.14098763465881348
Batch  30  loss:  0.14224016666412354
Batch  31  loss:  0.14413993060588837
Batch  32  loss:  0.14323991537094116
Batch  33  loss:  0.14281252026557922
Batch  34  loss:  0.1431252807378769
Batch  35  loss:  0.14445579051971436
Batch  36  loss:  0.1440998613834381
Batch  37  loss:  0.14151494204998016
Batch  38  loss:  0.1463794857263565
Batch  39  loss:  0.14479979872703552
Batch  40  loss:  0.14297057688236237
Batch  41  loss:  0.1452026516199112
Batch  42  loss:  0.14509619772434235
Batch  43  loss:  0.1457342654466629
Batch  44  loss:  0.14326581358909607
Batch  45  loss:  0.14523553848266602
Batch  46  loss:  0.14240776002407074
Batch  47  loss:  0.14030906558036804
Batch  48  loss:  0.14429795742034912
Batch  49  loss:  0.14506326615810394
Batch  50  loss:  0.14254753291606903
Batch  51  loss:  0.14352856576442719
Batch  52  loss:  0.14139346778392792
Batch  53  loss:  0.14258810877799988
Batch  54  loss:  0.14275233447551727
Batch  55  loss:  0.14234468340873718
Batch  56  loss:  0.1445852667093277
Batch  57  loss:  0.14409427344799042
Batch  58  loss:  0.14377765357494354
Batch  59  loss:  0.14169685542583466
Batch  60  loss:  0.14504478871822357
Batch  61  loss:  0.14335264265537262
Batch  62  loss:  0.1452246755361557
Batch  63  loss:  0.1430637091398239
Batch  64  loss:  0.14219139516353607
Batch  65  loss:  0.14222916960716248
Batch  66  loss:  0.1433512419462204
Batch  67  loss:  0.14645251631736755
Batch  68  loss:  0.14235028624534607
Batch  69  loss:  0.14631281793117523
Batch  70  loss:  0.14350415766239166
Batch  71  loss:  0.1435861587524414
Batch  72  loss:  0.14210155606269836
Batch  73  loss:  0.14332304894924164
Batch  74  loss:  0.14212177693843842
Batch  75  loss:  0.144334077835083
Batch  76  loss:  0.1437864601612091
Batch  77  loss:  0.14307506382465363
Batch  78  loss:  0.14324109256267548
Batch  79  loss:  0.1435118466615677
Batch  80  loss:  0.14274273812770844
Batch  81  loss:  0.14417658746242523
Batch  82  loss:  0.1444055140018463
Batch  83  loss:  0.14326506853103638
Batch  84  loss:  0.14240701496601105
Batch  85  loss:  0.1458704024553299
Batch  86  loss:  0.14475971460342407
Batch  87  loss:  0.14312930405139923
Batch  88  loss:  0.14251995086669922
Batch  89  loss:  0.14387787878513336
Batch  90  loss:  0.1446707844734192
Batch  91  loss:  0.1436387002468109
Batch  92  loss:  0.14302711188793182
Batch  93  loss:  0.14289142191410065
Batch  94  loss:  0.1425704061985016
Batch  95  loss:  0.14320755004882812
Batch  96  loss:  0.14355604350566864
Batch  97  loss:  0.14387524127960205
Batch  98  loss:  0.14384545385837555
Batch  99  loss:  0.14557825028896332
Batch  100  loss:  0.14393697679042816
Validation: 
LOSS train 0.1436323781311512, val 0.14551995694637299
EPOCH : 15 TIME:  2022-04-19 06:13:02.809061
Training: 
Batch  1  loss:  0.14303193986415863
Batch  2  loss:  0.14505897462368011
Batch  3  loss:  0.1425868421792984
Batch  4  loss:  0.14620961248874664
Batch  5  loss:  0.14516103267669678
Batch  6  loss:  0.1457696557044983
Batch  7  loss:  0.14233769476413727
Batch  8  loss:  0.14499568939208984
Batch  9  loss:  0.14347003400325775
Batch  10  loss:  0.14418557286262512
Batch  11  loss:  0.1431041657924652
Batch  12  loss:  0.1446179300546646
Batch  13  loss:  0.14531564712524414
Batch  14  loss:  0.142279714345932
Batch  15  loss:  0.1451282799243927
Batch  16  loss:  0.14501053094863892
Batch  17  loss:  0.14371401071548462
Batch  18  loss:  0.1429498791694641
Batch  19  loss:  0.14557038247585297
Batch  20  loss:  0.14362916350364685
Batch  21  loss:  0.14464309811592102
Batch  22  loss:  0.14576739072799683
Batch  23  loss:  0.1436813920736313
Batch  24  loss:  0.14487576484680176
Batch  25  loss:  0.1427336186170578
Batch  26  loss:  0.14363163709640503
Batch  27  loss:  0.14388449490070343
Batch  28  loss:  0.14241376519203186
Batch  29  loss:  0.14444991946220398
Batch  30  loss:  0.14225457608699799
Batch  31  loss:  0.14407750964164734
Batch  32  loss:  0.14521583914756775
Batch  33  loss:  0.1432121992111206
Batch  34  loss:  0.14282028377056122
Batch  35  loss:  0.14120067656040192
Batch  36  loss:  0.14599718153476715
Batch  37  loss:  0.14265458285808563
Batch  38  loss:  0.14527568221092224
Batch  39  loss:  0.1433546394109726
Batch  40  loss:  0.14300571382045746
Batch  41  loss:  0.1413498818874359
Batch  42  loss:  0.1453554481267929
Batch  43  loss:  0.14528721570968628
Batch  44  loss:  0.14173777401447296
Batch  45  loss:  0.14497798681259155
Batch  46  loss:  0.14321064949035645
Batch  47  loss:  0.14309310913085938
Batch  48  loss:  0.14496824145317078
Batch  49  loss:  0.14495298266410828
Batch  50  loss:  0.14349691569805145
Batch  51  loss:  0.14330993592739105
Batch  52  loss:  0.1434236615896225
Batch  53  loss:  0.14159391820430756
Batch  54  loss:  0.14525499939918518
Batch  55  loss:  0.14360587298870087
Batch  56  loss:  0.14600501954555511
Batch  57  loss:  0.1448611170053482
Batch  58  loss:  0.14152878522872925
Batch  59  loss:  0.14406658709049225
Batch  60  loss:  0.14494623243808746
Batch  61  loss:  0.1445319652557373
Batch  62  loss:  0.14291530847549438
Batch  63  loss:  0.14257851243019104
Batch  64  loss:  0.14405567944049835
Batch  65  loss:  0.14381326735019684
Batch  66  loss:  0.14274293184280396
Batch  67  loss:  0.14470937848091125
Batch  68  loss:  0.14069490134716034
Batch  69  loss:  0.1434668004512787
Batch  70  loss:  0.14164073765277863
Batch  71  loss:  0.14460691809654236
Batch  72  loss:  0.14156629145145416
Batch  73  loss:  0.14659671485424042
Batch  74  loss:  0.1456075757741928
Batch  75  loss:  0.14562244713306427
Batch  76  loss:  0.1434229463338852
Batch  77  loss:  0.14752857387065887
Batch  78  loss:  0.1433853954076767
Batch  79  loss:  0.1428283303976059
Batch  80  loss:  0.1458578109741211
Batch  81  loss:  0.14576615393161774
Batch  82  loss:  0.14282242953777313
Batch  83  loss:  0.14271394908428192
Batch  84  loss:  0.14542821049690247
Batch  85  loss:  0.14183969795703888
Batch  86  loss:  0.1412302702665329
Batch  87  loss:  0.14154618978500366
Batch  88  loss:  0.14423832297325134
Batch  89  loss:  0.1440630555152893
Batch  90  loss:  0.14238815009593964
Batch  91  loss:  0.14302890002727509
Batch  92  loss:  0.1449422836303711
Batch  93  loss:  0.1422404646873474
Batch  94  loss:  0.1451096534729004
Batch  95  loss:  0.1413070261478424
Batch  96  loss:  0.14211007952690125
Batch  97  loss:  0.1447727531194687
Batch  98  loss:  0.1444881558418274
Batch  99  loss:  0.14415712654590607
Batch  100  loss:  0.14182382822036743
Validation: 
LOSS train 0.14380492240190507, val 0.14390644431114197
EPOCH : 16 TIME:  2022-04-19 06:19:51.368806
Training: 
Batch  1  loss:  0.1444488912820816
Batch  2  loss:  0.14515912532806396
Batch  3  loss:  0.14320732653141022
Batch  4  loss:  0.14419671893119812
Batch  5  loss:  0.1413903832435608
Batch  6  loss:  0.14426353573799133
Batch  7  loss:  0.1426241546869278
Batch  8  loss:  0.14454659819602966
Batch  9  loss:  0.14541970193386078
Batch  10  loss:  0.14375610649585724
Batch  11  loss:  0.14232195913791656
Batch  12  loss:  0.1446237415075302
Batch  13  loss:  0.14316964149475098
Batch  14  loss:  0.14516744017601013
Batch  15  loss:  0.14332126080989838
Batch  16  loss:  0.14735278487205505
Batch  17  loss:  0.14468644559383392
Batch  18  loss:  0.14376167953014374
Batch  19  loss:  0.14282234013080597
Batch  20  loss:  0.14463366568088531
Batch  21  loss:  0.14428004622459412
Batch  22  loss:  0.1438579112291336
Batch  23  loss:  0.1461513638496399
Batch  24  loss:  0.14309313893318176
Batch  25  loss:  0.14591418206691742
Batch  26  loss:  0.14425238966941833
Batch  27  loss:  0.14408037066459656
Batch  28  loss:  0.1435771882534027
Batch  29  loss:  0.14312654733657837
Batch  30  loss:  0.14476275444030762
Batch  31  loss:  0.14479447901248932
Batch  32  loss:  0.1438496857881546
Batch  33  loss:  0.14288978278636932
Batch  34  loss:  0.14297978579998016
Batch  35  loss:  0.1424175649881363
Batch  36  loss:  0.14433664083480835
Batch  37  loss:  0.14369255304336548
Batch  38  loss:  0.14370863139629364
Batch  39  loss:  0.14510121941566467
Batch  40  loss:  0.1461806297302246
Batch  41  loss:  0.14366719126701355
Batch  42  loss:  0.14376266300678253
Batch  43  loss:  0.14554910361766815
Batch  44  loss:  0.1462269425392151
Batch  45  loss:  0.14427560567855835
Batch  46  loss:  0.14402629435062408
Batch  47  loss:  0.14031876623630524
Batch  48  loss:  0.14325225353240967
Batch  49  loss:  0.14395061135292053
Batch  50  loss:  0.14384974539279938
Batch  51  loss:  0.14318689703941345
Batch  52  loss:  0.14370179176330566
Batch  53  loss:  0.1427207738161087
Batch  54  loss:  0.14372582733631134
Batch  55  loss:  0.14683231711387634
Batch  56  loss:  0.1449422687292099
Batch  57  loss:  0.1425689160823822
Batch  58  loss:  0.14328844845294952
Batch  59  loss:  0.14516021311283112
Batch  60  loss:  0.1419549286365509
Batch  61  loss:  0.14342981576919556
Batch  62  loss:  0.14325162768363953
Batch  63  loss:  0.1424323171377182
Batch  64  loss:  0.14573577046394348
Batch  65  loss:  0.14271675050258636
Batch  66  loss:  0.144788458943367
Batch  67  loss:  0.14517439901828766
Batch  68  loss:  0.14254096150398254
Batch  69  loss:  0.1433849334716797
Batch  70  loss:  0.14553068578243256
Batch  71  loss:  0.14123930037021637
Batch  72  loss:  0.14448779821395874
Batch  73  loss:  0.14053170382976532
Batch  74  loss:  0.14399002492427826
Batch  75  loss:  0.14156904816627502
Batch  76  loss:  0.1453595906496048
Batch  77  loss:  0.14544545114040375
Batch  78  loss:  0.14357595145702362
Batch  79  loss:  0.14447984099388123
Batch  80  loss:  0.14479711651802063
Batch  81  loss:  0.14231233298778534
Batch  82  loss:  0.14597010612487793
Batch  83  loss:  0.14312422275543213
Batch  84  loss:  0.1418740451335907
Batch  85  loss:  0.14206047356128693
Batch  86  loss:  0.14198791980743408
Batch  87  loss:  0.14187286794185638
Batch  88  loss:  0.14331871271133423
Batch  89  loss:  0.1451244205236435
Batch  90  loss:  0.14456036686897278
Batch  91  loss:  0.14110307395458221
Batch  92  loss:  0.14545507729053497
Batch  93  loss:  0.14593850076198578
Batch  94  loss:  0.14647386968135834
Batch  95  loss:  0.14479760825634003
Batch  96  loss:  0.14463350176811218
Batch  97  loss:  0.1424182802438736
Batch  98  loss:  0.14369507133960724
Batch  99  loss:  0.1437387317419052
Batch  100  loss:  0.14345791935920715
Validation: 
LOSS train 0.14389260575175286, val 0.14412006735801697
EPOCH : 17 TIME:  2022-04-19 06:26:34.057350
Training: 
Batch  1  loss:  0.14423424005508423
Batch  2  loss:  0.1442766934633255
Batch  3  loss:  0.1429065316915512
Batch  4  loss:  0.14125806093215942
Batch  5  loss:  0.1464402675628662
Batch  6  loss:  0.14453062415122986
Batch  7  loss:  0.14238005876541138
Batch  8  loss:  0.14351557195186615
Batch  9  loss:  0.14188669621944427
Batch  10  loss:  0.144058957695961
Batch  11  loss:  0.14410962164402008
Batch  12  loss:  0.14466053247451782
Batch  13  loss:  0.14448532462120056
Batch  14  loss:  0.14577719569206238
Batch  15  loss:  0.14437024295330048
Batch  16  loss:  0.14286351203918457
Batch  17  loss:  0.14511442184448242
Batch  18  loss:  0.14314238727092743
Batch  19  loss:  0.1426832228899002
Batch  20  loss:  0.14136861264705658
Batch  21  loss:  0.1459439992904663
Batch  22  loss:  0.14312270283699036
Batch  23  loss:  0.1433044672012329
Batch  24  loss:  0.1450825035572052
Batch  25  loss:  0.1459490805864334
Batch  26  loss:  0.14208589494228363
Batch  27  loss:  0.1425747275352478
Batch  28  loss:  0.14348739385604858
Batch  29  loss:  0.14390651881694794
Batch  30  loss:  0.14597088098526
Batch  31  loss:  0.1446058601140976
Batch  32  loss:  0.14404888451099396
Batch  33  loss:  0.14201579988002777
Batch  34  loss:  0.14389944076538086
Batch  35  loss:  0.14620797336101532
Batch  36  loss:  0.1449788510799408
Batch  37  loss:  0.14263147115707397
Batch  38  loss:  0.14430055022239685
Batch  39  loss:  0.142447367310524
Batch  40  loss:  0.14687523245811462
Batch  41  loss:  0.14214572310447693
Batch  42  loss:  0.14311160147190094
Batch  43  loss:  0.14526012539863586
Batch  44  loss:  0.14477942883968353
Batch  45  loss:  0.1438656598329544
Batch  46  loss:  0.14481285214424133
Batch  47  loss:  0.14522017538547516
Batch  48  loss:  0.14146552979946136
Batch  49  loss:  0.14346162974834442
Batch  50  loss:  0.141001358628273
Batch  51  loss:  0.143661230802536
Batch  52  loss:  0.14428767561912537
Batch  53  loss:  0.1437547206878662
Batch  54  loss:  0.1447906345129013
Batch  55  loss:  0.1431303471326828
Batch  56  loss:  0.13993440568447113
Batch  57  loss:  0.14415501058101654
Batch  58  loss:  0.14363877475261688
Batch  59  loss:  0.14505821466445923
Batch  60  loss:  0.14517594873905182
Batch  61  loss:  0.14430354535579681
Batch  62  loss:  0.14229273796081543
Batch  63  loss:  0.1445031762123108
Batch  64  loss:  0.14384084939956665
Batch  65  loss:  0.14654557406902313
Batch  66  loss:  0.1441415697336197
Batch  67  loss:  0.14377619326114655
Batch  68  loss:  0.14377868175506592
Batch  69  loss:  0.14358001947402954
Batch  70  loss:  0.14255183935165405
Batch  71  loss:  0.1439448595046997
Batch  72  loss:  0.1417837142944336
Batch  73  loss:  0.14397968351840973
Batch  74  loss:  0.1452508270740509
Batch  75  loss:  0.14569847285747528
Batch  76  loss:  0.14293980598449707
Batch  77  loss:  0.1451491415500641
Batch  78  loss:  0.14471499621868134
Batch  79  loss:  0.14498233795166016
Batch  80  loss:  0.14468446373939514
Batch  81  loss:  0.14422258734703064
Batch  82  loss:  0.1444050371646881
Batch  83  loss:  0.14284472167491913
Batch  84  loss:  0.14303193986415863
Batch  85  loss:  0.1441500037908554
Batch  86  loss:  0.1414775252342224
Batch  87  loss:  0.14482207596302032
Batch  88  loss:  0.14181296527385712
Batch  89  loss:  0.1442299485206604
Batch  90  loss:  0.1434522420167923
Batch  91  loss:  0.144448384642601
Batch  92  loss:  0.1408495306968689
Batch  93  loss:  0.14245332777500153
Batch  94  loss:  0.14429593086242676
Batch  95  loss:  0.1441040188074112
Batch  96  loss:  0.14472214877605438
Batch  97  loss:  0.14353114366531372
Batch  98  loss:  0.14476925134658813
Batch  99  loss:  0.14106225967407227
Batch  100  loss:  0.14614632725715637
Validation: 
LOSS train 0.14383487284183502, val 0.142674520611763
EPOCH : 18 TIME:  2022-04-19 06:33:22.382248
Training: 
Batch  1  loss:  0.14355197548866272
Batch  2  loss:  0.14170707762241364
Batch  3  loss:  0.1455463171005249
Batch  4  loss:  0.14285683631896973
Batch  5  loss:  0.14291678369045258
Batch  6  loss:  0.1443851888179779
Batch  7  loss:  0.14701852202415466
Batch  8  loss:  0.14442899823188782
Batch  9  loss:  0.14334186911582947
Batch  10  loss:  0.1441177874803543
Batch  11  loss:  0.14518503844738007
Batch  12  loss:  0.14112256467342377
Batch  13  loss:  0.14437483251094818
Batch  14  loss:  0.1452452689409256
Batch  15  loss:  0.14425180852413177
Batch  16  loss:  0.14303217828273773
Batch  17  loss:  0.14624236524105072
Batch  18  loss:  0.1438336819410324
Batch  19  loss:  0.14478188753128052
Batch  20  loss:  0.14112500846385956
Batch  21  loss:  0.14093659818172455
Batch  22  loss:  0.14423950016498566
Batch  23  loss:  0.1416642963886261
Batch  24  loss:  0.14247377216815948
Batch  25  loss:  0.14314933121204376
Batch  26  loss:  0.14237521588802338
Batch  27  loss:  0.14372268319129944
Batch  28  loss:  0.1430330127477646
Batch  29  loss:  0.14229552447795868
Batch  30  loss:  0.14353656768798828
Batch  31  loss:  0.14241056144237518
Batch  32  loss:  0.14256088435649872
Batch  33  loss:  0.14417727291584015
Batch  34  loss:  0.14804597198963165
Batch  35  loss:  0.1444365531206131
Batch  36  loss:  0.1466817557811737
Batch  37  loss:  0.14196600019931793
Batch  38  loss:  0.14327876269817352
Batch  39  loss:  0.14502614736557007
Batch  40  loss:  0.1454738825559616
Batch  41  loss:  0.14429683983325958
Batch  42  loss:  0.14360922574996948
Batch  43  loss:  0.14319667220115662
Batch  44  loss:  0.14633683860301971
Batch  45  loss:  0.14140449464321136
Batch  46  loss:  0.14250332117080688
Batch  47  loss:  0.14441967010498047
Batch  48  loss:  0.14329521358013153
Batch  49  loss:  0.14343002438545227
Batch  50  loss:  0.14367307722568512
Batch  51  loss:  0.1423349380493164
Batch  52  loss:  0.14367642998695374
Batch  53  loss:  0.14367197453975677
Batch  54  loss:  0.145626500248909
Batch  55  loss:  0.14672160148620605
Batch  56  loss:  0.14218996465206146
Batch  57  loss:  0.14518392086029053
Batch  58  loss:  0.1452755481004715
Batch  59  loss:  0.14470486342906952
Batch  60  loss:  0.14395540952682495
Batch  61  loss:  0.14334721863269806
Batch  62  loss:  0.14338131248950958
Batch  63  loss:  0.1468326300382614
Batch  64  loss:  0.1442982405424118
Batch  65  loss:  0.14457061886787415
Batch  66  loss:  0.1431308090686798
Batch  67  loss:  0.14311736822128296
Batch  68  loss:  0.14579853415489197
Batch  69  loss:  0.14206171035766602
Batch  70  loss:  0.14214657247066498
Batch  71  loss:  0.14451982080936432
Batch  72  loss:  0.14415162801742554
Batch  73  loss:  0.1428477019071579
Batch  74  loss:  0.14440634846687317
Batch  75  loss:  0.14236485958099365
Batch  76  loss:  0.14436164498329163
Batch  77  loss:  0.14100690186023712
Batch  78  loss:  0.14239443838596344
Batch  79  loss:  0.14158567786216736
Batch  80  loss:  0.14082883298397064
Batch  81  loss:  0.14158517122268677
Batch  82  loss:  0.14570802450180054
Batch  83  loss:  0.1444975584745407
Batch  84  loss:  0.14688630402088165
Batch  85  loss:  0.14418131113052368
Batch  86  loss:  0.14310631155967712
Batch  87  loss:  0.14549025893211365
Batch  88  loss:  0.1449991911649704
Batch  89  loss:  0.14473584294319153
Batch  90  loss:  0.1432492434978485
Batch  91  loss:  0.14131000638008118
Batch  92  loss:  0.14260552823543549
Batch  93  loss:  0.14407150447368622
Batch  94  loss:  0.14340563118457794
Batch  95  loss:  0.14530478417873383
Batch  96  loss:  0.14232268929481506
Batch  97  loss:  0.14296698570251465
Batch  98  loss:  0.14559827744960785
Batch  99  loss:  0.14577051997184753
Batch  100  loss:  0.14325883984565735
Validation: 
LOSS train 0.14378263667225838, val 0.14225170016288757
EPOCH : 19 TIME:  2022-04-19 06:40:10.096802
Training: 
Batch  1  loss:  0.14462538063526154
Batch  2  loss:  0.144460067152977
Batch  3  loss:  0.1449609100818634
Batch  4  loss:  0.1450451761484146
Batch  5  loss:  0.1426350325345993
Batch  6  loss:  0.1429642289876938
Batch  7  loss:  0.14377205073833466
Batch  8  loss:  0.1434430032968521
Batch  9  loss:  0.14441095292568207
Batch  10  loss:  0.14372089505195618
Batch  11  loss:  0.1405361294746399
Batch  12  loss:  0.1426514834165573
Batch  13  loss:  0.14320093393325806
Batch  14  loss:  0.14400655031204224
Batch  15  loss:  0.14454419910907745
Batch  16  loss:  0.14284572005271912
Batch  17  loss:  0.142989382147789
Batch  18  loss:  0.14683258533477783
Batch  19  loss:  0.14026139676570892
Batch  20  loss:  0.1465419977903366
Batch  21  loss:  0.14252078533172607
Batch  22  loss:  0.14275255799293518
Batch  23  loss:  0.14462485909461975
Batch  24  loss:  0.14372849464416504
Batch  25  loss:  0.14693380892276764
Batch  26  loss:  0.14507585763931274
Batch  27  loss:  0.14512133598327637
Batch  28  loss:  0.14490720629692078
Batch  29  loss:  0.1433056741952896
Batch  30  loss:  0.14363734424114227
Batch  31  loss:  0.14535292983055115
Batch  32  loss:  0.14514897763729095
Batch  33  loss:  0.1456490457057953
Batch  34  loss:  0.1422560065984726
Batch  35  loss:  0.1452045738697052
Batch  36  loss:  0.14418676495552063
Batch  37  loss:  0.14240361750125885
Batch  38  loss:  0.1463615447282791
Batch  39  loss:  0.1438555121421814
Batch  40  loss:  0.14456307888031006
Batch  41  loss:  0.14441002905368805
Batch  42  loss:  0.14658984541893005
Batch  43  loss:  0.1435708850622177
Batch  44  loss:  0.14286579191684723
Batch  45  loss:  0.14598441123962402
Batch  46  loss:  0.1431395560503006
Batch  47  loss:  0.14483602344989777
Batch  48  loss:  0.14428868889808655
Batch  49  loss:  0.1439884752035141
Batch  50  loss:  0.1448870748281479
Batch  51  loss:  0.14485487341880798
Batch  52  loss:  0.1438208520412445
Batch  53  loss:  0.14382678270339966
Batch  54  loss:  0.14533482491970062
Batch  55  loss:  0.14306886494159698
Batch  56  loss:  0.14360834658145905
Batch  57  loss:  0.14264260232448578
Batch  58  loss:  0.144171804189682
Batch  59  loss:  0.145888090133667
Batch  60  loss:  0.1406945437192917
Batch  61  loss:  0.14284542202949524
Batch  62  loss:  0.1427030712366104
Batch  63  loss:  0.14443399012088776
Batch  64  loss:  0.14406631886959076
Batch  65  loss:  0.14383482933044434
Batch  66  loss:  0.14261817932128906
Batch  67  loss:  0.14487548172473907
Batch  68  loss:  0.14378352463245392
Batch  69  loss:  0.14200825989246368
Batch  70  loss:  0.14319470524787903
Batch  71  loss:  0.1404314935207367
Batch  72  loss:  0.14070409536361694
Batch  73  loss:  0.1431380659341812
Batch  74  loss:  0.14638583362102509
Batch  75  loss:  0.14693184196949005
Batch  76  loss:  0.14428828656673431
Batch  77  loss:  0.1434525102376938
Batch  78  loss:  0.14604908227920532
Batch  79  loss:  0.14239487051963806
Batch  80  loss:  0.1444120705127716
Batch  81  loss:  0.14365054666996002
Batch  82  loss:  0.14433743059635162
Batch  83  loss:  0.1438908576965332
Batch  84  loss:  0.14321374893188477
Batch  85  loss:  0.14593547582626343
Batch  86  loss:  0.14154760539531708
Batch  87  loss:  0.1428249180316925
Batch  88  loss:  0.14247044920921326
Batch  89  loss:  0.14327846467494965
Batch  90  loss:  0.14463923871517181
Batch  91  loss:  0.14283771812915802
Batch  92  loss:  0.14349959790706635
Batch  93  loss:  0.1414714902639389
Batch  94  loss:  0.14346468448638916
Batch  95  loss:  0.14421622455120087
Batch  96  loss:  0.1452917903661728
Batch  97  loss:  0.14564019441604614
Batch  98  loss:  0.14764301478862762
Batch  99  loss:  0.14395976066589355
Batch  100  loss:  0.14399687945842743
Validation: 
LOSS train 0.14394872441887854, val 0.14152969419956207
EPOCH : 20 TIME:  2022-04-19 06:46:53.983416
Training: 
Batch  1  loss:  0.14379602670669556
Batch  2  loss:  0.14326466619968414
Batch  3  loss:  0.1452922821044922
Batch  4  loss:  0.14598311483860016
Batch  5  loss:  0.14374487102031708
Batch  6  loss:  0.14413326978683472
Batch  7  loss:  0.14258605241775513
Batch  8  loss:  0.14593201875686646
Batch  9  loss:  0.14365720748901367
Batch  10  loss:  0.14698003232479095
Batch  11  loss:  0.14687411487102509
Batch  12  loss:  0.14153742790222168
Batch  13  loss:  0.14284645020961761
Batch  14  loss:  0.14341314136981964
Batch  15  loss:  0.14380402863025665
Batch  16  loss:  0.14493586122989655
Batch  17  loss:  0.1437302827835083
Batch  18  loss:  0.14484292268753052
Batch  19  loss:  0.14266380667686462
Batch  20  loss:  0.14192377030849457
Batch  21  loss:  0.1451302319765091
Batch  22  loss:  0.1439094841480255
Batch  23  loss:  0.14361879229545593
Batch  24  loss:  0.14437827467918396
Batch  25  loss:  0.1444360315799713
Batch  26  loss:  0.14236848056316376
Batch  27  loss:  0.14212965965270996
Batch  28  loss:  0.14581261575222015
Batch  29  loss:  0.14736461639404297
Batch  30  loss:  0.14334578812122345
Batch  31  loss:  0.14411523938179016
Batch  32  loss:  0.14328965544700623
Batch  33  loss:  0.14350086450576782
Batch  34  loss:  0.14312633872032166
Batch  35  loss:  0.1426466703414917
Batch  36  loss:  0.14237132668495178
Batch  37  loss:  0.14296776056289673
Batch  38  loss:  0.14531995356082916
Batch  39  loss:  0.14331363141536713
Batch  40  loss:  0.1435452103614807
Batch  41  loss:  0.14521555602550507
Batch  42  loss:  0.1451318860054016
Batch  43  loss:  0.14337320625782013
Batch  44  loss:  0.14307096600532532
Batch  45  loss:  0.1422385573387146
Batch  46  loss:  0.14407126605510712
Batch  47  loss:  0.14242985844612122
Batch  48  loss:  0.14442335069179535
Batch  49  loss:  0.14366693794727325
Batch  50  loss:  0.143863707780838
Batch  51  loss:  0.14571000635623932
Batch  52  loss:  0.1435338258743286
Batch  53  loss:  0.1472991406917572
Batch  54  loss:  0.14343595504760742
Batch  55  loss:  0.14405319094657898
Batch  56  loss:  0.13981737196445465
Batch  57  loss:  0.14252573251724243
Batch  58  loss:  0.1444634199142456
Batch  59  loss:  0.14549025893211365
Batch  60  loss:  0.14486654102802277
Batch  61  loss:  0.14257119596004486
Batch  62  loss:  0.144680917263031
Batch  63  loss:  0.14239941537380219
Batch  64  loss:  0.1431320607662201
Batch  65  loss:  0.1450778692960739
Batch  66  loss:  0.14442583918571472
Batch  67  loss:  0.14455538988113403
Batch  68  loss:  0.14089126884937286
Batch  69  loss:  0.14372020959854126
Batch  70  loss:  0.14543674886226654
Batch  71  loss:  0.14458490908145905
Batch  72  loss:  0.14508217573165894
Batch  73  loss:  0.14494140446186066
Batch  74  loss:  0.14093929529190063
Batch  75  loss:  0.14380855858325958
Batch  76  loss:  0.14556948840618134
Batch  77  loss:  0.14488860964775085
Batch  78  loss:  0.14302372932434082
Batch  79  loss:  0.14182965457439423
Batch  80  loss:  0.14415155351161957
Batch  81  loss:  0.1433977335691452
Batch  82  loss:  0.1441941112279892
Batch  83  loss:  0.14263533055782318
Batch  84  loss:  0.14316710829734802
Batch  85  loss:  0.14637412130832672
Batch  86  loss:  0.14426162838935852
Batch  87  loss:  0.14285580813884735
Batch  88  loss:  0.14405539631843567
Batch  89  loss:  0.14259423315525055
Batch  90  loss:  0.14297866821289062
Batch  91  loss:  0.14551492035388947
Batch  92  loss:  0.141445592045784
Batch  93  loss:  0.14314627647399902
Batch  94  loss:  0.14543649554252625
Batch  95  loss:  0.14539998769760132
Batch  96  loss:  0.1452399343252182
Batch  97  loss:  0.14476002752780914
Batch  98  loss:  0.14369705319404602
Batch  99  loss:  0.14179646968841553
Batch  100  loss:  0.14337392151355743
Validation: 
LOSS train 0.14389347821474074, val 0.14416128396987915
Tue 19 Apr 2022 06:53:40 AM EDT
TABLE:: POINT TRANSFORMER
INFO - 2022-04-19 06:53:42,289 - utils - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO - 2022-04-19 06:53:42,289 - utils - NumExpr defaulting to 8 threads.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Training:  table
--------------------
Running self_supervised_training:  2022-04-19 06:53:42.430218
--------------------
device is  cuda
--------------------
Number of trainable parameters:  891544
EPOCH : 1 TIME:  2022-04-19 06:53:44.320473
Training: 
Batch  1  loss:  0.6914997696876526
Batch  2  loss:  0.3405489921569824
Batch  3  loss:  0.7265167832374573
Batch  4  loss:  0.4060707092285156
Batch  5  loss:  0.2897510230541229
Batch  6  loss:  0.38405218720436096
Batch  7  loss:  0.3560205101966858
Batch  8  loss:  0.31813156604766846
Batch  9  loss:  0.2729940712451935
Batch  10  loss:  0.23365749418735504
Batch  11  loss:  0.2278471291065216
Batch  12  loss:  0.23810790479183197
Batch  13  loss:  0.2313246876001358
Batch  14  loss:  0.22379153966903687
Batch  15  loss:  0.2325802892446518
Batch  16  loss:  0.237010657787323
Batch  17  loss:  0.22606723010540009
Batch  18  loss:  0.23014815151691437
Batch  19  loss:  0.21769452095031738
Batch  20  loss:  0.21447144448757172
Batch  21  loss:  0.20851737260818481
Batch  22  loss:  0.20282422006130219
Batch  23  loss:  0.22078746557235718
Batch  24  loss:  0.2059098333120346
Batch  25  loss:  0.21827509999275208
Batch  26  loss:  0.2026234269142151
Batch  27  loss:  0.20375646650791168
Batch  28  loss:  0.2081660032272339
Batch  29  loss:  0.19806091487407684
Batch  30  loss:  0.2073420286178589
Batch  31  loss:  0.2053636908531189
Batch  32  loss:  0.20259417593479156
Batch  33  loss:  0.20065990090370178
Batch  34  loss:  0.19968585669994354
Batch  35  loss:  0.19490551948547363
Batch  36  loss:  0.19703640043735504
Batch  37  loss:  0.19655051827430725
Batch  38  loss:  0.1913873702287674
Batch  39  loss:  0.195139080286026
Batch  40  loss:  0.19275303184986115
Batch  41  loss:  0.18869081139564514
Batch  42  loss:  0.19339224696159363
Batch  43  loss:  0.19664710760116577
Batch  44  loss:  0.1877063363790512
Batch  45  loss:  0.18904514610767365
Batch  46  loss:  0.18918776512145996
Batch  47  loss:  0.19400671124458313
Batch  48  loss:  0.19210948050022125
Batch  49  loss:  0.18995994329452515
Batch  50  loss:  0.19007255136966705
Batch  51  loss:  0.18122577667236328
Batch  52  loss:  0.18689008057117462
Batch  53  loss:  0.19685813784599304
Batch  54  loss:  0.18475669622421265
Batch  55  loss:  0.18172059953212738
Batch  56  loss:  0.18746665120124817
Batch  57  loss:  0.1841726452112198
Batch  58  loss:  0.1807638555765152
Batch  59  loss:  0.18213537335395813
Batch  60  loss:  0.18804694712162018
Batch  61  loss:  0.1784934252500534
Batch  62  loss:  0.1770995706319809
Batch  63  loss:  0.18406875431537628
Batch  64  loss:  0.1867353916168213
Batch  65  loss:  0.1793082356452942
Batch  66  loss:  0.1825346201658249
Batch  67  loss:  0.18484824895858765
Batch  68  loss:  0.18718773126602173
Batch  69  loss:  0.1847977340221405
Batch  70  loss:  0.18683116137981415
Batch  71  loss:  0.181134432554245
Batch  72  loss:  0.1837729811668396
Batch  73  loss:  0.17996951937675476
Batch  74  loss:  0.18619707226753235
Batch  75  loss:  0.18363681435585022
Batch  76  loss:  0.18619148433208466
Batch  77  loss:  0.18141712248325348
Batch  78  loss:  0.19049596786499023
Batch  79  loss:  0.1841306984424591
Batch  80  loss:  0.18149589002132416
Batch  81  loss:  0.18297049403190613
Batch  82  loss:  0.18060065805912018
Batch  83  loss:  0.1861094981431961
Batch  84  loss:  0.1798904538154602
Batch  85  loss:  0.1845237761735916
Batch  86  loss:  0.1845913976430893
Batch  87  loss:  0.1857282668352127
Batch  88  loss:  0.17855121195316315
Batch  89  loss:  0.181065171957016
Batch  90  loss:  0.1821175217628479
Batch  91  loss:  0.17157936096191406
Batch  92  loss:  0.18499915301799774
Batch  93  loss:  0.1736421138048172
Batch  94  loss:  0.18272122740745544
Batch  95  loss:  0.17879268527030945
Batch  96  loss:  0.17958785593509674
Batch  97  loss:  0.1740235835313797
Batch  98  loss:  0.18313217163085938
Batch  99  loss:  0.1837635040283203
Batch  100  loss:  0.1795317381620407
Validation: 
LOSS train 0.21403748601675032, val 0.1769375205039978
EPOCH : 2 TIME:  2022-04-19 07:10:28.009480
Training: 
Batch  1  loss:  0.1743038147687912
Batch  2  loss:  0.17374137043952942
Batch  3  loss:  0.17819659411907196
Batch  4  loss:  0.1764281839132309
Batch  5  loss:  0.17625196278095245
Batch  6  loss:  0.18407702445983887
Batch  7  loss:  0.1852077841758728
Batch  8  loss:  0.18074409663677216
Batch  9  loss:  0.18162639439105988
Batch  10  loss:  0.1711151897907257
Batch  11  loss:  0.176762655377388
Batch  12  loss:  0.17899367213249207
Batch  13  loss:  0.17657428979873657
Batch  14  loss:  0.17842160165309906
Batch  15  loss:  0.1800989806652069
Batch  16  loss:  0.17540685832500458
Batch  17  loss:  0.18462467193603516
Batch  18  loss:  0.18053218722343445
Batch  19  loss:  0.1830773949623108
Batch  20  loss:  0.18072299659252167
Batch  21  loss:  0.17795173823833466
Batch  22  loss:  0.1820243000984192
Batch  23  loss:  0.17926833033561707
Batch  24  loss:  0.182876318693161
Batch  25  loss:  0.18294702470302582
Batch  26  loss:  0.17699572443962097
Batch  27  loss:  0.18271365761756897
Batch  28  loss:  0.17892463505268097
Batch  29  loss:  0.18189649283885956
Batch  30  loss:  0.17863181233406067
Batch  31  loss:  0.17625033855438232
Batch  32  loss:  0.17310380935668945
Batch  33  loss:  0.1709376871585846
Batch  34  loss:  0.1794796586036682
Batch  35  loss:  0.17759495973587036
Batch  36  loss:  0.17845018208026886
Batch  37  loss:  0.1800978183746338
Batch  38  loss:  0.17603355646133423
Batch  39  loss:  0.18399469554424286
Batch  40  loss:  0.1715032160282135
Batch  41  loss:  0.17403092980384827
Batch  42  loss:  0.16866342723369598
Batch  43  loss:  0.17885833978652954
Batch  44  loss:  0.1743115931749344
Batch  45  loss:  0.17305123805999756
Batch  46  loss:  0.18094290792942047
Batch  47  loss:  0.18570123612880707
Batch  48  loss:  0.17304210364818573
Batch  49  loss:  0.17944134771823883
Batch  50  loss:  0.17263932526111603
Batch  51  loss:  0.18026025593280792
Batch  52  loss:  0.17985011637210846
Batch  53  loss:  0.18017679452896118
Batch  54  loss:  0.17619673907756805
Batch  55  loss:  0.18138259649276733
Batch  56  loss:  0.17673951387405396
Batch  57  loss:  0.17584802210330963
Batch  58  loss:  0.17369773983955383
Batch  59  loss:  0.1771138310432434
Batch  60  loss:  0.18016491830348969
Batch  61  loss:  0.17751777172088623
Batch  62  loss:  0.1719701737165451
Batch  63  loss:  0.18459711968898773
Batch  64  loss:  0.1804056167602539
Batch  65  loss:  0.1813250631093979
Batch  66  loss:  0.18003474175930023
Batch  67  loss:  0.1741330325603485
Batch  68  loss:  0.18491412699222565
Batch  69  loss:  0.18195216357707977
Batch  70  loss:  0.1837621033191681
Batch  71  loss:  0.18488921225070953
Batch  72  loss:  0.1778770536184311
Batch  73  loss:  0.17781871557235718
Batch  74  loss:  0.17758731544017792
Batch  75  loss:  0.17812293767929077
Batch  76  loss:  0.16923804581165314
Batch  77  loss:  0.17576362192630768
Batch  78  loss:  0.17371824383735657
Batch  79  loss:  0.18341170251369476
Batch  80  loss:  0.1727983057498932
Batch  81  loss:  0.17687995731830597
Batch  82  loss:  0.1742280125617981
Batch  83  loss:  0.17508964240550995
Batch  84  loss:  0.1805475950241089
Batch  85  loss:  0.1722920536994934
Batch  86  loss:  0.17663997411727905
Batch  87  loss:  0.18208175897598267
Batch  88  loss:  0.17533889412879944
Batch  89  loss:  0.1761922836303711
Batch  90  loss:  0.17756038904190063
Batch  91  loss:  0.175418883562088
Batch  92  loss:  0.17175954580307007
Batch  93  loss:  0.1729617714881897
Batch  94  loss:  0.1711549311876297
Batch  95  loss:  0.18124988675117493
Batch  96  loss:  0.17045095562934875
Batch  97  loss:  0.18200385570526123
Batch  98  loss:  0.17107230424880981
Batch  99  loss:  0.17564556002616882
Batch  100  loss:  0.1781555861234665
Validation: 
LOSS train 0.1777925756573677, val 0.18148452043533325
EPOCH : 3 TIME:  2022-04-19 07:27:04.985331
Training: 
Batch  1  loss:  0.1689060628414154
Batch  2  loss:  0.17618052661418915
Batch  3  loss:  0.17560641467571259
Batch  4  loss:  0.17655429244041443
Batch  5  loss:  0.16842836141586304
Batch  6  loss:  0.17160463333129883
Batch  7  loss:  0.17546485364437103
Batch  8  loss:  0.18406394124031067
Batch  9  loss:  0.1715732365846634
Batch  10  loss:  0.17933310568332672
Batch  11  loss:  0.17443673312664032
Batch  12  loss:  0.17830035090446472
Batch  13  loss:  0.17863856256008148
Batch  14  loss:  0.17680838704109192
Batch  15  loss:  0.17668022215366364
Batch  16  loss:  0.17846953868865967
Batch  17  loss:  0.1785244643688202
Batch  18  loss:  0.17465533316135406
Batch  19  loss:  0.17612038552761078
Batch  20  loss:  0.1728944331407547
Batch  21  loss:  0.17462344467639923
Batch  22  loss:  0.17009902000427246
Batch  23  loss:  0.17226165533065796
Batch  24  loss:  0.17284239828586578
Batch  25  loss:  0.17570927739143372
Batch  26  loss:  0.16993027925491333
Batch  27  loss:  0.17905734479427338
Batch  28  loss:  0.16945786774158478
Batch  29  loss:  0.17296889424324036
Batch  30  loss:  0.17261403799057007
Batch  31  loss:  0.17277030646800995
Batch  32  loss:  0.1702328473329544
Batch  33  loss:  0.1752534955739975
Batch  34  loss:  0.17184604704380035
Batch  35  loss:  0.17135578393936157
Batch  36  loss:  0.17514720559120178
Batch  37  loss:  0.17229637503623962
Batch  38  loss:  0.1723572462797165
Batch  39  loss:  0.17491944134235382
Batch  40  loss:  0.17996162176132202
Batch  41  loss:  0.17857135832309723
Batch  42  loss:  0.17871502041816711
Batch  43  loss:  0.17217664420604706
Batch  44  loss:  0.17461267113685608
Batch  45  loss:  0.1783551573753357
Batch  46  loss:  0.17387056350708008
Batch  47  loss:  0.17219161987304688
Batch  48  loss:  0.16096265614032745
Batch  49  loss:  0.17614537477493286
Batch  50  loss:  0.17972834408283234
Batch  51  loss:  0.1705443114042282
Batch  52  loss:  0.1774185448884964
Batch  53  loss:  0.17475832998752594
Batch  54  loss:  0.1855648308992386
Batch  55  loss:  0.17044292390346527
Batch  56  loss:  0.17500945925712585
Batch  57  loss:  0.1737598031759262
Batch  58  loss:  0.17448227107524872
Batch  59  loss:  0.17289645969867706
Batch  60  loss:  0.17209911346435547
Batch  61  loss:  0.17218589782714844
Batch  62  loss:  0.1747896671295166
Batch  63  loss:  0.17982499301433563
Batch  64  loss:  0.17254996299743652
Batch  65  loss:  0.17626100778579712
Batch  66  loss:  0.18002068996429443
Batch  67  loss:  0.17432262003421783
Batch  68  loss:  0.18068347871303558
Batch  69  loss:  0.1668376475572586
Batch  70  loss:  0.17454998195171356
Batch  71  loss:  0.17381878197193146
Batch  72  loss:  0.17578332126140594
Batch  73  loss:  0.17648887634277344
Batch  74  loss:  0.17278708517551422
Batch  75  loss:  0.17106971144676208
Batch  76  loss:  0.16718246042728424
Batch  77  loss:  0.17126375436782837
Batch  78  loss:  0.18078729510307312
Batch  79  loss:  0.17994751036167145
Batch  80  loss:  0.17942526936531067
Batch  81  loss:  0.17612063884735107
Batch  82  loss:  0.17577125132083893
Batch  83  loss:  0.1794232577085495
Batch  84  loss:  0.18091981112957
Batch  85  loss:  0.17200669646263123
Batch  86  loss:  0.1773962378501892
Batch  87  loss:  0.1830025464296341
Batch  88  loss:  0.17843559384346008
Batch  89  loss:  0.1712479293346405
Batch  90  loss:  0.17882190644741058
Batch  91  loss:  0.17692133784294128
Batch  92  loss:  0.17487841844558716
Batch  93  loss:  0.17171567678451538
Batch  94  loss:  0.17205993831157684
Batch  95  loss:  0.17494697868824005
Batch  96  loss:  0.17398448288440704
Batch  97  loss:  0.17491614818572998
Batch  98  loss:  0.176507830619812
Batch  99  loss:  0.1775137037038803
Batch  100  loss:  0.17055724561214447
Validation: 
LOSS train 0.17488981530070305, val 0.17779439687728882
EPOCH : 4 TIME:  2022-04-19 07:43:41.450903
Training: 
Batch  1  loss:  0.17800571024417877
Batch  2  loss:  0.17298324406147003
Batch  3  loss:  0.16800855100154877
Batch  4  loss:  0.17568808794021606
Batch  5  loss:  0.16692839562892914
Batch  6  loss:  0.1712319552898407
Batch  7  loss:  0.17176656424999237
Batch  8  loss:  0.17119117081165314
Batch  9  loss:  0.1721789836883545
Batch  10  loss:  0.17538900673389435
Batch  11  loss:  0.17975978553295135
Batch  12  loss:  0.1693931221961975
Batch  13  loss:  0.17297881841659546
Batch  14  loss:  0.17481082677841187
Batch  15  loss:  0.16942468285560608
Batch  16  loss:  0.17388466000556946
Batch  17  loss:  0.17273548245429993
Batch  18  loss:  0.17175303399562836
Batch  19  loss:  0.17042136192321777
Batch  20  loss:  0.17677423357963562
Batch  21  loss:  0.17828038334846497
Batch  22  loss:  0.18236114084720612
Batch  23  loss:  0.1798362284898758
Batch  24  loss:  0.16844430565834045
Batch  25  loss:  0.17176517844200134
Batch  26  loss:  0.17028503119945526
Batch  27  loss:  0.17347152531147003
Batch  28  loss:  0.17431065440177917
Batch  29  loss:  0.17619334161281586
Batch  30  loss:  0.1726026087999344
Batch  31  loss:  0.17868657410144806
Batch  32  loss:  0.18083201348781586
Batch  33  loss:  0.1706022471189499
Batch  34  loss:  0.17842614650726318
Batch  35  loss:  0.1845179945230484
Batch  36  loss:  0.17032447457313538
Batch  37  loss:  0.16355715692043304
Batch  38  loss:  0.1757655292749405
Batch  39  loss:  0.17148242890834808
Batch  40  loss:  0.17935305833816528
Batch  41  loss:  0.17052990198135376
Batch  42  loss:  0.17714446783065796
Batch  43  loss:  0.1693210005760193
Batch  44  loss:  0.17093053460121155
Batch  45  loss:  0.18331404030323029
Batch  46  loss:  0.17942579090595245
Batch  47  loss:  0.170159712433815
Batch  48  loss:  0.1762615144252777
Batch  49  loss:  0.17468833923339844
Batch  50  loss:  0.18355071544647217
Batch  51  loss:  0.17880088090896606
Batch  52  loss:  0.17179188132286072
Batch  53  loss:  0.16581153869628906
Batch  54  loss:  0.1639253795146942
Batch  55  loss:  0.1700608730316162
Batch  56  loss:  0.16931062936782837
Batch  57  loss:  0.17847000062465668
Batch  58  loss:  0.17069198191165924
Batch  59  loss:  0.1732092797756195
Batch  60  loss:  0.16595859825611115
Batch  61  loss:  0.1837611347436905
Batch  62  loss:  0.17457978427410126
Batch  63  loss:  0.17515937983989716
Batch  64  loss:  0.17438837885856628
Batch  65  loss:  0.17575904726982117
Batch  66  loss:  0.17910075187683105
Batch  67  loss:  0.17768268287181854
Batch  68  loss:  0.1699170470237732
Batch  69  loss:  0.17244842648506165
Batch  70  loss:  0.16950881481170654
Batch  71  loss:  0.17372840642929077
Batch  72  loss:  0.17469631135463715
Batch  73  loss:  0.16875000298023224
Batch  74  loss:  0.17955026030540466
Batch  75  loss:  0.17292705178260803
Batch  76  loss:  0.17403483390808105
Batch  77  loss:  0.17988093197345734
Batch  78  loss:  0.1754096895456314
Batch  79  loss:  0.17692118883132935
Batch  80  loss:  0.17725612223148346
Batch  81  loss:  0.17709700763225555
Batch  82  loss:  0.17381159961223602
Batch  83  loss:  0.16364146769046783
Batch  84  loss:  0.17072100937366486
Batch  85  loss:  0.17919273674488068
Batch  86  loss:  0.17817169427871704
Batch  87  loss:  0.16667136549949646
Batch  88  loss:  0.1660679578781128
Batch  89  loss:  0.16898082196712494
Batch  90  loss:  0.1684146225452423
Batch  91  loss:  0.17821094393730164
Batch  92  loss:  0.17457276582717896
Batch  93  loss:  0.17854030430316925
Batch  94  loss:  0.17193345725536346
Batch  95  loss:  0.1788737028837204
Batch  96  loss:  0.17404048144817352
Batch  97  loss:  0.1712178736925125
Batch  98  loss:  0.17626388370990753
Batch  99  loss:  0.1676235944032669
Batch  100  loss:  0.16994275152683258
Validation: 
LOSS train 0.1737921305000782, val 0.17459364235401154
EPOCH : 5 TIME:  2022-04-19 08:00:18.413483
Training: 
Batch  1  loss:  0.1755882054567337
Batch  2  loss:  0.1708187311887741
Batch  3  loss:  0.17546525597572327
Batch  4  loss:  0.17312099039554596
Batch  5  loss:  0.17816168069839478
Batch  6  loss:  0.1681022197008133
Batch  7  loss:  0.17037193477153778
Batch  8  loss:  0.17695461213588715
Batch  9  loss:  0.176105335354805
Batch  10  loss:  0.17017842829227448
Batch  11  loss:  0.17014750838279724
Batch  12  loss:  0.1769324392080307
Batch  13  loss:  0.1669670045375824
Batch  14  loss:  0.17023639380931854
Batch  15  loss:  0.17317523062229156
Batch  16  loss:  0.17641611397266388
Batch  17  loss:  0.17504681646823883
Batch  18  loss:  0.17130935192108154
Batch  19  loss:  0.17718295753002167
Batch  20  loss:  0.17315642535686493
Batch  21  loss:  0.17291881144046783
Batch  22  loss:  0.1796955019235611
Batch  23  loss:  0.17113511264324188
Batch  24  loss:  0.1741807907819748
Batch  25  loss:  0.16495555639266968
Batch  26  loss:  0.17241908609867096
Batch  27  loss:  0.17652541399002075
Batch  28  loss:  0.1775849312543869
Batch  29  loss:  0.171524316072464
Batch  30  loss:  0.1767086684703827
Batch  31  loss:  0.16777051985263824
Batch  32  loss:  0.18404491245746613
Batch  33  loss:  0.1722395271062851
Batch  34  loss:  0.17793254554271698
Batch  35  loss:  0.17824989557266235
Batch  36  loss:  0.17548200488090515
Batch  37  loss:  0.1689540296792984
Batch  38  loss:  0.1717192828655243
Batch  39  loss:  0.17606639862060547
Batch  40  loss:  0.1716073900461197
Batch  41  loss:  0.17495320737361908
Batch  42  loss:  0.17627719044685364
Batch  43  loss:  0.16842222213745117
Batch  44  loss:  0.16797660291194916
Batch  45  loss:  0.17426170408725739
Batch  46  loss:  0.17363204061985016
Batch  47  loss:  0.17230172455310822
Batch  48  loss:  0.17423883080482483
Batch  49  loss:  0.17524024844169617
Batch  50  loss:  0.17580954730510712
Batch  51  loss:  0.16844505071640015
Batch  52  loss:  0.17733332514762878
Batch  53  loss:  0.16793762147426605
Batch  54  loss:  0.18013589084148407
Batch  55  loss:  0.17428699135780334
Batch  56  loss:  0.17765086889266968
Batch  57  loss:  0.16959139704704285
Batch  58  loss:  0.17170163989067078
Batch  59  loss:  0.1723838746547699
Batch  60  loss:  0.17505179345607758
Batch  61  loss:  0.17256465554237366
Batch  62  loss:  0.1721137911081314
Batch  63  loss:  0.17246055603027344
Batch  64  loss:  0.17102308571338654
Batch  65  loss:  0.17791293561458588
Batch  66  loss:  0.17389975488185883
Batch  67  loss:  0.17665225267410278
Batch  68  loss:  0.17537014186382294
Batch  69  loss:  0.17050887644290924
Batch  70  loss:  0.17054057121276855
Batch  71  loss:  0.16693852841854095
Batch  72  loss:  0.16983571648597717
Batch  73  loss:  0.16929931938648224
Batch  74  loss:  0.17611348628997803
Batch  75  loss:  0.17153014242649078
Batch  76  loss:  0.17414504289627075
Batch  77  loss:  0.16917681694030762
Batch  78  loss:  0.1746673583984375
Batch  79  loss:  0.17276617884635925
Batch  80  loss:  0.17201858758926392
Batch  81  loss:  0.17413249611854553
Batch  82  loss:  0.17636817693710327
Batch  83  loss:  0.17643705010414124
Batch  84  loss:  0.172367662191391
Batch  85  loss:  0.17865222692489624
Batch  86  loss:  0.17564964294433594
Batch  87  loss:  0.16683311760425568
Batch  88  loss:  0.1744987517595291
Batch  89  loss:  0.18133798241615295
Batch  90  loss:  0.16425010561943054
Batch  91  loss:  0.1739279329776764
Batch  92  loss:  0.1776873916387558
Batch  93  loss:  0.17404498159885406
Batch  94  loss:  0.17813701927661896
Batch  95  loss:  0.1733514815568924
Batch  96  loss:  0.17302192747592926
Batch  97  loss:  0.17000365257263184
Batch  98  loss:  0.1719655990600586
Batch  99  loss:  0.17004166543483734
Batch  100  loss:  0.17454089224338531
Validation: 
LOSS train 0.17341571658849717, val 0.17177332937717438
EPOCH : 6 TIME:  2022-04-19 08:16:51.722962
Training: 
Batch  1  loss:  0.17766143381595612
Batch  2  loss:  0.1753709316253662
Batch  3  loss:  0.17452841997146606
Batch  4  loss:  0.168263241648674
Batch  5  loss:  0.17473061382770538
Batch  6  loss:  0.17278864979743958
Batch  7  loss:  0.171551913022995
Batch  8  loss:  0.17013034224510193
Batch  9  loss:  0.1723622828722
Batch  10  loss:  0.17636768519878387
Batch  11  loss:  0.1745101660490036
Batch  12  loss:  0.17281940579414368
Batch  13  loss:  0.17228004336357117
Batch  14  loss:  0.16878437995910645
Batch  15  loss:  0.17432822287082672
Batch  16  loss:  0.17571040987968445
Batch  17  loss:  0.17220798134803772
Batch  18  loss:  0.1743285357952118
Batch  19  loss:  0.17314563691616058
Batch  20  loss:  0.17726844549179077
Batch  21  loss:  0.1677764654159546
Batch  22  loss:  0.17405372858047485
Batch  23  loss:  0.17469006776809692
Batch  24  loss:  0.17415741086006165
Batch  25  loss:  0.17483647167682648
Batch  26  loss:  0.16697734594345093
Batch  27  loss:  0.1695220172405243
Batch  28  loss:  0.1744471937417984
Batch  29  loss:  0.1785561740398407
Batch  30  loss:  0.1739852875471115
Batch  31  loss:  0.17130786180496216
Batch  32  loss:  0.16752400994300842
Batch  33  loss:  0.17777293920516968
Batch  34  loss:  0.17505797743797302
Batch  35  loss:  0.17040248215198517
Batch  36  loss:  0.18103961646556854
Batch  37  loss:  0.17749431729316711
Batch  38  loss:  0.17485211789608002
Batch  39  loss:  0.17772415280342102
Batch  40  loss:  0.17007564008235931
Batch  41  loss:  0.16825225949287415
Batch  42  loss:  0.17394939064979553
Batch  43  loss:  0.1698867231607437
Batch  44  loss:  0.17611248791217804
Batch  45  loss:  0.18055279552936554
Batch  46  loss:  0.17599031329154968
Batch  47  loss:  0.18003976345062256
Batch  48  loss:  0.17710290849208832
Batch  49  loss:  0.1711679995059967
Batch  50  loss:  0.17313562333583832
Batch  51  loss:  0.16973310708999634
Batch  52  loss:  0.17654092609882355
Batch  53  loss:  0.18011127412319183
Batch  54  loss:  0.1766635626554489
Batch  55  loss:  0.1716192364692688
Batch  56  loss:  0.1676066815853119
Batch  57  loss:  0.17279720306396484
Batch  58  loss:  0.1662311553955078
Batch  59  loss:  0.17179378867149353
Batch  60  loss:  0.176075741648674
Batch  61  loss:  0.1704687476158142
Batch  62  loss:  0.17627623677253723
Batch  63  loss:  0.17044475674629211
Batch  64  loss:  0.16331185400485992
Batch  65  loss:  0.1709500402212143
Batch  66  loss:  0.1726689636707306
Batch  67  loss:  0.1626310646533966
Batch  68  loss:  0.16651800274848938
Batch  69  loss:  0.1766703724861145
Batch  70  loss:  0.1734253466129303
Batch  71  loss:  0.17551584541797638
Batch  72  loss:  0.1720394343137741
Batch  73  loss:  0.17188367247581482
Batch  74  loss:  0.17381435632705688
Batch  75  loss:  0.17538362741470337
Batch  76  loss:  0.1783200353384018
Batch  77  loss:  0.17381660640239716
Batch  78  loss:  0.17192848026752472
Batch  79  loss:  0.18082192540168762
Batch  80  loss:  0.17802919447422028
Batch  81  loss:  0.16806922852993011
Batch  82  loss:  0.18112598359584808
Batch  83  loss:  0.1728590726852417
Batch  84  loss:  0.17407886683940887
Batch  85  loss:  0.16932456195354462
Batch  86  loss:  0.17790453135967255
Batch  87  loss:  0.16977615654468536
Batch  88  loss:  0.17056359350681305
Batch  89  loss:  0.1815355271100998
Batch  90  loss:  0.1820192039012909
Batch  91  loss:  0.17496398091316223
Batch  92  loss:  0.17546811699867249
Batch  93  loss:  0.17834265530109406
Batch  94  loss:  0.1737784743309021
Batch  95  loss:  0.17299506068229675
Batch  96  loss:  0.1726563572883606
Batch  97  loss:  0.1770828813314438
Batch  98  loss:  0.1715307980775833
Batch  99  loss:  0.17949356138706207
Batch  100  loss:  0.17408843338489532
Validation: 
LOSS train 0.17369330570101738, val 0.16982722282409668
EPOCH : 7 TIME:  2022-04-19 08:33:22.452037
Training: 
Batch  1  loss:  0.17367786169052124
Batch  2  loss:  0.17656414210796356
Batch  3  loss:  0.17326582968235016
Batch  4  loss:  0.1740024834871292
Batch  5  loss:  0.1769551783800125
Batch  6  loss:  0.17487838864326477
Batch  7  loss:  0.17292706668376923
Batch  8  loss:  0.1773780733346939
Batch  9  loss:  0.1694667786359787
Batch  10  loss:  0.1748744547367096
Batch  11  loss:  0.16969920694828033
Batch  12  loss:  0.17439761757850647
Batch  13  loss:  0.1740943342447281
Batch  14  loss:  0.17703165113925934
Batch  15  loss:  0.1696944385766983
Batch  16  loss:  0.1724766492843628
Batch  17  loss:  0.16876649856567383
Batch  18  loss:  0.16761772334575653
Batch  19  loss:  0.1725134253501892
Batch  20  loss:  0.17103643715381622
Batch  21  loss:  0.1693539172410965
Batch  22  loss:  0.1758897602558136
Batch  23  loss:  0.17942450940608978
Batch  24  loss:  0.177867129445076
Batch  25  loss:  0.16929744184017181
Batch  26  loss:  0.17073474824428558
Batch  27  loss:  0.18052007257938385
Batch  28  loss:  0.1730687916278839
Batch  29  loss:  0.17294961214065552
Batch  30  loss:  0.17226314544677734
Batch  31  loss:  0.1711612343788147
Batch  32  loss:  0.17507171630859375
Batch  33  loss:  0.16812007129192352
Batch  34  loss:  0.17312473058700562
Batch  35  loss:  0.16861754655838013
Batch  36  loss:  0.1765165477991104
Batch  37  loss:  0.16732317209243774
Batch  38  loss:  0.16201633214950562
Batch  39  loss:  0.1703868806362152
Batch  40  loss:  0.18036456406116486
Batch  41  loss:  0.16442367434501648
Batch  42  loss:  0.18157300353050232
Batch  43  loss:  0.16768313944339752
Batch  44  loss:  0.17723560333251953
Batch  45  loss:  0.1804720014333725
Batch  46  loss:  0.16921669244766235
Batch  47  loss:  0.18005728721618652
Batch  48  loss:  0.17978504300117493
Batch  49  loss:  0.17942796647548676
Batch  50  loss:  0.17704766988754272
Batch  51  loss:  0.17847071588039398
Batch  52  loss:  0.17142243683338165
Batch  53  loss:  0.17140036821365356
Batch  54  loss:  0.17202261090278625
Batch  55  loss:  0.17365872859954834
Batch  56  loss:  0.16677141189575195
Batch  57  loss:  0.1815328150987625
Batch  58  loss:  0.17599108815193176
Batch  59  loss:  0.17413340508937836
Batch  60  loss:  0.17567186057567596
Batch  61  loss:  0.17925651371479034
Batch  62  loss:  0.17667415738105774
Batch  63  loss:  0.1633889526128769
Batch  64  loss:  0.16572585701942444
Batch  65  loss:  0.172590970993042
Batch  66  loss:  0.17591552436351776
Batch  67  loss:  0.17731402814388275
Batch  68  loss:  0.17436619102954865
Batch  69  loss:  0.18083973228931427
Batch  70  loss:  0.17770202457904816
Batch  71  loss:  0.16829581558704376
Batch  72  loss:  0.1742459386587143
Batch  73  loss:  0.17377014458179474
Batch  74  loss:  0.1737229824066162
Batch  75  loss:  0.1726604402065277
Batch  76  loss:  0.1634148508310318
Batch  77  loss:  0.17060531675815582
Batch  78  loss:  0.17638307809829712
Batch  79  loss:  0.17080199718475342
Batch  80  loss:  0.17096371948719025
Batch  81  loss:  0.17148497700691223
Batch  82  loss:  0.17192339897155762
Batch  83  loss:  0.17007847130298615
Batch  84  loss:  0.17767561972141266
Batch  85  loss:  0.17933207750320435
Batch  86  loss:  0.16665412485599518
Batch  87  loss:  0.17250771820545197
Batch  88  loss:  0.17230293154716492
Batch  89  loss:  0.16693367063999176
Batch  90  loss:  0.1736656278371811
Batch  91  loss:  0.16640827059745789
Batch  92  loss:  0.17294584214687347
Batch  93  loss:  0.17120923101902008
Batch  94  loss:  0.17245063185691833
Batch  95  loss:  0.1739346981048584
Batch  96  loss:  0.17368373274803162
Batch  97  loss:  0.17507041990756989
Batch  98  loss:  0.17037241160869598
Batch  99  loss:  0.17108020186424255
Batch  100  loss:  0.1808607429265976
Validation: 
LOSS train 0.17322600722312928, val 0.17261569201946259
EPOCH : 8 TIME:  2022-04-19 08:49:47.433544
Training: 
Batch  1  loss:  0.17800810933113098
Batch  2  loss:  0.17602206766605377
Batch  3  loss:  0.170139878988266
Batch  4  loss:  0.17438597977161407
Batch  5  loss:  0.17845523357391357
Batch  6  loss:  0.17242750525474548
Batch  7  loss:  0.172265887260437
Batch  8  loss:  0.17887330055236816
Batch  9  loss:  0.17172080278396606
Batch  10  loss:  0.17645443975925446
Batch  11  loss:  0.17713099718093872
Batch  12  loss:  0.1746712476015091
Batch  13  loss:  0.17241474986076355
Batch  14  loss:  0.17582236230373383
Batch  15  loss:  0.17773543298244476
Batch  16  loss:  0.1687309592962265
Batch  17  loss:  0.17596635222434998
Batch  18  loss:  0.17475676536560059
Batch  19  loss:  0.1728622168302536
Batch  20  loss:  0.17548176646232605
Batch  21  loss:  0.17504490911960602
Batch  22  loss:  0.17464271187782288
Batch  23  loss:  0.178607776761055
Batch  24  loss:  0.16809837520122528
Batch  25  loss:  0.17299045622348785
Batch  26  loss:  0.16637645661830902
Batch  27  loss:  0.1687084585428238
Batch  28  loss:  0.16709212958812714
Batch  29  loss:  0.17944839596748352
Batch  30  loss:  0.16971105337142944
Batch  31  loss:  0.16965045034885406
Batch  32  loss:  0.1689504235982895
Batch  33  loss:  0.1806250661611557
Batch  34  loss:  0.16585516929626465
Batch  35  loss:  0.1752953678369522
Batch  36  loss:  0.1751701384782791
Batch  37  loss:  0.17260845005512238
Batch  38  loss:  0.1728409230709076
Batch  39  loss:  0.16873791813850403
Batch  40  loss:  0.175119549036026
Batch  41  loss:  0.17264260351657867
Batch  42  loss:  0.16658170521259308
Batch  43  loss:  0.17547743022441864
Batch  44  loss:  0.1733909547328949
Batch  45  loss:  0.17152319848537445
Batch  46  loss:  0.17703069746494293
Batch  47  loss:  0.17116595804691315
Batch  48  loss:  0.1816611886024475
Batch  49  loss:  0.17310574650764465
Batch  50  loss:  0.1733681857585907
Batch  51  loss:  0.1695183515548706
Batch  52  loss:  0.16922493278980255
Batch  53  loss:  0.1760617345571518
Batch  54  loss:  0.17497427761554718
Batch  55  loss:  0.1681291162967682
Batch  56  loss:  0.17132796347141266
Batch  57  loss:  0.17386771738529205
Batch  58  loss:  0.17507286369800568
Batch  59  loss:  0.17361174523830414
Batch  60  loss:  0.1677759885787964
Batch  61  loss:  0.17032814025878906
Batch  62  loss:  0.17213276028633118
Batch  63  loss:  0.17529137432575226
Batch  64  loss:  0.17173314094543457
Batch  65  loss:  0.1721094846725464
Batch  66  loss:  0.17131656408309937
Batch  67  loss:  0.17716757953166962
Batch  68  loss:  0.1757986843585968
Batch  69  loss:  0.16558727622032166
Batch  70  loss:  0.1690281331539154
Batch  71  loss:  0.1683402955532074
Batch  72  loss:  0.17150214314460754
Batch  73  loss:  0.17369653284549713
Batch  74  loss:  0.17101386189460754
Batch  75  loss:  0.17393207550048828
Batch  76  loss:  0.17498698830604553
Batch  77  loss:  0.17598678171634674
Batch  78  loss:  0.17730380594730377
Batch  79  loss:  0.169905424118042
Batch  80  loss:  0.17212529480457306
Batch  81  loss:  0.1722055822610855
Batch  82  loss:  0.17128413915634155
Batch  83  loss:  0.16620424389839172
Batch  84  loss:  0.17633788287639618
Batch  85  loss:  0.16701583564281464
Batch  86  loss:  0.17492739856243134
Batch  87  loss:  0.16564658284187317
Batch  88  loss:  0.1714916229248047
Batch  89  loss:  0.1702825129032135
Batch  90  loss:  0.16922633349895477
Batch  91  loss:  0.17361745238304138
Batch  92  loss:  0.1740269809961319
Batch  93  loss:  0.1779816746711731
Batch  94  loss:  0.17141446471214294
Batch  95  loss:  0.16833433508872986
Batch  96  loss:  0.17511574923992157
Batch  97  loss:  0.17204494774341583
Batch  98  loss:  0.17021746933460236
Batch  99  loss:  0.17467127740383148
Batch  100  loss:  0.17494453489780426
Validation: 
LOSS train 0.1728168995678425, val 0.1737067550420761
EPOCH : 9 TIME:  2022-04-19 09:06:09.967532
Training: 
Batch  1  loss:  0.1764124631881714
Batch  2  loss:  0.1747804433107376
Batch  3  loss:  0.16653293371200562
Batch  4  loss:  0.1677645444869995
Batch  5  loss:  0.16780321300029755
Batch  6  loss:  0.16819587349891663
Batch  7  loss:  0.17276206612586975
Batch  8  loss:  0.17642803490161896
Batch  9  loss:  0.17488710582256317
Batch  10  loss:  0.16446028649806976
Batch  11  loss:  0.17663760483264923
Batch  12  loss:  0.1757371872663498
Batch  13  loss:  0.182156041264534
Batch  14  loss:  0.1724255084991455
Batch  15  loss:  0.17429473996162415
Batch  16  loss:  0.1649756133556366
Batch  17  loss:  0.16969013214111328
Batch  18  loss:  0.17309196293354034
Batch  19  loss:  0.16286174952983856
Batch  20  loss:  0.1717262864112854
Batch  21  loss:  0.1717900186777115
Batch  22  loss:  0.17028716206550598
Batch  23  loss:  0.1689230352640152
Batch  24  loss:  0.16631166636943817
Batch  25  loss:  0.17298667132854462
Batch  26  loss:  0.1768154501914978
Batch  27  loss:  0.1789517104625702
Batch  28  loss:  0.17566779255867004
Batch  29  loss:  0.17142680287361145
Batch  30  loss:  0.1777775138616562
Batch  31  loss:  0.1752185970544815
Batch  32  loss:  0.1665850728750229
Batch  33  loss:  0.1753302365541458
Batch  34  loss:  0.16962768137454987
Batch  35  loss:  0.1754189431667328
Batch  36  loss:  0.17441269755363464
Batch  37  loss:  0.17819339036941528
Batch  38  loss:  0.1696363240480423
Batch  39  loss:  0.16765834391117096
Batch  40  loss:  0.16697463393211365
Batch  41  loss:  0.1736508458852768
Batch  42  loss:  0.1741904467344284
Batch  43  loss:  0.172053262591362
Batch  44  loss:  0.1742122620344162
Batch  45  loss:  0.17562372982501984
Batch  46  loss:  0.17447511851787567
Batch  47  loss:  0.1726420521736145
Batch  48  loss:  0.1724657416343689
Batch  49  loss:  0.16742706298828125
Batch  50  loss:  0.17178024351596832
Batch  51  loss:  0.17185035347938538
Batch  52  loss:  0.17318959534168243
Batch  53  loss:  0.17789097130298615
Batch  54  loss:  0.1769929677248001
Batch  55  loss:  0.16917189955711365
Batch  56  loss:  0.16961975395679474
Batch  57  loss:  0.1704903393983841
Batch  58  loss:  0.17134486138820648
Batch  59  loss:  0.17232315242290497
Batch  60  loss:  0.1756306290626526
Batch  61  loss:  0.1726718544960022
Batch  62  loss:  0.174465611577034
Batch  63  loss:  0.17361541092395782
Batch  64  loss:  0.1707773208618164
Batch  65  loss:  0.17137524485588074
Batch  66  loss:  0.16826705634593964
Batch  67  loss:  0.17748679220676422
Batch  68  loss:  0.16941337287425995
Batch  69  loss:  0.17798815667629242
Batch  70  loss:  0.1717848926782608
Batch  71  loss:  0.17253531515598297
Batch  72  loss:  0.17044390738010406
Batch  73  loss:  0.16536688804626465
Batch  74  loss:  0.18119345605373383
Batch  75  loss:  0.17571645975112915
Batch  76  loss:  0.17485032975673676
Batch  77  loss:  0.16734164953231812
Batch  78  loss:  0.17026706039905548
Batch  79  loss:  0.1719917207956314
Batch  80  loss:  0.17726829648017883
Batch  81  loss:  0.1761031299829483
Batch  82  loss:  0.1708841621875763
Batch  83  loss:  0.16866862773895264
Batch  84  loss:  0.1760810762643814
Batch  85  loss:  0.1748376041650772
Batch  86  loss:  0.17834308743476868
Batch  87  loss:  0.17249464988708496
Batch  88  loss:  0.17232714593410492
Batch  89  loss:  0.1713334023952484
Batch  90  loss:  0.17404988408088684
Batch  91  loss:  0.17180705070495605
Batch  92  loss:  0.17578046023845673
Batch  93  loss:  0.1720624566078186
Batch  94  loss:  0.17552892863750458
Batch  95  loss:  0.18105512857437134
Batch  96  loss:  0.1787276417016983
Batch  97  loss:  0.1816083937883377
Batch  98  loss:  0.17194849252700806
Batch  99  loss:  0.17497076094150543
Batch  100  loss:  0.18172897398471832
Validation: 
LOSS train 0.17295804679393767, val 0.172129288315773
EPOCH : 10 TIME:  2022-04-19 09:22:26.400038
Training: 
Batch  1  loss:  0.17240500450134277
Batch  2  loss:  0.1738552749156952
Batch  3  loss:  0.17209075391292572
Batch  4  loss:  0.17316436767578125
Batch  5  loss:  0.16952557861804962
Batch  6  loss:  0.17186881601810455
Batch  7  loss:  0.17591582238674164
Batch  8  loss:  0.1797424703836441
Batch  9  loss:  0.17467521131038666
Batch  10  loss:  0.1706032156944275
Batch  11  loss:  0.1789209544658661
Batch  12  loss:  0.16769404709339142
Batch  13  loss:  0.1767091453075409
Batch  14  loss:  0.17513304948806763
Batch  15  loss:  0.17163382470607758
Batch  16  loss:  0.17631247639656067
Batch  17  loss:  0.17094792425632477
Batch  18  loss:  0.17610420286655426
Batch  19  loss:  0.17258977890014648
Batch  20  loss:  0.17398616671562195
Batch  21  loss:  0.17528186738491058
Batch  22  loss:  0.17415519058704376
Batch  23  loss:  0.16734614968299866
Batch  24  loss:  0.1736667901277542
Batch  25  loss:  0.17511172592639923
Batch  26  loss:  0.16882741451263428
Batch  27  loss:  0.16551817953586578
Batch  28  loss:  0.1792561113834381
Batch  29  loss:  0.17581121623516083
Batch  30  loss:  0.17687730491161346
Batch  31  loss:  0.16334168612957
Batch  32  loss:  0.17152999341487885
Batch  33  loss:  0.18099063634872437
Batch  34  loss:  0.17347189784049988
Batch  35  loss:  0.17079171538352966
Batch  36  loss:  0.16672402620315552
Batch  37  loss:  0.16716252267360687
Batch  38  loss:  0.16193366050720215
Batch  39  loss:  0.16503995656967163
Batch  40  loss:  0.17446130514144897
Batch  41  loss:  0.16437777876853943
Batch  42  loss:  0.17618761956691742
Batch  43  loss:  0.17069417238235474
Batch  44  loss:  0.17005957663059235
Batch  45  loss:  0.17300915718078613
Batch  46  loss:  0.1662411093711853
Batch  47  loss:  0.1683342158794403
Batch  48  loss:  0.17070457339286804
Batch  49  loss:  0.17279651761054993
Batch  50  loss:  0.1709328591823578
Batch  51  loss:  0.17224420607089996
Batch  52  loss:  0.1760447472333908
Batch  53  loss:  0.1740061491727829
Batch  54  loss:  0.17356321215629578
Batch  55  loss:  0.17006336152553558
Batch  56  loss:  0.17118239402770996
Batch  57  loss:  0.1681804656982422
Batch  58  loss:  0.17690004408359528
Batch  59  loss:  0.17194430530071259
Batch  60  loss:  0.1682383120059967
Batch  61  loss:  0.17090445756912231
Batch  62  loss:  0.17204560339450836
Batch  63  loss:  0.17246928811073303
Batch  64  loss:  0.1772131621837616
Batch  65  loss:  0.17908069491386414
Batch  66  loss:  0.17184051871299744
Batch  67  loss:  0.18209226429462433
Batch  68  loss:  0.17949962615966797
Batch  69  loss:  0.16761817038059235
Batch  70  loss:  0.17596541345119476
Batch  71  loss:  0.17272959649562836
Batch  72  loss:  0.16576318442821503
Batch  73  loss:  0.1708817183971405
Batch  74  loss:  0.16377422213554382
Batch  75  loss:  0.17335492372512817
Batch  76  loss:  0.1770053207874298
Batch  77  loss:  0.17622219026088715
Batch  78  loss:  0.16904067993164062
Batch  79  loss:  0.1710621863603592
Batch  80  loss:  0.17024816572666168
Batch  81  loss:  0.18154078722000122
Batch  82  loss:  0.17246480286121368
Batch  83  loss:  0.1631564348936081
Batch  84  loss:  0.17282430827617645
Batch  85  loss:  0.1651633381843567
Batch  86  loss:  0.1742090880870819
Batch  87  loss:  0.17444021999835968
Batch  88  loss:  0.16768595576286316
Batch  89  loss:  0.172500878572464
Batch  90  loss:  0.17612847685813904
Batch  91  loss:  0.17840270698070526
Batch  92  loss:  0.17375853657722473
Batch  93  loss:  0.16280514001846313
Batch  94  loss:  0.16922207176685333
Batch  95  loss:  0.17214900255203247
Batch  96  loss:  0.17037904262542725
Batch  97  loss:  0.16765697300434113
Batch  98  loss:  0.1692073941230774
Batch  99  loss:  0.17116668820381165
Batch  100  loss:  0.1692519336938858
Validation: 
LOSS train 0.17209807381033898, val 0.1741560697555542
EPOCH : 11 TIME:  2022-04-19 09:38:43.684984
Training: 
Batch  1  loss:  0.17293794453144073
Batch  2  loss:  0.17722971737384796
Batch  3  loss:  0.1667243093252182
Batch  4  loss:  0.16653838753700256
Batch  5  loss:  0.1631762683391571
Batch  6  loss:  0.16987170279026031
Batch  7  loss:  0.1708645522594452
Batch  8  loss:  0.17544229328632355
Batch  9  loss:  0.17888079583644867
Batch  10  loss:  0.17136390507221222
Batch  11  loss:  0.1546846479177475
Batch  12  loss:  0.17638017237186432
Batch  13  loss:  0.17209388315677643
Batch  14  loss:  0.1728026568889618
Batch  15  loss:  0.17233242094516754
Batch  16  loss:  0.17147855460643768
Batch  17  loss:  0.18096353113651276
Batch  18  loss:  0.1671326756477356
Batch  19  loss:  0.16919265687465668
Batch  20  loss:  0.17305448651313782
Batch  21  loss:  0.17678339779376984
Batch  22  loss:  0.17657572031021118
Batch  23  loss:  0.16782909631729126
Batch  24  loss:  0.17540928721427917
Batch  25  loss:  0.16973264515399933
Batch  26  loss:  0.16730840504169464
Batch  27  loss:  0.17023877799510956
Batch  28  loss:  0.165983647108078
Batch  29  loss:  0.1742653250694275
Batch  30  loss:  0.17235597968101501
Batch  31  loss:  0.17538081109523773
Batch  32  loss:  0.17776790261268616
Batch  33  loss:  0.1728041023015976
Batch  34  loss:  0.16326457262039185
Batch  35  loss:  0.1729094684123993
Batch  36  loss:  0.16750727593898773
Batch  37  loss:  0.17213864624500275
Batch  38  loss:  0.16730663180351257
Batch  39  loss:  0.16774921119213104
Batch  40  loss:  0.16966459155082703
Batch  41  loss:  0.16894210875034332
Batch  42  loss:  0.16871772706508636
Batch  43  loss:  0.1740848571062088
Batch  44  loss:  0.1719714254140854
Batch  45  loss:  0.16998276114463806
Batch  46  loss:  0.1677541434764862
Batch  47  loss:  0.17342080175876617
Batch  48  loss:  0.16897568106651306
Batch  49  loss:  0.16566020250320435
Batch  50  loss:  0.16601155698299408
Batch  51  loss:  0.1754574030637741
Batch  52  loss:  0.17662140727043152
Batch  53  loss:  0.17334944009780884
Batch  54  loss:  0.16879889369010925
Batch  55  loss:  0.16855239868164062
Batch  56  loss:  0.17373599112033844
Batch  57  loss:  0.16602864861488342
Batch  58  loss:  0.1649511158466339
Batch  59  loss:  0.168879434466362
Batch  60  loss:  0.17543846368789673
Batch  61  loss:  0.16999801993370056
Batch  62  loss:  0.16516795754432678
Batch  63  loss:  0.17186976969242096
Batch  64  loss:  0.16782855987548828
Batch  65  loss:  0.16658958792686462
Batch  66  loss:  0.16468417644500732
Batch  67  loss:  0.17021547257900238
Batch  68  loss:  0.16865073144435883
Batch  69  loss:  0.1708051860332489
Batch  70  loss:  0.16981692612171173
Batch  71  loss:  0.16955003142356873
Batch  72  loss:  0.17220164835453033
Batch  73  loss:  0.1681918501853943
Batch  74  loss:  0.1715833693742752
Batch  75  loss:  0.17403650283813477
Batch  76  loss:  0.17353060841560364
Batch  77  loss:  0.1662464141845703
Batch  78  loss:  0.17308762669563293
Batch  79  loss:  0.17304359376430511
Batch  80  loss:  0.17617858946323395
Batch  81  loss:  0.1680663526058197
Batch  82  loss:  0.16954442858695984
Batch  83  loss:  0.1688041090965271
Batch  84  loss:  0.1806575506925583
Batch  85  loss:  0.17888261377811432
Batch  86  loss:  0.17615917325019836
Batch  87  loss:  0.1682266741991043
Batch  88  loss:  0.16608430445194244
Batch  89  loss:  0.17508304119110107
Batch  90  loss:  0.1604132056236267
Batch  91  loss:  0.1724182814359665
Batch  92  loss:  0.17081233859062195
Batch  93  loss:  0.1819218546152115
Batch  94  loss:  0.175038680434227
Batch  95  loss:  0.16732360422611237
Batch  96  loss:  0.17279087007045746
Batch  97  loss:  0.17748615145683289
Batch  98  loss:  0.179221048951149
Batch  99  loss:  0.1766233593225479
Batch  100  loss:  0.16366878151893616
Validation: 
LOSS train 0.17105958566069604, val 0.17305709421634674
EPOCH : 12 TIME:  2022-04-19 09:55:05.995028
Training: 
Batch  1  loss:  0.16286788880825043
Batch  2  loss:  0.1748235672712326
Batch  3  loss:  0.17045031487941742
Batch  4  loss:  0.16956882178783417
Batch  5  loss:  0.17644992470741272
Batch  6  loss:  0.17558100819587708
Batch  7  loss:  0.16919921338558197
Batch  8  loss:  0.17249920964241028
Batch  9  loss:  0.17540475726127625
Batch  10  loss:  0.1706390678882599
Batch  11  loss:  0.17572768032550812
Batch  12  loss:  0.17356020212173462
Batch  13  loss:  0.1694677770137787
Batch  14  loss:  0.17092567682266235
Batch  15  loss:  0.16754794120788574
Batch  16  loss:  0.17626041173934937
Batch  17  loss:  0.17247514426708221
Batch  18  loss:  0.17201897501945496
Batch  19  loss:  0.17086318135261536
Batch  20  loss:  0.17390267550945282
Batch  21  loss:  0.17181330919265747
Batch  22  loss:  0.16469542682170868
Batch  23  loss:  0.172682523727417
Batch  24  loss:  0.17132297158241272
Batch  25  loss:  0.17003872990608215
Batch  26  loss:  0.1741405874490738
Batch  27  loss:  0.1719634234905243
Batch  28  loss:  0.16429199278354645
Batch  29  loss:  0.16987517476081848
Batch  30  loss:  0.17666716873645782
Batch  31  loss:  0.1679777055978775
Batch  32  loss:  0.17497028410434723
Batch  33  loss:  0.17064616084098816
Batch  34  loss:  0.17638792097568512
Batch  35  loss:  0.17639221251010895
Batch  36  loss:  0.17245769500732422
Batch  37  loss:  0.17024005949497223
Batch  38  loss:  0.16966941952705383
Batch  39  loss:  0.17743533849716187
Batch  40  loss:  0.17057673633098602
Batch  41  loss:  0.1713414490222931
Batch  42  loss:  0.1767648309469223
Batch  43  loss:  0.1755785346031189
Batch  44  loss:  0.17189723253250122
Batch  45  loss:  0.16124315559864044
Batch  46  loss:  0.16918401420116425
Batch  47  loss:  0.1758401095867157
Batch  48  loss:  0.17004145681858063
Batch  49  loss:  0.17407271265983582
Batch  50  loss:  0.17250968515872955
Batch  51  loss:  0.1695636510848999
Batch  52  loss:  0.16965313255786896
Batch  53  loss:  0.1717844009399414
Batch  54  loss:  0.16805602610111237
Batch  55  loss:  0.17319945991039276
Batch  56  loss:  0.17163512110710144
Batch  57  loss:  0.16798025369644165
Batch  58  loss:  0.17371641099452972
Batch  59  loss:  0.1690053641796112
Batch  60  loss:  0.18401850759983063
Batch  61  loss:  0.16913536190986633
Batch  62  loss:  0.16994021832942963
Batch  63  loss:  0.17843444645404816
Batch  64  loss:  0.17026200890541077
Batch  65  loss:  0.17305536568164825
Batch  66  loss:  0.16902589797973633
Batch  67  loss:  0.16806794703006744
Batch  68  loss:  0.17339396476745605
Batch  69  loss:  0.171116903424263
Batch  70  loss:  0.1708543747663498
Batch  71  loss:  0.1760798543691635
Batch  72  loss:  0.1726398766040802
Batch  73  loss:  0.17250442504882812
Batch  74  loss:  0.1700402796268463
Batch  75  loss:  0.17059676349163055
Batch  76  loss:  0.1763640195131302
Batch  77  loss:  0.1673363894224167
Batch  78  loss:  0.16824862360954285
Batch  79  loss:  0.1738962084054947
Batch  80  loss:  0.17607933282852173
Batch  81  loss:  0.17069335281848907
Batch  82  loss:  0.1676013320684433
Batch  83  loss:  0.16849476099014282
Batch  84  loss:  0.1705319583415985
Batch  85  loss:  0.16774745285511017
Batch  86  loss:  0.1776943951845169
Batch  87  loss:  0.17284080386161804
Batch  88  loss:  0.1680963933467865
Batch  89  loss:  0.17410464584827423
Batch  90  loss:  0.16446952521800995
Batch  91  loss:  0.17242209613323212
Batch  92  loss:  0.1687384396791458
Batch  93  loss:  0.17393004894256592
Batch  94  loss:  0.1704472303390503
Batch  95  loss:  0.16710300743579865
Batch  96  loss:  0.17321167886257172
Batch  97  loss:  0.17308145761489868
Batch  98  loss:  0.17098236083984375
Batch  99  loss:  0.17540130019187927
Batch  100  loss:  0.17235049605369568
Validation: 
LOSS train 0.17168578788638114, val 0.16625991463661194
EPOCH : 13 TIME:  2022-04-19 10:11:37.236710
Training: 
Batch  1  loss:  0.17755205929279327
Batch  2  loss:  0.16946738958358765
Batch  3  loss:  0.16281351447105408
Batch  4  loss:  0.17973089218139648
Batch  5  loss:  0.1730518490076065
Batch  6  loss:  0.1733815222978592
Batch  7  loss:  0.17368392646312714
Batch  8  loss:  0.17082063853740692
Batch  9  loss:  0.17047452926635742
Batch  10  loss:  0.1728331595659256
Batch  11  loss:  0.1680387258529663
Batch  12  loss:  0.16497661173343658
Batch  13  loss:  0.1812811642885208
Batch  14  loss:  0.1666731834411621
Batch  15  loss:  0.17133828997612
Batch  16  loss:  0.17127500474452972
Batch  17  loss:  0.17985068261623383
Batch  18  loss:  0.16868792474269867
Batch  19  loss:  0.17229878902435303
Batch  20  loss:  0.17454852163791656
Batch  21  loss:  0.16864047944545746
Batch  22  loss:  0.17897666990756989
Batch  23  loss:  0.1693078726530075
Batch  24  loss:  0.17484839260578156
Batch  25  loss:  0.17206187546253204
Batch  26  loss:  0.17427848279476166
Batch  27  loss:  0.16763059794902802
Batch  28  loss:  0.167726069688797
Batch  29  loss:  0.17149022221565247
Batch  30  loss:  0.1739758998155594
Batch  31  loss:  0.17253746092319489
Batch  32  loss:  0.1707085818052292
Batch  33  loss:  0.16416454315185547
Batch  34  loss:  0.17724357545375824
Batch  35  loss:  0.16512683033943176
Batch  36  loss:  0.16611549258232117
Batch  37  loss:  0.16512586176395416
Batch  38  loss:  0.16403022408485413
Batch  39  loss:  0.1754883974790573
Batch  40  loss:  0.17543239891529083
Batch  41  loss:  0.16849806904792786
Batch  42  loss:  0.16521558165550232
Batch  43  loss:  0.1694379448890686
Batch  44  loss:  0.16589514911174774
Batch  45  loss:  0.17229151725769043
Batch  46  loss:  0.17744944989681244
Batch  47  loss:  0.17369593679904938
Batch  48  loss:  0.1744760125875473
Batch  49  loss:  0.17304664850234985
Batch  50  loss:  0.16908365488052368
Batch  51  loss:  0.16377170383930206
Batch  52  loss:  0.17397452890872955
Batch  53  loss:  0.16319167613983154
Batch  54  loss:  0.17230021953582764
Batch  55  loss:  0.17603155970573425
Batch  56  loss:  0.17658784985542297
Batch  57  loss:  0.17454738914966583
Batch  58  loss:  0.17385321855545044
Batch  59  loss:  0.1767013520002365
Batch  60  loss:  0.16030313074588776
Batch  61  loss:  0.17740269005298615
Batch  62  loss:  0.1807619035243988
Batch  63  loss:  0.17514632642269135
Batch  64  loss:  0.16581188142299652
Batch  65  loss:  0.1740344911813736
Batch  66  loss:  0.16514550149440765
Batch  67  loss:  0.16263511776924133
Batch  68  loss:  0.17269156873226166
Batch  69  loss:  0.16852052509784698
Batch  70  loss:  0.1753787398338318
Batch  71  loss:  0.1787065863609314
Batch  72  loss:  0.17269183695316315
Batch  73  loss:  0.1761707216501236
Batch  74  loss:  0.17274123430252075
Batch  75  loss:  0.16810302436351776
Batch  76  loss:  0.1686183512210846
Batch  77  loss:  0.1695350557565689
Batch  78  loss:  0.17081588506698608
Batch  79  loss:  0.16157615184783936
Batch  80  loss:  0.15973185002803802
Batch  81  loss:  0.16538241505622864
Batch  82  loss:  0.17072033882141113
Batch  83  loss:  0.17631682753562927
Batch  84  loss:  0.17328429222106934
Batch  85  loss:  0.17220976948738098
Batch  86  loss:  0.16148431599140167
Batch  87  loss:  0.16850902140140533
Batch  88  loss:  0.16740943491458893
Batch  89  loss:  0.16859491169452667
Batch  90  loss:  0.16930069029331207
Batch  91  loss:  0.17184299230575562
Batch  92  loss:  0.16665004193782806
Batch  93  loss:  0.17268620431423187
Batch  94  loss:  0.17499272525310516
Batch  95  loss:  0.17705127596855164
Batch  96  loss:  0.16861340403556824
Batch  97  loss:  0.17145614326000214
Batch  98  loss:  0.17473915219306946
Batch  99  loss:  0.1724179983139038
Batch  100  loss:  0.16174903512001038
Validation: 
LOSS train 0.17099671334028244, val 0.1726839393377304
EPOCH : 14 TIME:  2022-04-19 10:28:10.564071
Training: 
Batch  1  loss:  0.17395327985286713
Batch  2  loss:  0.17312446236610413
Batch  3  loss:  0.17388340830802917
Batch  4  loss:  0.16892395913600922
Batch  5  loss:  0.1721046417951584
Batch  6  loss:  0.1706000417470932
Batch  7  loss:  0.16753962635993958
Batch  8  loss:  0.1721644103527069
Batch  9  loss:  0.17305713891983032
Batch  10  loss:  0.1690191626548767
Batch  11  loss:  0.17416812479496002
Batch  12  loss:  0.16562151908874512
Batch  13  loss:  0.1661558747291565
Batch  14  loss:  0.16693517565727234
Batch  15  loss:  0.17210730910301208
Batch  16  loss:  0.16921508312225342
Batch  17  loss:  0.17643947899341583
Batch  18  loss:  0.17046134173870087
Batch  19  loss:  0.16724830865859985
Batch  20  loss:  0.1765034794807434
Batch  21  loss:  0.1676279753446579
Batch  22  loss:  0.1679253727197647
Batch  23  loss:  0.17653056979179382
Batch  24  loss:  0.16506490111351013
Batch  25  loss:  0.1797632873058319
Batch  26  loss:  0.16982775926589966
Batch  27  loss:  0.17552879452705383
Batch  28  loss:  0.17162901163101196
Batch  29  loss:  0.174976646900177
Batch  30  loss:  0.16910213232040405
Batch  31  loss:  0.17521695792675018
Batch  32  loss:  0.1652308851480484
Batch  33  loss:  0.17228733003139496
Batch  34  loss:  0.168630450963974
Batch  35  loss:  0.17556044459342957
Batch  36  loss:  0.1717853844165802
Batch  37  loss:  0.17065022885799408
Batch  38  loss:  0.1712072193622589
Batch  39  loss:  0.1735764592885971
Batch  40  loss:  0.16790901124477386
Batch  41  loss:  0.15819332003593445
Batch  42  loss:  0.175617516040802
Batch  43  loss:  0.16209235787391663
Batch  44  loss:  0.16935163736343384
Batch  45  loss:  0.16746176779270172
Batch  46  loss:  0.1713830530643463
Batch  47  loss:  0.17487098276615143
Batch  48  loss:  0.1740754246711731
Batch  49  loss:  0.1738935112953186
Batch  50  loss:  0.1748681217432022
Batch  51  loss:  0.17099449038505554
Batch  52  loss:  0.1704169362783432
Batch  53  loss:  0.17010384798049927
Batch  54  loss:  0.16666261851787567
Batch  55  loss:  0.1704866737127304
Batch  56  loss:  0.17874275147914886
Batch  57  loss:  0.16867592930793762
Batch  58  loss:  0.17742057144641876
Batch  59  loss:  0.1720324456691742
Batch  60  loss:  0.163966104388237
Batch  61  loss:  0.17424871027469635
Batch  62  loss:  0.16974280774593353
Batch  63  loss:  0.16289785504341125
Batch  64  loss:  0.1706855595111847
Batch  65  loss:  0.1855846494436264
Batch  66  loss:  0.17423796653747559
Batch  67  loss:  0.17813341319561005
Batch  68  loss:  0.17298099398612976
Batch  69  loss:  0.1691901981830597
Batch  70  loss:  0.16889090836048126
Batch  71  loss:  0.17487800121307373
Batch  72  loss:  0.1725180447101593
Batch  73  loss:  0.17338134348392487
Batch  74  loss:  0.17540985345840454
Batch  75  loss:  0.16954021155834198
Batch  76  loss:  0.1728159338235855
Batch  77  loss:  0.17157678306102753
Batch  78  loss:  0.17461153864860535
Batch  79  loss:  0.17846626043319702
Batch  80  loss:  0.18056795001029968
Batch  81  loss:  0.1684240698814392
Batch  82  loss:  0.16867223381996155
Batch  83  loss:  0.17043238878250122
Batch  84  loss:  0.16655276715755463
Batch  85  loss:  0.17826853692531586
Batch  86  loss:  0.16312572360038757
Batch  87  loss:  0.17036117613315582
Batch  88  loss:  0.17790348827838898
Batch  89  loss:  0.17783042788505554
Batch  90  loss:  0.1716877669095993
Batch  91  loss:  0.1743050366640091
Batch  92  loss:  0.16992299258708954
Batch  93  loss:  0.16921596229076385
Batch  94  loss:  0.16779354214668274
Batch  95  loss:  0.16758906841278076
Batch  96  loss:  0.16307277977466583
Batch  97  loss:  0.17506322264671326
Batch  98  loss:  0.16569390892982483
Batch  99  loss:  0.17027507722377777
Batch  100  loss:  0.17541271448135376
Validation: 
LOSS train 0.1714252857863903, val 0.1721131056547165
EPOCH : 15 TIME:  2022-04-19 10:44:32.096231
Training: 
Batch  1  loss:  0.17328400909900665
Batch  2  loss:  0.17271460592746735
Batch  3  loss:  0.17871727049350739
Batch  4  loss:  0.17132839560508728
Batch  5  loss:  0.17204953730106354
Batch  6  loss:  0.17376253008842468
Batch  7  loss:  0.17293879389762878
Batch  8  loss:  0.17292420566082
Batch  9  loss:  0.17105282843112946
Batch  10  loss:  0.17294815182685852
Batch  11  loss:  0.1753215193748474
Batch  12  loss:  0.17110560834407806
Batch  13  loss:  0.16744714975357056
Batch  14  loss:  0.16371354460716248
Batch  15  loss:  0.1685694456100464
Batch  16  loss:  0.17280060052871704
Batch  17  loss:  0.16699214279651642
Batch  18  loss:  0.17198817431926727
Batch  19  loss:  0.17584769427776337
Batch  20  loss:  0.1694699376821518
Batch  21  loss:  0.17091526091098785
Batch  22  loss:  0.16779746115207672
Batch  23  loss:  0.17031818628311157
Batch  24  loss:  0.16928982734680176
Batch  25  loss:  0.1691669076681137
Batch  26  loss:  0.17527854442596436
Batch  27  loss:  0.17105640470981598
Batch  28  loss:  0.16961669921875
Batch  29  loss:  0.16876697540283203
Batch  30  loss:  0.17053824663162231
Batch  31  loss:  0.16794006526470184
Batch  32  loss:  0.1718323528766632
Batch  33  loss:  0.17070214450359344
Batch  34  loss:  0.17009200155735016
Batch  35  loss:  0.17299985885620117
Batch  36  loss:  0.17468449473381042
Batch  37  loss:  0.16649310290813446
Batch  38  loss:  0.1749325692653656
Batch  39  loss:  0.1672067642211914
Batch  40  loss:  0.16709905862808228
Batch  41  loss:  0.1729702651500702
Batch  42  loss:  0.17365872859954834
Batch  43  loss:  0.17267072200775146
Batch  44  loss:  0.1710645854473114
Batch  45  loss:  0.1707863062620163
Batch  46  loss:  0.17594027519226074
Batch  47  loss:  0.17000682651996613
Batch  48  loss:  0.16792616248130798
Batch  49  loss:  0.16942806541919708
Batch  50  loss:  0.17083412408828735
Batch  51  loss:  0.16600780189037323
Batch  52  loss:  0.1758871078491211
Batch  53  loss:  0.17332854866981506
Batch  54  loss:  0.16527239978313446
Batch  55  loss:  0.17381031811237335
Batch  56  loss:  0.17360033094882965
Batch  57  loss:  0.17086726427078247
Batch  58  loss:  0.17641738057136536
Batch  59  loss:  0.17358747124671936
Batch  60  loss:  0.1712409406900406
Batch  61  loss:  0.17189240455627441
Batch  62  loss:  0.17294563353061676
Batch  63  loss:  0.17061248421669006
Batch  64  loss:  0.1712501049041748
Batch  65  loss:  0.16684047877788544
Batch  66  loss:  0.170999675989151
Batch  67  loss:  0.1698121577501297
Batch  68  loss:  0.1655726581811905
Batch  69  loss:  0.1784421056509018
Batch  70  loss:  0.16918212175369263
Batch  71  loss:  0.1713690161705017
Batch  72  loss:  0.1776607781648636
Batch  73  loss:  0.16604392230510712
Batch  74  loss:  0.16713231801986694
Batch  75  loss:  0.1653103530406952
Batch  76  loss:  0.17212262749671936
Batch  77  loss:  0.1740112006664276
Batch  78  loss:  0.17468003928661346
Batch  79  loss:  0.1704864799976349
Batch  80  loss:  0.16284574568271637
Batch  81  loss:  0.16830627620220184
Batch  82  loss:  0.1722317487001419
Batch  83  loss:  0.16816319525241852
Batch  84  loss:  0.17221170663833618
Batch  85  loss:  0.1696036159992218
Batch  86  loss:  0.16473087668418884
Batch  87  loss:  0.17327401041984558
Batch  88  loss:  0.1801692545413971
Batch  89  loss:  0.17456582188606262
Batch  90  loss:  0.1679917722940445
Batch  91  loss:  0.17574714124202728
Batch  92  loss:  0.17008620500564575
Batch  93  loss:  0.1684606522321701
Batch  94  loss:  0.17211082577705383
Batch  95  loss:  0.16775593161582947
Batch  96  loss:  0.17074421048164368
Batch  97  loss:  0.16612328588962555
Batch  98  loss:  0.16303744912147522
Batch  99  loss:  0.17158423364162445
Batch  100  loss:  0.1681298464536667
Validation: 
LOSS train 0.1709124906361103, val 0.17438222467899323
EPOCH : 16 TIME:  2022-04-19 11:01:15.725953
Training: 
Batch  1  loss:  0.16437946259975433
Batch  2  loss:  0.170427605509758
Batch  3  loss:  0.1643112599849701
Batch  4  loss:  0.17029857635498047
Batch  5  loss:  0.18523354828357697
Batch  6  loss:  0.17067603766918182
Batch  7  loss:  0.16983012855052948
Batch  8  loss:  0.16551513969898224
Batch  9  loss:  0.18062354624271393
Batch  10  loss:  0.17810474336147308
Batch  11  loss:  0.17074301838874817
Batch  12  loss:  0.16573382914066315
Batch  13  loss:  0.17489959299564362
Batch  14  loss:  0.17062310874462128
Batch  15  loss:  0.16894157230854034
Batch  16  loss:  0.16686053574085236
Batch  17  loss:  0.17416925728321075
Batch  18  loss:  0.1593979299068451
Batch  19  loss:  0.1779613047838211
Batch  20  loss:  0.16752010583877563
Batch  21  loss:  0.1619042158126831
Batch  22  loss:  0.1678667813539505
Batch  23  loss:  0.17035116255283356
Batch  24  loss:  0.17201049625873566
Batch  25  loss:  0.16552047431468964
Batch  26  loss:  0.16699253022670746
Batch  27  loss:  0.1636248677968979
Batch  28  loss:  0.16780447959899902
Batch  29  loss:  0.16719810664653778
Batch  30  loss:  0.16656236350536346
Batch  31  loss:  0.18003211915493011
Batch  32  loss:  0.17226433753967285
Batch  33  loss:  0.16884571313858032
Batch  34  loss:  0.16717790067195892
Batch  35  loss:  0.17455428838729858
Batch  36  loss:  0.1728825718164444
Batch  37  loss:  0.17688491940498352
Batch  38  loss:  0.17800578474998474
Batch  39  loss:  0.1728821098804474
Batch  40  loss:  0.1782194823026657
Batch  41  loss:  0.17647111415863037
Batch  42  loss:  0.17537438869476318
Batch  43  loss:  0.17068782448768616
Batch  44  loss:  0.17050732672214508
Batch  45  loss:  0.17045213282108307
Batch  46  loss:  0.17132049798965454
Batch  47  loss:  0.16824224591255188
Batch  48  loss:  0.1810830980539322
Batch  49  loss:  0.1725383698940277
Batch  50  loss:  0.17398467659950256
Batch  51  loss:  0.18041253089904785
Batch  52  loss:  0.16903232038021088
Batch  53  loss:  0.167851060628891
Batch  54  loss:  0.17322711646556854
Batch  55  loss:  0.17406558990478516
Batch  56  loss:  0.17030753195285797
Batch  57  loss:  0.17333433032035828
Batch  58  loss:  0.17268630862236023
Batch  59  loss:  0.16255328059196472
Batch  60  loss:  0.16532224416732788
Batch  61  loss:  0.17044313251972198
Batch  62  loss:  0.18160602450370789
Batch  63  loss:  0.17268027365207672
Batch  64  loss:  0.17445087432861328
Batch  65  loss:  0.17000456154346466
Batch  66  loss:  0.175161674618721
Batch  67  loss:  0.1725986897945404
Batch  68  loss:  0.1748870462179184
Batch  69  loss:  0.18335863947868347
Batch  70  loss:  0.1701238453388214
Batch  71  loss:  0.16740576922893524
Batch  72  loss:  0.16947822272777557
Batch  73  loss:  0.17134441435337067
Batch  74  loss:  0.17479848861694336
Batch  75  loss:  0.17203500866889954
Batch  76  loss:  0.1630638986825943
Batch  77  loss:  0.1725957691669464
Batch  78  loss:  0.17901372909545898
Batch  79  loss:  0.1677711009979248
Batch  80  loss:  0.17569032311439514
Batch  81  loss:  0.17212232947349548
Batch  82  loss:  0.167551189661026
Batch  83  loss:  0.17578250169754028
Batch  84  loss:  0.17708459496498108
Batch  85  loss:  0.1712459921836853
Batch  86  loss:  0.17459715902805328
Batch  87  loss:  0.17859426140785217
Batch  88  loss:  0.17217646539211273
Batch  89  loss:  0.16863280534744263
Batch  90  loss:  0.16756033897399902
Batch  91  loss:  0.1716630905866623
Batch  92  loss:  0.17424064874649048
Batch  93  loss:  0.17120203375816345
Batch  94  loss:  0.16568292677402496
Batch  95  loss:  0.17624258995056152
Batch  96  loss:  0.17723187804222107
Batch  97  loss:  0.1758824735879898
Batch  98  loss:  0.1726418137550354
Batch  99  loss:  0.17134219408035278
Batch  100  loss:  0.16851425170898438
Validation: 
LOSS train 0.17175792023539543, val 0.18120108544826508
EPOCH : 17 TIME:  2022-04-19 11:17:33.508202
Training: 
Batch  1  loss:  0.1678629070520401
Batch  2  loss:  0.16838032007217407
Batch  3  loss:  0.16384296119213104
Batch  4  loss:  0.17140525579452515
Batch  5  loss:  0.17062033712863922
Batch  6  loss:  0.1679370105266571
Batch  7  loss:  0.17603060603141785
Batch  8  loss:  0.16936832666397095
Batch  9  loss:  0.16367851197719574
Batch  10  loss:  0.16551333665847778
Batch  11  loss:  0.16878624260425568
Batch  12  loss:  0.17631478607654572
Batch  13  loss:  0.1733500063419342
Batch  14  loss:  0.17036311328411102
Batch  15  loss:  0.17131900787353516
Batch  16  loss:  0.169447660446167
Batch  17  loss:  0.17296060919761658
Batch  18  loss:  0.17412304878234863
Batch  19  loss:  0.17765170335769653
Batch  20  loss:  0.17351920902729034
Batch  21  loss:  0.1784387230873108
Batch  22  loss:  0.1655544936656952
Batch  23  loss:  0.16854262351989746
Batch  24  loss:  0.16442236304283142
Batch  25  loss:  0.17555129528045654
Batch  26  loss:  0.16837552189826965
Batch  27  loss:  0.1625247597694397
Batch  28  loss:  0.16990627348423004
Batch  29  loss:  0.17393867671489716
Batch  30  loss:  0.16691482067108154
Batch  31  loss:  0.16579997539520264
Batch  32  loss:  0.1738951951265335
Batch  33  loss:  0.17407718300819397
Batch  34  loss:  0.16592048108577728
Batch  35  loss:  0.16925141215324402
Batch  36  loss:  0.17000208795070648
Batch  37  loss:  0.1808985471725464
Batch  38  loss:  0.16782398521900177
Batch  39  loss:  0.16909457743167877
Batch  40  loss:  0.17159876227378845
Batch  41  loss:  0.17087310552597046
Batch  42  loss:  0.17396613955497742
Batch  43  loss:  0.16334044933319092
Batch  44  loss:  0.16932693123817444
Batch  45  loss:  0.16250160336494446
Batch  46  loss:  0.16943584382534027
Batch  47  loss:  0.17684490978717804
Batch  48  loss:  0.17096801102161407
Batch  49  loss:  0.17140395939350128
Batch  50  loss:  0.1788201481103897
Batch  51  loss:  0.1707441806793213
Batch  52  loss:  0.1700737178325653
Batch  53  loss:  0.17377234995365143
Batch  54  loss:  0.1717713326215744
Batch  55  loss:  0.1766625940799713
Batch  56  loss:  0.17077529430389404
Batch  57  loss:  0.17290663719177246
Batch  58  loss:  0.17588435113430023
Batch  59  loss:  0.1715480387210846
Batch  60  loss:  0.16293062269687653
Batch  61  loss:  0.17482535541057587
Batch  62  loss:  0.17847564816474915
Batch  63  loss:  0.1680070012807846
Batch  64  loss:  0.1652860939502716
Batch  65  loss:  0.17266559600830078
Batch  66  loss:  0.17241410911083221
Batch  67  loss:  0.1734151840209961
Batch  68  loss:  0.1706254482269287
Batch  69  loss:  0.17300213873386383
Batch  70  loss:  0.17133289575576782
Batch  71  loss:  0.16857531666755676
Batch  72  loss:  0.16958333551883698
Batch  73  loss:  0.1699482649564743
Batch  74  loss:  0.17056392133235931
Batch  75  loss:  0.1690984070301056
Batch  76  loss:  0.17099997401237488
Batch  77  loss:  0.18051870167255402
Batch  78  loss:  0.16877037286758423
Batch  79  loss:  0.1723724752664566
Batch  80  loss:  0.1701943725347519
Batch  81  loss:  0.18104058504104614
Batch  82  loss:  0.17516246438026428
Batch  83  loss:  0.16251124441623688
Batch  84  loss:  0.17127487063407898
Batch  85  loss:  0.17670197784900665
Batch  86  loss:  0.1739734262228012
Batch  87  loss:  0.16994857788085938
Batch  88  loss:  0.17129161953926086
Batch  89  loss:  0.17653459310531616
Batch  90  loss:  0.17255179584026337
Batch  91  loss:  0.16588672995567322
Batch  92  loss:  0.17082801461219788
Batch  93  loss:  0.17264968156814575
Batch  94  loss:  0.1705167144536972
Batch  95  loss:  0.17343340814113617
Batch  96  loss:  0.1691792607307434
Batch  97  loss:  0.17232516407966614
Batch  98  loss:  0.16727271676063538
Batch  99  loss:  0.16770446300506592
Batch  100  loss:  0.17456293106079102
Validation: 
LOSS train 0.17105581790208815, val 0.1724461168050766
EPOCH : 18 TIME:  2022-04-19 11:33:45.209027
Training: 
Batch  1  loss:  0.1712888479232788
Batch  2  loss:  0.1720549762248993
Batch  3  loss:  0.16762851178646088
Batch  4  loss:  0.16642941534519196
Batch  5  loss:  0.1674574315547943
Batch  6  loss:  0.17276999354362488
Batch  7  loss:  0.17092350125312805
Batch  8  loss:  0.17577025294303894
Batch  9  loss:  0.17174965143203735
Batch  10  loss:  0.16949598491191864
Batch  11  loss:  0.17010660469532013
Batch  12  loss:  0.16976317763328552
Batch  13  loss:  0.17257142066955566
Batch  14  loss:  0.17270272970199585
Batch  15  loss:  0.1743393987417221
Batch  16  loss:  0.16690883040428162
Batch  17  loss:  0.1607969105243683
Batch  18  loss:  0.17584790289402008
Batch  19  loss:  0.17414680123329163
Batch  20  loss:  0.16775111854076385
Batch  21  loss:  0.1663815677165985
Batch  22  loss:  0.16367778182029724
Batch  23  loss:  0.16792498528957367
Batch  24  loss:  0.171629399061203
Batch  25  loss:  0.1776190847158432
Batch  26  loss:  0.17080868780612946
Batch  27  loss:  0.17340798676013947
Batch  28  loss:  0.17375488579273224
Batch  29  loss:  0.17597201466560364
Batch  30  loss:  0.1713806837797165
Batch  31  loss:  0.17469340562820435
Batch  32  loss:  0.17248381674289703
Batch  33  loss:  0.16706867516040802
Batch  34  loss:  0.17567205429077148
Batch  35  loss:  0.17385847866535187
Batch  36  loss:  0.1788570135831833
Batch  37  loss:  0.162809818983078
Batch  38  loss:  0.1751944124698639
Batch  39  loss:  0.16907909512519836
Batch  40  loss:  0.17378325760364532
Batch  41  loss:  0.16758933663368225
Batch  42  loss:  0.1692904680967331
Batch  43  loss:  0.17240002751350403
Batch  44  loss:  0.17165309190750122
Batch  45  loss:  0.16650432348251343
Batch  46  loss:  0.1668461412191391
Batch  47  loss:  0.1721554547548294
Batch  48  loss:  0.1712643951177597
Batch  49  loss:  0.16841956973075867
Batch  50  loss:  0.16983535885810852
Batch  51  loss:  0.1734926402568817
Batch  52  loss:  0.17018267512321472
Batch  53  loss:  0.16895164549350739
Batch  54  loss:  0.16877132654190063
Batch  55  loss:  0.15880322456359863
Batch  56  loss:  0.17437277734279633
Batch  57  loss:  0.1713205873966217
Batch  58  loss:  0.1686929613351822
Batch  59  loss:  0.17073611915111542
Batch  60  loss:  0.16052968800067902
Batch  61  loss:  0.16831718385219574
Batch  62  loss:  0.1633937805891037
Batch  63  loss:  0.1744612604379654
Batch  64  loss:  0.17483745515346527
Batch  65  loss:  0.17274878919124603
Batch  66  loss:  0.1762242466211319
Batch  67  loss:  0.16666610538959503
Batch  68  loss:  0.17404796183109283
Batch  69  loss:  0.16888689994812012
Batch  70  loss:  0.17004632949829102
Batch  71  loss:  0.1772615760564804
Batch  72  loss:  0.16854067146778107
Batch  73  loss:  0.1734382063150406
Batch  74  loss:  0.17172053456306458
Batch  75  loss:  0.1788446456193924
Batch  76  loss:  0.17012524604797363
Batch  77  loss:  0.172836571931839
Batch  78  loss:  0.17388680577278137
Batch  79  loss:  0.16198274493217468
Batch  80  loss:  0.1762804538011551
Batch  81  loss:  0.17474116384983063
Batch  82  loss:  0.16579188406467438
Batch  83  loss:  0.16943538188934326
Batch  84  loss:  0.1629328429698944
Batch  85  loss:  0.17298667132854462
Batch  86  loss:  0.16595149040222168
Batch  87  loss:  0.17643237113952637
Batch  88  loss:  0.17564991116523743
Batch  89  loss:  0.16979779303073883
Batch  90  loss:  0.1641199141740799
Batch  91  loss:  0.17637348175048828
Batch  92  loss:  0.17721493542194366
Batch  93  loss:  0.16834554076194763
Batch  94  loss:  0.16676487028598785
Batch  95  loss:  0.1734003722667694
Batch  96  loss:  0.17061403393745422
Batch  97  loss:  0.1782912015914917
Batch  98  loss:  0.16883212327957153
Batch  99  loss:  0.17100493609905243
Batch  100  loss:  0.16660930216312408
Validation: 
LOSS train 0.17078210070729255, val 0.16865774989128113
EPOCH : 19 TIME:  2022-04-19 11:49:59.403629
Training: 
Batch  1  loss:  0.176233172416687
Batch  2  loss:  0.1710258573293686
Batch  3  loss:  0.17446033656597137
Batch  4  loss:  0.17225991189479828
Batch  5  loss:  0.1693284809589386
Batch  6  loss:  0.17117595672607422
Batch  7  loss:  0.17669795453548431
Batch  8  loss:  0.17393100261688232
Batch  9  loss:  0.1637454330921173
Batch  10  loss:  0.1755727082490921
Batch  11  loss:  0.16784220933914185
Batch  12  loss:  0.16974425315856934
Batch  13  loss:  0.1767892837524414
Batch  14  loss:  0.17556464672088623
Batch  15  loss:  0.17323248088359833
Batch  16  loss:  0.1724105179309845
Batch  17  loss:  0.16789329051971436
Batch  18  loss:  0.1686442494392395
Batch  19  loss:  0.1636558324098587
Batch  20  loss:  0.18805694580078125
Batch  21  loss:  0.17376038432121277
Batch  22  loss:  0.16764895617961884
Batch  23  loss:  0.1681685596704483
Batch  24  loss:  0.17566509544849396
Batch  25  loss:  0.1739291399717331
Batch  26  loss:  0.17315544188022614
Batch  27  loss:  0.16896425187587738
Batch  28  loss:  0.17064619064331055
Batch  29  loss:  0.16451092064380646
Batch  30  loss:  0.1711147129535675
Batch  31  loss:  0.1674956977367401
Batch  32  loss:  0.17072592675685883
Batch  33  loss:  0.1729808747768402
Batch  34  loss:  0.16608697175979614
Batch  35  loss:  0.17241179943084717
Batch  36  loss:  0.17710040509700775
Batch  37  loss:  0.16798457503318787
Batch  38  loss:  0.17325973510742188
Batch  39  loss:  0.16877569258213043
Batch  40  loss:  0.17096824944019318
Batch  41  loss:  0.17357563972473145
Batch  42  loss:  0.16915452480316162
Batch  43  loss:  0.17194576561450958
Batch  44  loss:  0.17016398906707764
Batch  45  loss:  0.1776561737060547
Batch  46  loss:  0.1731598675251007
Batch  47  loss:  0.17671817541122437
Batch  48  loss:  0.17384913563728333
Batch  49  loss:  0.17173300683498383
Batch  50  loss:  0.17290765047073364
Batch  51  loss:  0.17391648888587952
Batch  52  loss:  0.16934560239315033
Batch  53  loss:  0.17230693995952606
Batch  54  loss:  0.1798921376466751
Batch  55  loss:  0.17375990748405457
Batch  56  loss:  0.17483823001384735
Batch  57  loss:  0.16743281483650208
Batch  58  loss:  0.16553428769111633
Batch  59  loss:  0.16726888716220856
Batch  60  loss:  0.16665026545524597
Batch  61  loss:  0.16911785304546356
Batch  62  loss:  0.17091567814350128
Batch  63  loss:  0.16808387637138367
Batch  64  loss:  0.1659010499715805
Batch  65  loss:  0.17478561401367188
Batch  66  loss:  0.1718280166387558
Batch  67  loss:  0.1694374829530716
Batch  68  loss:  0.17363405227661133
Batch  69  loss:  0.1690547913312912
Batch  70  loss:  0.16874071955680847
Batch  71  loss:  0.17424698173999786
Batch  72  loss:  0.1693831980228424
Batch  73  loss:  0.1758963018655777
Batch  74  loss:  0.16531208157539368
Batch  75  loss:  0.1714043766260147



Batch  76  loss:  0.16834425926208496
Batch  77  loss:  0.17228078842163086
Batch  78  loss:  0.17291712760925293
Batch  79  loss:  0.16586028039455414
Batch  80  loss:  0.16726604104042053
Batch  81  loss:  0.17219483852386475
Batch  82  loss:  0.17097969353199005
Batch  83  loss:  0.17336037755012512
Batch  84  loss:  0.16828499734401703
Batch  85  loss:  0.16075223684310913
Batch  86  loss:  0.1710418164730072
Batch  87  loss:  0.1734522134065628
Batch  88  loss:  0.16646453738212585
Batch  89  loss:  0.16776500642299652
Batch  90  loss:  0.16522328555583954
Batch  91  loss:  0.16384755074977875
Batch  92  loss:  0.17250514030456543
Batch  93  loss:  0.17076903581619263
Batch  94  loss:  0.1687908172607422
Batch  95  loss:  0.16529297828674316
Batch  96  loss:  0.17324699461460114
Batch  97  loss:  0.17590934038162231
Batch  98  loss:  0.17218920588493347
Batch  99  loss:  0.17285193502902985
Batch  100  loss:  0.17237043380737305
Validation: 
LOSS train 0.17107130602002144, val 0.1753784865140915
EPOCH : 20 TIME:  2022-04-19 12:06:32.465957

